diff --git a/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java b/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
index 0d8c5d6e..cb2227df 100644
--- a/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
+++ b/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
@@ -46,45 +46,24 @@
 public class DebuggableThreadPoolExecutor extends ThreadPoolExecutor
 {
     protected static Logger logger = LoggerFactory.getLogger(DebuggableThreadPoolExecutor.class);
-
-    public DebuggableThreadPoolExecutor(String threadPoolName, int priority)
-    {
-        this(1, Integer.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(), new NamedThreadFactory(threadPoolName, priority));
-    }
-
-    public DebuggableThreadPoolExecutor(int corePoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> queue, ThreadFactory factory)
-    {
-        this(corePoolSize, corePoolSize, keepAliveTime, unit, queue, factory);
-    }
-
-    protected DebuggableThreadPoolExecutor(int corePoolSize, int maxPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue, ThreadFactory threadFactory)
-    {
-        super(corePoolSize, maxPoolSize, keepAliveTime, unit, workQueue, threadFactory);
-        allowCoreThreadTimeOut(true);
-
-        // block task submissions until queue has room.
-        // this is fighting TPE's design a bit because TPE rejects if queue.offer reports a full queue.
-        // we'll just override this with a handler that retries until it gets in.  ugly, but effective.
-        // (there is an extensive analysis of the options here at
-        //  http://today.java.net/pub/a/today/2008/10/23/creating-a-notifying-blocking-thread-pool-executor.html)
-        this.setRejectedExecutionHandler(new RejectedExecutionHandler()
+    public static final RejectedExecutionHandler blockingExecutionHandler = new RejectedExecutionHandler()
         {
             public void rejectedExecution(Runnable task, ThreadPoolExecutor executor)
             {
-                ((DebuggableThreadPoolExecutor)executor).onInitialRejection(task);
+            ((DebuggableThreadPoolExecutor) executor).onInitialRejection(task);
                 BlockingQueue<Runnable> queue = executor.getQueue();
                 while (true)
                 {
                     if (executor.isShutdown())
                     {
-                        ((DebuggableThreadPoolExecutor)executor).onFinalRejection(task);
+                    ((DebuggableThreadPoolExecutor) executor).onFinalRejection(task);
                         throw new RejectedExecutionException("ThreadPoolExecutor has shut down");
                     }
                     try
                     {
                         if (queue.offer(task, 1000, TimeUnit.MILLISECONDS))
                         {
-                            ((DebuggableThreadPoolExecutor)executor).onFinalAccept(task);
+                        ((DebuggableThreadPoolExecutor) executor).onFinalAccept(task);
                             break;
                         }
                     }
@@ -94,7 +73,29 @@ public void rejectedExecution(Runnable task, ThreadPoolExecutor executor)
                     }
                 }
             }
-        });
+    };
+
+    public DebuggableThreadPoolExecutor(String threadPoolName, int priority)
+    {
+        this(1, Integer.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(), new NamedThreadFactory(threadPoolName, priority));
+    }
+
+    public DebuggableThreadPoolExecutor(int corePoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> queue, ThreadFactory factory)
+    {
+        this(corePoolSize, corePoolSize, keepAliveTime, unit, queue, factory);
+    }
+
+    protected DebuggableThreadPoolExecutor(int corePoolSize, int maxPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue, ThreadFactory threadFactory)
+    {
+        super(corePoolSize, maxPoolSize, keepAliveTime, unit, workQueue, threadFactory);
+        allowCoreThreadTimeOut(true);
+
+        // block task submissions until queue has room.
+        // this is fighting TPE's design a bit because TPE rejects if queue.offer reports a full queue.
+        // we'll just override this with a handler that retries until it gets in.  ugly, but effective.
+        // (there is an extensive analysis of the options here at
+        //  http://today.java.net/pub/a/today/2008/10/23/creating-a-notifying-blocking-thread-pool-executor.html)
+        this.setRejectedExecutionHandler(blockingExecutionHandler);
     }
 
     protected void onInitialRejection(Runnable task) {}
diff --git a/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java b/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
index 7e5da9af..f1f4b0da 100644
--- a/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
+++ b/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
@@ -120,7 +120,7 @@ protected AbstractCompactedRow getReduced()
 
         try
         {
-            AbstractCompactedRow compactedRow = controller.getCompactedRow(rows);
+            AbstractCompactedRow compactedRow = controller.getCompactedRow(new ArrayList<SSTableIdentityIterator>(rows));
             if (compactedRow.isEmpty())
             {
                 controller.invalidateCachedRow(compactedRow.key);
diff --git a/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java b/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
index 59883aad..a0829552 100644
--- a/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
+++ b/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
@@ -25,7 +25,6 @@
 import java.io.IOError;
 import java.io.IOException;
 import java.security.MessageDigest;
-import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
 
@@ -33,7 +32,11 @@
 import com.google.common.collect.Iterators;
 import org.apache.commons.collections.iterators.CollatingIterator;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.columniterator.IColumnIterator;
 import org.apache.cassandra.db.marshal.AbstractType;
 import org.apache.cassandra.io.sstable.SSTableIdentityIterator;
 import org.apache.cassandra.io.util.DataOutputBuffer;
@@ -53,23 +56,25 @@
  */
 public class LazilyCompactedRow extends AbstractCompactedRow implements IIterableColumns
 {
+    private static Logger logger = LoggerFactory.getLogger(LazilyCompactedRow.class);
+
     private final List<SSTableIdentityIterator> rows;
     private final CompactionController controller;
     private final boolean shouldPurge;
     private final DataOutputBuffer headerBuffer;
     private ColumnFamily emptyColumnFamily;
-    private LazyColumnIterator iter;
+    private LazyColumnIterator reducer;
     private int columnCount;
     private long columnSerializedSize;
 
     public LazilyCompactedRow(CompactionController controller, List<SSTableIdentityIterator> rows)
     {
         super(rows.get(0).getKey());
+        this.rows = rows;
         this.controller = controller;
         this.shouldPurge = controller.shouldPurge(key);
-        this.rows = new ArrayList<SSTableIdentityIterator>(rows);
 
-        for (SSTableIdentityIterator row : rows)
+        for (IColumnIterator row : rows)
         {
             ColumnFamily cf = row.getColumnFamily();
 
@@ -83,9 +88,10 @@ public LazilyCompactedRow(CompactionController controller, List<SSTableIdentityI
         headerBuffer = new DataOutputBuffer();
         ColumnIndexer.serialize(this, headerBuffer);
         // reach into iterator used by ColumnIndexer to get column count and size
-        columnCount = iter.size;
-        columnSerializedSize = iter.serializedSize;
-        iter = null;
+        // (however, if there are zero columns, iterator() will not be called by ColumnIndexer and reducer will be null)
+        columnCount = reducer == null ? 0 : reducer.size;
+        columnSerializedSize = reducer == null ? 0 : reducer.serializedSize;
+        reducer = null;
     }
 
     public void write(DataOutput out) throws IOException
@@ -94,6 +100,9 @@ public void write(DataOutput out) throws IOException
         ColumnFamily.serializer().serializeCFInfo(emptyColumnFamily, clockOut);
 
         long dataSize = headerBuffer.getLength() + clockOut.getLength() + columnSerializedSize;
+        if (logger.isDebugEnabled())
+            logger.debug(String.format("header / clock / column sizes are %s / %s / %s",
+                         headerBuffer.getLength(), clockOut.getLength(), columnSerializedSize));
         assert dataSize > 0;
         out.writeLong(dataSize);
         out.write(headerBuffer.getData(), 0, headerBuffer.getLength());
@@ -106,6 +115,9 @@ public void write(DataOutput out) throws IOException
             IColumn column = iter.next();
             emptyColumnFamily.getColumnSerializer().serialize(column, out);
         }
+        long secondPassColumnSize = reducer == null ? 0 : reducer.serializedSize;
+        assert secondPassColumnSize == columnSerializedSize
+               : "originally calculated column size of " + columnSerializedSize + " but now it is " + secondPassColumnSize;
     }
 
     public void update(MessageDigest digest)
@@ -157,8 +169,8 @@ public AbstractType getComparator()
         {
             row.reset();
         }
-        iter = new LazyColumnIterator(new CollatingIterator(getComparator().columnComparator, rows));
-        return Iterators.filter(iter, Predicates.notNull());
+        reducer = new LazyColumnIterator(new CollatingIterator(getComparator().columnComparator, rows));
+        return Iterators.filter(reducer, Predicates.notNull());
     }
 
     public int columnCount()
@@ -190,18 +202,13 @@ public void reduce(IColumn current)
 
         protected IColumn getReduced()
         {
-            assert container != null;
-            IColumn reduced = container.iterator().next();
-            ColumnFamily purged = shouldPurge ? ColumnFamilyStore.removeDeleted(container, controller.gcBefore) : container;
-            if (shouldPurge && purged != null && purged.metadata().getDefaultValidator().isCommutative())
-            {
-                CounterColumn.removeOldShards(purged, controller.gcBefore);
-            }
+            ColumnFamily purged = PrecompactedRow.removeDeletedAndOldShards(shouldPurge, controller, container);
             if (purged == null || !purged.iterator().hasNext())
             {
                 container.clear();
                 return null;
             }
+            IColumn reduced = purged.iterator().next();
             container.clear();
             serializedSize += reduced.serializedSize();
             size++;
diff --git a/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java b/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
index 6f151fee..9db39c13 100644
--- a/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
+++ b/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
@@ -36,6 +36,7 @@
 import org.apache.cassandra.db.DecoratedKey;
 import org.apache.cassandra.io.sstable.SSTableIdentityIterator;
 import org.apache.cassandra.io.util.DataOutputBuffer;
+import sun.tools.tree.ThisExpression;
 
 /**
  * PrecompactedRow merges its rows in its constructor in memory.
@@ -55,11 +56,28 @@ public PrecompactedRow(DecoratedKey key, ColumnFamily compacted)
         this.gcBefore = Integer.MAX_VALUE;
     }
 
+    public static ColumnFamily removeDeletedAndOldShards(DecoratedKey key, CompactionController controller, ColumnFamily cf)
+    {
+        return removeDeletedAndOldShards(controller.shouldPurge(key), controller, cf);
+    }
+
+    public static ColumnFamily removeDeletedAndOldShards(boolean shouldPurge, CompactionController controller, ColumnFamily cf)
+    {
+        ColumnFamily compacted = shouldPurge ? ColumnFamilyStore.removeDeleted(cf, controller.gcBefore) : cf;
+        if (shouldPurge && compacted != null && compacted.metadata().getDefaultValidator().isCommutative())
+            CounterColumn.removeOldShards(compacted, controller.gcBefore);
+        return compacted;
+    }
+
     public PrecompactedRow(CompactionController controller, List<SSTableIdentityIterator> rows)
     {
         super(rows.get(0).getKey());
-        this.gcBefore = controller.gcBefore;
+        gcBefore = controller.gcBefore;
+        compactedCf = removeDeletedAndOldShards(rows.get(0).getKey(), controller, merge(rows));
+    }
 
+    private static ColumnFamily merge(List<SSTableIdentityIterator> rows)
+    {
         ColumnFamily cf = null;
         for (SSTableIdentityIterator row : rows)
         {
@@ -70,7 +88,7 @@ public PrecompactedRow(CompactionController controller, List<SSTableIdentityIter
             }
             catch (IOException e)
             {
-                logger.error("Skipping row " + key + " in " + row.getPath(), e);
+                logger.error("Skipping row " + row.getKey() + " in " + row.getPath(), e);
                 continue;
             }
             if (cf == null)
@@ -82,18 +100,12 @@ public PrecompactedRow(CompactionController controller, List<SSTableIdentityIter
                 cf.addAll(thisCF);
             }
         }
-        boolean shouldPurge = controller.shouldPurge(key);
-        compactedCf = shouldPurge ? ColumnFamilyStore.removeDeleted(cf, controller.gcBefore) : cf;
-        if (shouldPurge && compactedCf != null && compactedCf.metadata().getDefaultValidator().isCommutative())
-        {
-            CounterColumn.removeOldShards(compactedCf, controller.gcBefore);
-        }
+        return cf;
     }
 
     public void write(DataOutput out) throws IOException
     {
-        if (compactedCf != null)
-        {
+        assert compactedCf != null;
             DataOutputBuffer buffer = new DataOutputBuffer();
             DataOutputBuffer headerBuffer = new DataOutputBuffer();
             ColumnIndexer.serialize(compactedCf, headerBuffer);
@@ -102,12 +114,10 @@ public void write(DataOutput out) throws IOException
             out.write(headerBuffer.getData(), 0, headerBuffer.getLength());
             out.write(buffer.getData(), 0, buffer.getLength());
         }
-    }
 
     public void update(MessageDigest digest)
     {
-        if (compactedCf != null)
-        {
+        assert compactedCf != null;
             DataOutputBuffer buffer = new DataOutputBuffer();
             try
             {
@@ -121,7 +131,6 @@ public void update(MessageDigest digest)
             }
             compactedCf.updateDigest(digest);
         }
-    }
 
     public boolean isEmpty()
     {
diff --git a/cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java b/cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
index 6a18b513..21d23716 100644
--- a/cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
+++ b/cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
@@ -32,6 +32,7 @@
 import org.apache.cassandra.Util;
 import org.apache.cassandra.concurrent.Stage;
 import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.db.*;
 import org.apache.cassandra.db.compaction.PrecompactedRow;
 import org.apache.cassandra.dht.IPartitioner;
@@ -150,7 +151,8 @@ public void testValidatorAdd() throws Throwable
         validator.prepare(store);
 
         // add a row
-        validator.add(new PrecompactedRow(new DecoratedKey(mid, ByteBufferUtil.bytes("inconceivable!")), null));
+        validator.add(new PrecompactedRow(new DecoratedKey(mid, ByteBufferUtil.bytes("inconceivable!")),
+                                          new ColumnFamily(DatabaseDescriptor.getCFMetaData(tablename, cfname))));
         validator.completeTree();
 
         // confirm that the tree was validated
