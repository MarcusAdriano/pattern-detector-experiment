diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/AnalyticsContext.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/AnalyticsContext.java
index e69de29b..5ee5a774 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/AnalyticsContext.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/AnalyticsContext.java
@@ -0,0 +1,788 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.analytics;
+
+import java.io.IOException;
+import java.net.DatagramPacket;
+import java.net.DatagramSocket;
+import java.net.InetSocketAddress;
+import java.net.SocketAddress;
+import java.net.SocketException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.TreeMap;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.service.IComponentShutdown;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+
+/**
+ * Context for sending metrics to Ganglia. This class drives the entire metric collection process.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com ) & Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+public class AnalyticsContext implements IComponentShutdown
+{
+	private static Logger logger_ = Logger.getLogger(AnalyticsContext.class);
+
+	private static final String PERIOD_PROPERTY = "period";
+	private static final String SERVERS_PROPERTY = "servers";
+	private static final String UNITS_PROPERTY = "units";
+	private static final String SLOPE_PROPERTY = "slope";
+	private static final String TMAX_PROPERTY = "tmax";
+	private static final String DMAX_PROPERTY = "dmax";
+
+	private static final String DEFAULT_UNITS = "";
+	private static final String DEFAULT_SLOPE = "both";
+	private static final int DEFAULT_TMAX = 60;
+	private static final int DEFAULT_DMAX = 0;
+	private static final int DEFAULT_PORT = 8649;
+	private static final int BUFFER_SIZE = 1500;			 // as per libgmond.c
+
+	private static final Map<Class,String> typeTable_ = new HashMap<Class,String>(5);
+
+	private Map<String,RecordMap> bufferedData_ = new HashMap<String,RecordMap>();
+    /* Keeps the MetricRecord for each abstraction that implements IAnalyticsSource */
+    private Map<String, MetricsRecord> recordMap_ = new HashMap<String, MetricsRecord>();
+	private Map<String,Object> attributeMap_ = new HashMap<String,Object>();
+	private Set<IAnalyticsSource> updaters = new HashSet<IAnalyticsSource>(1);
+	private List<InetSocketAddress> metricsServers_;
+
+	private Map<String, String> unitsTable_;
+	private Map<String, String> slopeTable_;
+	private Map<String, String> tmaxTable_;
+	private Map<String, String> dmaxTable_;
+
+	/* singleton instance */
+	private static AnalyticsContext instance_;
+    /* Used to lock the factory for creation of StorageService instance */
+    private static Lock createLock_ = new ReentrantLock();
+
+	/**
+	 * Default period in seconds at which data is sent to the metrics system.
+	*/
+	private static final int DEFAULT_PERIOD = 5;
+
+	/**
+	 * Port to which we should write the data.
+	 */
+	private int port_ = DEFAULT_PORT;
+
+	private Timer timer = null;
+	private int period_ = DEFAULT_PERIOD;
+	private volatile boolean isMonitoring = false;
+	private byte[] buffer_ = new byte[BUFFER_SIZE];
+	private int offset_;
+
+	private DatagramSocket datagramSocket_;
+
+	static class TagMap extends TreeMap<String,Object>
+	{
+		private static final long serialVersionUID = 3546309335061952993L;
+		TagMap()
+		{
+			super();
+		}
+		TagMap(TagMap orig)
+		{
+			super(orig);
+		}
+	}
+
+	static class MetricMap extends TreeMap<String,Number>
+	{
+		private static final long serialVersionUID = -7495051861141631609L;
+	}
+
+	static class RecordMap extends HashMap<TagMap,MetricMap>
+	{
+		private static final long serialVersionUID = 259835619700264611L;
+	}
+
+	static
+	{
+		typeTable_.put(String.class, "string");
+		typeTable_.put(Byte.class, "int8");
+		typeTable_.put(Short.class, "int16");
+		typeTable_.put(Integer.class, "int32");
+		typeTable_.put(Float.class, "float");
+	}
+
+
+	/**
+	 * Creates a new instance of AnalyticsReporter
+	 */
+	public AnalyticsContext()
+	{
+		StorageService.instance().registerComponentForShutdown(this);
+	}
+
+	/**
+	* Initializes the context.
+	*/
+	public void init(String contextName, String serverSpecList)
+	{
+		String periodStr = getAttribute(PERIOD_PROPERTY);
+
+		if (periodStr != null)
+		{
+			int period = 0;
+			try
+			{
+				period = Integer.parseInt(periodStr);
+			}
+			catch (NumberFormatException nfe)
+			{
+			}
+
+			if (period <= 0)
+			{
+				throw new AnalyticsException("Invalid period: " + periodStr);
+			}
+
+			setPeriod(period);
+		}
+
+		metricsServers_ = parse(serverSpecList, port_);
+		unitsTable_ = getAttributeTable(UNITS_PROPERTY);
+		slopeTable_ = getAttributeTable(SLOPE_PROPERTY);
+		tmaxTable_ = getAttributeTable(TMAX_PROPERTY);
+		dmaxTable_ = getAttributeTable(DMAX_PROPERTY);
+
+		try
+		{
+			datagramSocket_ = new DatagramSocket();
+		}
+		catch (SocketException se)
+		{
+			se.printStackTrace();
+		}
+	}
+
+	/**
+	 * Sends a record to the metrics system.
+	 */
+	public void emitRecord(String recordName, OutputRecord outRec) throws IOException
+	{
+		// emit each metric in turn
+		for (String metricName : outRec.getMetricNames())
+		{
+			Object metric = outRec.getMetric(metricName);
+			String type = (String) typeTable_.get(metric.getClass());
+			emitMetric(metricName, type, metric.toString());
+		}
+	}
+
+	/**
+	 * Helper which actually writes the metric in XDR format.
+	 *
+	 * @param name
+	 * @param type
+	 * @param value
+	 * @throws IOException
+	 */
+	private void emitMetric(String name, String type, String value) throws IOException
+	{
+		String units = getUnits(name);
+		int slope = getSlope(name);
+		int tmax = getTmax(name);
+		int dmax = getDmax(name);
+		offset_ = 0;
+
+		xdr_int(0); // metric_user_defined
+		xdr_string(type);
+		xdr_string(name);
+		xdr_string(value);
+		xdr_string(units);
+		xdr_int(slope);
+		xdr_int(tmax);
+		xdr_int(dmax);
+
+		for (InetSocketAddress socketAddress : metricsServers_)
+		{
+			DatagramPacket packet = new DatagramPacket(buffer_, offset_, socketAddress);
+			datagramSocket_.send(packet);
+		}
+	}
+
+	private String getUnits(String metricName)
+	{
+		String result = (String) unitsTable_.get(metricName);
+		if (result == null)
+		{
+			result = DEFAULT_UNITS;
+		}
+
+		return result;
+	}
+
+	private int getSlope(String metricName)
+	{
+		String slopeString = (String) slopeTable_.get(metricName);
+		if (slopeString == null)
+		{
+			slopeString = DEFAULT_SLOPE;
+		}
+
+		return ("zero".equals(slopeString) ? 0 : 3); // see gmetric.c
+	}
+
+	private int getTmax(String metricName)
+	{
+		String tmaxString = (String) tmaxTable_.get(metricName);
+		if (tmaxString == null)
+		{
+			return DEFAULT_TMAX;
+		}
+		else
+		{
+			return Integer.parseInt(tmaxString);
+		}
+	}
+
+	private int getDmax(String metricName)
+	{
+		String dmaxString = (String) dmaxTable_.get(metricName);
+		if (dmaxString == null)
+		{
+			return DEFAULT_DMAX;
+		}
+		else
+		{
+			return Integer.parseInt(dmaxString);
+		}
+	}
+
+	/**
+	 * Puts a string into the buffer by first writing the size of the string
+	 * as an int, followed by the bytes of the string, padded if necessary to
+	 * a multiple of 4.
+	 */
+	private void xdr_string(String s)
+	{
+		byte[] bytes = s.getBytes();
+		int len = bytes.length;
+		xdr_int(len);
+		System.arraycopy(bytes, 0, buffer_, offset_, len);
+		offset_ += len;
+		pad();
+	}
+
+	/**
+	 * Pads the buffer with zero bytes up to the nearest multiple of 4.
+	 */
+	private void pad()
+	{
+		int newOffset = ((offset_ + 3) / 4) * 4;
+		while (offset_ < newOffset)
+		{
+			buffer_[offset_++] = 0;
+		}
+	}
+
+	/**
+	 * Puts an integer into the buffer as 4 bytes, big-endian.
+	 */
+	private void xdr_int(int i)
+	{
+		buffer_[offset_++] = (byte) ((i >> 24) & 0xff);
+		buffer_[offset_++] = (byte) ((i >> 16) & 0xff);
+		buffer_[offset_++] = (byte) ((i >> 8) & 0xff);
+		buffer_[offset_++] = (byte) (i & 0xff);
+	}
+
+
+
+	/**
+	 * Returns the names of all the factory's attributes.
+	 *
+	 * @return the attribute names
+	 */
+	public String[] getAttributeNames()
+	{
+		String[] result = new String[attributeMap_.size()];
+		int i = 0;
+		// for (String attributeName : attributeMap.keySet()) {
+		Iterator<String> it = attributeMap_.keySet().iterator();
+		while (it.hasNext())
+		{
+			result[i++] = it.next();
+		}
+		return result;
+	}
+
+	/**
+	 * Sets the named factory attribute to the specified value, creating it
+	 * if it did not already exist.	If the value is null, this is the same as
+	 * calling removeAttribute.
+	 *
+	 * @param attributeName the attribute name
+	 * @param value the new attribute value
+	 */
+	public void setAttribute(String attributeName, Object value)
+	{
+		attributeMap_.put(attributeName, value);
+	}
+
+	/**
+	 * Removes the named attribute if it exists.
+	 *
+	 * @param attributeName the attribute name
+	 */
+	public void removeAttribute(String attributeName)
+	{
+		attributeMap_.remove(attributeName);
+	}
+
+	/**
+	 * Returns the value of the named attribute, or null if there is no
+	 * attribute of that name.
+	 *
+	 * @param attributeName the attribute name
+	 * @return the attribute value
+	 */
+	public String getAttribute(String attributeName)
+	{
+		return (String)attributeMap_.get(attributeName);
+	}
+
+
+	/**
+	 * Returns an attribute-value map derived from the factory attributes
+	 * by finding all factory attributes that begin with
+	 * <i>contextName</i>.<i>tableName</i>.	The returned map consists of
+	 * those attributes with the contextName and tableName stripped off.
+	 */
+	protected Map<String,String> getAttributeTable(String tableName)
+	{
+		String prefix = tableName + ".";
+		Map<String,String> result = new HashMap<String,String>();
+		for (String attributeName : getAttributeNames())
+		{
+			if (attributeName.startsWith(prefix))
+			{
+				String name = attributeName.substring(prefix.length());
+				String value = (String) getAttribute(attributeName);
+				result.put(name, value);
+			}
+		}
+		return result;
+	}
+
+	/**
+	 * Starts or restarts monitoring, the emitting of metrics records.
+	 */
+	public void startMonitoring() throws IOException {
+		if (!isMonitoring)
+		{
+			startTimer();
+			isMonitoring = true;
+		}
+	}
+
+	/**
+	 * Stops monitoring.	This does not free buffered data.
+	 * @see #close()
+	 */
+	public void stopMonitoring() {
+		if (isMonitoring)
+		{
+			shutdown();
+			isMonitoring = false;
+		}
+	}
+
+	/**
+	 * Returns true if monitoring is currently in progress.
+	 */
+	public boolean isMonitoring() {
+		return isMonitoring;
+	}
+
+	/**
+	 * Stops monitoring and frees buffered data, returning this
+	 * object to its initial state.
+	 */
+	public void close()
+	{
+		stopMonitoring();
+		clearUpdaters();
+	}
+
+	/**
+	 * Creates a new AbstractMetricsRecord instance with the given <code>recordName</code>.
+	 * Throws an exception if the metrics implementation is configured with a fixed
+	 * set of record names and <code>recordName</code> is not in that set.
+	 *
+	 * @param recordName the name of the record
+	 * @throws AnalyticsException if recordName conflicts with configuration data
+	 */
+	public final void createRecord(String recordName)
+	{
+		if (bufferedData_.get(recordName) == null)
+		{
+			bufferedData_.put(recordName, new RecordMap());
+		}
+        recordMap_.put(recordName, new MetricsRecord(recordName, this));
+	}
+
+	/**
+	 * Return the MetricsRecord associated with this record name.
+	 * @param recordName the name of the record
+	 * @return newly created instance of MetricsRecordImpl or subclass
+	 */
+	public MetricsRecord getMetricsRecord(String recordName)
+	{
+		return recordMap_.get(recordName);
+	}
+
+	/**
+	 * Registers a callback to be called at time intervals determined by
+	 * the configuration.
+	 *
+	 * @param updater object to be run periodically; it should update
+	 * some metrics records
+	 */
+	public void registerUpdater(final IAnalyticsSource updater)
+	{
+		if (!updaters.contains(updater)) {
+			updaters.add(updater);
+		}
+	}
+
+	/**
+	 * Removes a callback, if it exists.
+	 *
+	 * @param updater object to be removed from the callback list
+	 */
+	public void unregisterUpdater(IAnalyticsSource updater)
+	{
+		updaters.remove(updater);
+	}
+
+	private void clearUpdaters()
+	{
+		updaters.clear();
+	}
+
+	/**
+	 * Starts timer if it is not already started
+	 */
+	private void startTimer()
+	{
+		if (timer == null)
+		{
+			timer = new Timer("Timer thread for monitoring AnalyticsContext", true);
+			TimerTask task = new TimerTask()
+			{
+				public void run()
+				{
+					try
+					{
+						timerEvent();
+					}
+					catch (IOException ioe)
+					{
+						ioe.printStackTrace();
+					}
+				}
+			};
+			long millis = period_ * 1000;
+			timer.scheduleAtFixedRate(task, millis, millis);
+		}
+	}
+
+	/**
+	 * Stops timer if it is running
+	 */
+	public void shutdown()
+	{
+		if (timer != null)
+		{
+			timer.cancel();
+			timer = null;
+		}
+	}
+
+	/**
+	 * Timer callback.
+	 */
+	private void timerEvent() throws IOException
+	{
+		if (isMonitoring)
+		{
+			Collection<IAnalyticsSource> myUpdaters;
+
+			// we dont need to synchronize as there will not be any
+			// addition or removal of listeners
+			myUpdaters = new ArrayList<IAnalyticsSource>(updaters);
+
+			// Run all the registered updates without holding a lock
+			// on this context
+			for (IAnalyticsSource updater : myUpdaters)
+			{
+				try
+				{
+					updater.doUpdates(this);
+				}
+				catch (Throwable throwable)
+				{
+					throwable.printStackTrace();
+				}
+			}
+			emitRecords();
+		}
+	}
+
+	/**
+	 *	Emits the records.
+	 */
+	private void emitRecords() throws IOException
+	{
+		for (String recordName : bufferedData_.keySet())
+		{
+			RecordMap recordMap = bufferedData_.get(recordName);
+			synchronized (recordMap)
+			{
+				for (TagMap tagMap : recordMap.keySet())
+				{
+					MetricMap metricMap = recordMap.get(tagMap);
+					OutputRecord outRec = new OutputRecord(tagMap, metricMap);
+					emitRecord(recordName, outRec);
+				}
+			}
+		}
+		flush();
+	}
+
+	/**
+	 * Called each period after all records have been emitted, this method does nothing.
+	 * Subclasses may override it in order to perform some kind of flush.
+	 */
+	protected void flush() throws IOException
+	{
+	}
+
+	/**
+	 * Called by MetricsRecordImpl.update().	Creates or updates a row in
+	 * the internal table of metric data.
+	 */
+	protected void update(MetricsRecord record)
+	{
+		String recordName = record.getRecordName();
+		TagMap tagTable = record.getTagTable();
+		Map<String,MetricValue> metricUpdates = record.getMetricTable();
+
+		RecordMap recordMap = getRecordMap(recordName);
+		synchronized (recordMap)
+		{
+			MetricMap metricMap = recordMap.get(tagTable);
+			if (metricMap == null)
+			{
+				metricMap = new MetricMap();
+				TagMap tagMap = new TagMap(tagTable); // clone tags
+				recordMap.put(tagMap, metricMap);
+			}
+			for (String metricName : metricUpdates.keySet())
+			{
+				MetricValue updateValue = metricUpdates.get(metricName);
+				Number updateNumber = updateValue.getNumber();
+				Number currentNumber = metricMap.get(metricName);
+				if (currentNumber == null || updateValue.isAbsolute())
+				{
+					metricMap.put(metricName, updateNumber);
+				}
+				else
+				{
+					Number newNumber = sum(updateNumber, currentNumber);
+					metricMap.put(metricName, newNumber);
+				}
+			}
+		}
+	}
+
+	private RecordMap getRecordMap(String recordName)
+	{
+		return bufferedData_.get(recordName);
+	}
+
+	/**
+	 * Adds two numbers, coercing the second to the type of the first.
+	 *
+	 */
+	private Number sum(Number a, Number b)
+	{
+		if (a instanceof Integer)
+		{
+			return new Integer(a.intValue() + b.intValue());
+		}
+		else if (a instanceof Float)
+		{
+			return new Float(a.floatValue() + b.floatValue());
+		}
+		else if (a instanceof Short)
+		{
+			return new Short((short)(a.shortValue() + b.shortValue()));
+		}
+		else if (a instanceof Byte)
+		{
+			return new Byte((byte)(a.byteValue() + b.byteValue()));
+		}
+		else
+		{
+			// should never happen
+			throw new AnalyticsException("Invalid number type");
+		}
+	}
+
+	/**
+	 * Called by MetricsRecordImpl.remove().	Removes any matching row in
+	 * the internal table of metric data.	A row matches if it has the same
+	 * tag names and tag values.
+	 */
+	protected void remove(MetricsRecord record)
+	{
+		String recordName = record.getRecordName();
+		TagMap tagTable = record.getTagTable();
+
+		RecordMap recordMap = getRecordMap(recordName);
+
+		recordMap.remove(tagTable);
+	}
+
+	/**
+	 * Returns the timer period.
+	 */
+	public int getPeriod()
+	{
+		return period_;
+	}
+
+	/**
+	 * Sets the timer period
+	 */
+	protected void setPeriod(int period)
+	{
+		this.period_ = period;
+	}
+
+	/**
+	 * Sets the default port to listen on
+	 */
+	public void setPort(int port)
+	{
+		port_ = port;
+	}
+
+	/**
+	 * Parses a space and/or comma separated sequence of server specifications
+	 * of the form <i>hostname</i> or <i>hostname:port</i>.	If
+	 * the specs string is null, defaults to localhost:defaultPort.
+	 *
+	 * @return a list of InetSocketAddress objects.
+	 */
+	private static List<InetSocketAddress> parse(String specs, int defaultPort)
+	{
+		List<InetSocketAddress> result = new ArrayList<InetSocketAddress>(1);
+		if (specs == null) {
+			result.add(new InetSocketAddress("localhost", defaultPort));
+		}
+		else {
+			String[] specStrings = specs.split("[ ,]+");
+			for (String specString : specStrings) {
+				int colon = specString.indexOf(':');
+				if (colon < 0 || colon == specString.length() - 1)
+				{
+					result.add(new InetSocketAddress(specString, defaultPort));
+				} else
+				{
+					String hostname = specString.substring(0, colon);
+					int port = Integer.parseInt(specString.substring(colon+1));
+					result.add(new InetSocketAddress(hostname, port));
+				}
+			}
+		}
+		return result;
+	}
+
+	/**
+	 * Starts up the analytics context and registers the VM metrics.
+	 */
+	public void start()
+	{
+		// register the vm analytics object with the analytics context to update the data
+		registerUpdater(new VMAnalyticsSource());
+
+
+        init("analyticsContext", DatabaseDescriptor.getGangliaServers());
+
+		try
+		{
+			startMonitoring();
+		}
+		catch(IOException e)
+		{
+			logger_.error(LogUtil.throwableToString(e));
+		}
+	}
+
+	public void stop()
+	{
+		close();
+	}
+
+    /**
+     * Factory method that gets an instance of the StorageService
+     * class.
+     */
+    public static AnalyticsContext instance()
+    {
+        if ( instance_ == null )
+        {
+        	AnalyticsContext.createLock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                {
+                    instance_ = new AnalyticsContext();
+                }
+            }
+            finally
+            {
+                createLock_.unlock();
+            }
+        }
+        return instance_;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/AnalyticsException.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/AnalyticsException.java
index e69de29b..053287c1 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/AnalyticsException.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/AnalyticsException.java
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.analytics;
+
+/**
+ * General-purpose, unchecked metrics exception.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com ) & Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+public class AnalyticsException extends RuntimeException
+{
+
+	  private static final long serialVersionUID = -1643257498540498497L;
+
+	  /**
+	   * Creates a new instance of MetricsException
+	   */
+	  public AnalyticsException()
+	  {
+	  }
+
+	  /** Creates a new instance of MetricsException
+	   *
+	   * @param message an error message
+	   */
+	  public AnalyticsException(String message)
+	  {
+	    super(message);
+	  }
+
+	}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/DBAnalyticsSource.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/DBAnalyticsSource.java
index e69de29b..fc7a5266 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/DBAnalyticsSource.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/DBAnalyticsSource.java
@@ -0,0 +1,182 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.analytics;
+
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicLong;
+
+/**
+ * This class sets up the analytics package to report metrics into
+ * Ganglia for the various DB operations such as: reads per second,
+ * average read latency, writes per second, average write latency.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com ) & Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+public class DBAnalyticsSource implements IAnalyticsSource
+{
+    private static final String METRIC_READ_OPS = "Read Operations";
+    private static final String RECORD_READ_OPS = "ReadOperationsRecord";
+    private static final String TAG_READOPS = "ReadOperationsTag";
+    private static final String TAG_READ_OPS = "ReadOperationsTagValue";
+
+    private static final String METRIC_READ_AVG = "Average Read Latency";
+    private static final String RECORD_READ_AVG = "ReadLatencyRecord";
+    private static final String TAG_READAVG = "AverageReadLatencyTag";
+    private static final String TAG_READ_AVG = "ReadLatencyTagValue";
+
+    private static final String METRIC_WRITE_OPS = "Write Operations";
+    private static final String RECORD_WRITE_OPS = "WriteOperationsRecord";
+    private static final String TAG_WRITEOPS = "WriteOperationsTag";
+    private static final String TAG_WRITE_OPS = "WriteOperationsTagValue";
+
+    private static final String METRIC_WRITE_AVG = "Average Write Latency";
+    private static final String RECORD_WRITE_AVG = "WriteLatencyRecord";
+    private static final String TAG_WRITEAVG = "AverageWriteLatencyTag";
+    private static final String TAG_WRITE_AVG = "WriteLatencyTagValue";
+
+    /* keep track of the number of read operations */
+    private AtomicInteger readOperations_ = new AtomicInteger(0);
+
+    /* keep track of the number of read latencies */
+    private AtomicLong readLatencies_ = new AtomicLong(0);
+
+    /* keep track of the number of write operations */
+    private AtomicInteger writeOperations_ = new AtomicInteger(0);
+
+    /* keep track of the number of write latencies */
+    private AtomicLong writeLatencies_ = new AtomicLong(0);
+
+    /**
+     * Create all the required records we intend to display, and
+     * register with the AnalyticsContext.
+     */
+    public DBAnalyticsSource()
+    {
+        /* register with the AnalyticsContext */
+        AnalyticsContext.instance().registerUpdater(this);
+        /* set the units for the metric type */
+        AnalyticsContext.instance().setAttribute("units." + METRIC_READ_OPS, "r/s");
+        /* create the record */
+        AnalyticsContext.instance().createRecord(RECORD_READ_OPS);
+
+        /* set the units for the metric type */
+        AnalyticsContext.instance().setAttribute("units." + METRIC_READ_AVG, "ms");
+        /* create the record */
+        AnalyticsContext.instance().createRecord(RECORD_READ_AVG);
+
+        /* set the units for the metric type */
+        AnalyticsContext.instance().setAttribute("units." + METRIC_WRITE_OPS, "w/s");
+        /* create the record */
+        AnalyticsContext.instance().createRecord(RECORD_WRITE_OPS);
+
+        /* set the units for the metric type */
+        AnalyticsContext.instance().setAttribute("units." + METRIC_WRITE_AVG, "ms");
+        /* create the record */
+        AnalyticsContext.instance().createRecord(RECORD_WRITE_AVG);
+    }
+
+    /**
+     * Update each of the records with the relevant data
+     *
+     * @param context the reference to the context which has called this callback
+     */
+    public void doUpdates(AnalyticsContext context)
+    {
+        // update the read operations record
+        MetricsRecord readUsageRecord = context.getMetricsRecord(RECORD_READ_OPS);
+        int period = context.getPeriod();
+
+        if(readUsageRecord != null)
+        {
+            if ( readOperations_.get() > 0 )
+            {
+                readUsageRecord.setTag(TAG_READOPS, TAG_READ_OPS);
+                readUsageRecord.setMetric(METRIC_READ_OPS, readOperations_.get() / period);
+                readUsageRecord.update();
+            }
+        }
+
+        // update the read latency record
+        MetricsRecord readLatencyRecord = context.getMetricsRecord(RECORD_READ_AVG);
+        if(readLatencyRecord != null)
+        {
+            if ( readOperations_.get() > 0 )
+            {
+                readLatencyRecord.setTag(TAG_READAVG, TAG_READ_AVG);
+                readLatencyRecord.setMetric(METRIC_READ_AVG, readLatencies_.get() / readOperations_.get() );
+                readLatencyRecord.update();
+            }
+        }
+
+        // update the write operations record
+        MetricsRecord writeUsageRecord = context.getMetricsRecord(RECORD_WRITE_OPS);
+        if(writeUsageRecord != null)
+        {
+            if ( writeOperations_.get() > 0 )
+            {
+                writeUsageRecord.setTag(TAG_WRITEOPS, TAG_WRITE_OPS);
+                writeUsageRecord.setMetric(METRIC_WRITE_OPS, writeOperations_.get() / period);
+                writeUsageRecord.update();
+            }
+        }
+
+        // update the write latency record
+        MetricsRecord writeLatencyRecord = context.getMetricsRecord(RECORD_WRITE_AVG);
+        if(writeLatencyRecord != null)
+        {
+            if ( writeOperations_.get() > 0 )
+            {
+                writeLatencyRecord.setTag(TAG_WRITEAVG, TAG_WRITE_AVG);
+                writeLatencyRecord.setMetric(METRIC_WRITE_AVG, writeLatencies_.get() / writeOperations_.get() );
+                writeLatencyRecord.update();
+            }
+        }
+
+        clear();
+    }
+
+    /**
+     * Reset all the metric records
+     */
+    private void clear()
+    {
+        readOperations_.set(0);
+        readLatencies_.set(0);
+        writeOperations_.set(0);
+        writeLatencies_.set(0);
+    }
+
+    /**
+     * Update the read statistics.
+     */
+    public void updateReadStatistics(long latency)
+    {
+        readOperations_.incrementAndGet();
+        readLatencies_.addAndGet(latency);
+    }
+
+    /**
+     * Update the write statistics.
+     */
+    public void updateWriteStatistics(long latency)
+    {
+        writeOperations_.incrementAndGet();
+        writeLatencies_.addAndGet(latency);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/IAnalyticsSource.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/IAnalyticsSource.java
index e69de29b..78f63cdb 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/IAnalyticsSource.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/IAnalyticsSource.java
@@ -0,0 +1,35 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.analytics;
+
+/**
+ * Call-back interface.  See <code>AnalyticsContext.registerUpdater()</code>.
+ * This callback is called at a regular (pre-registered time interval) in
+ * order to update the metric values.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com ) & Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+public interface IAnalyticsSource
+{
+  /**
+   * Timer-based call-back from the metric library.
+   */
+  public abstract void doUpdates(AnalyticsContext context);
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/MetricValue.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/MetricValue.java
index e69de29b..a0dac24d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/MetricValue.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/MetricValue.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.analytics;
+
+
+/**
+ * A Number that is either an absolute or an incremental amount.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com ) & Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+public class MetricValue
+{
+	public static final boolean ABSOLUTE = false;
+	public static final boolean INCREMENT = true;
+
+	private boolean isIncrement;
+	private Number number;
+
+	/**
+	 * Creates a new instance of MetricValue
+	 *
+	 *  @param number this initializes the initial value of this metric
+	 *  @param isIncrement sets if the metric can be incremented or only set
+	 */
+	public MetricValue(Number number, boolean isIncrement)
+	{
+		this.number = number;
+		this.isIncrement = isIncrement;
+	}
+
+	/**
+	 * Checks if this metric can be incremented.
+	 *
+	 * @return true if the value of this metric can be incremented, false otherwise
+	 */
+	public boolean isIncrement()
+	{
+		return isIncrement;
+	}
+
+	/**
+	 * Checks if the value of this metric is always an absolute value. This is the
+	 * inverse of isIncrement.
+	 *
+	 * @return true if the
+	 */
+	public boolean isAbsolute()
+	{
+		return !isIncrement;
+	}
+
+	/**
+	 * Returns the current number value of the metric.
+	 *
+	 * @return the Number value of this metric
+	 */
+	public Number getNumber()
+	{
+		return number;
+	}
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/MetricsRecord.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/MetricsRecord.java
index e69de29b..4371dd16 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/MetricsRecord.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/MetricsRecord.java
@@ -0,0 +1,265 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.analytics;
+
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+
+/**
+ * This class keeps a back-pointer to the AnalyticsContext
+ * and delegates back to it on <code>update</code> and <code>remove()</code>.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com ) & Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+public class MetricsRecord {
+
+	private AnalyticsContext.TagMap tagTable = new AnalyticsContext.TagMap();
+	private Map<String,MetricValue> metricTable = new LinkedHashMap<String,MetricValue>();
+
+	private String recordName;
+	private AnalyticsContext context;
+
+
+	/**
+	 * Creates a new instance of MetricsRecord
+	 *
+	 *  @param recordName name of this record
+	 *  @param context the context which this record is a part of
+	 */
+	protected MetricsRecord(String recordName, AnalyticsContext context)
+	{
+		this.recordName = recordName;
+		this.context = context;
+	}
+
+	/**
+	 * Returns the record name.
+	 *
+	 * @return the record name
+	 */
+	public String getRecordName() {
+		return recordName;
+	}
+
+	/**
+	 * Sets the named tag to the specified value.
+	 *
+	 * @param tagName name of the tag
+	 * @param tagValue new value of the tag
+	 * @throws MetricsException if the tagName conflicts with the configuration
+	 */
+	public void setTag(String tagName, String tagValue) {
+		if (tagValue == null) {
+			tagValue = "";
+		}
+		tagTable.put(tagName, tagValue);
+	}
+
+	/**
+	 * Sets the named tag to the specified value.
+	 *
+	 * @param tagName name of the tag
+	 * @param tagValue new value of the tag
+	 * @throws MetricsException if the tagName conflicts with the configuration
+	 */
+	public void setTag(String tagName, int tagValue) {
+		tagTable.put(tagName, new Integer(tagValue));
+	}
+
+	/**
+	 * Sets the named tag to the specified value.
+	 *
+	 * @param tagName name of the tag
+	 * @param tagValue new value of the tag
+	 * @throws MetricsException if the tagName conflicts with the configuration
+	 */
+	public void setTag(String tagName, short tagValue) {
+		tagTable.put(tagName, new Short(tagValue));
+	}
+
+	/**
+	 * Sets the named tag to the specified value.
+	 *
+	 * @param tagName name of the tag
+	 * @param tagValue new value of the tag
+	 * @throws MetricsException if the tagName conflicts with the configuration
+	 */
+	public void setTag(String tagName, byte tagValue)
+	{
+		tagTable.put(tagName, new Byte(tagValue));
+	}
+
+	/**
+	 * Sets the named metric to the specified value.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue new value of the metric
+	 * @throws MetricsException if the metricName or the type of the metricValue
+	 * conflicts with the configuration
+	 */
+	public void setMetric(String metricName, int metricValue)
+	{
+		setAbsolute(metricName, new Integer(metricValue));
+	}
+
+	/**
+	 * Sets the named metric to the specified value.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue new value of the metric
+	 * @throws MetricsException if the metricName or the type of the metricValue
+	 * conflicts with the configuration
+	 */
+	public void setMetric(String metricName, short metricValue)
+	{
+		setAbsolute(metricName, new Short(metricValue));
+	}
+
+	/**
+	 * Sets the named metric to the specified value.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue new value of the metric
+	 * @throws MetricsException if the metricName or the type of the metricValue
+	 * conflicts with the configuration
+	 */
+	public void setMetric(String metricName, byte metricValue)
+	{
+		setAbsolute(metricName, new Byte(metricValue));
+	}
+
+	/**
+	 * Sets the named metric to the specified value.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue new value of the metric
+	 * @throws MetricsException if the metricName or the type of the metricValue
+	 * conflicts with the configuration
+	 */
+	public void setMetric(String metricName, float metricValue)
+	{
+		setAbsolute(metricName, new Float(metricValue));
+	}
+
+	/**
+	 * Increments the named metric by the specified value.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue incremental value
+	 * @throws MetricsException if the metricName or the type of the metricValue
+	 * conflicts with the configuration
+	 */
+	public void incrMetric(String metricName, int metricValue)
+	{
+		setIncrement(metricName, new Integer(metricValue));
+	}
+
+	/**
+	 * Increments the named metric by the specified value.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue incremental value
+	 * @throws MetricsException if the metricName or the type of the metricValue
+	 * conflicts with the configuration
+	 */
+	public void incrMetric(String metricName, short metricValue)
+	{
+		setIncrement(metricName, new Short(metricValue));
+	}
+
+	/**
+	 * Increments the named metric by the specified value.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue incremental value
+	 * @throws MetricsException if the metricName or the type of the metricValue
+	 * conflicts with the configuration
+	 */
+	public void incrMetric(String metricName, byte metricValue)
+	{
+		setIncrement(metricName, new Byte(metricValue));
+	}
+
+	/**
+	 * Increments the named metric by the specified value.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue incremental value
+	 * @throws MetricsException if the metricName or the type of the metricValue
+	 * conflicts with the configuration
+	 */
+	public void incrMetric(String metricName, float metricValue)
+	{
+		setIncrement(metricName, new Float(metricValue));
+	}
+
+	/**
+	 * Sets the value of the metric identified by metricName with the
+	 * number metricValue.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue number value to which it should be updated
+	 */
+	private void setAbsolute(String metricName, Number metricValue)
+	{
+		metricTable.put(metricName, new MetricValue(metricValue, MetricValue.ABSOLUTE));
+	}
+
+	/**
+	 * Increments the value of the metric identified by metricName with the
+	 * number metricValue.
+	 *
+	 * @param metricName name of the metric
+	 * @param metricValue number value by which it should be incremented
+	 */
+	private void setIncrement(String metricName, Number metricValue)
+	{
+		metricTable.put(metricName, new MetricValue(metricValue, MetricValue.INCREMENT));
+	}
+
+	/**
+	 * Updates the table of buffered data which is to be sent periodically.
+	 * If the tag values match an existing row, that row is updated;
+	 * otherwise, a new row is added.
+	 */
+	public void update()
+	{
+		context.update(this);
+	}
+
+	/**
+	 * Removes the row, if it exists, in the buffered data table having tags
+	 * that equal the tags that have been set on this record.
+	 */
+	public void remove()
+	{
+		context.remove(this);
+	}
+
+	AnalyticsContext.TagMap getTagTable()
+	{
+		return tagTable;
+	}
+
+	Map<String, MetricValue> getMetricTable()
+	{
+		return metricTable;
+	}
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/OutputRecord.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/OutputRecord.java
index e69de29b..ea3fff58 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/OutputRecord.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/OutputRecord.java
@@ -0,0 +1,82 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.analytics;
+
+import java.util.Collections;
+import java.util.Set;
+
+/**
+ * Represents a record of metric data to be sent to a metrics system.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com ) & Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+public class OutputRecord
+{
+	private AnalyticsContext.TagMap tagMap;
+	private AnalyticsContext.MetricMap metricMap;
+
+	/**
+	 * Creates a new instance of OutputRecord
+	 */
+	OutputRecord(AnalyticsContext.TagMap tagMap, AnalyticsContext.MetricMap metricMap)
+	{
+		this.tagMap = tagMap;
+		this.metricMap = metricMap;
+	}
+
+	/**
+	 * Returns the set of tag names.
+	 */
+	public Set<String> getTagNames()
+	{
+		return Collections.unmodifiableSet(tagMap.keySet());
+	}
+
+	/**
+	 * Returns a tag object which is can be a String, Integer, Short or Byte.
+	 *
+	 * @return the tag value, or null if there is no such tag
+	 */
+	public Object getTag(String name)
+	{
+		return tagMap.get(name);
+	}
+
+	/**
+	 * Returns the set of metric names.
+	 *
+	 * @return the set of metric names
+	 */
+	public Set<String> getMetricNames()
+	{
+		return Collections.unmodifiableSet(metricMap.keySet());
+	}
+
+	/**
+	 * Returns the metric object which can be a Float, Integer, Short or Byte.
+	 *
+	 * @param name name of the metric for which the value is being requested
+	 * @return return the tag value, or null if there is no such tag
+	 */
+	public Number getMetric(String name)
+	{
+		return (Number) metricMap.get(name);
+	}
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/VMAnalyticsSource.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/VMAnalyticsSource.java
index e69de29b..2e1b3d91 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/VMAnalyticsSource.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/analytics/VMAnalyticsSource.java
@@ -0,0 +1,86 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.analytics;
+
+import java.lang.management.ManagementFactory;
+import java.lang.management.MemoryMXBean;
+import java.lang.management.MemoryUsage;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+/**
+ * This class sets up the analytics package to report metrics into
+ * Ganglia for VM heap utilization.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com ) & Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+
+public class VMAnalyticsSource implements IAnalyticsSource
+{
+	private static final String METRIC_MEMUSAGE = "VM Heap Utilization";
+	private static final String RECORD_MEMUSAGE = "MemoryUsageRecord";
+	private static final String TAG_MEMUSAGE = "MemoryUsageTag";
+	private static final String TAG_MEMUSAGE_MEMUSED = "MemoryUsedTagValue";
+
+	/**
+	 * Setup the Ganglia record to display the VM heap utilization.
+	 */
+	public VMAnalyticsSource()
+	{
+		// set the units for the metric type
+		AnalyticsContext.instance().setAttribute("units." + METRIC_MEMUSAGE, "MB");
+		// create the record
+        AnalyticsContext.instance().createRecord(RECORD_MEMUSAGE);
+  	}
+
+	/**
+	 * Update the VM heap utilization record with the relevant data.
+	 *
+	 * @param context the reference to the context which has called this callback
+	 */
+	public void doUpdates(AnalyticsContext context)
+	{
+        // update the memory used record
+		MetricsRecord memUsageRecord = context.getMetricsRecord(RECORD_MEMUSAGE);
+		if(memUsageRecord != null)
+		{
+			updateUsedMemory(memUsageRecord);
+		}
+	}
+
+	private void updateUsedMemory(MetricsRecord memUsageRecord)
+	{
+		memUsageRecord.setTag(TAG_MEMUSAGE, TAG_MEMUSAGE_MEMUSED);
+		memUsageRecord.setMetric(METRIC_MEMUSAGE, getMemoryUsed());
+		memUsageRecord.update();
+	}
+
+	private float getMemoryUsed()
+	{
+        MemoryMXBean memoryMxBean = ManagementFactory.getMemoryMXBean();
+        MemoryUsage memUsage = memoryMxBean.getHeapMemoryUsage();
+        return (float)memUsage.getUsed()/(1024 * 1024);
+	}
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java
index e69de29b..0d10ba33 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java
@@ -0,0 +1,308 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.cli;
+
+import org.apache.thrift.*;
+
+import org.antlr.runtime.tree.*;
+import org.apache.cassandra.cql.common.Utils;
+import org.apache.cassandra.service.Cassandra;
+import org.apache.cassandra.service.CqlResult_t;
+import org.apache.cassandra.service.column_t;
+import org.apache.cassandra.service.NotFoundException;
+import org.apache.cassandra.service.InvalidRequestException;
+import org.apache.cassandra.utils.LogUtil;
+
+import java.util.*;
+
+// Cli Client Side Library
+public class CliClient 
+{
+    private Cassandra.Client thriftClient_ = null;
+    private CliSessionState css_ = null;
+
+    public CliClient(CliSessionState css, Cassandra.Client thriftClient)
+    {
+        css_ = css;
+        thriftClient_ = thriftClient;
+    }
+
+    // Execute a CLI Statement 
+    public void executeCLIStmt(String stmt) throws TException, NotFoundException, InvalidRequestException
+    {
+        CommonTree ast = null;
+
+        ast = CliCompiler.compileQuery(stmt);
+
+        switch (ast.getType()) {
+        case CliParser.NODE_EXIT:
+            cleanupAndExit();
+            break;
+        case CliParser.NODE_THRIFT_GET:
+            executeGet(ast);
+            break;
+        case CliParser.NODE_HELP:
+            printCmdHelp();
+            break;
+        case CliParser.NODE_THRIFT_SET:
+            executeSet(ast);
+            break;
+        case CliParser.NODE_SHOW_CLUSTER_NAME:
+            executeShowProperty(ast, "cluster name");
+            break;
+        case CliParser.NODE_SHOW_CONFIG_FILE:
+            executeShowProperty(ast, "config file");
+            break;
+        case CliParser.NODE_SHOW_VERSION:
+            executeShowProperty(ast, "version");
+            break;
+        case CliParser.NODE_SHOW_TABLES:
+            executeShowTables(ast);
+            break;
+        case CliParser.NODE_DESCRIBE_TABLE:
+            executeDescribeTable(ast);
+            break;
+        case CliParser.NODE_CONNECT:
+            executeConnect(ast);
+            break;
+        case CliParser.NODE_NO_OP:
+            // comment lines come here; they are treated as no ops.
+            break;
+        default:
+            css_.err.println("Invalid Statement (Type: " + ast.getType() + ")");
+            break;
+        }
+    }
+    
+    private void printCmdHelp()
+    {
+       css_.out.println("List of all CLI commands:");
+       css_.out.println("?                                                         Same as help.");
+       css_.out.println("connect <hostname>/<port>                                 Connect to Cassandra's thrift service.");
+       css_.out.println("describe table <tbl>                                      Describe table.");
+       css_.out.println("exit                                                      Exit CLI.");
+       css_.out.println("explain plan [<set stmt>|<get stmt>|<select stmt>]        Explains the PLAN for specified stmt.");
+       css_.out.println("help                                                      Display this help.");
+       css_.out.println("quit                                                      Exit CLI.");
+       css_.out.println("show config file                                          Display contents of config file");
+       css_.out.println("show cluster name                                         Display cassandra server version");
+       css_.out.println("show tables                                               Show list of tables.");
+       css_.out.println("show version                                              Show server version.");
+       css_.out.println("select ...                                                CQL select statement (TBD).");
+       css_.out.println("get ...                                                   CQL data retrieval statement.");
+       css_.out.println("set ...                                                   CQL DML statement.");
+       css_.out.println("thrift get <tbl>.<cf>['<rowKey>']                         (will be deprecated)");            
+       css_.out.println("thrift get <tbl>.<cf>['<rowKey>']['<colKey>']             (will be deprecated)");            
+       css_.out.println("thrift set <tbl>.<cf>['<rowKey>']['<colKey>'] = '<value>' (will be deprecated)");    
+    }
+
+    private void cleanupAndExit()
+    {
+        CliMain.disconnect();
+        System.exit(0);
+    }
+
+    // Execute GET statement
+    private void executeGet(CommonTree ast) throws TException, NotFoundException, InvalidRequestException
+    {
+        if (!CliMain.isConnected())
+            return;
+
+        int childCount = ast.getChildCount();
+        assert(childCount == 1);
+
+        CommonTree columnFamilySpec = (CommonTree)ast.getChild(0);
+        assert(columnFamilySpec.getType() == CliParser.NODE_COLUMN_ACCESS);
+
+        String tableName     = CliCompiler.getTableName(columnFamilySpec);
+        String key           = CliCompiler.getKey(columnFamilySpec);
+        String columnFamily  = CliCompiler.getColumnFamily(columnFamilySpec);
+        int    columnSpecCnt = CliCompiler.numColumnSpecifiers(columnFamilySpec);
+
+        // assume simple columnFamily for now
+        if (columnSpecCnt == 0)
+        {
+            // table.cf['key']
+        	List<column_t> columns = new ArrayList<column_t>();
+      		columns = thriftClient_.get_slice(tableName, key, columnFamily, -1, 1000000);
+            int size = columns.size();
+            for (Iterator<column_t> colIter = columns.iterator(); colIter.hasNext(); )
+            {
+                column_t col = colIter.next();
+                css_.out.printf("  (column=%s, value=%s; timestamp=%d)\n",
+                                 col.columnName, col.value, col.timestamp);
+            }
+            css_.out.println("Returned " + size + " rows.");
+        }
+        else if (columnSpecCnt == 1)
+        {
+            // table.cf['key']['column']
+            String columnName = CliCompiler.getColumn(columnFamilySpec, 0);
+            column_t col = new column_t();
+           	col = thriftClient_.get_column(tableName, key, columnFamily + ":" + columnName);
+            css_.out.printf("==> (name=%s, value=%s; timestamp=%d)\n",
+                            col.columnName, col.value, col.timestamp);
+        }
+        else
+        {
+            assert(false);
+        }
+    }
+
+    // Execute SET statement
+    private void executeSet(CommonTree ast) throws TException
+    {
+        if (!CliMain.isConnected())
+            return;
+
+        int childCount = ast.getChildCount();
+        assert(childCount == 2);
+
+        CommonTree columnFamilySpec = (CommonTree)ast.getChild(0);
+        assert(columnFamilySpec.getType() == CliParser.NODE_COLUMN_ACCESS);
+
+        String tableName     = CliCompiler.getTableName(columnFamilySpec);
+        String key           = CliCompiler.getKey(columnFamilySpec);
+        String columnFamily  = CliCompiler.getColumnFamily(columnFamilySpec);
+        int    columnSpecCnt = CliCompiler.numColumnSpecifiers(columnFamilySpec);
+        String value         = Utils.unescapeSQLString(ast.getChild(1).getText());
+
+        // assume simple columnFamily for now
+        if (columnSpecCnt == 1)
+        {
+            // We have the table.cf['key']['column'] = 'value' case.
+
+            // get the column name
+            String columnName = CliCompiler.getColumn(columnFamilySpec, 0);
+
+            // do the insert
+            thriftClient_.insert(tableName, key, columnFamily + ":" + columnName,
+                                 value.getBytes(), System.currentTimeMillis());
+
+            css_.out.println("Value inserted.");
+        }
+        else
+        {
+            /* for now (until we support batch sets) */
+            assert(false);
+        }
+    }
+
+    private void executeShowProperty(CommonTree ast, String propertyName) throws TException
+    {
+        if (!CliMain.isConnected())
+            return;
+
+        String propertyValue = thriftClient_.getStringProperty(propertyName);
+        css_.out.println(propertyValue);
+        return;
+    }
+
+    // process "show tables" statement
+    private void executeShowTables(CommonTree ast) throws TException
+    {
+        if (!CliMain.isConnected())
+            return;
+        
+        List<String> tables = thriftClient_.getStringListProperty("tables");
+        for (String table : tables)
+        {
+            css_.out.println(table);
+        }
+    }
+
+    // process a statement of the form: describe table <tablename> 
+    private void executeDescribeTable(CommonTree ast) throws TException
+    {
+        if (!CliMain.isConnected())
+            return;
+
+        // Get table name
+        int childCount = ast.getChildCount();
+        assert(childCount == 1);
+        String tableName = ast.getChild(0).getText();
+        
+        // Describe and display
+        String describe = thriftClient_.describeTable(tableName);
+        css_.out.println(describe);
+        return;
+    }
+
+    // process a statement of the form: connect hostname/port
+    private void executeConnect(CommonTree ast) throws TException
+    {
+        int portNumber = Integer.parseInt(ast.getChild(1).getText());
+        Tree idList = ast.getChild(0);
+        
+        StringBuffer hostName = new StringBuffer();
+        int idCount = idList.getChildCount(); 
+        for (int idx = 0; idx < idCount; idx++)
+        {
+            hostName.append(idList.getChild(idx).getText());
+        }
+        
+        // disconnect current connection, if any.
+        // This is a no-op, if you aren't currently connected.
+        CliMain.disconnect();
+
+        // now, connect to the newly specified host name and port
+        css_.hostName = hostName.toString();
+        css_.thriftPort = portNumber;
+        CliMain.connect(css_.hostName, css_.thriftPort);
+    }
+
+    // execute CQL query on server
+    public void executeQueryOnServer(String query) throws TException
+    {
+        if (!CliMain.isConnected())
+            return;
+        
+        CqlResult_t result = thriftClient_.executeQuery(query);
+        
+        if (result == null)
+        {
+            css_.out.println("Unexpected error. Received null result from server.");
+            return;
+        }
+
+        if ((result.errorTxt != null) || (result.errorCode != 0))
+        {
+            css_.out.println("Error: " + result.errorTxt);
+        }
+        else
+        {
+            List<Map<String, String>> rows = result.resultSet;
+            
+            if (rows != null)
+            {
+                for (Map<String, String> row : rows)
+                {
+                    for (Iterator<Map.Entry<String, String>> it = row.entrySet().iterator(); it.hasNext(); )
+                    {
+                        Map.Entry<String, String> entry = it.next();
+                        String key = entry.getKey();
+                        String value = entry.getValue();
+                        css_.out.print(key + " = " + value + "; ");
+                    }
+                    css_.out.println();
+                }
+            }
+            css_.out.println("Statement processed.");
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliCompiler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliCompiler.java
index e69de29b..31a6d2ec 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliCompiler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliCompiler.java
@@ -0,0 +1,123 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cli;
+
+import org.antlr.runtime.*;
+import org.antlr.runtime.tree.*;
+import org.apache.cassandra.cql.common.Utils;
+
+
+public class CliCompiler
+{
+
+    // ANTLR does not provide case-insensitive tokenization support
+    // out of the box. So we override the LA (lookahead) function
+    // of the ANTLRStringStream class. Note: This doesn't change the
+    // token text-- but just relaxes the matching rules to match
+    // in upper case. [Logic borrowed from Hive code.]
+    // 
+    // Also see discussion on this topic in:
+    // http://www.antlr.org/wiki/pages/viewpage.action?pageId=1782.
+    public static class ANTLRNoCaseStringStream  extends ANTLRStringStream
+    {
+        public ANTLRNoCaseStringStream(String input)
+        {
+            super(input);
+        }
+    
+        public int LA(int i)
+        {
+            int returnChar = super.LA(i);
+            if (returnChar == CharStream.EOF)
+            {
+                return returnChar; 
+            }
+            else if (returnChar == 0) 
+            {
+                return returnChar;
+            }
+
+            return Character.toUpperCase((char)returnChar);
+        }
+    }
+
+    public static CommonTree compileQuery(String query)
+    {
+        CommonTree queryTree = null;
+        try
+        {
+            ANTLRStringStream input = new ANTLRNoCaseStringStream(query);
+
+            CliLexer lexer = new CliLexer(input);
+            CommonTokenStream tokens = new CommonTokenStream(lexer);
+
+            CliParser parser = new CliParser(tokens);
+
+            // start parsing...
+            queryTree = (CommonTree)(parser.root().getTree());
+
+            // semantic analysis if any...
+            //  [tbd]
+
+        }
+        catch(Exception e)
+        {
+            System.err.println("Exception " + e.getMessage());
+            e.printStackTrace(System.err);
+        }
+        return queryTree;
+    }
+    /*
+     * NODE_COLUMN_ACCESS related functions.
+     */
+    public static String getTableName(CommonTree astNode)
+    {
+        assert(astNode.getType() == CliParser.NODE_COLUMN_ACCESS);
+
+        return astNode.getChild(0).getText();
+    }
+
+    public static String getColumnFamily(CommonTree astNode)
+    {
+        assert(astNode.getType() == CliParser.NODE_COLUMN_ACCESS);
+
+        return astNode.getChild(1).getText();
+    }
+
+    public static String getKey(CommonTree astNode)
+    {
+        assert(astNode.getType() == CliParser.NODE_COLUMN_ACCESS);
+
+        return Utils.unescapeSQLString(astNode.getChild(2).getText());
+    }
+
+    public static int numColumnSpecifiers(CommonTree astNode)
+    {
+        // Skip over table, column family and rowKey
+        return astNode.getChildCount() - 3;
+    }
+
+    // Returns the pos'th (0-based index) column specifier in the astNode
+    public static String getColumn(CommonTree astNode, int pos)
+    {
+        // Skip over table, column family and rowKey
+        return Utils.unescapeSQLString(astNode.getChild(pos + 3).getText()); 
+    }
+ 
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliMain.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliMain.java
index e69de29b..d3c2fe74 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliMain.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliMain.java
@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cli;
+
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.transport.TTransport;
+
+import jline.*;
+import java.io.*;
+import java.util.*;
+
+import org.apache.cassandra.service.Cassandra;
+
+//
+// Cassandra Command Line Interface (CLI) Main
+//
+public class CliMain
+{
+    public final static String PROMPT = "cassandra";
+    public final static String HISTORYFILE = ".cassandra.history";
+
+    private static TTransport transport_ = null;
+    private static Cassandra.Client thriftClient_ = null;
+    private static CliSessionState css_ = new CliSessionState();
+    private static CliClient cliClient_;
+
+    // Establish a thrift connection to cassandra instance
+    public static void connect(String server, int port)
+    {
+        TSocket socket = new TSocket(server, port);
+
+        if (transport_ != null)
+            transport_.close();
+
+        transport_ = socket;
+
+        TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport_, false, false);
+        Cassandra.Client cassandraClient = new Cassandra.Client(binaryProtocol);
+
+        try
+        {
+            transport_.open();
+        }
+        catch(Exception e)
+        {
+            // Should move this to Log4J as well probably...
+            System.err.println("Exception " + e.getMessage());            
+            e.printStackTrace();
+        }
+
+        thriftClient_ = cassandraClient;
+        cliClient_ = new CliClient(css_, thriftClient_);
+
+        css_.out.printf("Connected to %s/%d\n", server, port);
+    }
+
+    // Disconnect thrift connection to cassandra instance
+    public static void disconnect()
+    {
+        if (transport_ != null)
+        {
+            transport_.close();
+            transport_ = null;
+        }
+    }
+
+    private static void printBanner()
+    {
+        css_.out.println("Welcome to cassandra CLI.\n");
+        css_.out.println("Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.");
+    }
+
+    public static boolean isConnected()
+    {
+        if (thriftClient_ == null)
+        {
+            css_.out.println("Not connected to a cassandra instance.");
+            return false;
+        }
+        return true;
+    }
+    
+    private static void processServerQuery(String query)
+    {
+        if (!isConnected())
+            return;
+
+        try
+        {
+            cliClient_.executeQueryOnServer(query);
+        }
+        catch(Exception e)
+        {
+            System.err.println("Exception " + e.getMessage());
+            e.printStackTrace(System.err);
+        }
+        return;
+    }
+
+    private static void processCLIStmt(String query)
+    {
+        try
+        {
+            cliClient_.executeCLIStmt(query);
+        }
+        catch(Exception e)
+        {
+            System.err.println("Exception " + e.getMessage());
+            e.printStackTrace(System.err);
+        }
+        return;
+    }
+
+    private static void processLine(String line)
+    {
+        StringTokenizer tokenizer = new StringTokenizer(line);
+        if (tokenizer.hasMoreTokens())
+        {
+            // Use first token for now to determine if this statement is
+            // a CQL statement. Technically, the line could start with
+            // a comment token followed by a CQL statement. That case
+            // isn't handled right now.
+            String token = tokenizer.nextToken().toUpperCase();
+            if (token.startsWith("GET")
+                || token.startsWith("SELECT")
+                || token.startsWith("SET")
+                || token.startsWith("DELETE")
+                || token.startsWith("EXPLAIN")) // explain plan statement
+            {
+                // these are CQL Statements that are compiled and executed on server-side
+                processServerQuery(line);
+            }
+            else 
+            {
+                // These are CLI statements processed locally
+                processCLIStmt(line);
+            }
+        }
+    } 
+
+    public static void main(String args[]) throws IOException  
+    {
+        // process command line args
+        CliOptions cliOptions = new CliOptions();
+        cliOptions.processArgs(css_, args);
+
+        // connect to cassandra server if host argument specified.
+        if (css_.hostName != null)
+        {
+            connect(css_.hostName, css_.thriftPort);
+        }
+        else
+        {
+            // If not, client must connect explicitly using the "connect" CLI statement.
+            cliClient_ = new CliClient(css_, null);
+        }
+
+        ConsoleReader reader = new ConsoleReader(); 
+        reader.setBellEnabled(false);
+
+        String historyFile = System.getProperty("user.home") + File.separator  + HISTORYFILE;
+
+        reader.setHistory(new History(new File(historyFile)));
+
+        printBanner();
+
+        String line;
+        while ((line = reader.readLine(PROMPT+"> ")) != null)
+        {
+            processLine(line);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliOptions.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliOptions.java
index e69de29b..177aa8c4 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliOptions.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliOptions.java
@@ -0,0 +1,91 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.cli;
+
+import org.apache.commons.cli.*;
+
+public class CliOptions {
+
+    private static Options options = null; // Info about command line options
+    private CommandLine cmd = null;        // Command Line arguments
+
+    // Command line options
+    private static final String HOST_OPTION = "host";
+    private static final String PORT_OPTION = "port";
+
+    // Default values for optional command line arguments
+    private static final int    DEFAULT_THRIFT_PORT = 9160;
+
+    // Register the command line options and their properties (such as
+    // whether they take an extra argument, etc.
+    static
+    {
+        options = new Options();
+        options.addOption(HOST_OPTION, true, "cassandra server's host name");
+        options.addOption(PORT_OPTION, true, "cassandra server's thrift port");  
+    }
+
+    private static void printUsage()
+    {
+        System.err.println("");
+        System.err.println("Usage: cascli --host hostname [--port <portname>]");
+        System.err.println("");
+    }
+
+    public void processArgs(CliSessionState css, String[] args)
+    {
+        CommandLineParser parser = new PosixParser();
+        try
+        {
+            cmd = parser.parse(options, args);
+        }
+        catch (ParseException e)
+        {
+            printUsage();
+            e.printStackTrace();
+            System.exit(1);
+        }
+
+        if (!cmd.hasOption(HOST_OPTION))
+        {
+            // host name not specified in command line.
+            // In this case, we don't implicitly connect at CLI startup. In this case,
+            // the user must use the "connect" CLI statement to connect.
+            //
+            css.hostName = null;
+            
+            // HelpFormatter formatter = new HelpFormatter();
+            // formatter.printHelp("java com.facebook.infrastructure.cli.CliMain ", options);
+            // System.exit(1);
+        }
+        else 
+        {
+            css.hostName = cmd.getOptionValue(HOST_OPTION);
+        }
+
+        // Look for optional args.
+        if (cmd.hasOption(PORT_OPTION))
+        {
+            css.thriftPort = Integer.parseInt(cmd.getOptionValue(PORT_OPTION));
+        }
+        else
+        {
+            css.thriftPort = DEFAULT_THRIFT_PORT;
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliSessionState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliSessionState.java
index e69de29b..50bf9eca 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliSessionState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cli/CliSessionState.java
@@ -0,0 +1,42 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cli;
+
+import java.io.InputStream;
+import java.io.PrintStream;
+
+public class CliSessionState {
+
+    public boolean timingOn = false;
+    public String  hostName;       // cassandra server name
+    public int     thriftPort;     // cassandra server's thrift port
+
+    /*
+     * Streams to read/write from
+     */
+    public InputStream in;
+    public PrintStream out;
+    public PrintStream err;
+
+    public CliSessionState() {
+        in = System.in;
+        out = System.out;
+        err = System.err;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/AIOExecutorService.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/AIOExecutorService.java
index e69de29b..1679dfad 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/AIOExecutorService.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/AIOExecutorService.java
@@ -0,0 +1,319 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.RejectedExecutionException;
+import java.util.concurrent.ThreadFactory;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+public class AIOExecutorService implements ExecutorService
+{
+    private ExecutorService executorService_;
+    
+    public AIOExecutorService(int corePoolSize,
+            int maximumPoolSize,
+            long keepAliveTime,
+            TimeUnit unit,
+            BlockingQueue<Runnable> workQueue,
+            ThreadFactory threadFactory)
+    {
+        executorService_ = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory);        
+    }
+    
+    /**
+     * Executes the given command at some time in the future.  The command
+     * may execute in a new thread, in a pooled thread, or in the calling
+     * thread, at the discretion of the <tt>Executor</tt> implementation.
+     *
+     * @param command the runnable task
+     * @throws RejectedExecutionException if this task cannot be
+     * accepted for execution.
+     * @throws NullPointerException if command is null
+     */
+    public void execute(Runnable command)
+    {
+        executorService_.execute(command);
+    }
+    
+    /**
+     * Initiates an orderly shutdown in which previously submitted
+     * tasks are executed, but no new tasks will be accepted.
+     * Invocation has no additional effect if already shut down.
+     *
+     * <p>This method does not wait for previously submitted tasks to
+     * complete execution.  Use {@link #awaitTermination awaitTermination}
+     * to do that.
+     *
+     * @throws SecurityException if a security manager exists and
+     *         shutting down this ExecutorService may manipulate
+     *         threads that the caller is not permitted to modify
+     *         because it does not hold {@link
+     *         java.lang.RuntimePermission}<tt>("modifyThread")</tt>,
+     *         or the security manager's <tt>checkAccess</tt> method
+     *         denies access.
+     */
+    public void shutdown()
+    {    
+        /* This is a noop. */     
+    }
+
+    /**
+     * Attempts to stop all actively executing tasks, halts the
+     * processing of waiting tasks, and returns a list of the tasks
+     * that were awaiting execution.
+     *
+     * <p>This method does not wait for actively executing tasks to
+     * terminate.  Use {@link #awaitTermination awaitTermination} to
+     * do that.
+     *
+     * <p>There are no guarantees beyond best-effort attempts to stop
+     * processing actively executing tasks.  For example, typical
+     * implementations will cancel via {@link Thread#interrupt}, so any
+     * task that fails to respond to interrupts may never terminate.
+     *
+     * @return list of tasks that never commenced execution
+     * @throws SecurityException if a security manager exists and
+     *         shutting down this ExecutorService may manipulate
+     *         threads that the caller is not permitted to modify
+     *         because it does not hold {@link
+     *         java.lang.RuntimePermission}<tt>("modifyThread")</tt>,
+     *         or the security manager's <tt>checkAccess</tt> method
+     *         denies access.
+     */
+    public List<Runnable> shutdownNow()
+    {
+        return executorService_.shutdownNow();
+    }
+
+    /**
+     * Returns <tt>true</tt> if this executor has been shut down.
+     *
+     * @return <tt>true</tt> if this executor has been shut down
+     */
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }
+
+    /**
+     * Returns <tt>true</tt> if all tasks have completed following shut down.
+     * Note that <tt>isTerminated</tt> is never <tt>true</tt> unless
+     * either <tt>shutdown</tt> or <tt>shutdownNow</tt> was called first.
+     *
+     * @return <tt>true</tt> if all tasks have completed following shut down
+     */
+    public boolean isTerminated()
+    {
+        return executorService_.isTerminated();
+    }
+
+    /**
+     * Blocks until all tasks have completed execution after a shutdown
+     * request, or the timeout occurs, or the current thread is
+     * interrupted, whichever happens first.
+     *
+     * @param timeout the maximum time to wait
+     * @param unit the time unit of the timeout argument
+     * @return <tt>true</tt> if this executor terminated and
+     *         <tt>false</tt> if the timeout elapsed before termination
+     * @throws InterruptedException if interrupted while waiting
+     */
+    public boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException
+    {
+        return executorService_.awaitTermination(timeout, unit);
+    }
+
+    /**
+     * Submits a value-returning task for execution and returns a
+     * Future representing the pending results of the task. The
+     * Future's <tt>get</tt> method will return the task's result upon
+     * successful completion.
+     *
+     * <p>
+     * If you would like to immediately block waiting
+     * for a task, you can use constructions of the form
+     * <tt>result = exec.submit(aCallable).get();</tt>
+     *
+     * <p> Note: The {@link Executors} class includes a set of methods
+     * that can convert some other common closure-like objects,
+     * for example, {@link java.security.PrivilegedAction} to
+     * {@link Callable} form so they can be submitted.
+     *
+     * @param task the task to submit
+     * @return a Future representing pending completion of the task
+     * @throws RejectedExecutionException if the task cannot be
+     *         scheduled for execution
+     * @throws NullPointerException if the task is null
+     */
+    public <T> Future<T> submit(Callable<T> task)
+    {
+        return executorService_.submit(task);
+    }
+
+    /**
+     * Submits a Runnable task for execution and returns a Future
+     * representing that task. The Future's <tt>get</tt> method will
+     * return the given result upon successful completion.
+     *
+     * @param task the task to submit
+     * @param result the result to return
+     * @return a Future representing pending completion of the task
+     * @throws RejectedExecutionException if the task cannot be
+     *         scheduled for execution
+     * @throws NullPointerException if the task is null
+     */
+    public <T> Future<T> submit(Runnable task, T result)
+    {
+        return executorService_.submit(task, result);
+    }
+
+    /**
+     * Submits a Runnable task for execution and returns a Future
+     * representing that task. The Future's <tt>get</tt> method will
+     * return <tt>null</tt> upon <em>successful</em> completion.
+     *
+     * @param task the task to submit
+     * @return a Future representing pending completion of the task
+     * @throws RejectedExecutionException if the task cannot be
+     *         scheduled for execution
+     * @throws NullPointerException if the task is null
+     */
+    public Future<?> submit(Runnable task)
+    {
+        return executorService_.submit(task);
+    }
+
+    /**
+     * Executes the given tasks, returning a list of Futures holding
+     * their status and results when all complete.
+     * {@link Future#isDone} is <tt>true</tt> for each
+     * element of the returned list.
+     * Note that a <em>completed</em> task could have
+     * terminated either normally or by throwing an exception.
+     * The results of this method are undefined if the given
+     * collection is modified while this operation is in progress.
+     *
+     * @param tasks the collection of tasks
+     * @return A list of Futures representing the tasks, in the same
+     *         sequential order as produced by the iterator for the
+     *         given task list, each of which has completed.
+     * @throws InterruptedException if interrupted while waiting, in
+     *         which case unfinished tasks are cancelled.
+     * @throws NullPointerException if tasks or any of its elements are <tt>null</tt>
+     * @throws RejectedExecutionException if any task cannot be
+     *         scheduled for execution
+     */
+
+    public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks) throws InterruptedException
+    {
+        return executorService_.invokeAll(tasks);
+    }
+
+    /**
+     * Executes the given tasks, returning a list of Futures holding
+     * their status and results
+     * when all complete or the timeout expires, whichever happens first.
+     * {@link Future#isDone} is <tt>true</tt> for each
+     * element of the returned list.
+     * Upon return, tasks that have not completed are cancelled.
+     * Note that a <em>completed</em> task could have
+     * terminated either normally or by throwing an exception.
+     * The results of this method are undefined if the given
+     * collection is modified while this operation is in progress.
+     *
+     * @param tasks the collection of tasks
+     * @param timeout the maximum time to wait
+     * @param unit the time unit of the timeout argument
+     * @return a list of Futures representing the tasks, in the same
+     *         sequential order as produced by the iterator for the
+     *         given task list. If the operation did not time out,
+     *         each task will have completed. If it did time out, some
+     *         of these tasks will not have completed.
+     * @throws InterruptedException if interrupted while waiting, in
+     *         which case unfinished tasks are cancelled
+     * @throws NullPointerException if tasks, any of its elements, or
+     *         unit are <tt>null</tt>
+     * @throws RejectedExecutionException if any task cannot be scheduled
+     *         for execution
+     */
+    public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks, long timeout, TimeUnit unit) throws InterruptedException
+    {
+        return executorService_.invokeAll(tasks, timeout, unit);
+    }
+    
+    /**
+     * Executes the given tasks, returning the result
+     * of one that has completed successfully (i.e., without throwing
+     * an exception), if any do. Upon normal or exceptional return,
+     * tasks that have not completed are cancelled.
+     * The results of this method are undefined if the given
+     * collection is modified while this operation is in progress.
+     *
+     * @param tasks the collection of tasks
+     * @return the result returned by one of the tasks
+     * @throws InterruptedException if interrupted while waiting
+     * @throws NullPointerException if tasks or any of its elements
+     *         are <tt>null</tt>
+     * @throws IllegalArgumentException if tasks is empty
+     * @throws ExecutionException if no task successfully completes
+     * @throws RejectedExecutionException if tasks cannot be scheduled
+     *         for execution
+     */
+    public <T> T invokeAny(Collection<? extends Callable<T>> tasks) throws InterruptedException, ExecutionException
+    {
+        return executorService_.invokeAny(tasks);
+    }
+
+    /**
+     * Executes the given tasks, returning the result
+     * of one that has completed successfully (i.e., without throwing
+     * an exception), if any do before the given timeout elapses.
+     * Upon normal or exceptional return, tasks that have not
+     * completed are cancelled.
+     * The results of this method are undefined if the given
+     * collection is modified while this operation is in progress.
+     *
+     * @param tasks the collection of tasks
+     * @param timeout the maximum time to wait
+     * @param unit the time unit of the timeout argument
+     * @return the result returned by one of the tasks.
+     * @throws InterruptedException if interrupted while waiting
+     * @throws NullPointerException if tasks, any of its elements, or
+     *         unit are <tt>null</tt>
+     * @throws TimeoutException if the given timeout elapses before
+     *         any task successfully completes
+     * @throws ExecutionException if no task successfully completes
+     * @throws RejectedExecutionException if tasks cannot be scheduled
+     *         for execution
+     */
+    public <T> T invokeAny(Collection<? extends Callable<T>> tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException
+    {
+        return executorService_.invokeAny(tasks, timeout, unit);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/Context.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/Context.java
index e69de29b..cddde144 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/Context.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/Context.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * Context object adding a collection of key/value pairs into ThreadLocalContext.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Context
+{
+    private Map<Object, Object> ht_;
+    
+    public Context()
+    {
+        ht_ = new HashMap<Object, Object>();
+    }
+    
+    public Object put(Object key, Object value)
+    {
+        return ht_.put(key, value);
+    }
+    
+    public Object get(Object key)
+    {
+        return ht_.get(key);
+    }
+    
+    public void remove(Object key)
+    {
+        ht_.remove(key);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationContext.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationContext.java
index e69de29b..d08577ff 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationContext.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationContext.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import org.apache.cassandra.continuations.Suspendable;
+import org.apache.commons.javaflow.Continuation;
+
+public class ContinuationContext
+{
+    private Continuation continuation_;
+    private Object result_;
+    
+    public ContinuationContext(Continuation continuation)
+    {
+        continuation_ = continuation;        
+    }
+    
+    public Continuation getContinuation()
+    {
+        return continuation_;
+    }
+    
+    public void setContinuation(Continuation continuation)
+    {
+        continuation_ = continuation;
+    }
+    
+    public Object result()
+    {
+        return result_;
+    }
+    
+    public void result(Object result)
+    {
+        result_ = result;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationStage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationStage.java
index e69de29b..7ebce74d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationStage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationStage.java
@@ -0,0 +1,91 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.cassandra.continuations.Suspendable;
+
+
+public class ContinuationStage implements IStage
+{
+    private String name_;
+    private ContinuationsExecutor executorService_;
+            
+    public ContinuationStage(String name, int numThreads)
+    {        
+        name_ = name;        
+        executorService_ = new ContinuationsExecutor( numThreads,
+                numThreads,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl(name)
+                );        
+    }
+    
+    public String getName() 
+    {        
+        return name_;
+    }
+    
+    public ExecutorService getInternalThreadPool()
+    {
+        return executorService_;
+    }
+
+    public Future<Object> execute(Callable<Object> callable) {
+        return executorService_.submit(callable);
+    }
+    
+    public void execute(Runnable runnable) {
+        executorService_.execute(runnable);
+    }
+    
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
+    {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public void shutdown() {  
+        executorService_.shutdownNow(); 
+    }
+    
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }
+    
+    public long getTaskCount(){
+        return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationsExecutor.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationsExecutor.java
index e69de29b..0b70a231 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationsExecutor.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ContinuationsExecutor.java
@@ -0,0 +1,2244 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.AbstractExecutorService;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.RejectedExecutionException;
+import java.util.concurrent.ThreadFactory;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.*;
+import java.util.concurrent.atomic.*;
+import java.util.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.Cachetable;
+import org.apache.cassandra.utils.GuidGenerator;
+import org.apache.cassandra.utils.ICachetable;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.commons.javaflow.Continuation;
+import org.apache.log4j.Logger;
+
+
+/**
+ * An {@link ExecutorService} that executes each submitted task using one of
+ * possibly several pooled threads, normally configured using {@link Executors}
+ * factory methods.
+ * 
+ * <p>
+ * Thread pools address two different problems: they usually provide improved
+ * performance when executing large numbers of asynchronous tasks, due to
+ * reduced per-task invocation overhead, and they provide a means of bounding
+ * and managing the resources, including threads, consumed when executing a
+ * collection of tasks. Each {@code ContinuationsExecutor} also maintains some
+ * basic statistics, such as the number of completed tasks.
+ * 
+ * <p>
+ * To be useful across a wide range of contexts, this class provides many
+ * adjustable parameters and extensibility hooks. However, programmers are urged
+ * to use the more convenient {@link Executors} factory methods {@link
+ * Executors#newCachedThreadPool} (unbounded thread pool, with automatic thread
+ * reclamation), {@link Executors#newFixedThreadPool} (fixed size thread pool)
+ * and {@link Executors#newSingleThreadExecutor} (single background thread),
+ * that preconfigure settings for the most common usage scenarios. Otherwise,
+ * use the following guide when manually configuring and tuning this class:
+ * 
+ * <dl>
+ * 
+ * <dt>Core and maximum pool sizes</dt>
+ * 
+ * <dd>A {@code ContinuationsExecutor} will automatically adjust the pool size
+ * (see {@link #getPoolSize}) according to the bounds set by corePoolSize (see
+ * {@link #getCorePoolSize}) and maximumPoolSize (see
+ * {@link #getMaximumPoolSize}).
+ * 
+ * When a new task is submitted in method {@link #execute}, and fewer than
+ * corePoolSize threads are running, a new thread is created to handle the
+ * request, even if other worker threads are idle. If there are more than
+ * corePoolSize but less than maximumPoolSize threads running, a new thread will
+ * be created only if the queue is full. By setting corePoolSize and
+ * maximumPoolSize the same, you create a fixed-size thread pool. By setting
+ * maximumPoolSize to an essentially unbounded value such as
+ * {@code Integer.MAX_VALUE}, you allow the pool to accommodate an arbitrary
+ * number of concurrent tasks. Most typically, core and maximum pool sizes are
+ * set only upon construction, but they may also be changed dynamically using
+ * {@link #setCorePoolSize} and {@link #setMaximumPoolSize}. </dd>
+ * 
+ * <dt>On-demand construction</dt>
+ * 
+ * <dd> By default, even core threads are initially created and started only
+ * when new tasks arrive, but this can be overridden dynamically using method
+ * {@link #prestartCoreThread} or {@link #prestartAllCoreThreads}. You probably
+ * want to prestart threads if you construct the pool with a non-empty queue.
+ * </dd>
+ * 
+ * <dt>Creating new threads</dt>
+ * 
+ * <dd>New threads are created using a {@link ThreadFactory}. If not otherwise
+ * specified, a {@link Executors#defaultThreadFactory} is used, that creates
+ * threads to all be in the same {@link ThreadGroup} and with the same
+ * {@code NORM_PRIORITY} priority and non-daemon status. By supplying a
+ * different ThreadFactory, you can alter the thread's name, thread group,
+ * priority, daemon status, etc. If a {@code ThreadFactory} fails to create a
+ * thread when asked by returning null from {@code newThread}, the executor
+ * will continue, but might not be able to execute any tasks. Threads should
+ * possess the "modifyThread" {@code RuntimePermission}. If worker threads or
+ * other threads using the pool do not possess this permission, service may be
+ * degraded: configuration changes may not take effect in a timely manner, and a
+ * shutdown pool may remain in a state in which termination is possible but not
+ * completed.</dd>
+ * 
+ * <dt>Keep-alive times</dt>
+ * 
+ * <dd>If the pool currently has more than corePoolSize threads, excess threads
+ * will be terminated if they have been idle for more than the keepAliveTime
+ * (see {@link #getKeepAliveTime}). This provides a means of reducing resource
+ * consumption when the pool is not being actively used. If the pool becomes
+ * more active later, new threads will be constructed. This parameter can also
+ * be changed dynamically using method {@link #setKeepAliveTime}. Using a value
+ * of {@code Long.MAX_VALUE} {@link TimeUnit#NANOSECONDS} effectively disables
+ * idle threads from ever terminating prior to shut down. By default, the
+ * keep-alive policy applies only when there are more than corePoolSizeThreads.
+ * But method {@link #allowCoreThreadTimeOut(boolean)} can be used to apply this
+ * time-out policy to core threads as well, so long as the keepAliveTime value
+ * is non-zero. </dd>
+ * 
+ * <dt>Queuing</dt>
+ * 
+ * <dd>Any {@link BlockingQueue} may be used to transfer and hold submitted
+ * tasks. The use of this queue interacts with pool sizing:
+ * 
+ * <ul>
+ * 
+ * <li> If fewer than corePoolSize threads are running, the Executor always
+ * prefers adding a new thread rather than queuing.</li>
+ * 
+ * <li> If corePoolSize or more threads are running, the Executor always prefers
+ * queuing a request rather than adding a new thread.</li>
+ * 
+ * <li> If a request cannot be queued, a new thread is created unless this would
+ * exceed maximumPoolSize, in which case, the task will be rejected.</li>
+ * 
+ * </ul>
+ * 
+ * There are three general strategies for queuing:
+ * <ol>
+ * 
+ * <li> <em> Direct handoffs.</em> A good default choice for a work queue is a
+ * {@link SynchronousQueue} that hands off tasks to threads without otherwise
+ * holding them. Here, an attempt to queue a task will fail if no threads are
+ * immediately available to run it, so a new thread will be constructed. This
+ * policy avoids lockups when handling sets of requests that might have internal
+ * dependencies. Direct handoffs generally require unbounded maximumPoolSizes to
+ * avoid rejection of new submitted tasks. This in turn admits the possibility
+ * of unbounded thread growth when commands continue to arrive on average faster
+ * than they can be processed. </li>
+ * 
+ * <li><em> Unbounded queues.</em> Using an unbounded queue (for example a
+ * {@link LinkedBlockingQueue} without a predefined capacity) will cause new
+ * tasks to wait in the queue when all corePoolSize threads are busy. Thus, no
+ * more than corePoolSize threads will ever be created. (And the value of the
+ * maximumPoolSize therefore doesn't have any effect.) This may be appropriate
+ * when each task is completely independent of others, so tasks cannot affect
+ * each others execution; for example, in a web page server. While this style of
+ * queuing can be useful in smoothing out transient bursts of requests, it
+ * admits the possibility of unbounded work queue growth when commands continue
+ * to arrive on average faster than they can be processed. </li>
+ * 
+ * <li><em>Bounded queues.</em> A bounded queue (for example, an
+ * {@link ArrayBlockingQueue}) helps prevent resource exhaustion when used with
+ * finite maximumPoolSizes, but can be more difficult to tune and control. Queue
+ * sizes and maximum pool sizes may be traded off for each other: Using large
+ * queues and small pools minimizes CPU usage, OS resources, and
+ * context-switching overhead, but can lead to artificially low throughput. If
+ * tasks frequently block (for example if they are I/O bound), a system may be
+ * able to schedule time for more threads than you otherwise allow. Use of small
+ * queues generally requires larger pool sizes, which keeps CPUs busier but may
+ * encounter unacceptable scheduling overhead, which also decreases throughput.
+ * </li>
+ * 
+ * </ol>
+ * 
+ * </dd>
+ * 
+ * <dt>Rejected tasks</dt>
+ * 
+ * <dd> New tasks submitted in method {@link #execute} will be <em>rejected</em>
+ * when the Executor has been shut down, and also when the Executor uses finite
+ * bounds for both maximum threads and work queue capacity, and is saturated. In
+ * either case, the {@code execute} method invokes the {@link
+ * RejectedExecutionHandler#rejectedExecution} method of its {@link
+ * RejectedExecutionHandler}. Four predefined handler policies are provided:
+ * 
+ * <ol>
+ * 
+ * <li> In the default {@link ContinuationsExecutor.AbortPolicy}, the handler
+ * throws a runtime {@link RejectedExecutionException} upon rejection. </li>
+ * 
+ * <li> In {@link ContinuationsExecutor.CallerRunsPolicy}, the thread that invokes
+ * {@code execute} itself runs the task. This provides a simple feedback control
+ * mechanism that will slow down the rate that new tasks are submitted. </li>
+ * 
+ * <li> In {@link ContinuationsExecutor.DiscardPolicy}, a task that cannot be
+ * executed is simply dropped. </li>
+ * 
+ * <li>In {@link ContinuationsExecutor.DiscardOldestPolicy}, if the executor is
+ * not shut down, the task at the head of the work queue is dropped, and then
+ * execution is retried (which can fail again, causing this to be repeated.)
+ * </li>
+ * 
+ * </ol>
+ * 
+ * It is possible to define and use other kinds of {@link
+ * RejectedExecutionHandler} classes. Doing so requires some care especially
+ * when policies are designed to work only under particular capacity or queuing
+ * policies. </dd>
+ * 
+ * <dt>Hook methods</dt>
+ * 
+ * <dd>This class provides {@code protected} overridable {@link #beforeExecute}
+ * and {@link #afterExecute} methods that are called before and after execution
+ * of each task. These can be used to manipulate the execution environment; for
+ * example, reinitializing ThreadLocals, gathering statistics, or adding log
+ * entries. Additionally, method {@link #terminated} can be overridden to
+ * perform any special processing that needs to be done once the Executor has
+ * fully terminated.
+ * 
+ * <p>
+ * If hook or callback methods throw exceptions, internal worker threads may in
+ * turn fail and abruptly terminate.</dd>
+ * 
+ * <dt>Queue maintenance</dt>
+ * 
+ * <dd> Method {@link #getQueue} allows access to the work queue for purposes of
+ * monitoring and debugging. Use of this method for any other purpose is
+ * strongly discouraged. Two supplied methods, {@link #remove} and
+ * {@link #purge} are available to assist in storage reclamation when large
+ * numbers of queued tasks become cancelled.</dd>
+ * 
+ * <dt>Finalization</dt>
+ * 
+ * <dd> A pool that is no longer referenced in a program <em>AND</em> has no
+ * remaining threads will be {@code shutdown} automatically. If you would like
+ * to ensure that unreferenced pools are reclaimed even if users forget to call
+ * {@link #shutdown}, then you must arrange that unused threads eventually die,
+ * by setting appropriate keep-alive times, using a lower bound of zero core
+ * threads and/or setting {@link #allowCoreThreadTimeOut(boolean)}. </dd>
+ * 
+ * </dl>
+ * 
+ * <p>
+ * <b>Extension example</b>. Most extensions of this class override one or more
+ * of the protected hook methods. For example, here is a subclass that adds a
+ * simple pause/resume feature:
+ * 
+ * <pre>
+ *  {@code
+ *  class PausableThreadPoolExecutor extends ContinuationsExecutor {
+ *    private boolean isPaused;
+ *    private ReentrantLock pauseLock = new ReentrantLock();
+ *    private Condition unpaused = pauseLock.newCondition();
+ * 
+ *    public PausableThreadPoolExecutor(...) { super(...); }
+ * 
+ *    protected void beforeExecute(Thread t, Runnable r) {
+ *      super.beforeExecute(t, r);
+ *      pauseLock.lock();
+ *      try {
+ *        while (isPaused) unpaused.await();
+ *      } catch (InterruptedException ie) {
+ *        t.interrupt();
+ *      } finally {
+ *        pauseLock.unlock();
+ *      }
+ *    }
+ * 
+ *    public void pause() {
+ *      pauseLock.lock();
+ *      try {
+ *        isPaused = true;
+ *      } finally {
+ *        pauseLock.unlock();
+ *      }
+ *    }
+ * 
+ *    public void resume() {
+ *      pauseLock.lock();
+ *      try {
+ *        isPaused = false;
+ *        unpaused.signalAll();
+ *      } finally {
+ *        pauseLock.unlock();
+ *      }
+ *    }
+ *  }}
+ * </pre>
+ * 
+ * @since 1.5
+ * @author Doug Lea
+ */
+public class ContinuationsExecutor extends AbstractExecutorService
+{
+    /**
+     * The main pool control state, ctl, is an atomic integer packing two
+     * conceptual fields workerCount, indicating the effective number of threads
+     * runState, indicating whether running, shutting down etc
+     * 
+     * In order to pack them into one int, we limit workerCount to (2^29)-1
+     * (about 500 million) threads rather than (2^31)-1 (2 billion) otherwise
+     * representable. If this is ever an issue in the future, the variable can
+     * be changed to be an AtomicLong, and the shift/mask constants below
+     * adjusted. But until the need arises, this code is a bit faster and
+     * simpler using an int.
+     * 
+     * The workerCount is the number of workers that have been permitted to
+     * start and not permitted to stop. The value may be transiently different
+     * from the actual number of live threads, for example when a ThreadFactory
+     * fails to create a thread when asked, and when exiting threads are still
+     * performing bookkeeping before terminating. The user-visible pool size is
+     * reported as the current size of the workers set.
+     * 
+     * The runState provides the main lifecyle control, taking on values:
+     * 
+     * RUNNING: Accept new tasks and process queued tasks SHUTDOWN: Don't accept
+     * new tasks, but process queued tasks STOP: Don't accept new tasks, don't
+     * process queued tasks, and interrupt in-progress tasks TIDYING: All tasks
+     * have terminated, workerCount is zero, the thread transitioning to state
+     * TIDYING will run the terminated() hook method TERMINATED: terminated()
+     * has completed
+     * 
+     * The numerical order among these values matters, to allow ordered
+     * comparisons. The runState monotonically increases over time, but need not
+     * hit each state. The transitions are:
+     * 
+     * RUNNING -> SHUTDOWN On invocation of shutdown(), perhaps implicitly in
+     * finalize() (RUNNING or SHUTDOWN) -> STOP On invocation of shutdownNow()
+     * SHUTDOWN -> TIDYING When both queue and pool are empty STOP -> TIDYING
+     * When pool is empty TIDYING -> TERMINATED When the terminated() hook
+     * method has completed
+     * 
+     * Threads waiting in awaitTermination() will return when the state reaches
+     * TERMINATED.
+     * 
+     * Detecting the transition from SHUTDOWN to TIDYING is less straightforward
+     * than you'd like because the queue may become empty after non-empty and
+     * vice versa during SHUTDOWN state, but we can only terminate if, after
+     * seeing that it is empty, we see that workerCount is 0 (which sometimes
+     * entails a recheck -- see below).
+     */
+    private final static Logger logger_ = Logger.getLogger( ContinuationsExecutor.class );
+    private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));
+    private final static ThreadLocal<IContinuable> tls_ = new ThreadLocal<IContinuable>();
+    private static final int COUNT_BITS = Integer.SIZE - 3;
+    private static final int CAPACITY = (1 << COUNT_BITS) - 1;
+
+    // runState is stored in the high-order bits
+    private static final int RUNNING = -1 << COUNT_BITS;
+    private static final int SHUTDOWN = 0 << COUNT_BITS;
+    private static final int STOP = 1 << COUNT_BITS;
+    private static final int TIDYING = 2 << COUNT_BITS;
+    private static final int TERMINATED = 3 << COUNT_BITS;
+    
+    public static void putInTls(IContinuable run)
+    {
+        tls_.set(run);
+    }
+    
+    public static void doPostProcessing(Continuation c)
+    {
+        /* post process the call if need be */ 
+        IContinuable run = ContinuationsExecutor.tls_.get();
+        if ( run != null )
+        {              
+            run.run(c);                        
+        }
+        ContinuationsExecutor.tls_.remove();
+    }
+    
+    // Packing and unpacking ctl
+    private static int runStateOf(int c)
+    {
+        return c & ~CAPACITY;
+    }
+
+    private static int workerCountOf(int c)
+    {
+        return c & CAPACITY;
+    }
+
+    private static int ctlOf(int rs, int wc)
+    {
+        return rs | wc;
+    }
+
+    /*
+     * Bit field accessors that don't require unpacking ctl. These depend on the
+     * bit layout and on workerCount being never negative.
+     */
+
+    private static boolean runStateLessThan(int c, int s)
+    {
+        return c < s;
+    }
+
+    private static boolean runStateAtLeast(int c, int s)
+    {
+        return c >= s;
+    }
+
+    private static boolean isRunning(int c)
+    {
+        return c < SHUTDOWN;
+    }
+
+    /**
+     * Attempt to CAS-increment the workerCount field of ctl.
+     */
+    private boolean compareAndIncrementWorkerCount(int expect)
+    {
+        return ctl.compareAndSet(expect, expect + 1);
+    }
+    
+    /**
+     * Attempt to CAS-decrement the workerCount field of ctl.
+     */
+    private boolean compareAndDecrementWorkerCount(int expect)
+    {
+        return ctl.compareAndSet(expect, expect - 1);
+    }
+
+    /**
+     * Decrements the workerCount field of ctl. This is called only on abrupt
+     * termination of a thread (see processWorkerExit). Other decrements are
+     * performed within getTask.
+     */
+    private void decrementWorkerCount()
+    {
+        do
+        {
+        }
+        while (!compareAndDecrementWorkerCount(ctl.get()));
+    }
+
+    /**
+     * The queue used for holding tasks and handing off to worker threads. We do
+     * not require that workQueue.poll() returning null necessarily means that
+     * workQueue.isEmpty(), so rely solely on isEmpty to see if the queue is
+     * empty (which we must do for example when deciding whether to transition
+     * from SHUTDOWN to TIDYING). This accommodates special-purpose queues such
+     * as DelayQueues for which poll() is allowed to return null even if it may
+     * later return non-null when delays expire.
+     */
+    private final BlockingQueue<Runnable> workQueue;
+
+    /**
+     * Lock held on access to workers set and related bookkeeping. While we
+     * could use a concurrent set of some sort, it turns out to be generally
+     * preferable to use a lock. Among the reasons is that this serializes
+     * interruptIdleWorkers, which avoids unnecessary interrupt storms,
+     * especially during shutdown. Otherwise exiting threads would concurrently
+     * interrupt those that have not yet interrupted. It also simplifies some of
+     * the associated statistics bookkeeping of largestPoolSize etc. We also
+     * hold mainLock on shutdown and shutdownNow, for the sake of ensuring
+     * workers set is stable while separately checking permission to interrupt
+     * and actually interrupting.
+     */
+    private final ReentrantLock mainLock = new ReentrantLock();
+
+    /**
+     * Set containing all worker threads in pool. Accessed only when holding
+     * mainLock.
+     */
+    private final HashSet<Worker> workers = new HashSet<Worker>();
+
+    /**
+     * Wait condition to support awaitTermination
+     */
+    private final Condition termination = mainLock.newCondition();
+
+    /**
+     * Tracks largest attained pool size. Accessed only under mainLock.
+     */
+    private int largestPoolSize;
+
+    /**
+     * Counter for completed tasks. Updated only on termination of worker
+     * threads. Accessed only under mainLock.
+     */
+    private long completedTaskCount;
+
+    /*
+     * All user control parameters are declared as volatiles so that ongoing
+     * actions are based on freshest values, but without need for locking, since
+     * no internal invariants depend on them changing synchronously with respect
+     * to other actions.
+     */
+
+    /**
+     * Factory for new threads. All threads are created using this factory (via
+     * method addWorker). All callers must be prepared for addWorker to fail,
+     * which may reflect a system or user's policy limiting the number of
+     * threads. Even though it is not treated as an error, failure to create
+     * threads may result in new tasks being rejected or existing ones remaining
+     * stuck in the queue. On the other hand, no special precautions exist to
+     * handle OutOfMemoryErrors that might be thrown while trying to create
+     * threads, since there is generally no recourse from within this class.
+     */
+    private volatile ThreadFactory threadFactory;
+
+    /**
+     * Handler called when saturated or shutdown in execute.
+     */
+    private volatile RejectedExecutionHandler handler;
+
+    /**
+     * Timeout in nanoseconds for idle threads waiting for work. Threads use
+     * this timeout when there are more than corePoolSize present or if
+     * allowCoreThreadTimeOut. Otherwise they wait forever for new work.
+     */
+    private volatile long keepAliveTime;
+
+    /**
+     * If false (default), core threads stay alive even when idle. If true, core
+     * threads use keepAliveTime to time out waiting for work.
+     */
+    private volatile boolean allowCoreThreadTimeOut;
+
+    /**
+     * Core pool size is the minimum number of workers to keep alive (and not
+     * allow to time out etc) unless allowCoreThreadTimeOut is set, in which
+     * case the minimum is zero.
+     */
+    private volatile int corePoolSize;
+
+    /**
+     * Maximum pool size. Note that the actual maximum is internally bounded by
+     * CAPACITY.
+     */
+    private volatile int maximumPoolSize;
+
+    /**
+     * The default rejected execution handler
+     */
+    private static final RejectedExecutionHandler defaultHandler = new AbortPolicy();
+
+    /**
+     * Permission required for callers of shutdown and shutdownNow. We
+     * additionally require (see checkShutdownAccess) that callers have
+     * permission to actually interrupt threads in the worker set (as governed
+     * by Thread.interrupt, which relies on ThreadGroup.checkAccess, which in
+     * turn relies on SecurityManager.checkAccess). Shutdowns are attempted only
+     * if these checks pass.
+     * 
+     * All actual invocations of Thread.interrupt (see interruptIdleWorkers and
+     * interruptWorkers) ignore SecurityExceptions, meaning that the attempted
+     * interrupts silently fail. In the case of shutdown, they should not fail
+     * unless the SecurityManager has inconsistent policies, sometimes allowing
+     * access to a thread and sometimes not. In such cases, failure to actually
+     * interrupt threads may disable or delay full termination. Other uses of
+     * interruptIdleWorkers are advisory, and failure to actually interrupt will
+     * merely delay response to configuration changes so is not handled
+     * exceptionally.
+     */
+    private static final RuntimePermission shutdownPerm = new RuntimePermission(
+            "modifyThread");
+
+    /**
+     * Class Worker mainly maintains interrupt control state for threads running
+     * tasks, along with other minor bookkeeping. This class opportunistically
+     * extends AbstractQueuedSynchronizer to simplify acquiring and releasing a
+     * lock surrounding each task execution. This protects against interrupts
+     * that are intended to wake up a worker thread waiting for a task from
+     * instead interrupting a task being run. We implement a simple
+     * non-reentrant mutual exclusion lock rather than use ReentrantLock because
+     * we do not want worker tasks to be able to reacquire the lock when they
+     * invoke pool control methods like setCorePoolSize.
+     */
+    private final class Worker extends AbstractQueuedSynchronizer implements Runnable
+    {
+        /**
+         * This class will never be serialized, but we provide a
+         * serialVersionUID to suppress a javac warning.
+         */
+        private static final long serialVersionUID = 6138294804551838833L;
+
+        /** Thread this worker is running in. Null if factory fails. */
+        final Thread thread;
+        /** Initial task to run. Possibly null. */
+        Runnable firstTask;
+        /** Per-thread task counter */
+        volatile long completedTasks;
+
+        /**
+         * Creates with given first task and thread from ThreadFactory.
+         * 
+         * @param firstTask
+         *            the first task (null if none)
+         */
+        Worker(Runnable firstTask)
+        {
+            this.firstTask = firstTask;
+            this.thread = getThreadFactory().newThread(this);
+        }
+
+        /** Delegates main run loop to outer runWorker */
+        public void run()
+        {
+            runWorker(this);
+        }
+
+        // Lock methods
+        //
+        // The value 0 represents the unlocked state.
+        // The value 1 represents the locked state.
+
+        protected boolean isHeldExclusively()
+        {
+            return getState() == 1;
+        }
+
+        protected boolean tryAcquire(int unused)
+        {
+            if (compareAndSetState(0, 1))
+            {
+                setExclusiveOwnerThread(Thread.currentThread());
+                return true;
+            }
+            return false;
+        }
+
+        protected boolean tryRelease(int unused)
+        {
+            setExclusiveOwnerThread(null);
+            setState(0);
+            return true;
+        }
+
+        public void lock()
+        {
+            acquire(1);
+        }
+
+        public boolean tryLock()
+        {
+            return tryAcquire(1);
+        }
+
+        public void unlock()
+        {
+            release(1);
+        }
+
+        public boolean isLocked()
+        {
+            return isHeldExclusively();
+        }
+    }
+
+    /*
+     * Methods for setting control state
+     */
+
+    /**
+     * Transitions runState to given target, or leaves it alone if already at
+     * least the given target.
+     * 
+     * @param targetState
+     *            the desired state, either SHUTDOWN or STOP (but not TIDYING or
+     *            TERMINATED -- use tryTerminate for that)
+     */
+    private void advanceRunState(int targetState)
+    {
+        for (;;)
+        {
+            int c = ctl.get();
+            if (runStateAtLeast(c, targetState)
+                    || ctl.compareAndSet(c,
+                            ctlOf(targetState, workerCountOf(c))))
+                break;
+        }
+    }
+
+    /**
+     * Transitions to TERMINATED state if either (SHUTDOWN and pool and queue
+     * empty) or (STOP and pool empty). If otherwise eligible to terminate but
+     * workerCount is nonzero, interrupts an idle worker to ensure that shutdown
+     * signals propagate. This method must be called following any action that
+     * might make termination possible -- reducing worker count or removing
+     * tasks from the queue during shutdown. The method is non-private to allow
+     * access from ScheduledThreadPoolExecutor.
+     */
+    final void tryTerminate()
+    {
+        for (;;)
+        {
+            int c = ctl.get();
+            if (isRunning(c) || runStateAtLeast(c, TIDYING)
+                    || (runStateOf(c) == SHUTDOWN && !workQueue.isEmpty()))
+                return;
+            if (workerCountOf(c) != 0)
+            { // Eligible to terminate
+                interruptIdleWorkers(ONLY_ONE);
+                return;
+            }
+
+            final ReentrantLock mainLock = this.mainLock;
+            mainLock.lock();
+            try
+            {
+                if (ctl.compareAndSet(c, ctlOf(TIDYING, 0)))
+                {
+                    try
+                    {
+                        terminated();
+                    }
+                    finally
+                    {
+                        ctl.set(ctlOf(TERMINATED, 0));
+                        termination.signalAll();
+                    }
+                    return;
+                }
+            }
+            finally
+            {
+                mainLock.unlock();
+            }
+            // else retry on failed CAS
+        }
+    }
+
+    /*
+     * Methods for controlling interrupts to worker threads.
+     */
+
+    /**
+     * If there is a security manager, makes sure caller has permission to shut
+     * down threads in general (see shutdownPerm). If this passes, additionally
+     * makes sure the caller is allowed to interrupt each worker thread. This
+     * might not be true even if first check passed, if the SecurityManager
+     * treats some threads specially.
+     */
+    private void checkShutdownAccess()
+    {
+        SecurityManager security = System.getSecurityManager();
+        if (security != null)
+        {
+            security.checkPermission(shutdownPerm);
+            final ReentrantLock mainLock = this.mainLock;
+            mainLock.lock();
+            try
+            {
+                for (Worker w : workers)
+                    security.checkAccess(w.thread);
+            }
+            finally
+            {
+                mainLock.unlock();
+            }
+        }
+    }
+
+    /**
+     * Interrupts all threads, even if active. Ignores SecurityExceptions (in
+     * which case some threads may remain uninterrupted).
+     */
+    private void interruptWorkers()
+    {
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            for (Worker w : workers)
+            {
+                try
+                {
+                    w.thread.interrupt();
+                }
+                catch (SecurityException ignore)
+                {
+                }
+            }
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+    }
+
+    /**
+     * Interrupts threads that might be waiting for tasks (as indicated by not
+     * being locked) so they can check for termination or configuration changes.
+     * Ignores SecurityExceptions (in which case some threads may remain
+     * uninterrupted).
+     * 
+     * @param onlyOne
+     *            If true, interrupt at most one worker. This is called only
+     *            from tryTerminate when termination is otherwise enabled but
+     *            there are still other workers. In this case, at most one
+     *            waiting worker is interrupted to propagate shutdown signals in
+     *            case all threads are currently waiting. Interrupting any
+     *            arbitrary thread ensures that newly arriving workers since
+     *            shutdown began will also eventually exit. To guarantee
+     *            eventual termination, it suffices to always interrupt only one
+     *            idle worker, but shutdown() interrupts all idle workers so
+     *            that redundant workers exit promptly, not waiting for a
+     *            straggler task to finish.
+     */
+    private void interruptIdleWorkers(boolean onlyOne)
+    {
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            for (Worker w : workers)
+            {
+                Thread t = w.thread;
+                if (!t.isInterrupted() && w.tryLock())
+                {
+                    try
+                    {
+                        t.interrupt();
+                    }
+                    catch (SecurityException ignore)
+                    {
+                    }
+                    finally
+                    {
+                        w.unlock();
+                    }
+                }
+                if (onlyOne)
+                    break;
+            }
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+    }
+
+    /**
+     * Common form of interruptIdleWorkers, to avoid having to remember what the
+     * boolean argument means.
+     */
+    private void interruptIdleWorkers()
+    {
+        interruptIdleWorkers(false);
+    }
+
+    private static final boolean ONLY_ONE = true;
+
+    /**
+     * Ensures that unless the pool is stopping, the current thread does not
+     * have its interrupt set. This requires a double-check of state in case the
+     * interrupt was cleared concurrently with a shutdownNow -- if so, the
+     * interrupt is re-enabled.
+     */
+    private void clearInterruptsForTaskRun()
+    {
+        if (runStateLessThan(ctl.get(), STOP) && Thread.interrupted()
+                && runStateAtLeast(ctl.get(), STOP))
+            Thread.currentThread().interrupt();
+    }
+
+    /*
+     * Misc utilities, most of which are also exported to
+     * ScheduledThreadPoolExecutor
+     */
+
+    /**
+     * Invokes the rejected execution handler for the given command.
+     * Package-protected for use by ScheduledThreadPoolExecutor.
+     */
+    final void reject(Runnable command)
+    {
+        handler.rejectedExecution(command, this);
+    }
+
+    /**
+     * Performs any further cleanup following run state transition on invocation
+     * of shutdown. A no-op here, but used by ScheduledThreadPoolExecutor to
+     * cancel delayed tasks.
+     */
+    void onShutdown()
+    {
+    }
+
+    /**
+     * State check needed by ScheduledThreadPoolExecutor to enable running tasks
+     * during shutdown.
+     * 
+     * @param shutdownOK
+     *            true if should return true if SHUTDOWN
+     */
+    final boolean isRunningOrShutdown(boolean shutdownOK)
+    {
+        int rs = runStateOf(ctl.get());
+        return rs == RUNNING || (rs == SHUTDOWN && shutdownOK);
+    }
+
+    /**
+     * Drains the task queue into a new list, normally using drainTo. But if the
+     * queue is a DelayQueue or any other kind of queue for which poll or
+     * drainTo may fail to remove some elements, it deletes them one by one.
+     */
+    private List<Runnable> drainQueue()
+    {
+        BlockingQueue<Runnable> q = workQueue;
+        List<Runnable> taskList = new ArrayList<Runnable>();
+        q.drainTo(taskList);
+        if (!q.isEmpty())
+        {
+            for (Runnable r : q.toArray(new Runnable[0]))
+            {
+                if (q.remove(r))
+                    taskList.add(r);
+            }
+        }
+        return taskList;
+    }
+
+    /*
+     * Methods for creating, running and cleaning up after workers
+     */
+
+    /**
+     * Checks if a new worker can be added with respect to current pool state
+     * and the given bound (either core or maximum). If so, the worker count is
+     * adjusted accordingly, and, if possible, a new worker is created and
+     * started running firstTask as its first task. This method returns false if
+     * the pool is stopped or eligible to shut down. It also returns false if
+     * the thread factory fails to create a thread when asked, which requires a
+     * backout of workerCount, and a recheck for termination, in case the
+     * existence of this worker was holding up termination.
+     * 
+     * @param firstTask
+     *            the task the new thread should run first (or null if none).
+     *            Workers are created with an initial first task (in method
+     *            execute()) to bypass queuing when there are fewer than
+     *            corePoolSize threads (in which case we always start one), or
+     *            when the queue is full (in which case we must bypass queue).
+     *            Initially idle threads are usually created via
+     *            prestartCoreThread or to replace other dying workers.
+     * 
+     * @param core
+     *            if true use corePoolSize as bound, else maximumPoolSize. (A
+     *            boolean indicator is used here rather than a value to ensure
+     *            reads of fresh values after checking other pool state).
+     * @return true if successful
+     */
+    private boolean addWorker(Runnable firstTask, boolean core)
+    {
+        retry: for (;;)
+        {
+            int c = ctl.get();
+            int rs = runStateOf(c);
+
+            // Check if queue empty only if necessary.
+            if (rs >= SHUTDOWN
+                    && !(rs == SHUTDOWN && firstTask == null && !workQueue
+                            .isEmpty()))
+                return false;
+
+            for (;;)
+            {
+                int wc = workerCountOf(c);
+                if (wc >= CAPACITY
+                        || wc >= (core ? corePoolSize : maximumPoolSize))
+                    return false;
+                if (compareAndIncrementWorkerCount(c))
+                    break retry;
+                c = ctl.get(); // Re-read ctl
+                if (runStateOf(c) != rs)
+                    continue retry;
+                // else CAS failed due to workerCount change; retry inner loop
+            }
+        }
+
+        Worker w = new Worker(firstTask);
+        Thread t = w.thread;
+
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            // Recheck while holding lock.
+            // Back out on ThreadFactory failure or if
+            // shut down before lock acquired.
+            int c = ctl.get();
+            int rs = runStateOf(c);
+
+            if (t == null
+                    || (rs >= SHUTDOWN && !(rs == SHUTDOWN && firstTask == null)))
+            {
+                decrementWorkerCount();
+                tryTerminate();
+                return false;
+            }
+
+            workers.add(w);
+
+            int s = workers.size();
+            if (s > largestPoolSize)
+                largestPoolSize = s;
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+
+        t.start();
+        // It is possible (but unlikely) for a thread to have been
+        // added to workers, but not yet started, during transition to
+        // STOP, which could result in a rare missed interrupt,
+        // because Thread.interrupt is not guaranteed to have any effect
+        // on a non-yet-started Thread (see Thread#interrupt).
+        if (runStateOf(ctl.get()) == STOP && !t.isInterrupted())
+            t.interrupt();
+
+        return true;
+    }
+
+    /**
+     * Performs cleanup and bookkeeping for a dying worker. Called only from
+     * worker threads. Unless completedAbruptly is set, assumes that workerCount
+     * has already been adjusted to account for exit. This method removes thread
+     * from worker set, and possibly terminates the pool or replaces the worker
+     * if either it exited due to user task exception or if fewer than
+     * corePoolSize workers are running or queue is non-empty but there are no
+     * workers.
+     * 
+     * @param w
+     *            the worker
+     * @param completedAbruptly
+     *            if the worker died due to user exception
+     */
+    private void processWorkerExit(Worker w, boolean completedAbruptly)
+    {
+        if (completedAbruptly) // If abrupt, then workerCount wasn't adjusted
+            decrementWorkerCount();
+
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            completedTaskCount += w.completedTasks;
+            workers.remove(w);
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+
+        tryTerminate();
+
+        int c = ctl.get();
+        if (runStateLessThan(c, STOP))
+        {
+            if (!completedAbruptly)
+            {
+                int min = allowCoreThreadTimeOut ? 0 : corePoolSize;
+                if (min == 0 && !workQueue.isEmpty())
+                    min = 1;
+                if (workerCountOf(c) >= min)
+                    return; // replacement not needed
+            }
+            addWorker(null, false);
+        }
+    }
+
+    /**
+     * Performs blocking or timed wait for a task, depending on current
+     * configuration settings, or returns null if this worker must exit because
+     * of any of: 1. There are more than maximumPoolSize workers (due to a call
+     * to setMaximumPoolSize). 2. The pool is stopped. 3. The pool is shutdown
+     * and the queue is empty. 4. This worker timed out waiting for a task, and
+     * timed-out workers are subject to termination (that is,
+     * {@code allowCoreThreadTimeOut || workerCount > corePoolSize}) both
+     * before and after the timed wait.
+     * 
+     * @return task, or null if the worker must exit, in which case workerCount
+     *         is decremented
+     */
+    private Runnable getTask()
+    {
+        boolean timedOut = false; // Did the last poll() time out?
+
+        retry: for (;;)
+        {
+            int c = ctl.get();
+            int rs = runStateOf(c);
+
+            // Check if queue empty only if necessary.
+            if (rs >= SHUTDOWN && (rs >= STOP || workQueue.isEmpty()))
+            {
+                decrementWorkerCount();
+                return null;
+            }
+
+            boolean timed; // Are workers subject to culling?
+
+            for (;;)
+            {
+                int wc = workerCountOf(c);
+                timed = allowCoreThreadTimeOut || wc > corePoolSize;
+
+                if (wc <= maximumPoolSize && !(timedOut && timed))
+                    break;
+                if (compareAndDecrementWorkerCount(c))
+                    return null;
+                c = ctl.get(); // Re-read ctl
+                if (runStateOf(c) != rs)
+                    continue retry;
+                // else CAS failed due to workerCount change; retry inner loop
+            }
+
+            try
+            {
+                Runnable r = timed ? workQueue.poll(keepAliveTime,
+                        TimeUnit.NANOSECONDS) : workQueue.take();
+                if (r != null)
+                    return r;
+                timedOut = true;
+            }
+            catch (InterruptedException retry)
+            {
+                timedOut = false;
+            }
+        }
+    }
+
+    /**
+     * Main worker run loop. Repeatedly gets tasks from queue and executes them,
+     * while coping with a number of issues:
+     * 
+     * 1. We may start out with an initial task, in which case we don't need to
+     * get the first one. Otherwise, as long as pool is running, we get tasks
+     * from getTask. If it returns null then the worker exits due to changed
+     * pool state or configuration parameters. Other exits result from exception
+     * throws in external code, in which case completedAbruptly holds, which
+     * usually leads processWorkerExit to replace this thread.
+     * 
+     * 2. Before running any task, the lock is acquired to prevent other pool
+     * interrupts while the task is executing, and clearInterruptsForTaskRun
+     * called to ensure that unless pool is stopping, this thread does not have
+     * its interrupt set.
+     * 
+     * 3. Each task run is preceded by a call to beforeExecute, which might
+     * throw an exception, in which case we cause thread to die (breaking loop
+     * with completedAbruptly true) without processing the task.
+     * 
+     * 4. Assuming beforeExecute completes normally, we run the task, gathering
+     * any of its thrown exceptions to send to afterExecute. We separately
+     * handle RuntimeException, Error (both of which the specs guarantee that we
+     * trap) and arbitrary Throwables. Because we cannot rethrow Throwables
+     * within Runnable.run, we wrap them within Errors on the way out (to the
+     * thread's UncaughtExceptionHandler). Any thrown exception also
+     * conservatively causes thread to die.
+     * 
+     * 5. After task.run completes, we call afterExecute, which may also throw
+     * an exception, which will also cause thread to die. According to JLS Sec
+     * 14.20, this exception is the one that will be in effect even if task.run
+     * throws.
+     * 
+     * The net effect of the exception mechanics is that afterExecute and the
+     * thread's UncaughtExceptionHandler have as accurate information as we can
+     * provide about any problems encountered by user code.
+     * 
+     * @param w
+     *            the worker
+     */
+    final void runWorker(Worker w)
+    {
+        Runnable task = w.firstTask;
+        w.firstTask = null;
+        boolean completedAbruptly = true;
+        try
+        {
+            while (task != null || (task = getTask()) != null)
+            {
+                w.lock();
+                clearInterruptsForTaskRun();
+                try
+                {
+                    beforeExecute(w.thread, task);
+                    Throwable thrown = null;
+                    try
+                    {                                 
+                        /* start in suspended mode to get a handle to the continuation */
+                        Continuation c = Continuation.startSuspendedWith(task);                                              
+                        /* resume the damn continuation */                                                    
+                        c = Continuation.continueWith(c, new ContinuationContext(c));
+                        /* post process the call if need be */                        
+                        ContinuationsExecutor.doPostProcessing(c);
+                    }
+                    catch (RuntimeException x)
+                    {
+                        thrown = x;
+                        throw x;
+                    }
+                    catch (Error x)
+                    {
+                        thrown = x;
+                        throw x;
+                    }
+                    catch (Throwable x)
+                    {
+                        thrown = x;
+                        throw new Error(x);
+                    }
+                    finally
+                    {
+                        afterExecute(task, thrown);
+                    }
+                }
+                finally
+                {
+                    task = null;
+                    w.completedTasks++;
+                    w.unlock();
+                }
+            }
+            completedAbruptly = false;
+        }
+        finally
+        {
+            processWorkerExit(w, completedAbruptly);
+        }
+    }
+
+    // Public constructors and methods
+
+    /**
+     * Creates a new {@code ContinuationsExecutor} with the given initial
+     * parameters and default thread factory and rejected execution handler. It
+     * may be more convenient to use one of the {@link Executors} factory
+     * methods instead of this general purpose constructor.
+     * 
+     * @param corePoolSize
+     *            the number of threads to keep in the pool, even if they are
+     *            idle, unless {@code allowCoreThreadTimeOut} is set
+     * @param maximumPoolSize
+     *            the maximum number of threads to allow in the pool
+     * @param keepAliveTime
+     *            when the number of threads is greater than the core, this is
+     *            the maximum time that excess idle threads will wait for new
+     *            tasks before terminating.
+     * @param unit
+     *            the time unit for the {@code keepAliveTime} argument
+     * @param workQueue
+     *            the queue to use for holding tasks before they are executed.
+     *            This queue will hold only the {@code Runnable} tasks submitted
+     *            by the {@code execute} method.
+     * @throws IllegalArgumentException
+     *             if one of the following holds:<br>
+     *             {@code corePoolSize < 0}<br>
+     *             {@code keepAliveTime < 0}<br>
+     *             {@code maximumPoolSize <= 0}<br>
+     *             {@code maximumPoolSize < corePoolSize}
+     * @throws NullPointerException
+     *             if {@code workQueue} is null
+     */
+    public ContinuationsExecutor(int corePoolSize, int maximumPoolSize,
+            long keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue)
+    {
+        this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
+                Executors.defaultThreadFactory(), defaultHandler);
+    }
+
+    /**
+     * Creates a new {@code ContinuationsExecutor} with the given initial
+     * parameters and default rejected execution handler.
+     * 
+     * @param corePoolSize
+     *            the number of threads to keep in the pool, even if they are
+     *            idle, unless {@code allowCoreThreadTimeOut} is set
+     * @param maximumPoolSize
+     *            the maximum number of threads to allow in the pool
+     * @param keepAliveTime
+     *            when the number of threads is greater than the core, this is
+     *            the maximum time that excess idle threads will wait for new
+     *            tasks before terminating.
+     * @param unit
+     *            the time unit for the {@code keepAliveTime} argument
+     * @param workQueue
+     *            the queue to use for holding tasks before they are executed.
+     *            This queue will hold only the {@code Runnable} tasks submitted
+     *            by the {@code execute} method.
+     * @param threadFactory
+     *            the factory to use when the executor creates a new thread
+     * @throws IllegalArgumentException
+     *             if one of the following holds:<br>
+     *             {@code corePoolSize < 0}<br>
+     *             {@code keepAliveTime < 0}<br>
+     *             {@code maximumPoolSize <= 0}<br>
+     *             {@code maximumPoolSize < corePoolSize}
+     * @throws NullPointerException
+     *             if {@code workQueue} or {@code threadFactory} is null
+     */
+    public ContinuationsExecutor(int corePoolSize, int maximumPoolSize,
+            long keepAliveTime, TimeUnit unit,
+            BlockingQueue<Runnable> workQueue, ThreadFactory threadFactory)
+    {
+        this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
+                threadFactory, defaultHandler);
+    }
+
+    /**
+     * Creates a new {@code ContinuationsExecutor} with the given initial
+     * parameters and default thread factory.
+     * 
+     * @param corePoolSize
+     *            the number of threads to keep in the pool, even if they are
+     *            idle, unless {@code allowCoreThreadTimeOut} is set
+     * @param maximumPoolSize
+     *            the maximum number of threads to allow in the pool
+     * @param keepAliveTime
+     *            when the number of threads is greater than the core, this is
+     *            the maximum time that excess idle threads will wait for new
+     *            tasks before terminating.
+     * @param unit
+     *            the time unit for the {@code keepAliveTime} argument
+     * @param workQueue
+     *            the queue to use for holding tasks before they are executed.
+     *            This queue will hold only the {@code Runnable} tasks submitted
+     *            by the {@code execute} method.
+     * @param handler
+     *            the handler to use when execution is blocked because the
+     *            thread bounds and queue capacities are reached
+     * @throws IllegalArgumentException
+     *             if one of the following holds:<br>
+     *             {@code corePoolSize < 0}<br>
+     *             {@code keepAliveTime < 0}<br>
+     *             {@code maximumPoolSize <= 0}<br>
+     *             {@code maximumPoolSize < corePoolSize}
+     * @throws NullPointerException
+     *             if {@code workQueue} or {@code handler} is null
+     */
+    public ContinuationsExecutor(int corePoolSize, int maximumPoolSize,
+            long keepAliveTime, TimeUnit unit,
+            BlockingQueue<Runnable> workQueue, RejectedExecutionHandler handler)
+    {
+        this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
+                Executors.defaultThreadFactory(), handler);
+    }
+
+    /**
+     * Creates a new {@code ContinuationsExecutor} with the given initial
+     * parameters.
+     * 
+     * @param corePoolSize
+     *            the number of threads to keep in the pool, even if they are
+     *            idle, unless {@code allowCoreThreadTimeOut} is set
+     * @param maximumPoolSize
+     *            the maximum number of threads to allow in the pool
+     * @param keepAliveTime
+     *            when the number of threads is greater than the core, this is
+     *            the maximum time that excess idle threads will wait for new
+     *            tasks before terminating.
+     * @param unit
+     *            the time unit for the {@code keepAliveTime} argument
+     * @param workQueue
+     *            the queue to use for holding tasks before they are executed.
+     *            This queue will hold only the {@code Runnable} tasks submitted
+     *            by the {@code execute} method.
+     * @param threadFactory
+     *            the factory to use when the executor creates a new thread
+     * @param handler
+     *            the handler to use when execution is blocked because the
+     *            thread bounds and queue capacities are reached
+     * @throws IllegalArgumentException
+     *             if one of the following holds:<br>
+     *             {@code corePoolSize < 0}<br>
+     *             {@code keepAliveTime < 0}<br>
+     *             {@code maximumPoolSize <= 0}<br>
+     *             {@code maximumPoolSize < corePoolSize}
+     * @throws NullPointerException
+     *             if {@code workQueue} or {@code threadFactory} or
+     *             {@code handler} is null
+     */
+    public ContinuationsExecutor(int corePoolSize, int maximumPoolSize,
+            long keepAliveTime, TimeUnit unit,
+            BlockingQueue<Runnable> workQueue, ThreadFactory threadFactory,
+            RejectedExecutionHandler handler)
+    {
+        if (corePoolSize < 0 || maximumPoolSize <= 0
+                || maximumPoolSize < corePoolSize || keepAliveTime < 0)
+            throw new IllegalArgumentException();
+        if (workQueue == null || threadFactory == null || handler == null)
+            throw new NullPointerException();
+        this.corePoolSize = corePoolSize;
+        this.maximumPoolSize = maximumPoolSize;
+        this.workQueue = workQueue;
+        this.keepAliveTime = unit.toNanos(keepAliveTime);
+        this.threadFactory = threadFactory;
+        this.handler = handler;
+    }
+
+    /**
+     * Executes the given task sometime in the future. The task may execute in a
+     * new thread or in an existing pooled thread.
+     * 
+     * If the task cannot be submitted for execution, either because this
+     * executor has been shutdown or because its capacity has been reached, the
+     * task is handled by the current {@code RejectedExecutionHandler}.
+     * 
+     * @param command
+     *            the task to execute
+     * @throws RejectedExecutionException
+     *             at discretion of {@code RejectedExecutionHandler}, if the
+     *             task cannot be accepted for execution
+     * @throws NullPointerException
+     *             if {@code command} is null
+     */
+    public void execute(Runnable command)
+    {
+        if (command == null)
+            throw new NullPointerException();
+        /*
+         * Proceed in 3 steps:
+         * 
+         * 1. If fewer than corePoolSize threads are running, try to start a new
+         * thread with the given command as its first task. The call to
+         * addWorker atomically checks runState and workerCount, and so prevents
+         * false alarms that would add threads when it shouldn't, by returning
+         * false.
+         * 
+         * 2. If a task can be successfully queued, then we still need to
+         * double-check whether we should have added a thread (because existing
+         * ones died since last checking) or that the pool shut down since entry
+         * into this method. So we recheck state and if necessary roll back the
+         * enqueuing if stopped, or start a new thread if there are none.
+         * 
+         * 3. If we cannot queue task, then we try to add a new thread. If it
+         * fails, we know we are shut down or saturated and so reject the task.
+         */
+        int c = ctl.get();
+        if (workerCountOf(c) < corePoolSize)
+        {
+            if (addWorker(command, true))
+                return;
+            c = ctl.get();
+        }
+        if (isRunning(c) && workQueue.offer(command))
+        {
+            int recheck = ctl.get();
+            if (!isRunning(recheck) && remove(command))
+                reject(command);
+            else if (workerCountOf(recheck) == 0)
+                addWorker(null, false);
+        }
+        else if (!addWorker(command, false))
+            reject(command);
+    }
+
+    /**
+     * Initiates an orderly shutdown in which previously submitted tasks are
+     * executed, but no new tasks will be accepted. Invocation has no additional
+     * effect if already shut down.
+     * 
+     * @throws SecurityException
+     *             {@inheritDoc}
+     */
+    public void shutdown()
+    { 
+        /*
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            checkShutdownAccess();
+            advanceRunState(SHUTDOWN);
+            interruptIdleWorkers();
+            onShutdown(); // hook for ScheduledThreadPoolExecutor
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+        tryTerminate(); 
+        */           
+    }
+
+    /**
+     * Attempts to stop all actively executing tasks, halts the processing of
+     * waiting tasks, and returns a list of the tasks that were awaiting
+     * execution. These tasks are drained (removed) from the task queue upon
+     * return from this method.
+     * 
+     * <p>
+     * There are no guarantees beyond best-effort attempts to stop processing
+     * actively executing tasks. This implementation cancels tasks via
+     * {@link Thread#interrupt}, so any task that fails to respond to
+     * interrupts may never terminate.
+     * 
+     * @throws SecurityException
+     *             {@inheritDoc}
+     */
+    public List<Runnable> shutdownNow()
+    {
+        List<Runnable> tasks;
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            checkShutdownAccess();
+            advanceRunState(STOP);
+            interruptWorkers();
+            tasks = drainQueue();
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+        tryTerminate();
+        return tasks;
+    }
+
+    public boolean isShutdown()
+    {
+        return !isRunning(ctl.get());
+    }
+
+    /**
+     * Returns true if this executor is in the process of terminating after
+     * {@link #shutdown} or {@link #shutdownNow} but has not completely
+     * terminated. This method may be useful for debugging. A return of
+     * {@code true} reported a sufficient period after shutdown may indicate
+     * that submitted tasks have ignored or suppressed interruption, causing
+     * this executor not to properly terminate.
+     * 
+     * @return true if terminating but not yet terminated
+     */
+    public boolean isTerminating()
+    {
+        int c = ctl.get();
+        return !isRunning(c) && runStateLessThan(c, TERMINATED);
+    }
+
+    public boolean isTerminated()
+    {
+        return runStateAtLeast(ctl.get(), TERMINATED);
+    }
+
+    public boolean awaitTermination(long timeout, TimeUnit unit)
+            throws InterruptedException
+    {
+        long nanos = unit.toNanos(timeout);
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            for (;;)
+            {
+                if (runStateAtLeast(ctl.get(), TERMINATED))
+                    return true;
+                if (nanos <= 0)
+                    return false;
+                nanos = termination.awaitNanos(nanos);
+            }
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+    }
+
+    /**
+     * Invokes {@code shutdown} when this executor is no longer referenced and
+     * it has no threads.
+     */
+    protected void finalize()
+    {
+        shutdown();
+    }
+
+    /**
+     * Sets the thread factory used to create new threads.
+     * 
+     * @param threadFactory
+     *            the new thread factory
+     * @throws NullPointerException
+     *             if threadFactory is null
+     * @see #getThreadFactory
+     */
+    public void setThreadFactory(ThreadFactory threadFactory)
+    {
+        if (threadFactory == null)
+            throw new NullPointerException();
+        this.threadFactory = threadFactory;
+    }
+
+    /**
+     * Returns the thread factory used to create new threads.
+     * 
+     * @return the current thread factory
+     * @see #setThreadFactory
+     */
+    public ThreadFactory getThreadFactory()
+    {
+        return threadFactory;
+    }
+
+    /**
+     * Sets a new handler for unexecutable tasks.
+     * 
+     * @param handler
+     *            the new handler
+     * @throws NullPointerException
+     *             if handler is null
+     * @see #getRejectedExecutionHandler
+     */
+    public void setRejectedExecutionHandler(RejectedExecutionHandler handler)
+    {
+        if (handler == null)
+            throw new NullPointerException();
+        this.handler = handler;
+    }
+
+    /**
+     * Returns the current handler for unexecutable tasks.
+     * 
+     * @return the current handler
+     * @see #setRejectedExecutionHandler
+     */
+    public RejectedExecutionHandler getRejectedExecutionHandler()
+    {
+        return handler;
+    }
+
+    /**
+     * Sets the core number of threads. This overrides any value set in the
+     * constructor. If the new value is smaller than the current value, excess
+     * existing threads will be terminated when they next become idle. If
+     * larger, new threads will, if needed, be started to execute any queued
+     * tasks.
+     * 
+     * @param corePoolSize
+     *            the new core size
+     * @throws IllegalArgumentException
+     *             if {@code corePoolSize < 0}
+     * @see #getCorePoolSize
+     */
+    public void setCorePoolSize(int corePoolSize)
+    {
+        if (corePoolSize < 0)
+            throw new IllegalArgumentException();
+        int delta = corePoolSize - this.corePoolSize;
+        this.corePoolSize = corePoolSize;
+        if (workerCountOf(ctl.get()) > corePoolSize)
+            interruptIdleWorkers();
+        else if (delta > 0)
+        {
+            // We don't really know how many new threads are "needed".
+            // As a heuristic, prestart enough new workers (up to new
+            // core size) to handle the current number of tasks in
+            // queue, but stop if queue becomes empty while doing so.
+            int k = Math.min(delta, workQueue.size());
+            while (k-- > 0 && addWorker(null, true))
+            {
+                if (workQueue.isEmpty())
+                    break;
+            }
+        }
+    }
+
+    /**
+     * Returns the core number of threads.
+     * 
+     * @return the core number of threads
+     * @see #setCorePoolSize
+     */
+    public int getCorePoolSize()
+    {
+        return corePoolSize;
+    }
+
+    /**
+     * Starts a core thread, causing it to idly wait for work. This overrides
+     * the default policy of starting core threads only when new tasks are
+     * executed. This method will return {@code false} if all core threads have
+     * already been started.
+     * 
+     * @return {@code true} if a thread was started
+     */
+    public boolean prestartCoreThread()
+    {
+        return workerCountOf(ctl.get()) < corePoolSize && addWorker(null, true);
+    }
+
+    /**
+     * Starts all core threads, causing them to idly wait for work. This
+     * overrides the default policy of starting core threads only when new tasks
+     * are executed.
+     * 
+     * @return the number of threads started
+     */
+    public int prestartAllCoreThreads()
+    {
+        int n = 0;
+        while (addWorker(null, true))
+            ++n;
+        return n;
+    }
+
+    /**
+     * Returns true if this pool allows core threads to time out and terminate
+     * if no tasks arrive within the keepAlive time, being replaced if needed
+     * when new tasks arrive. When true, the same keep-alive policy applying to
+     * non-core threads applies also to core threads. When false (the default),
+     * core threads are never terminated due to lack of incoming tasks.
+     * 
+     * @return {@code true} if core threads are allowed to time out, else
+     *         {@code false}
+     * 
+     * @since 1.6
+     */
+    public boolean allowsCoreThreadTimeOut()
+    {
+        return allowCoreThreadTimeOut;
+    }
+
+    /**
+     * Sets the policy governing whether core threads may time out and terminate
+     * if no tasks arrive within the keep-alive time, being replaced if needed
+     * when new tasks arrive. When false, core threads are never terminated due
+     * to lack of incoming tasks. When true, the same keep-alive policy applying
+     * to non-core threads applies also to core threads. To avoid continual
+     * thread replacement, the keep-alive time must be greater than zero when
+     * setting {@code true}. This method should in general be called before the
+     * pool is actively used.
+     * 
+     * @param value
+     *            {@code true} if should time out, else {@code false}
+     * @throws IllegalArgumentException
+     *             if value is {@code true} and the current keep-alive time is
+     *             not greater than zero
+     * 
+     * @since 1.6
+     */
+    public void allowCoreThreadTimeOut(boolean value)
+    {
+        if (value && keepAliveTime <= 0)
+            throw new IllegalArgumentException(
+                    "Core threads must have nonzero keep alive times");
+        if (value != allowCoreThreadTimeOut)
+        {
+            allowCoreThreadTimeOut = value;
+            if (value)
+                interruptIdleWorkers();
+        }
+    }
+
+    /**
+     * Sets the maximum allowed number of threads. This overrides any value set
+     * in the constructor. If the new value is smaller than the current value,
+     * excess existing threads will be terminated when they next become idle.
+     * 
+     * @param maximumPoolSize
+     *            the new maximum
+     * @throws IllegalArgumentException
+     *             if the new maximum is less than or equal to zero, or less
+     *             than the {@linkplain #getCorePoolSize core pool size}
+     * @see #getMaximumPoolSize
+     */
+    public void setMaximumPoolSize(int maximumPoolSize)
+    {
+        if (maximumPoolSize <= 0 || maximumPoolSize < corePoolSize)
+            throw new IllegalArgumentException();
+        this.maximumPoolSize = maximumPoolSize;
+        if (workerCountOf(ctl.get()) > maximumPoolSize)
+            interruptIdleWorkers();
+    }
+
+    /**
+     * Returns the maximum allowed number of threads.
+     * 
+     * @return the maximum allowed number of threads
+     * @see #setMaximumPoolSize
+     */
+    public int getMaximumPoolSize()
+    {
+        return maximumPoolSize;
+    }
+
+    /**
+     * Sets the time limit for which threads may remain idle before being
+     * terminated. If there are more than the core number of threads currently
+     * in the pool, after waiting this amount of time without processing a task,
+     * excess threads will be terminated. This overrides any value set in the
+     * constructor.
+     * 
+     * @param time
+     *            the time to wait. A time value of zero will cause excess
+     *            threads to terminate immediately after executing tasks.
+     * @param unit
+     *            the time unit of the {@code time} argument
+     * @throws IllegalArgumentException
+     *             if {@code time} less than zero or if {@code time} is zero and
+     *             {@code allowsCoreThreadTimeOut}
+     * @see #getKeepAliveTime
+     */
+    public void setKeepAliveTime(long time, TimeUnit unit)
+    {
+        if (time < 0)
+            throw new IllegalArgumentException();
+        if (time == 0 && allowsCoreThreadTimeOut())
+            throw new IllegalArgumentException(
+                    "Core threads must have nonzero keep alive times");
+        long keepAliveTime = unit.toNanos(time);
+        long delta = keepAliveTime - this.keepAliveTime;
+        this.keepAliveTime = keepAliveTime;
+        if (delta < 0)
+            interruptIdleWorkers();
+    }
+
+    /**
+     * Returns the thread keep-alive time, which is the amount of time that
+     * threads in excess of the core pool size may remain idle before being
+     * terminated.
+     * 
+     * @param unit
+     *            the desired time unit of the result
+     * @return the time limit
+     * @see #setKeepAliveTime
+     */
+    public long getKeepAliveTime(TimeUnit unit)
+    {
+        return unit.convert(keepAliveTime, TimeUnit.NANOSECONDS);
+    }
+
+    /* User-level queue utilities */
+
+    /**
+     * Returns the task queue used by this executor. Access to the task queue is
+     * intended primarily for debugging and monitoring. This queue may be in
+     * active use. Retrieving the task queue does not prevent queued tasks from
+     * executing.
+     * 
+     * @return the task queue
+     */
+    public BlockingQueue<Runnable> getQueue()
+    {
+        return workQueue;
+    }
+
+    /**
+     * Removes this task from the executor's internal queue if it is present,
+     * thus causing it not to be run if it has not already started.
+     * 
+     * <p>
+     * This method may be useful as one part of a cancellation scheme. It may
+     * fail to remove tasks that have been converted into other forms before
+     * being placed on the internal queue. For example, a task entered using
+     * {@code submit} might be converted into a form that maintains
+     * {@code Future} status. However, in such cases, method {@link #purge} may
+     * be used to remove those Futures that have been cancelled.
+     * 
+     * @param task
+     *            the task to remove
+     * @return true if the task was removed
+     */
+    public boolean remove(Runnable task)
+    {
+        boolean removed = workQueue.remove(task);
+        tryTerminate(); // In case SHUTDOWN and now empty
+        return removed;
+    }
+
+    /**
+     * Tries to remove from the work queue all {@link Future} tasks that have
+     * been cancelled. This method can be useful as a storage reclamation
+     * operation, that has no other impact on functionality. Cancelled tasks are
+     * never executed, but may accumulate in work queues until worker threads
+     * can actively remove them. Invoking this method instead tries to remove
+     * them now. However, this method may fail to remove tasks in the presence
+     * of interference by other threads.
+     */
+    public void purge()
+    {
+        final BlockingQueue<Runnable> q = workQueue;
+        try
+        {
+            Iterator<Runnable> it = q.iterator();
+            while (it.hasNext())
+            {
+                Runnable r = it.next();
+                if (r instanceof Future<?> && ((Future<?>) r).isCancelled())
+                    it.remove();
+            }
+        }
+        catch (ConcurrentModificationException fallThrough)
+        {
+            // Take slow path if we encounter interference during traversal.
+            // Make copy for traversal and call remove for cancelled entries.
+            // The slow path is more likely to be O(N*N).
+            for (Object r : q.toArray())
+                if (r instanceof Future<?> && ((Future<?>) r).isCancelled())
+                    q.remove(r);
+        }
+
+        tryTerminate(); // In case SHUTDOWN and now empty
+    }
+
+    /* Statistics */
+
+    /**
+     * Returns the current number of threads in the pool.
+     * 
+     * @return the number of threads
+     */
+    public int getPoolSize()
+    {
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            // Remove rare and surprising possibility of
+            // isTerminated() && getPoolSize() > 0
+            return runStateAtLeast(ctl.get(), TIDYING) ? 0 : workers.size();
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+    }
+
+    /**
+     * Returns the approximate number of threads that are actively executing
+     * tasks.
+     * 
+     * @return the number of threads
+     */
+    public int getActiveCount()
+    {
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            int n = 0;
+            for (Worker w : workers)
+                if (w.isLocked())
+                    ++n;
+            return n;
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+    }
+
+    /**
+     * Returns the largest number of threads that have ever simultaneously been
+     * in the pool.
+     * 
+     * @return the number of threads
+     */
+    public int getLargestPoolSize()
+    {
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            return largestPoolSize;
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+    }
+
+    /**
+     * Returns the approximate total number of tasks that have ever been
+     * scheduled for execution. Because the states of tasks and threads may
+     * change dynamically during computation, the returned value is only an
+     * approximation.
+     * 
+     * @return the number of tasks
+     */
+    public long getTaskCount()
+    {
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            long n = completedTaskCount;
+            for (Worker w : workers)
+            {
+                n += w.completedTasks;
+                if (w.isLocked())
+                    ++n;
+            }
+            return n + workQueue.size();
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+    }
+
+    /**
+     * Returns the approximate total number of tasks that have completed
+     * execution. Because the states of tasks and threads may change dynamically
+     * during computation, the returned value is only an approximation, but one
+     * that does not ever decrease across successive calls.
+     * 
+     * @return the number of tasks
+     */
+    public long getCompletedTaskCount()
+    {
+        final ReentrantLock mainLock = this.mainLock;
+        mainLock.lock();
+        try
+        {
+            long n = completedTaskCount;
+            for (Worker w : workers)
+                n += w.completedTasks;
+            return n;
+        }
+        finally
+        {
+            mainLock.unlock();
+        }
+    }
+
+    /* Extension hooks */
+
+    /**
+     * Method invoked prior to executing the given Runnable in the given thread.
+     * This method is invoked by thread {@code t} that will execute task
+     * {@code r}, and may be used to re-initialize ThreadLocals, or to perform
+     * logging.
+     * 
+     * <p>
+     * This implementation does nothing, but may be customized in subclasses.
+     * Note: To properly nest multiple overridings, subclasses should generally
+     * invoke {@code super.beforeExecute} at the end of this method.
+     * 
+     * @param t
+     *            the thread that will run task {@code r}
+     * @param r
+     *            the task that will be executed
+     */
+    protected void beforeExecute(Thread t, Runnable r)
+    {         
+    }
+
+    /**
+     * Method invoked upon completion of execution of the given Runnable. This
+     * method is invoked by the thread that executed the task. If non-null, the
+     * Throwable is the uncaught {@code RuntimeException} or {@code Error} that
+     * caused execution to terminate abruptly.
+     * 
+     * <p>
+     * This implementation does nothing, but may be customized in subclasses.
+     * Note: To properly nest multiple overridings, subclasses should generally
+     * invoke {@code super.afterExecute} at the beginning of this method.
+     * 
+     * <p>
+     * <b>Note:</b> When actions are enclosed in tasks (such as
+     * {@link FutureTask}) either explicitly or via methods such as
+     * {@code submit}, these task objects catch and maintain computational
+     * exceptions, and so they do not cause abrupt termination, and the internal
+     * exceptions are <em>not</em> passed to this method. If you would like to
+     * trap both kinds of failures in this method, you can further probe for
+     * such cases, as in this sample subclass that prints either the direct
+     * cause or the underlying exception if a task has been aborted:
+     * 
+     * <pre>
+     * {
+     *     @code
+     *     class ExtendedExecutor extends ContinuationsExecutor
+     *     {
+     *         // ...
+     *         protected void afterExecute(Runnable r, Throwable t)
+     *         {
+     *             super.afterExecute(r, t);
+     *             if (t == null &amp;&amp; r instanceof Future&lt;?&gt;)
+     *             {
+     *                 try
+     *                 {
+     *                     Object result = ((Future&lt;?&gt;) r).get();
+     *                 }
+     *                 catch (CancellationException ce)
+     *                 {
+     *                     t = ce;
+     *                 }
+     *                 catch (ExecutionException ee)
+     *                 {
+     *                     t = ee.getCause();
+     *                 }
+     *                 catch (InterruptedException ie)
+     *                 {
+     *                     Thread.currentThread().interrupt(); // ignore/reset
+     *                 }
+     *             }
+     *             if (t != null)
+     *                 System.out.println(t);
+     *         }
+     *     }
+     * }
+     * </pre>
+     * 
+     * @param r
+     *            the runnable that has completed
+     * @param t
+     *            the exception that caused termination, or null if execution
+     *            completed normally
+     */
+    protected void afterExecute(Runnable r, Throwable t)
+    {
+        if ( t != null )
+            logger_.info( LogUtil.throwableToString(t) );
+    }
+
+    /**
+     * Method invoked when the Executor has terminated. Default implementation
+     * does nothing. Note: To properly nest multiple overridings, subclasses
+     * should generally invoke {@code super.terminated} within this method.
+     */
+    protected void terminated()
+    {
+    }
+
+    /* Predefined RejectedExecutionHandlers */
+
+    /**
+     * A handler for rejected tasks that runs the rejected task directly in the
+     * calling thread of the {@code execute} method, unless the executor has
+     * been shut down, in which case the task is discarded.
+     */
+    public static class CallerRunsPolicy implements RejectedExecutionHandler
+    {
+        /**
+         * Creates a {@code CallerRunsPolicy}.
+         */
+        public CallerRunsPolicy()
+        {
+        }
+
+        /**
+         * Executes task r in the caller's thread, unless the executor has been
+         * shut down, in which case the task is discarded.
+         * 
+         * @param r
+         *            the runnable task requested to be executed
+         * @param e
+         *            the executor attempting to execute this task
+         */
+        public void rejectedExecution(Runnable r, ContinuationsExecutor e)
+        {
+            if (!e.isShutdown())
+            {
+                r.run();
+            }
+        }
+    }
+
+    /**
+     * A handler for rejected tasks that throws a
+     * {@code RejectedExecutionException}.
+     */
+    public static class AbortPolicy implements RejectedExecutionHandler
+    {
+        /**
+         * Creates an {@code AbortPolicy}.
+         */
+        public AbortPolicy()
+        {
+        }
+
+        /**
+         * Always throws RejectedExecutionException.
+         * 
+         * @param r
+         *            the runnable task requested to be executed
+         * @param e
+         *            the executor attempting to execute this task
+         * @throws RejectedExecutionException
+         *             always.
+         */
+        public void rejectedExecution(Runnable r, ContinuationsExecutor e)
+        {
+            throw new RejectedExecutionException();
+        }
+    }
+
+    /**
+     * A handler for rejected tasks that silently discards the rejected task.
+     */
+    public static class DiscardPolicy implements RejectedExecutionHandler
+    {
+        /**
+         * Creates a {@code DiscardPolicy}.
+         */
+        public DiscardPolicy()
+        {
+        }
+
+        /**
+         * Does nothing, which has the effect of discarding task r.
+         * 
+         * @param r
+         *            the runnable task requested to be executed
+         * @param e
+         *            the executor attempting to execute this task
+         */
+        public void rejectedExecution(Runnable r, ContinuationsExecutor e)
+        {
+        }
+    }
+
+    /**
+     * A handler for rejected tasks that discards the oldest unhandled request
+     * and then retries {@code execute}, unless the executor is shut down, in
+     * which case the task is discarded.
+     */
+    public static class DiscardOldestPolicy implements RejectedExecutionHandler
+    {
+        /**
+         * Creates a {@code DiscardOldestPolicy} for the given executor.
+         */
+        public DiscardOldestPolicy()
+        {
+        }
+
+        /**
+         * Obtains and ignores the next task that the executor
+         * would otherwise execute, if one is immediately available,
+         * and then retries execution of task r, unless the executor
+         * is shut down, in which case task r is instead discarded.
+         *
+         * @param r the runnable task requested to be executed
+         * @param e the executor attempting to execute this task
+         */
+        public void rejectedExecution(Runnable r, ContinuationsExecutor e)
+        {
+            if (!e.isShutdown())
+            {
+                e.getQueue().poll();
+                e.execute(r);
+            }
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java
index e69de29b..6b2abfc3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java
@@ -0,0 +1,76 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
+
+/**
+ * This is a wrapper class for the <i>ScheduledThreadPoolExecutor</i>. It provides an implementation
+ * for the <i>afterExecute()</i> found in the <i>ThreadPoolExecutor</i> class to log any unexpected 
+ * Runtime Exceptions.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public final class DebuggableScheduledThreadPoolExecutor extends ScheduledThreadPoolExecutor
+{
+    private static Logger logger_ = Logger.getLogger(DebuggableScheduledThreadPoolExecutor.class);
+    
+    public DebuggableScheduledThreadPoolExecutor(int threads,
+            ThreadFactory threadFactory)
+    {
+        super(threads, threadFactory);        
+    }
+    
+    /**
+     *  (non-Javadoc)
+     * @see java.util.concurrent.ThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)
+     */
+    public void afterExecute(Runnable r, Throwable t)
+    {
+        super.afterExecute(r,t);
+        if ( t != null )
+        {  
+            Context ctx = ThreadLocalContext.get();
+            if ( ctx != null )
+            {
+                Object object = ctx.get(r.getClass().getName());
+                
+                if ( object != null )
+                {
+                    logger_.info("**** In afterExecute() " + t.getClass().getName() + " occured while working with " + object + " ****");
+                }
+                else
+                {
+                    logger_.info("**** In afterExecute() " + t.getClass().getName() + " occured ****");
+                }
+            }
+            
+            Throwable cause = t.getCause();
+            if ( cause != null )
+            {
+                logger_.info( LogUtil.throwableToString(cause) );
+            }
+            logger_.info( LogUtil.throwableToString(t) );
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
index e69de29b..88b9606f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
@@ -0,0 +1,92 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
+
+/**
+ * This is a wrapper class for the <i>ScheduledThreadPoolExecutor</i>. It provides an implementation
+ * for the <i>afterExecute()</i> found in the <i>ThreadPoolExecutor</i> class to log any unexpected 
+ * Runtime Exceptions.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class DebuggableThreadPoolExecutor extends ThreadPoolExecutor
+{
+    private static Logger logger_ = Logger.getLogger(DebuggableThreadPoolExecutor.class);    
+    
+    public DebuggableThreadPoolExecutor(int corePoolSize,
+            int maximumPoolSize,
+            long keepAliveTime,
+            TimeUnit unit,
+            BlockingQueue<Runnable> workQueue,
+            ThreadFactory threadFactory)
+    {
+        super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory);
+        super.prestartAllCoreThreads();
+    }
+    
+    /*
+     * 
+     *  (non-Javadoc)
+     * @see java.util.concurrent.ThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)
+     * Helps us in figuring out why sometimes the threads are getting 
+     * killed and replaced by new ones.
+     */
+    public void afterExecute(Runnable r, Throwable t)
+    {
+        super.afterExecute(r,t);
+
+        if (r instanceof FutureTask) {
+            assert t == null;
+            try
+            {
+                ((FutureTask)r).get();
+            }
+            catch (InterruptedException e)
+            {
+                throw new RuntimeException(e);
+            }
+            catch (ExecutionException e)
+            {
+                t = e;
+            }
+        }
+
+        if ( t != null )
+        {  
+            Context ctx = ThreadLocalContext.get();
+            if ( ctx != null )
+            {
+                Object object = ctx.get(r.getClass().getName());
+                
+                if ( object != null )
+                {
+                    logger_.error("In afterExecute() " + t.getClass().getName() + " occured while working with " + object);
+                }
+            }
+            logger_.error("Error in ThreadPoolExecutor", t);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/IContinuable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/IContinuable.java
index e69de29b..99e61363 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/IContinuable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/IContinuable.java
@@ -0,0 +1,26 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import org.apache.commons.javaflow.Continuation;
+
+public interface IContinuable
+{
+    public void run(Continuation c);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/IStage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/IStage.java
index e69de29b..90cd75c5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/IStage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/IStage.java
@@ -0,0 +1,120 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+
+/**
+ * An abstraction for stages as described in the SEDA paper by Matt Welsh. 
+ * For reference to the paper look over here 
+ * <a href="http://www.eecs.harvard.edu/~mdw/papers/seda-sosp01.pdf">SEDA: An Architecture for WellConditioned,
+   Scalable Internet Services</a>.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IStage 
+{
+    /**
+     * Get the name of the associated stage.
+     * @return name of the associated stage.
+     */
+    public String getName();
+    
+    /**
+     * Get the thread pool used by this stage 
+     * internally.
+     */
+    public ExecutorService getInternalThreadPool();
+    
+    /**
+     * This method is used to execute a piece of code on
+     * this stage. The idea is that the <i>run()</i> method
+     * of this Runnable instance is invoked on a thread from a
+     * thread pool that belongs to this stage.
+     * @param runnable instance whose run() method needs to be invoked.
+     */
+    public void execute(Runnable runnable);
+    
+    /**
+     * This method is used to execute a piece of code on
+     * this stage which returns a Future pointer. The idea
+     * is that the <i>call()</i> method of this Runnable 
+     * instance is invoked on a thread from a thread pool 
+     * that belongs to this stage.
+     
+     * @param callable instance that needs to be invoked.
+     * @return the future return object from the callable.
+     */
+    public Future<Object> execute(Callable<Object> callable);
+    
+    /**
+     * This method is used to submit tasks to this stage
+     * that execute periodically. 
+     * 
+     * @param command the task to execute.
+     * @param delay the time to delay first execution 
+     * @param unit the time unit of the initialDelay and period parameters 
+     * @return the future return object from the runnable.
+     */
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit); 
+      
+    /**
+     * This method is used to submit tasks to this stage
+     * that execute periodically. 
+     * @param command the task to execute.
+     * @param initialDelay the time to delay first execution
+     * @param period the period between successive executions
+     * @param unit the time unit of the initialDelay and period parameters 
+     * @return the future return object from the runnable.
+     */
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit); 
+    
+    /**
+     * This method is used to submit tasks to this stage
+     * that execute periodically. 
+     * @param command the task to execute.
+     * @param initialDelay the time to delay first execution
+     * @param delay  the delay between the termination of one execution and the commencement of the next.
+     * @param unit the time unit of the initialDelay and delay parameters 
+     * @return the future return object from the runnable.
+     */
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit);
+    
+    /**
+     * Shutdown the stage. All the threads of this stage
+     * are forcefully shutdown. Any pending tasks on this
+     * stage could be dropped or the stage could wait for 
+     * these tasks to be completed. This is however an 
+     * implementation detail.
+     */
+    public void shutdown();  
+    
+    /**
+     * Checks if the stage has been shutdown.
+     * @return true if shut down, otherwise false.
+     */
+    public boolean isShutdown();
+    
+    /**
+     * This method returns the number of tasks that are 
+     * pending on this stage to be executed.
+     * @return task count.
+     */
+    public long getTaskCount();
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/MultiThreadedStage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/MultiThreadedStage.java
index e69de29b..e36be420 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/MultiThreadedStage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/MultiThreadedStage.java
@@ -0,0 +1,98 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+
+import javax.naming.OperationNotSupportedException;
+
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * This class is an implementation of the <i>IStage</i> interface. In particular
+ * it is for a stage that has a thread pool with multiple threads. For details 
+ * please refer to the <i>IStage</i> documentation.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MultiThreadedStage implements IStage 
+{    
+    private String name_;
+    private DebuggableThreadPoolExecutor executorService_;
+            
+    public MultiThreadedStage(String name, int numThreads)
+    {        
+        name_ = name;        
+        executorService_ = new DebuggableThreadPoolExecutor( numThreads,
+                numThreads,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl(name)
+                );        
+    }
+    
+    public String getName() 
+    {        
+        return name_;
+    }    
+    
+    public ExecutorService getInternalThreadPool()
+    {
+        return executorService_;
+    }
+
+    public Future<Object> execute(Callable<Object> callable) {
+        return executorService_.submit(callable);
+    }
+    
+    public void execute(Runnable runnable) {
+        executorService_.execute(runnable);
+    }
+    
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
+    {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public void shutdown() {  
+        executorService_.shutdownNow(); 
+    }
+    
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }
+    
+    public long getTaskCount(){
+        return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/RejectedExecutionHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/RejectedExecutionHandler.java
index e69de29b..0ac88d44 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/RejectedExecutionHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/RejectedExecutionHandler.java
@@ -0,0 +1,24 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+interface RejectedExecutionHandler
+{
+    public void rejectedExecution(Runnable r, ContinuationsExecutor executor); 
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/SingleThreadedContinuationStage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/SingleThreadedContinuationStage.java
index e69de29b..65bac3dd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/SingleThreadedContinuationStage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/SingleThreadedContinuationStage.java
@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+
+public class SingleThreadedContinuationStage implements IStage 
+{
+    protected ContinuationsExecutor executorService_;
+    private String name_;
+
+    public SingleThreadedContinuationStage(String name)
+    {        
+        executorService_ = new ContinuationsExecutor( 1,
+                1,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl(name)
+                );        
+        name_ = name;        
+    }
+    
+    /* Implementing the IStage interface methods */
+    
+    public String getName()
+    {
+        return name_;
+    }
+    
+    public ExecutorService getInternalThreadPool()
+    {
+        return executorService_;
+    }
+    
+    public void execute(Runnable runnable)
+    {
+        executorService_.execute(runnable);
+    }
+    
+    public Future<Object> execute(Callable<Object> callable)
+    {
+        return executorService_.submit(callable);
+    }
+    
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
+    {
+        //return executorService_.schedule(command, delay, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)
+    {
+        //return executorService_.scheduleAtFixedRate(command, initialDelay, period, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit)
+    {
+        //return executorService_.scheduleWithFixedDelay(command, initialDelay, delay, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public void shutdown()
+    {
+        executorService_.shutdownNow();
+    }
+    
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }    
+    
+    public long getTaskCount(){
+        return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
+    }
+    /* Finished implementing the IStage interface methods */
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/SingleThreadedStage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/SingleThreadedStage.java
index e69de29b..3c390d51 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/SingleThreadedStage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/SingleThreadedStage.java
@@ -0,0 +1,108 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+import org.apache.cassandra.net.*;
+
+/**
+ * This class is an implementation of the <i>IStage</i> interface. In particular
+ * it is for a stage that has a thread pool with a single thread. For details 
+ * please refer to the <i>IStage</i> documentation.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SingleThreadedStage implements IStage 
+{
+    protected DebuggableThreadPoolExecutor executorService_;
+    private String name_;
+
+	public SingleThreadedStage(String name)
+    {
+        //executorService_ = new DebuggableScheduledThreadPoolExecutor(1,new ThreadFactoryImpl(name));
+        executorService_ = new DebuggableThreadPoolExecutor( 1,
+                1,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl(name)
+                );        
+        name_ = name;        
+	}
+	
+    /* Implementing the IStage interface methods */
+    
+    public String getName()
+    {
+        return name_;
+    }
+    
+    public ExecutorService getInternalThreadPool()
+    {
+        return executorService_;
+    }
+    
+    public void execute(Runnable runnable)
+    {
+        executorService_.execute(runnable);
+    }
+    
+    public Future<Object> execute(Callable<Object> callable)
+    {
+        return executorService_.submit(callable);
+    }
+    
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
+    {
+        //return executorService_.schedule(command, delay, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)
+    {
+        //return executorService_.scheduleAtFixedRate(command, initialDelay, period, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit)
+    {
+        //return executorService_.scheduleWithFixedDelay(command, initialDelay, delay, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public void shutdown()
+    {
+        executorService_.shutdownNow();
+    }
+    
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }    
+    
+    public long getTaskCount(){
+        return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
+    }
+    /* Finished implementing the IStage interface methods */
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/StageManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/StageManager.java
index e69de29b..462c143a 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/StageManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/StageManager.java
@@ -0,0 +1,119 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ExecutorService;
+
+import org.apache.cassandra.continuations.Suspendable;
+
+
+/**
+ * This class manages all stages that exist within a process. The application registers
+ * and de-registers stages with this abstraction. Any component that has the <i>ID</i> 
+ * associated with a stage can obtain a handle to actual stage.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class StageManager
+{
+    private static Map<String, IStage > stageQueues_ = new HashMap<String, IStage>();
+    
+    /**
+     * Register a stage with the StageManager
+     * @param stageName stage name.
+     * @param stage stage for the respective message types.
+     */
+    public static void registerStage(String stageName, IStage stage)
+    {
+        stageQueues_.put(stageName, stage);
+    }
+    
+    /**
+     * Returns the stage that we are currently executing on.
+     * This relies on the fact that the thread names in the
+     * stage have the name of the stage as the prefix.
+     * @return Returns the stage that we are currently executing on.
+     */
+    public static IStage getCurrentStage()
+    {
+        String name = Thread.currentThread().getName();
+        String[] peices = name.split(":");
+        IStage stage = getStage(peices[0]);
+        return stage;
+    }
+
+    /**
+     * Retrieve a stage from the StageManager
+     * @param stageName name of the stage to be retrieved.
+    */
+    public static IStage getStage(String stageName)
+    {
+        return stageQueues_.get(stageName);
+    }
+    
+    /**
+     * Retrieve the internal thread pool associated with the
+     * specified stage name.
+     * @param stageName name of the stage.
+     */
+    public static ExecutorService getStageInternalThreadPool(String stageName)
+    {
+        IStage stage = getStage(stageName);
+        if ( stage == null )
+            throw new IllegalArgumentException("No stage registered with name " + stageName);
+        return stage.getInternalThreadPool();
+    }
+
+    /**
+     * Deregister a stage from StageManager
+     * @param stageName stage name.
+     */
+    public static void deregisterStage(String stageName)
+    {
+        stageQueues_.remove(stageName);
+    }
+
+    /**
+     * This method gets the number of tasks on the
+     * stage's internal queue.
+     * @param stage name of the stage
+     * @return stage task count.
+     */
+    public static long getStageTaskCount(String stage)
+    {
+        return stageQueues_.get(stage).getTaskCount();
+    }
+
+    /**
+     * This method shuts down all registered stages.
+     */
+    public static void shutdown()
+    {
+        Set<String> stages = stageQueues_.keySet();
+        for ( String stage : stages )
+        {
+            IStage registeredStage = stageQueues_.get(stage);
+            registeredStage.shutdown();
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ThreadFactoryImpl.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ThreadFactoryImpl.java
index e69de29b..ae71d8b5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ThreadFactoryImpl.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ThreadFactoryImpl.java
@@ -0,0 +1,51 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+import java.util.concurrent.atomic.*;
+import org.apache.cassandra.utils.*;
+
+/**
+ * This class is an implementation of the <i>ThreadFactory</i> interface. This 
+ * is useful to give Java threads meaningful names which is useful when using 
+ * a tool like JConsole.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ThreadFactoryImpl implements ThreadFactory
+{
+    protected String id_;
+    protected ThreadGroup threadGroup_;
+    protected final AtomicInteger threadNbr_ = new AtomicInteger(1);
+    
+    public ThreadFactoryImpl(String id)
+    {
+        SecurityManager sm = System.getSecurityManager();
+        threadGroup_ = ( sm != null ) ? sm.getThreadGroup() : Thread.currentThread().getThreadGroup();
+        id_ = id;
+    }    
+    
+    public Thread newThread(Runnable runnable)
+    {        
+        String name = id_ + ":" + threadNbr_.getAndIncrement();       
+        Thread thread = new Thread(threadGroup_, runnable, name);        
+        return thread;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ThreadLocalContext.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ThreadLocalContext.java
index e69de29b..f7b0259f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ThreadLocalContext.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/concurrent/ThreadLocalContext.java
@@ -0,0 +1,42 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+/**
+ * Use this implementation over Java's ThreadLocal or InheritableThreadLocal when 
+ * you need to add multiple key/value pairs into ThreadLocalContext for a given thread.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+
+public class ThreadLocalContext
+{
+    private static InheritableThreadLocal<Context> tls_ = new InheritableThreadLocal<Context>();
+
+    public static void put(Context value)
+    {
+        tls_.set(value);
+    }
+
+    public static Context get()
+    {
+        return tls_.get();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/config/CFMetaData.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/config/CFMetaData.java
index e69de29b..0e5e82e7 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/config/CFMetaData.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/config/CFMetaData.java
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.config;
+
+public class CFMetaData
+{
+    public String tableName;            // name of table which has this column family
+    public String cfName;               // name of the column family
+    public String columnType;           // type: super, standard, etc.
+    public String indexProperty_;       // name sorted, time stamp sorted etc. 
+
+    // The user chosen names (n_) for various parts of data in a column family.
+    // CQL queries, for instance, will refer to/extract data within a column
+    // family using these logical names.
+    public String n_rowKey;               
+    public String n_superColumnMap;     // only used if this is a super column family
+    public String n_superColumnKey;     // only used if this is a super column family
+    public String n_columnMap;
+    public String n_columnKey;
+    public String n_columnValue;
+    public String n_columnTimestamp;
+ 
+    // a quick and dirty pretty printer for describing the column family...
+    public String pretty()
+    {
+        String desc;
+        desc = n_columnMap + "(" + n_columnKey + ", " + n_columnValue + ", " + n_columnTimestamp + ")";
+        if ("Super".equals(columnType))
+        {
+            desc = n_superColumnMap + "(" + n_superColumnKey + ", " + desc + ")"; 
+        }
+        desc = tableName + "." + cfName + "(" + n_rowKey + ", " + desc + ")\n";
+        
+        desc += "Column Family Type: " + columnType + "\n" +
+                "Columns Sorted By: " + indexProperty_ + "\n";
+        return desc;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
index e69de29b..a29f72d3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
@@ -0,0 +1,813 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.config;
+
+import java.util.*;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.io.*;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.db.TypeInfo;
+import org.apache.cassandra.db.SystemTable;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.XMLUtils;
+import org.w3c.dom.Node;
+import org.w3c.dom.NodeList;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class DatabaseDescriptor
+{
+    private static Logger logger_ = Logger.getLogger(DatabaseDescriptor.class);
+
+    public static final String random_ = "RANDOM";
+    public static final String ophf_ = "OPHF";
+    private static int storagePort_ = 7000;
+    private static int controlPort_ = 7001;
+    private static int httpPort_ = 7002;
+    private static int thriftPort_ = 9160;
+    private static String listenAddress_; // leave null so we can fall through to getLocalHost
+    private static String clusterName_ = "Test";
+    private static int replicationFactor_ = 3;
+    private static long rpcTimeoutInMillis_ = 2000;
+    private static Set<String> seeds_ = new HashSet<String>();
+    private static String metadataDirectory_;
+    private static String snapshotDirectory_;
+    /* Keeps the list of Ganglia servers to contact */
+    private static String[] gangliaServers_ ;
+    /* Keeps the list of data file directories */
+    private static String[] dataFileDirectories_;
+    /* Current index into the above list of directories */
+    private static int currentIndex_ = 0;
+    private static String logFileDirectory_;
+    private static String bootstrapFileDirectory_;
+    private static int logRotationThreshold_ = 128*1024*1024;
+    private static boolean fastSync_ = false;
+    private static boolean rackAware_ = false;
+    private static int threadsPerPool_ = 4;
+    private static List<String> tables_ = new ArrayList<String>();
+    private static Set<String> applicationColumnFamilies_ = new HashSet<String>();
+
+    // Default descriptive names for use in CQL. The user can override
+    // these choices in the config file. These are not case sensitive.
+    // Hence, these are stored in UPPER case for easy comparison.
+    private static String d_rowKey_           = "ROW_KEY";
+    private static String d_superColumnMap_   = "SUPER_COLUMN_MAP";
+    private static String d_superColumnKey_   = "SUPER_COLUMN_KEY";
+    private static String d_columnMap_        = "COLUMN_MAP";
+    private static String d_columnKey_        = "COLUMN_KEY";
+    private static String d_columnValue_      = "COLUMN_VALUE";
+    private static String d_columnTimestamp_  = "COLUMN_TIMESTAMP";
+
+    /*
+     * A map from table names to the set of column families for the table and the
+     * corresponding meta data for that column family.
+    */
+    private static Map<String, Map<String, CFMetaData>> tableToCFMetaDataMap_;
+    /* Hashing strategy Random or OPHF */
+    private static String partitionerClass_;
+    /* if the size of columns or super-columns are more than this, indexing will kick in */
+    private static int columnIndexSizeInKB_;
+    /* Size of touch key cache */
+    private static int touchKeyCacheSize_ = 1024;
+    /* Number of hours to keep a memtable in memory */
+    private static int memtableLifetime_ = 6;
+    /* Size of the memtable in memory before it is dumped */
+    private static int memtableSize_ = 128;
+    /* Number of objects in millions in the memtable before it is dumped */
+    private static int memtableObjectCount_ = 1;
+    /* 
+     * This parameter enables or disables consistency checks. 
+     * If set to false the read repairs are disable for very
+     * high throughput on reads but at the cost of consistency.
+    */
+    private static boolean doConsistencyCheck_ = true;
+    /* Address of ZooKeeper cell */
+    private static String zkAddress_;
+    /* Callout directories */
+    private static String calloutLocation_;
+    /* Job Jar Location */
+    private static String jobJarFileLocation_;
+    /* Address where to run the job tracker */
+    private static String jobTrackerHost_;    
+    /* Zookeeper session timeout. */
+    private static int zkSessionTimeout_ = 30000;
+    /* time to wait before garbage collecting tombstones (deletion markers) */
+    private static int gcGraceInSeconds_ = 10 * 24 * 3600; // 10 days
+
+    // the path qualified config file (storage-conf.xml) name
+    private static String configFileName_;
+
+    static
+    {
+        try
+        {
+            String file = System.getProperty("storage-config") + System.getProperty("file.separator") + "storage-conf.xml";
+            XMLUtils xmlUtils = new XMLUtils(file);
+
+            /* Cluster Name */
+            clusterName_ = xmlUtils.getNodeValue("/Storage/ClusterName");
+
+            /* Ganglia servers contact list */
+            gangliaServers_ = xmlUtils.getNodeValues("/Storage/GangliaServers/GangliaServer");
+
+            /* ZooKeeper's address */
+            zkAddress_ = xmlUtils.getNodeValue("/Storage/ZookeeperAddress");
+
+            /* Hashing strategy */
+            partitionerClass_ = xmlUtils.getNodeValue("/Storage/Partitioner");
+            /* Callout location */
+            calloutLocation_ = xmlUtils.getNodeValue("/Storage/CalloutLocation");
+
+            /* JobTracker address */
+            jobTrackerHost_ = xmlUtils.getNodeValue("/Storage/JobTrackerHost");
+
+            /* Job Jar file location */
+            jobJarFileLocation_ = xmlUtils.getNodeValue("/Storage/JobJarFileLocation");
+
+            String gcGrace = xmlUtils.getNodeValue("/Storage/GCGraceSeconds");
+            if ( gcGrace != null )
+                gcGraceInSeconds_ = Integer.parseInt(gcGrace);
+
+            /* Zookeeper's session timeout */
+            String zkSessionTimeout = xmlUtils.getNodeValue("/Storage/ZookeeperSessionTimeout");
+            if ( zkSessionTimeout != null )
+                zkSessionTimeout_ = Integer.parseInt(zkSessionTimeout);
+
+            /* Data replication factor */
+            String replicationFactor = xmlUtils.getNodeValue("/Storage/ReplicationFactor");
+            if ( replicationFactor != null )
+                replicationFactor_ = Integer.parseInt(replicationFactor);
+
+            /* RPC Timeout */
+            String rpcTimeoutInMillis = xmlUtils.getNodeValue("/Storage/RpcTimeoutInMillis");
+            if ( rpcTimeoutInMillis != null )
+                rpcTimeoutInMillis_ = Integer.parseInt(rpcTimeoutInMillis);
+
+            /* Thread per pool */
+            String threadsPerPool = xmlUtils.getNodeValue("/Storage/ThreadsPerPool");
+            if ( threadsPerPool != null )
+                threadsPerPool_ = Integer.parseInt(threadsPerPool);
+
+            /* TCP port on which the storage system listens */
+            String port = xmlUtils.getNodeValue("/Storage/StoragePort");
+            if ( port != null )
+                storagePort_ = Integer.parseInt(port);
+
+            /* Local IP or hostname to bind services to */
+            String listenAddress = xmlUtils.getNodeValue("/Storage/ListenAddress");
+            if ( listenAddress != null)
+                listenAddress_ = listenAddress;
+            
+            /* UDP port for control messages */
+            port = xmlUtils.getNodeValue("/Storage/ControlPort");
+            if ( port != null )
+                controlPort_ = Integer.parseInt(port);
+
+            /* HTTP port for HTTP messages */
+            port = xmlUtils.getNodeValue("/Storage/HttpPort");
+            if ( port != null )
+                httpPort_ = Integer.parseInt(port);
+
+            /* get the thrift port from conf file */
+            port = xmlUtils.getNodeValue("/Storage/ThriftPort");
+            if (port != null)
+                thriftPort_ = Integer.parseInt(port);
+
+
+            /* Touch Key Cache Size */
+            String touchKeyCacheSize = xmlUtils.getNodeValue("/Storage/TouchKeyCacheSize");
+            if ( touchKeyCacheSize != null )
+                touchKeyCacheSize_ = Integer.parseInt(touchKeyCacheSize);
+
+            /* Number of days to keep the memtable around w/o flushing */
+            String lifetime = xmlUtils.getNodeValue("/Storage/MemtableLifetimeInDays");
+            if ( lifetime != null )
+                memtableLifetime_ = Integer.parseInt(lifetime);
+
+            /* Size of the memtable in memory in MB before it is dumped */
+            String memtableSize = xmlUtils.getNodeValue("/Storage/MemtableSizeInMB");
+            if ( memtableSize != null )
+                memtableSize_ = Integer.parseInt(memtableSize);
+            /* Number of objects in millions in the memtable before it is dumped */
+            String memtableObjectCount = xmlUtils.getNodeValue("/Storage/MemtableObjectCountInMillions");
+            if ( memtableObjectCount != null )
+                memtableObjectCount_ = Integer.parseInt(memtableObjectCount);
+
+            /* This parameter enables or disables consistency checks.
+             * If set to false the read repairs are disable for very
+             * high throughput on reads but at the cost of consistency.*/
+            String doConsistencyCheck = xmlUtils.getNodeValue("/Storage/DoConsistencyChecksBoolean");
+            if ( doConsistencyCheck != null )
+                doConsistencyCheck_ = Boolean.parseBoolean(doConsistencyCheck);
+
+
+            /* read the size at which we should do column indexes */
+            String columnIndexSizeInKB = xmlUtils.getNodeValue("/Storage/ColumnIndexSizeInKB");
+            if(columnIndexSizeInKB == null)
+            {
+                columnIndexSizeInKB_ = 64;
+            }
+            else
+            {
+                columnIndexSizeInKB_ = Integer.parseInt(columnIndexSizeInKB);
+            }
+
+            /* metadata directory */
+            metadataDirectory_ = xmlUtils.getNodeValue("/Storage/MetadataDirectory");
+            if (metadataDirectory_ == null)
+            {
+                throw new ConfigurationException("MetadataDirectory must be specified");
+            }
+            FileUtils.createDirectory(metadataDirectory_);
+
+            /* snapshot directory */
+            snapshotDirectory_ = xmlUtils.getNodeValue("/Storage/SnapshotDirectory");
+            if ( snapshotDirectory_ != null )
+                FileUtils.createDirectory(snapshotDirectory_);
+            else
+            {
+                snapshotDirectory_ = metadataDirectory_ + System.getProperty("file.separator") + "snapshot";
+            }
+
+            /* data file directory */
+            dataFileDirectories_ = xmlUtils.getNodeValues("/Storage/DataFileDirectories/DataFileDirectory");
+            if (dataFileDirectories_.length == 0)
+            {
+                throw new ConfigurationException("At least one DataFileDirectory must be specified");
+            }
+            for ( String dataFileDirectory : dataFileDirectories_ )
+                FileUtils.createDirectory(dataFileDirectory);
+
+            /* bootstrap file directory */
+            bootstrapFileDirectory_ = xmlUtils.getNodeValue("/Storage/BootstrapFileDirectory");
+            if (bootstrapFileDirectory_ == null)
+            {
+                throw new ConfigurationException("BootstrapFileDirectory must be specified");
+            }
+            FileUtils.createDirectory(bootstrapFileDirectory_);
+
+            /* commit log directory */
+            logFileDirectory_ = xmlUtils.getNodeValue("/Storage/CommitLogDirectory");
+            if (logFileDirectory_ == null)
+            {
+                throw new ConfigurationException("CommitLogDirectory must be specified");
+            }
+            FileUtils.createDirectory(logFileDirectory_);
+
+            /* threshold after which commit log should be rotated. */
+            String value = xmlUtils.getNodeValue("/Storage/CommitLogRotationThresholdInMB");
+            if ( value != null)
+                logRotationThreshold_ = Integer.parseInt(value) * 1024 * 1024;
+
+            /* fast sync option */
+            value = xmlUtils.getNodeValue("/Storage/CommitLogFastSync");
+            if ( value != null )
+                fastSync_ = Boolean.parseBoolean(value);
+
+            tableToCFMetaDataMap_ = new HashMap<String, Map<String, CFMetaData>>();
+
+            /* Rack Aware option */
+            value = xmlUtils.getNodeValue("/Storage/RackAware");
+            if ( value != null )
+                rackAware_ = Boolean.parseBoolean(value);
+
+            /* Read the table related stuff from config */
+            NodeList tables = xmlUtils.getRequestedNodeList("/Storage/Tables/Table");
+            int size = tables.getLength();
+            for ( int i = 0; i < size; ++i )
+            {
+                Node table = tables.item(i);
+
+                /* parsing out the table name */
+                String tName = XMLUtils.getAttributeValue(table, "Name");
+                if (tName == null)
+                {
+                    throw new ConfigurationException("Table name attribute is required");
+                }
+                tables_.add(tName);
+                tableToCFMetaDataMap_.put(tName, new HashMap<String, CFMetaData>());
+
+                String xqlTable = "/Storage/Tables/Table[@Name='" + tName + "']/";
+                NodeList columnFamilies = xmlUtils.getRequestedNodeList(xqlTable + "ColumnFamily");
+
+                // get name of the rowKey for this table
+                String n_rowKey = xmlUtils.getNodeValue(xqlTable + "RowKey");
+                if (n_rowKey == null)
+                    n_rowKey = d_rowKey_;
+
+                //NodeList columnFamilies = xmlUtils.getRequestedNodeList(table, "ColumnFamily");
+                int size2 = columnFamilies.getLength();
+
+                for ( int j = 0; j < size2; ++j )
+                {
+                    Node columnFamily = columnFamilies.item(j);
+                    String cName = XMLUtils.getAttributeValue(columnFamily, "Name");
+                    if (cName == null)
+                    {
+                        throw new ConfigurationException("ColumnFamily name attribute is required");
+                    }
+                    String xqlCF = xqlTable + "ColumnFamily[@Name='" + cName + "']/";
+
+                    /* squirrel away the application column families */
+                    applicationColumnFamilies_.add(cName);
+
+                    // Parse out the column type
+                    String rawColumnType = XMLUtils.getAttributeValue(columnFamily, "ColumnType");
+                    String columnType = ColumnFamily.getColumnType(rawColumnType);
+                    if (columnType == null)
+                    {
+                        throw new ConfigurationException("Column " + cName + " has invalid type " + rawColumnType);
+                    }
+
+                    // Parse out the column family sorting property for columns
+                    String rawColumnIndexType = XMLUtils.getAttributeValue(columnFamily, "ColumnSort");
+                    String columnIndexType = ColumnFamily.getColumnSortProperty(rawColumnIndexType);
+                    if (columnIndexType == null)
+                    {
+                        throw new ConfigurationException("invalid column sort value " + rawColumnIndexType);
+                    }
+
+                    // Parse out user-specified logical names for the various dimensions
+                    // of a the column family from the config.
+                    String n_superColumnMap = xmlUtils.getNodeValue(xqlCF + "SuperColumnMap");
+                    if (n_superColumnMap == null)
+                        n_superColumnMap = d_superColumnMap_;
+
+                    String n_superColumnKey = xmlUtils.getNodeValue(xqlCF + "SuperColumnKey");
+                    if (n_superColumnKey == null)
+                        n_superColumnKey = d_superColumnKey_;
+
+                    String n_columnMap = xmlUtils.getNodeValue(xqlCF + "ColumnMap");
+                    if (n_columnMap == null)
+                        n_columnMap = d_columnMap_;
+
+                    String n_columnKey = xmlUtils.getNodeValue(xqlCF + "ColumnKey");
+                    if (n_columnKey == null)
+                        n_columnKey = d_columnKey_;
+
+                    String n_columnValue = xmlUtils.getNodeValue(xqlCF + "ColumnValue");
+                    if (n_columnValue == null)
+                        n_columnValue = d_columnValue_;
+
+                    String n_columnTimestamp = xmlUtils.getNodeValue(xqlCF + "ColumnTimestamp");
+                    if (n_columnTimestamp == null)
+                        n_columnTimestamp = d_columnTimestamp_;
+
+                    // now populate the column family meta data and
+                    // insert it into the table dictionary.
+                    CFMetaData cfMetaData = new CFMetaData();
+
+                    cfMetaData.tableName = tName;
+                    cfMetaData.cfName = cName;
+
+                    cfMetaData.columnType = columnType;
+                    cfMetaData.indexProperty_ = columnIndexType;
+
+                    cfMetaData.n_rowKey = n_rowKey;
+                    cfMetaData.n_columnMap = n_columnMap;
+                    cfMetaData.n_columnKey = n_columnKey;
+                    cfMetaData.n_columnValue = n_columnValue;
+                    cfMetaData.n_columnTimestamp = n_columnTimestamp;
+                    if ("Super".equals(columnType))
+                    {
+                        cfMetaData.n_superColumnKey = n_superColumnKey;
+                        cfMetaData.n_superColumnMap = n_superColumnMap;
+                    }
+
+                    tableToCFMetaDataMap_.get(tName).put(cName, cfMetaData);
+                }
+            }
+
+            /* Load the seeds for node contact points */
+            String[] seeds = xmlUtils.getNodeValues("/Storage/Seeds/Seed");
+            for( int i = 0; i < seeds.length; ++i )
+            {
+                seeds_.add( seeds[i] );
+            }
+        }
+        catch (ConfigurationException e)
+        {
+            logger_.error("Fatal error: " + e.getMessage());
+            System.exit(1);
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+        
+        try
+        {
+            storeMetadata();
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+    
+
+    /*
+     * Create the metadata tables. This table has information about
+     * the table name and the column families that make up the table.
+     * Each column family also has an associated ID which is an int.
+    */
+    private static void storeMetadata() throws IOException
+    {
+        AtomicInteger idGenerator = new AtomicInteger(0);
+        Set<String> tables = tableToCFMetaDataMap_.keySet();
+
+        for ( String table : tables )
+        {
+            Table.TableMetadata tmetadata = Table.TableMetadata.instance();
+            if (tmetadata.isEmpty())
+            {
+                tmetadata = Table.TableMetadata.instance();
+                /* Column families associated with this table */
+                Map<String, CFMetaData> columnFamilies = tableToCFMetaDataMap_.get(table);
+
+                for (String columnFamily : columnFamilies.keySet())
+                {
+                    tmetadata.add(columnFamily, idGenerator.getAndIncrement(), DatabaseDescriptor.getColumnType(columnFamily));
+                }
+
+                /*
+                 * Here we add all the system related column families.
+                */
+                /* Add the TableMetadata column family to this map. */
+                tmetadata.add(Table.TableMetadata.cfName_, idGenerator.getAndIncrement());
+                /* Add the LocationInfo column family to this map. */
+                tmetadata.add(SystemTable.cfName_, idGenerator.getAndIncrement());
+                /* Add the recycle column family to this map. */
+                tmetadata.add(Table.recycleBin_, idGenerator.getAndIncrement());
+                /* Add the Hints column family to this map. */
+                tmetadata.add(Table.hints_, idGenerator.getAndIncrement(), ColumnFamily.getColumnType("Super"));
+                tmetadata.apply();
+                idGenerator.set(0);
+            }
+        }
+    }
+
+    public static int getGcGraceInSeconds()
+    {
+        return gcGraceInSeconds_;
+    }
+
+    public static String getPartitionerClass()
+    {
+        return partitionerClass_;
+    }
+    
+    public static String getZkAddress()
+    {
+        return zkAddress_;
+    }
+    
+    public static String getCalloutLocation()
+    {
+        return calloutLocation_;
+    }
+    
+    public static String getJobTrackerAddress()
+    {
+        return jobTrackerHost_;
+    }
+    
+    public static int getZkSessionTimeout()
+    {
+        return zkSessionTimeout_;
+    }
+
+    public static int getColumnIndexSize()
+    {
+    	return columnIndexSizeInKB_ * 1024;
+    }
+
+   
+    public static int getMemtableLifetime()
+    {
+      return memtableLifetime_;
+    }
+
+    public static int getMemtableSize()
+    {
+      return memtableSize_;
+    }
+
+    public static int getMemtableObjectCount()
+    {
+      return memtableObjectCount_;
+    }
+
+    public static boolean getConsistencyCheck()
+    {
+      return doConsistencyCheck_;
+    }
+
+    public static String getClusterName()
+    {
+        return clusterName_;
+    }
+
+    public static String getConfigFileName() {
+        return configFileName_;
+    }
+    
+    public static boolean isApplicationColumnFamily(String columnFamily)
+    {
+        return applicationColumnFamilies_.contains(columnFamily);
+    }
+
+    public static int getTouchKeyCacheSize()
+    {
+        return touchKeyCacheSize_;
+    }
+    
+    public static String getJobJarLocation()
+    {
+        return jobJarFileLocation_;
+    }
+
+    public static String getGangliaServers()
+    {
+    	StringBuilder sb = new StringBuilder();
+    	for ( int i = 0; i < gangliaServers_.length; ++i )
+    	{
+    		sb.append(gangliaServers_[i]);
+    		if ( i != (gangliaServers_.length - 1) )
+    			sb.append(", ");
+    	}
+    	return sb.toString();
+    }
+    
+    public static Map<String, CFMetaData> getTableMetaData(String table)
+    {
+        return tableToCFMetaDataMap_.get(table);
+    }
+
+    /*
+     * Given a table name & column family name, get the column family
+     * meta data. If the table name or column family name is not valid
+     * this function returns null.
+     */
+    public static CFMetaData getCFMetaData(String table, String cfName)
+    {
+        Map<String, CFMetaData> cfInfo = tableToCFMetaDataMap_.get(table);
+        if (cfInfo == null)
+            return null;
+        
+        return cfInfo.get(cfName);
+    }
+    
+    public static String getColumnType(String cfName)
+    {
+        String table = getTables().get(0);
+        CFMetaData cfMetaData = getCFMetaData(table, cfName);
+        
+        if (cfMetaData == null)
+            return null;
+        return cfMetaData.columnType;
+    }
+
+    public static boolean isNameSortingEnabled(String cfName)
+    {
+        String table = getTables().get(0);
+        CFMetaData cfMetaData = getCFMetaData(table, cfName);
+
+        if (cfMetaData == null)
+            return false;
+
+    	return "Name".equals(cfMetaData.indexProperty_);
+    }
+    
+    public static boolean isTimeSortingEnabled(String cfName)
+    {
+        String table = getTables().get(0);
+        CFMetaData cfMetaData = getCFMetaData(table, cfName);
+
+        if (cfMetaData == null)
+            return false;
+
+        return "Time".equals(cfMetaData.indexProperty_);
+    }
+    
+
+    public static List<String> getTables()
+    {
+        return tables_;
+    }
+
+    public static void  setTables(String table)
+    {
+        tables_.add(table);
+    }
+
+    public static int getStoragePort()
+    {
+        return storagePort_;
+    }
+
+    public static int getControlPort()
+    {
+        return controlPort_;
+    }
+
+    public static int getHttpPort()
+    {
+        return httpPort_;
+    }
+
+    public static int getThriftPort()
+    {
+        return thriftPort_;
+    }
+
+    public static int getReplicationFactor()
+    {
+        return replicationFactor_;
+    }
+
+    public static long getRpcTimeout()
+    {
+        return rpcTimeoutInMillis_;
+    }
+
+    public static int getThreadsPerPool()
+    {
+        return threadsPerPool_;
+    }
+
+    public static String getMetadataDirectory()
+    {
+        return metadataDirectory_;
+    }
+
+    public static void setMetadataDirectory(String metadataDirectory)
+    {
+        metadataDirectory_ = metadataDirectory;
+    }
+
+    public static String getSnapshotDirectory()
+    {
+        return snapshotDirectory_;
+    }
+
+    public static void setSnapshotDirectory(String snapshotDirectory)
+    {
+    	snapshotDirectory_ = snapshotDirectory;
+    }
+
+    public static String[] getAllDataFileLocations()
+    {
+        return dataFileDirectories_;
+    }
+
+    public static String getDataFileLocation()
+    {
+    	String dataFileDirectory = dataFileDirectories_[currentIndex_];
+        return dataFileDirectory;
+    }
+    
+    public static String getCompactionFileLocation()
+    {
+    	String dataFileDirectory = dataFileDirectories_[currentIndex_];
+    	currentIndex_ = (currentIndex_ + 1 )%dataFileDirectories_.length ;
+        return dataFileDirectory;
+    }
+
+    public static String getBootstrapFileLocation()
+    {
+        return bootstrapFileDirectory_;
+    }
+
+    public static void setBootstrapFileLocation(String bfLocation)
+    {
+        bootstrapFileDirectory_ = bfLocation;
+    }
+
+    public static int getLogFileSizeThreshold()
+    {
+        return logRotationThreshold_;
+    }
+
+    public static String getLogFileLocation()
+    {
+        return logFileDirectory_;
+    }
+
+    public static void setLogFileLocation(String logLocation)
+    {
+        logFileDirectory_ = logLocation;
+    }
+
+    public static boolean isFastSync()
+    {
+        return fastSync_;
+    }
+
+    public static boolean isRackAware()
+    {
+        return rackAware_;
+    }
+
+    public static Set<String> getSeeds()
+    {
+        return seeds_;
+    }
+
+    public static String getColumnFamilyType(String cfName)
+    {
+        String cfType = getColumnType(cfName);
+        if ( cfType == null )
+            cfType = "Standard";
+    	return cfType;
+    }
+
+    /*
+     * Loop through all the disks to see which disk has the max free space
+     * return the disk with max free space for compactions. If the size of the expected
+     * compacted file is greater than the max disk space available return null, we cannot
+     * do compaction in this case.
+     */
+    public static String getCompactionFileLocation(long expectedCompactedFileSize)
+    {
+      long maxFreeDisk = 0;
+      int maxDiskIndex = 0;
+      String dataFileDirectory = null;
+      for ( int i = 0 ; i < dataFileDirectories_.length ; i++ )
+      {
+        File f = new File(dataFileDirectories_[i]);
+        if( maxFreeDisk < f.getUsableSpace())
+        {
+          maxFreeDisk = f.getUsableSpace();
+          maxDiskIndex = i;
+        }
+      }
+      // Load factor of 0.9 we do not want to use the entire disk that is too risky.
+      maxFreeDisk = (long)(0.9 * maxFreeDisk);
+      if( expectedCompactedFileSize < maxFreeDisk )
+      {
+        dataFileDirectory = dataFileDirectories_[maxDiskIndex];
+        currentIndex_ = (maxDiskIndex + 1 )%dataFileDirectories_.length ;
+      }
+      else
+      {
+        currentIndex_ = maxDiskIndex;
+      }
+        return dataFileDirectory;
+    }
+    
+    public static TypeInfo getTypeInfo(String cfName)
+    {
+        String table = DatabaseDescriptor.getTables().get(0);
+        CFMetaData cfMetadata = DatabaseDescriptor.getCFMetaData(table, cfName);
+        if ( cfMetadata.indexProperty_.equals("Name") )
+        {
+            return TypeInfo.STRING;
+        }
+        else
+        {
+            return TypeInfo.LONG;
+        }
+    }
+
+    public static Map<String, Map<String, CFMetaData>> getTableToColumnFamilyMap()
+    {
+        return tableToCFMetaDataMap_;
+    }
+
+    private static class ConfigurationException extends Exception
+    {
+        public ConfigurationException(String message)
+        {
+            super(message);
+        }
+    }
+
+    public static String getListenAddress()
+    {
+        return listenAddress_;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/CAgent.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/CAgent.java
index e69de29b..7963fe2e 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/CAgent.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/CAgent.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.continuations;
+
+import java.lang.instrument.Instrumentation;
+
+import org.apache.commons.javaflow.bytecode.transformation.bcel.BcelClassTransformer;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class CAgent
+{
+    public static void premain(String agentArguments, Instrumentation instrumentation)
+    {        
+        instrumentation.addTransformer(new ContinuationClassTransformer(agentArguments, new BcelClassTransformer()));        
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/ContinuationClassTransformer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/ContinuationClassTransformer.java
index e69de29b..9199d081 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/ContinuationClassTransformer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/ContinuationClassTransformer.java
@@ -0,0 +1,74 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.continuations;
+
+import java.lang.annotation.Annotation;
+import java.lang.instrument.ClassFileTransformer;
+import java.lang.instrument.IllegalClassFormatException;
+import java.security.ProtectionDomain;
+import java.util.List;
+import java.util.Set;
+import java.util.HashSet;
+import java.util.StringTokenizer;
+import org.objectweb.asm.ClassReader;
+import org.objectweb.asm.tree.AnnotationNode;
+import org.objectweb.asm.tree.ClassNode;
+import org.objectweb.asm.tree.MethodNode;
+import org.apache.commons.javaflow.bytecode.transformation.ResourceTransformer;
+import org.apache.commons.javaflow.bytecode.transformation.bcel.BcelClassTransformer;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class ContinuationClassTransformer implements ClassFileTransformer
+{
+    private static final String targetAnnotation_ = "Suspendable"; 
+    private ResourceTransformer transformer_;    
+    
+    public ContinuationClassTransformer(String agentArguments, ResourceTransformer transformer)
+    {
+        super();
+        transformer_ = transformer;        
+    }
+    
+    public byte[] transform(ClassLoader classLoader, String className, Class redefiningClass, ProtectionDomain domain, byte[] bytes) throws IllegalClassFormatException
+    {              
+        /*
+         * Use the ASM class reader to see which classes support
+         * the Suspendable annotation. If they do then those 
+         * classes need to have their bytecodes transformed for
+         * Continuation support. 
+        */                
+        ClassReader classReader = new ClassReader(bytes);
+        ClassNode classNode = new ClassNode();
+        classReader.accept(classNode, true);       
+        List<AnnotationNode> annotationNodes = classNode.visibleAnnotations;
+        
+        for( AnnotationNode annotationNode : annotationNodes )
+        {            
+            if (annotationNode.desc.contains(ContinuationClassTransformer.targetAnnotation_))
+            {                
+                bytes = transformer_.transform(bytes);
+            }
+        }                                
+        return bytes;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/Suspendable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/Suspendable.java
index e69de29b..9b85232f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/Suspendable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/continuations/Suspendable.java
@@ -0,0 +1,33 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.continuations;
+
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+
+@Retention(RetentionPolicy.RUNTIME)
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public @interface Suspendable
+{
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/BindOperand.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/BindOperand.java
index e69de29b..2ea2b9af 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/BindOperand.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/BindOperand.java
@@ -0,0 +1,51 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import org.apache.cassandra.cql.execution.RuntimeErrorMsg;
+
+/**
+ * BindOperand: 
+ * Represents a bind variable in the CQL statement. Lives
+ * in the shared execution plan.
+ */
+public class BindOperand implements OperandDef 
+{
+    int bindIndex_;  // bind position
+
+    public BindOperand(int bindIndex)
+    {
+        bindIndex_ = bindIndex;
+    }
+
+    public Object get()
+    {
+        // TODO: Once bind variables are supported, the get() will extract
+        // the value of the bind at position "bindIndex_" from the execution
+        // context.
+        throw new RuntimeException(RuntimeErrorMsg.IMPLEMENTATION_RESTRICTION
+                                   .getMsg("bind params not yet supported"));
+    }
+    
+    public String explain()
+    {
+        return "Bind #: " + bindIndex_;
+    }
+
+};
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CExpr.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CExpr.java
index e69de29b..fe78a2f3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CExpr.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CExpr.java
@@ -0,0 +1,30 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+//Note: This class is CQL related work in progress.
+public class CExpr
+{
+    public static interface Expr
+    {
+        CType  getType();
+        String toString();
+    };
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CType.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CType.java
index e69de29b..a1d513bb 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CType.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CType.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.cql.common;
+
+import java.util.ArrayList;
+
+//Note: This class is CQL related work in progress.
+public class CType
+{
+    public static interface Type
+    {
+        String toString(); 
+    };
+
+    public static class IntegerType implements Type
+    {
+        public String toString() { return "Integer"; };
+    }
+
+    public static class StringType implements Type
+    {
+        public String toString() { return "String"; };
+    }
+
+    public static class RowType implements Type
+    {
+        ArrayList<Type> types_;
+        public RowType(ArrayList<Type> types)
+        {
+            types_ = types;
+        }
+
+        public String toString()
+        {
+            StringBuffer sb = new StringBuffer("<");
+            for (int idx = types_.size(); idx > 0; idx--)
+            {
+                sb.append(types_.toString());
+                if (idx != 1)
+                {
+                    sb.append(", ");
+                }
+            }
+            sb.append(">");
+            return sb.toString();
+        }
+    }
+
+    public static class ArrayType
+    {
+        Type elementType_;
+        public ArrayType(Type elementType)
+        {
+            elementType_ = elementType;
+        }
+
+        public String toString()
+        {
+            return "Array(" + elementType_.toString() + ")";
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ColumnMapExpr.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ColumnMapExpr.java
index e69de29b..35f40723 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ColumnMapExpr.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ColumnMapExpr.java
@@ -0,0 +1,25 @@
+/* Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import java.util.ArrayList;
+
+public class ColumnMapExpr extends ArrayList<Pair<OperandDef, OperandDef>>
+{
+    private static final long serialVersionUID = 1L;
+};
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ColumnRangeQueryRSD.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ColumnRangeQueryRSD.java
index e69de29b..eaa58f70 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ColumnRangeQueryRSD.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ColumnRangeQueryRSD.java
@@ -0,0 +1,172 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.List;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.cql.execution.RuntimeErrorMsg;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.SliceReadCommand;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * A Row Source Defintion (RSD) for doing a range query on a column map
+ * (in Standard or Super Column Family).
+ */
+public class ColumnRangeQueryRSD extends RowSourceDef
+{
+    private final static Logger logger_ = Logger.getLogger(ColumnRangeQueryRSD.class);
+    private CFMetaData cfMetaData_;
+    private OperandDef rowKey_;
+    private OperandDef superColumnKey_;
+    private int        offset_;
+    private int        limit_;
+
+    /**
+     * Set up a range query on column map in a simple column family.
+     * The column map in a simple column family is identified by the rowKey.
+     * 
+     * Note: "limit" of -1 is the equivalent of no limit.
+     *       "offset" specifies the number of rows to skip. An offset of 0 implies from the first row.
+     */
+    public ColumnRangeQueryRSD(CFMetaData cfMetaData, OperandDef rowKey, int offset, int limit)
+    {
+        cfMetaData_     = cfMetaData;
+        rowKey_         = rowKey;
+        superColumnKey_ = null;
+        offset_         = offset;
+        limit_          = limit;
+    }
+
+    /**
+     * Setup a range query on a column map in a super column family.
+     * The column map in a super column family is identified by the rowKey & superColumnKey.
+     *  
+     * Note: "limit" of -1 is the equivalent of no limit.
+     *       "offset" specifies the number of rows to skip. An offset of 0 implies the first row.  
+     */
+    public ColumnRangeQueryRSD(CFMetaData cfMetaData, ConstantOperand rowKey, ConstantOperand superColumnKey,
+                               int offset, int limit)
+    {
+        cfMetaData_     = cfMetaData;
+        rowKey_         = rowKey;
+        superColumnKey_ = superColumnKey;
+        offset_         = offset;
+        limit_          = limit;
+    }
+
+    public List<Map<String,String>> getRows()
+    {
+        String columnFamily_column;
+        String superColumnKey = null;      
+
+        if (superColumnKey_ != null)
+        {
+            superColumnKey = (String)(superColumnKey_.get());
+            columnFamily_column = cfMetaData_.cfName + ":" + superColumnKey;
+        }
+        else
+        {
+            columnFamily_column = cfMetaData_.cfName;
+        }
+
+        Row row = null;
+        try
+        {
+            String key = (String)(rowKey_.get());
+            ReadCommand readCommand = new SliceReadCommand(cfMetaData_.tableName, key, columnFamily_column, offset_, limit_);
+            row = StorageProxy.readProtocol(readCommand, StorageService.ConsistencyLevel.WEAK);
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+            throw new RuntimeException(RuntimeErrorMsg.GENERIC_ERROR.getMsg());
+        }
+
+        List<Map<String, String>> rows = new LinkedList<Map<String, String>>();
+        if (row != null)
+        {
+            Map<String, ColumnFamily> cfMap = row.getColumnFamilyMap();
+            if (cfMap != null && cfMap.size() > 0)
+            {
+                ColumnFamily cfamily = cfMap.get(cfMetaData_.cfName);
+                if (cfamily != null)
+                {
+                    Collection<IColumn> columns = null;
+                    if (superColumnKey_ != null)
+                    {
+                        // this is the super column case 
+                        IColumn column = cfamily.getColumn(superColumnKey);
+                        if (column != null)
+                            columns = column.getSubColumns();
+                    }
+                    else
+                    {
+                        columns = cfamily.getAllColumns();
+                    }
+
+                    if (columns != null && columns.size() > 0)
+                    {
+                        for (IColumn column : columns)
+                        {
+                            Map<String, String> result = new HashMap<String, String>();
+                            
+                            result.put(cfMetaData_.n_columnKey, column.name());
+                            result.put(cfMetaData_.n_columnValue, new String(column.value()));
+                            result.put(cfMetaData_.n_columnTimestamp, Long.toString(column.timestamp()));
+                            
+                            rows.add(result);
+                        }
+                    }
+                }
+            }
+        }
+        return rows;
+    }
+
+    public String explainPlan()
+    {
+        return String.format("%s Column Family: Column Range Query: \n" +
+                "  Table Name:       %s\n" +
+                "  Column Family:    %s\n" +
+                "  RowKey:           %s\n" +
+                "%s"                   +
+                "  Offset:           %d\n" +
+                "  Limit:            %d\n" +
+                "  Order By:         %s",
+                cfMetaData_.columnType,
+                cfMetaData_.tableName,
+                cfMetaData_.cfName,
+                rowKey_.explain(),
+                (superColumnKey_ == null) ? "" : "  SuperColumnKey:   " + superColumnKey_.explain() + "\n",
+                offset_, limit_,
+                cfMetaData_.indexProperty_);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ConstantOperand.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ConstantOperand.java
index e69de29b..cfabb500 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ConstantOperand.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ConstantOperand.java
@@ -0,0 +1,43 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+/**
+ * ConstantOperand:
+ * Represents a literal/constant operand in the CQL statement.
+ * Lives as part of the shared execution plan.
+ */
+public class ConstantOperand implements OperandDef 
+{
+    Object value_;
+    public ConstantOperand(Object value)
+    {
+        value_ = value;
+    }
+
+    public Object get()
+    {
+        return value_;
+    }
+
+    public String explain()
+    {
+        return "Constant: '" + value_ + "'";
+    }
+};
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CqlResult.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CqlResult.java
index e69de29b..2908e86e 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CqlResult.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/CqlResult.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.cql.common;
+
+import java.util.List;
+import java.util.Map;
+
+public class CqlResult
+{
+    public int                       errorCode; // 0 - success
+    public String                    errorTxt;
+    public List<Map<String, String>> resultSet;
+
+    public CqlResult(List<Map<String, String>> rows)
+    {
+        resultSet = rows;
+        errorTxt  = null;
+        errorCode = 0; // success
+    }
+    
+};
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/DMLPlan.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/DMLPlan.java
index e69de29b..c592473d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/DMLPlan.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/DMLPlan.java
@@ -0,0 +1,25 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+/**
+ * This class represents the execution plan for DML (data manipulation language)
+ * CQL statements. 
+ */
+public abstract class DMLPlan extends Plan {};
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ExplainPlan.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ExplainPlan.java
index e69de29b..ed421ab2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ExplainPlan.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/ExplainPlan.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import java.util.*;
+import org.apache.log4j.Logger;
+
+/**
+ * The "Plan" for the EXPLAIN PLAN statement itself!
+ * 
+ * It is nothing but a simple wrapper around the "Plan" for the statement
+ * on which an EXPLAIN PLAN has been requested.
+ */
+public class ExplainPlan extends Plan
+{
+    private final static Logger logger_ = Logger.getLogger(ExplainPlan.class);
+    
+    // the execution plan for the statement on which an 
+    // EXPLAIN PLAN was requested.
+    private Plan plan_ = null;
+
+    /**
+     *  Construct an ExplainPlan instance for the statement whose
+     * "plan" has been passed in.
+     */
+    public ExplainPlan(Plan plan)
+    {
+        plan_ = plan;
+    }
+
+    public CqlResult execute()
+    {
+        String planText = plan_.explainPlan();
+
+        List<Map<String, String>> rows = new LinkedList<Map<String, String>>(); 
+        Map<String, String> row = new HashMap<String, String>();
+        row.put("PLAN", planText);
+        rows.add(row);
+        
+        return new CqlResult(rows);
+    }
+
+    public String explainPlan()
+    {
+        // We never expect this method to get invoked for ExplainPlan instances
+        // (i.e. those that correspond to the EXPLAIN PLAN statement).
+        logger_.error("explainPlan() invoked on an ExplainPlan instance");
+        return null;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/OperandDef.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/OperandDef.java
index e69de29b..fa91e112 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/OperandDef.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/OperandDef.java
@@ -0,0 +1,33 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+/**
+ * OperandDef:
+ *
+ * The abstract definition of an operand (i.e. data item) in 
+ * CQL compiler/runtime. Examples, include a Constant operand
+ * or a Bind operand. This is the part of an operand definition
+ * that lives in the share-able execution plan.
+ */
+public abstract interface OperandDef
+{
+    public abstract Object get();
+    public abstract String explain();
+};
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Pair.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Pair.java
index e69de29b..59523bbd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Pair.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Pair.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+/* Would have expected java.util.* to have this class!
+ * Code cut-paste from wikipedia.
+ */
+
+/**
+ * Generic for representing a "typed" 2-tuple.
+ */
+public class Pair<T, S>
+{
+  public Pair(T f, S s)
+  { 
+    first = f;
+    second = s;   
+  }
+
+  public T getFirst()
+  {
+    return first;
+  }
+ 
+  public S getSecond() 
+  {
+    return second;
+  }
+ 
+  public String toString()
+  { 
+    return "(" + first.toString() + ", " + second.toString() + ")"; 
+  }
+ 
+  private T first;
+  private S second;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Plan.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Plan.java
index e69de29b..9b0d8609 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Plan.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Plan.java
@@ -0,0 +1,30 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+/**
+ * Abstract class representing the shared execution plan for a CQL
+ * statement (query or DML operation).
+ * 
+ */
+public abstract class Plan
+{
+    public abstract CqlResult execute();
+    public abstract String explainPlan();
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/QueryPlan.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/QueryPlan.java
index e69de29b..d88595c0 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/QueryPlan.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/QueryPlan.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import org.apache.log4j.Logger;
+
+/**
+ * This class represents the execution plan for Query (data retrieval) statement. 
+ */
+public class QueryPlan extends Plan
+{
+    private final static Logger logger_ = Logger.getLogger(QueryPlan.class);    
+
+    public RowSourceDef root;    // the root of the row source tree
+
+    public QueryPlan(RowSourceDef rwsDef)
+    {
+        root = rwsDef;
+    }
+    
+    public CqlResult execute()
+    {
+        if (root != null)
+        {
+            return new CqlResult(root.getRows());
+        }
+        else
+            logger_.error("No rowsource to execute");
+        return null;
+    }
+    
+    public String explainPlan()
+    {
+        return root.explainPlan();
+    }
+    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/RowSourceDef.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/RowSourceDef.java
index e69de29b..234ae0b2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/RowSourceDef.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/RowSourceDef.java
@@ -0,0 +1,48 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import java.util.List;
+import java.util.Map;
+
+/**
+ * The abstract notion of a row source definition. A row source
+ * is literally just anything that returns rows back.
+ * 
+ * The concrete implementations of row source might be things like a 
+ * column family row source, a "super column family" row source, 
+ * a table row source, etc.
+ *
+ * Note: Instances of sub-classes of this class are part of the "shared" 
+ * execution plan of CQL. And hence they should not contain any mutable
+ * (i.e. session specific) execution state. Mutable state, such a bind
+ * variable values (corresponding to say a rowKey or a column Key) are
+ * note part of the RowSourceDef tree.
+ * 
+ * [Eventually the notion of a "mutable" portion of the RowSource (RowSourceMut)
+ * will be introduced to hold session-specific execution state of the RowSource.
+ * For example, this would be needed when implementing iterator style rowsources
+ * that yields rows back one at a time as opposed to returning them in one
+ * shot.]
+ */
+public abstract class RowSourceDef
+{
+    public abstract List<Map<String,String>> getRows();
+    public abstract String explainPlan();  
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetColumnMap.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetColumnMap.java
index e69de29b..aa25da11 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetColumnMap.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetColumnMap.java
@@ -0,0 +1,133 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.cql.execution.RuntimeErrorMsg;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.cql.execution.*;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.service.*;
+
+/**
+ * Execution plan for batch setting a set of columns in a Simple/Super column family.
+ *   SET table.standard_cf[<rowKey>] = <columnMapExpr>;                
+ *   SET table.super_cf[<rowKey>][<superColumn>] = <columnMapExpr>;
+ */
+public class SetColumnMap extends DMLPlan
+{
+    private final static Logger logger_ = Logger.getLogger(SetUniqueKey.class);    
+    private CFMetaData    cfMetaData_;
+    private OperandDef    rowKey_;
+    private OperandDef    superColumnKey_;
+    private ColumnMapExpr columnMapExpr_;
+
+    /**
+     *  construct an execution plan node to set the column map for a Standard Column Family.
+     *
+     *    SET table.standard_cf[<rowKey>] = <columnMapExpr>;                
+     */
+    public SetColumnMap(CFMetaData cfMetaData, OperandDef rowKey, ColumnMapExpr columnMapExpr)
+    {
+        cfMetaData_     = cfMetaData;
+        rowKey_         = rowKey;
+        superColumnKey_ = null;        
+        columnMapExpr_  = columnMapExpr;
+    }
+
+    /**
+     * Construct an execution plan node to set the column map for a Super Column Family
+     * 
+     *   SET table.super_cf[<rowKey>][<superColumn>] = <columnMapExpr>;
+     */
+    public SetColumnMap(CFMetaData cfMetaData, OperandDef rowKey, OperandDef superColumnKey, ColumnMapExpr columnMapExpr)
+    {
+        cfMetaData_     = cfMetaData;
+        rowKey_         = rowKey;
+        superColumnKey_ = superColumnKey;
+        columnMapExpr_  = columnMapExpr;
+    }
+
+    public CqlResult execute()
+    {
+        String columnFamily_column;
+        
+        if (superColumnKey_ != null)
+        {
+            String superColumnKey = (String)(superColumnKey_.get());
+            columnFamily_column = cfMetaData_.cfName + ":" + superColumnKey + ":";
+        }
+        else
+        {
+            columnFamily_column = cfMetaData_.cfName + ":";
+        }
+
+        try
+        {
+            RowMutation rm = new RowMutation(cfMetaData_.tableName, (String)(rowKey_.get()));
+            long time = System.currentTimeMillis();
+
+            for (Pair<OperandDef, OperandDef> entry : columnMapExpr_)
+            {
+                OperandDef columnKey = entry.getFirst();
+                OperandDef value     = entry.getSecond();
+
+                rm.add(columnFamily_column + (String)(columnKey.get()), ((String)value.get()).getBytes(), time);
+            }
+            StorageProxy.insert(rm);
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+            throw new RuntimeException(RuntimeErrorMsg.GENERIC_ERROR.getMsg());            
+        }
+        return null;
+    }
+
+    public String explainPlan()
+    {
+        StringBuffer sb = new StringBuffer();
+        
+        String prefix =
+            String.format("%s Column Family: Batch SET a set of columns: \n" +
+            "   Table Name:     %s\n" +
+            "   Column Famly:   %s\n" +
+            "   RowKey:         %s\n" +
+            "%s",
+            cfMetaData_.columnType,
+            cfMetaData_.tableName,
+            cfMetaData_.cfName,
+            rowKey_.explain(),
+            (superColumnKey_ == null) ? "" : "   SuperColumnKey: " + superColumnKey_.explain() + "\n");                
+
+        for (Pair<OperandDef, OperandDef> entry : columnMapExpr_)
+        {
+            OperandDef columnKey = entry.getFirst();
+            OperandDef value     = entry.getSecond();
+            sb.append(String.format("   ColumnKey:        %s\n" +
+                                    "   Value:            %s\n",
+                                    columnKey.explain(), value.explain()));
+        }
+        
+        return prefix + sb.toString();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetSuperColumnMap.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetSuperColumnMap.java
index e69de29b..e6a19df5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetSuperColumnMap.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetSuperColumnMap.java
@@ -0,0 +1,119 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.cql.execution.RuntimeErrorMsg;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.cql.execution.*;
+
+/**
+ * Execution plan for batch setting a set of super columns in a Super column family.
+  *   SET table.super_cf[<rowKey>] = <superColumnMapExpr>;
+ */
+public class SetSuperColumnMap extends DMLPlan
+{
+    private final static Logger logger_ = Logger.getLogger(SetUniqueKey.class);    
+    private CFMetaData         cfMetaData_;
+    private OperandDef         rowKey_;
+    private SuperColumnMapExpr superColumnMapExpr_;
+
+    /**
+     *  construct an execution plan node to batch set a bunch of super columns in a 
+     *  super column family.
+     *
+     *    SET table.super_cf[<rowKey>] = <superColumnMapExpr>;
+     */
+    public SetSuperColumnMap(CFMetaData cfMetaData, OperandDef rowKey, SuperColumnMapExpr superColumnMapExpr)
+    {
+        cfMetaData_         = cfMetaData;
+        rowKey_             = rowKey;
+        superColumnMapExpr_ = superColumnMapExpr;
+    }
+    
+    public CqlResult execute()
+    {
+        try
+        {
+            RowMutation rm = new RowMutation(cfMetaData_.tableName, (String)(rowKey_.get()));
+            long time = System.currentTimeMillis();
+
+            for (Pair<OperandDef, ColumnMapExpr> superColumn : superColumnMapExpr_)
+            {
+                OperandDef    superColumnKey = superColumn.getFirst();
+                ColumnMapExpr columnMapExpr = superColumn.getSecond();
+                
+                String columnFamily_column = cfMetaData_.cfName + ":" + (String)(superColumnKey.get()) + ":";
+                
+                for (Pair<OperandDef, OperandDef> entry : columnMapExpr)
+                {
+                    OperandDef columnKey = entry.getFirst();
+                    OperandDef value     = entry.getSecond();
+                    rm.add(columnFamily_column + (String)(columnKey.get()), ((String)value.get()).getBytes(), time);
+                }
+            }
+            StorageProxy.insert(rm);
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+            throw new RuntimeException(RuntimeErrorMsg.GENERIC_ERROR.getMsg());            
+        }
+        return null;
+
+    }
+
+    public String explainPlan()
+    {
+        StringBuffer sb = new StringBuffer();
+        
+        String prefix =
+            String.format("%s Column Family: Batch SET a set of Super Columns: \n" +
+            "   Table Name:     %s\n" +
+            "   Column Famly:   %s\n" +
+            "   RowKey:         %s\n",
+            cfMetaData_.columnType,
+            cfMetaData_.tableName,
+            cfMetaData_.cfName,
+            rowKey_.explain());
+
+        for (Pair<OperandDef, ColumnMapExpr> superColumn : superColumnMapExpr_)
+        {
+            OperandDef    superColumnKey = superColumn.getFirst();
+            ColumnMapExpr columnMapExpr = superColumn.getSecond();
+
+            for (Pair<OperandDef, OperandDef> entry : columnMapExpr)
+            {
+                OperandDef columnKey = entry.getFirst();
+                OperandDef value     = entry.getSecond();
+                sb.append(String.format("     SuperColumnKey: %s\n" + 
+                                        "     ColumnKey:      %s\n" +
+                                        "     Value:          %s\n",
+                                        superColumnKey.explain(),
+                                        columnKey.explain(),
+                                        value.explain()));
+            }
+        }
+        
+        return prefix + sb.toString();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetUniqueKey.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetUniqueKey.java
index e69de29b..0b7731de 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetUniqueKey.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SetUniqueKey.java
@@ -0,0 +1,118 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.cql.execution.RuntimeErrorMsg;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+/**
+ * Execution plan for setting a specific column in a Simple/Super column family.
+ *   SET table.standard_cf[<rowKey>][<columnKey>] = <value>;
+ *   SET table.super_cf[<rowKey>][<superColumnKey>][<columnKey>] = <value>; 
+ */
+public class SetUniqueKey extends DMLPlan
+{
+    private final static Logger logger_ = Logger.getLogger(SetUniqueKey.class);    
+    private CFMetaData cfMetaData_;
+    private OperandDef rowKey_;
+    private OperandDef superColumnKey_;
+    private OperandDef columnKey_;
+    private OperandDef value_;
+
+    /**
+     *  Construct an execution plan for setting a column in a simple column family
+     * 
+     *   SET table.standard_cf[<rowKey>][<columnKey>] = <value>;
+     */
+    public SetUniqueKey(CFMetaData cfMetaData, OperandDef rowKey, OperandDef columnKey, OperandDef value)
+    {
+        cfMetaData_     = cfMetaData;
+        rowKey_         = rowKey;
+        columnKey_      = columnKey;
+        superColumnKey_ = null;
+        value_          = value;
+    }
+    
+    /**
+     * Construct execution plan for setting a column in a super column family.
+     * 
+     *  SET table.super_cf[<rowKey>][<superColumnKey>][<columnKey>] = <value>;
+     */
+    public SetUniqueKey(CFMetaData cfMetaData, OperandDef rowKey, OperandDef superColumnKey, OperandDef columnKey, OperandDef value)
+    {
+        cfMetaData_     = cfMetaData;
+        rowKey_         = rowKey;
+        superColumnKey_ = superColumnKey;
+        columnKey_      = columnKey;
+        value_          = value;
+    }
+
+    public CqlResult execute()
+    {
+        String columnKey = (String)(columnKey_.get());
+        String columnFamily_column;
+
+        if (superColumnKey_ != null)
+        {
+            String superColumnKey = (String)(superColumnKey_.get());
+            columnFamily_column = cfMetaData_.cfName + ":" + superColumnKey + ":" + columnKey;
+        }
+        else
+        {
+            columnFamily_column = cfMetaData_.cfName + ":" + columnKey;
+        }
+
+        try
+        {
+            RowMutation rm = new RowMutation(cfMetaData_.tableName, (String)(rowKey_.get()));
+            rm.add(columnFamily_column, ((String)value_.get()).getBytes(), System.currentTimeMillis());
+            StorageProxy.insert(rm);
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+            throw new RuntimeException(RuntimeErrorMsg.GENERIC_ERROR.getMsg());            
+        }
+        return null;
+    }
+
+    public String explainPlan()
+    {
+        return
+            String.format("%s Column Family: Unique Key SET: \n" +
+                "   Table Name:     %s\n" +
+                "   Column Famly:   %s\n" +
+                "   RowKey:         %s\n" +
+                "%s" +
+                "   ColumnKey:      %s\n" +
+                "   Value:          %s\n",
+                cfMetaData_.columnType,
+                cfMetaData_.tableName,
+                cfMetaData_.cfName,
+                rowKey_.explain(),
+                (superColumnKey_ == null) ? "" : "   SuperColumnKey: " + superColumnKey_.explain() + "\n",                
+                columnKey_.explain(),
+                value_.explain());
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SuperColumnMapExpr.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SuperColumnMapExpr.java
index e69de29b..57f6d276 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SuperColumnMapExpr.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SuperColumnMapExpr.java
@@ -0,0 +1,25 @@
+/* Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import java.util.ArrayList;
+
+public class SuperColumnMapExpr extends ArrayList<Pair<OperandDef, ColumnMapExpr>>
+{
+    private static final long serialVersionUID = 1L;
+};
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SuperColumnRangeQueryRSD.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SuperColumnRangeQueryRSD.java
index e69de29b..a20c1c11 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SuperColumnRangeQueryRSD.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/SuperColumnRangeQueryRSD.java
@@ -0,0 +1,130 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.List;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.cql.execution.RuntimeErrorMsg;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.SliceReadCommand;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * A Row Source Defintion (RSD) for doing a super column range query on a Super Column Family.
+ */
+public class SuperColumnRangeQueryRSD extends RowSourceDef
+{
+    private final static Logger logger_ = Logger.getLogger(SuperColumnRangeQueryRSD.class);
+    private CFMetaData cfMetaData_;
+    private OperandDef rowKey_;
+    private OperandDef superColumnKey_;
+    private int        offset_;
+    private int        limit_;
+
+    /**
+     * Set up a range query on super column map in a super column family.
+     * The super column map is identified by the rowKey.
+     * 
+     * Note: "limit" of -1 is the equivalent of no limit.
+     *       "offset" specifies the number of rows to skip.
+     *        An offset of 0 implies from the first row.
+     */
+    public SuperColumnRangeQueryRSD(CFMetaData cfMetaData, OperandDef rowKey, int offset, int limit)
+    {
+        cfMetaData_     = cfMetaData;
+        rowKey_         = rowKey;
+        offset_         = offset;
+        limit_          = limit;
+    }
+
+    public List<Map<String,String>> getRows()
+    {
+        Row row = null;
+        try
+        {
+            String key = (String)(rowKey_.get());
+            ReadCommand readCommand = new SliceReadCommand(cfMetaData_.tableName, key, cfMetaData_.cfName, offset_, limit_);
+            row = StorageProxy.readProtocol(readCommand, StorageService.ConsistencyLevel.WEAK);
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+            throw new RuntimeException(RuntimeErrorMsg.GENERIC_ERROR.getMsg());
+        }
+
+        List<Map<String, String>> rows = new LinkedList<Map<String, String>>();
+        if (row != null)
+        {
+            Map<String, ColumnFamily> cfMap = row.getColumnFamilyMap();
+            if (cfMap != null && cfMap.size() > 0)
+            {
+                ColumnFamily cfamily = cfMap.get(cfMetaData_.cfName);
+                if (cfamily != null)
+                {
+                    Collection<IColumn> columns = cfamily.getAllColumns();
+                    if (columns != null && columns.size() > 0)
+                    {
+                        for (IColumn column : columns)
+                        {
+                            Collection<IColumn> subColumns = column.getSubColumns();
+                            for( IColumn subColumn : subColumns )
+                            {
+                               Map<String, String> result = new HashMap<String, String>();
+                               result.put(cfMetaData_.n_superColumnKey, column.name());                               
+                               result.put(cfMetaData_.n_columnKey, subColumn.name());
+                               result.put(cfMetaData_.n_columnValue, new String(subColumn.value()));
+                               result.put(cfMetaData_.n_columnTimestamp, Long.toString(subColumn.timestamp()));
+                               rows.add(result);
+                            }
+                        }
+                    }
+                }
+            }
+        }
+        return rows;
+    }
+
+    public String explainPlan()
+    {
+        return String.format("%s Column Family: Super Column Range Query: \n" +
+                "  Table Name:       %s\n" +
+                "  Column Family:    %s\n" +
+                "  RowKey:           %s\n" +
+                "  Offset:           %d\n" +
+                "  Limit:            %d\n" +
+                "  Order By:         %s",
+                cfMetaData_.columnType,
+                cfMetaData_.tableName,
+                cfMetaData_.cfName,
+                rowKey_.explain(),
+                offset_, limit_,
+                cfMetaData_.indexProperty_);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/UniqueKeyQueryRSD.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/UniqueKeyQueryRSD.java
index e69de29b..5d5f09c8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/UniqueKeyQueryRSD.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/UniqueKeyQueryRSD.java
@@ -0,0 +1,166 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.List;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.cql.execution.RuntimeErrorMsg;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.ColumnReadCommand;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.service.StorageProxy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * A Row Source Defintion (RSD) for looking up a unique column within a column family.
+ */
+public class UniqueKeyQueryRSD extends RowSourceDef
+{
+    private final static Logger logger_ = Logger.getLogger(UniqueKeyQueryRSD.class);    
+    private CFMetaData cfMetaData_;
+    private OperandDef rowKey_;
+    private OperandDef superColumnKey_;
+    private OperandDef columnKey_;
+
+    // super column family
+    public UniqueKeyQueryRSD(CFMetaData cfMetaData, OperandDef rowKey, OperandDef superColumnKey, OperandDef columnKey)
+    {
+        cfMetaData_     = cfMetaData;
+        rowKey_         = rowKey;
+        superColumnKey_ = superColumnKey;
+        columnKey_      = columnKey;
+    }
+
+    // simple column family
+    public UniqueKeyQueryRSD(CFMetaData cfMetaData, OperandDef rowKey, OperandDef columnKey)
+    {
+        cfMetaData_ = cfMetaData;
+        rowKey_     = rowKey;
+        columnKey_  = columnKey;
+        superColumnKey_ = null;
+    }
+
+    // specific column lookup
+    public List<Map<String,String>> getRows() throws RuntimeException
+    {
+        String columnKey = (String)(columnKey_.get());
+        String columnFamily_column;
+        String superColumnKey = null;
+
+        if (superColumnKey_ != null)
+        {
+            superColumnKey = (String)(superColumnKey_.get());
+            columnFamily_column = cfMetaData_.cfName + ":" + superColumnKey + ":" + columnKey;
+        }
+        else
+        {
+            columnFamily_column = cfMetaData_.cfName + ":" + columnKey;
+        }
+
+        Row row = null;
+        try
+        {
+            String key = (String)(rowKey_.get());
+            ReadCommand readCommand = new ColumnReadCommand(cfMetaData_.tableName, key, columnFamily_column);
+            row = StorageProxy.readProtocol(readCommand, StorageService.ConsistencyLevel.WEAK);
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+            throw new RuntimeException(RuntimeErrorMsg.GENERIC_ERROR.getMsg());
+        }
+
+        if (row != null)
+        {
+            Map<String, ColumnFamily> cfMap = row.getColumnFamilyMap();
+            if (cfMap != null && cfMap.size() > 0)
+            {
+                ColumnFamily cfamily = cfMap.get(cfMetaData_.cfName);
+                if (cfamily != null)
+                {
+                    Collection<IColumn> columns = null;
+                    if (superColumnKey_ != null)
+                    {
+                        // this is the super column case 
+                        IColumn column = cfamily.getColumn(superColumnKey);
+                        if (column != null)
+                            columns = column.getSubColumns();
+                    }
+                    else
+                    {
+                        columns = cfamily.getAllColumns();
+                    }
+                    
+                    if (columns != null && columns.size() > 0)
+                    {
+                        if (columns.size() > 1)
+                        {
+                            // We are looking up by a rowKey & columnKey. There should
+                            // be at most one column that matches. If we find more than
+                            // one, then it is an internal error.
+                            throw new RuntimeException(RuntimeErrorMsg.INTERNAL_ERROR.getMsg("Too many columns found for: " + columnKey));
+                        }
+                        for (IColumn column : columns)
+                        {
+                            List<Map<String, String>> rows = new LinkedList<Map<String, String>>();
+
+                            Map<String, String> result = new HashMap<String, String>();
+                            result.put(cfMetaData_.n_columnKey, column.name());
+                            result.put(cfMetaData_.n_columnValue, new String(column.value()));
+                            result.put(cfMetaData_.n_columnTimestamp, Long.toString(column.timestamp()));
+                            
+                            rows.add(result);
+                                
+                            // at this point, due to the prior checks, we are guaranteed that
+                            // there is only one item in "columns".
+                            return rows;
+                        }
+                        return null;
+                    }
+                }
+            }
+        }
+        throw new RuntimeException(RuntimeErrorMsg.NO_DATA_FOUND.getMsg());
+    }
+
+    public String explainPlan()
+    {
+        return String.format("%s Column Family: Unique Key Query: \n" +
+                "   Table Name:     %s\n" +
+                "   Column Famly:   %s\n" +
+                "   RowKey:         %s\n" +
+                "%s" +
+                "   ColumnKey:      %s",
+                cfMetaData_.columnType,
+                cfMetaData_.tableName,
+                cfMetaData_.cfName,
+                rowKey_.explain(),
+                (superColumnKey_ == null) ? "" : "   SuperColumnKey: " + superColumnKey_.explain() + "\n",                
+                columnKey_.explain());
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Utils.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Utils.java
index e69de29b..e030d84e 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Utils.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/common/Utils.java
@@ -0,0 +1,62 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.common;
+
+public class Utils
+{
+    /*
+     * Strips leading and trailing "'" characters, and handles
+     * and escaped characters such as \n, \r, etc.
+     * [Shameless clone from hive.]
+     */
+    public static String unescapeSQLString(String b) 
+    {
+        assert(b.charAt(0) == '\'');
+        assert(b.charAt(b.length()-1) == '\'');
+        StringBuilder sb = new StringBuilder(b.length());
+        
+        for (int i=1; i+1<b.length(); i++)
+        {
+            if (b.charAt(i) == '\\' && i+2<b.length())
+            {
+                char n=b.charAt(i+1);
+                switch(n)
+                {
+                case '0': sb.append("\0"); break;
+                case '\'': sb.append("'"); break;
+                case '"': sb.append("\""); break;
+                case 'b': sb.append("\b"); break;
+                case 'n': sb.append("\n"); break;
+                case 'r': sb.append("\r"); break;
+                case 't': sb.append("\t"); break;
+                case 'Z': sb.append("\u001A"); break;
+                case '\\': sb.append("\\"); break;
+                case '%': sb.append("%"); break;
+                case '_': sb.append("_"); break;
+                default: sb.append(n);
+                }
+            } 
+            else
+            {
+                sb.append(b.charAt(i));
+            }
+        }
+        return sb.toString();
+    } 
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/common/CompilerErrorMsg.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/common/CompilerErrorMsg.java
index e69de29b..73927e53 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/common/CompilerErrorMsg.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/common/CompilerErrorMsg.java
@@ -0,0 +1,70 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.cql.compiler.common;
+
+import org.antlr.runtime.tree.CommonTree;
+
+/**
+ * List of error messages thrown by the CQL Compiler
+ **/
+
+public enum CompilerErrorMsg
+{
+    // Error messages with String.format() style format specifiers
+    GENERIC_ERROR("CQL Compilation Error"),
+    INTERNAL_ERROR("CQL Compilation Internal Error"),
+    INVALID_TABLE("Table '%s' does not exist"),
+    INVALID_COLUMN_FAMILY("Column Family '%s' not found in table '%s'"),
+    TOO_MANY_DIMENSIONS("Too many dimensions specified for %s Column Family"),
+    INVALID_TYPE("Expression is of invalid type")
+    ;
+
+    private String mesg;
+    CompilerErrorMsg(String mesg)
+    {
+        this.mesg = mesg;
+    }
+    
+    private static String getLineAndPosition(CommonTree tree) 
+    {
+        if (tree.getChildCount() == 0)
+        {
+            return tree.getToken().getLine() + ":" + tree.getToken().getCharPositionInLine();
+        }
+        return getLineAndPosition((CommonTree)tree.getChild(0));
+    }
+
+    // Returns the formatted error message. Derives line/position information
+    // from the "tree" node passed in.
+    public String getMsg(CommonTree tree, Object... args)
+    {
+        // We allocate another array since we want to add line and position as an 
+        // implicit additional first argument to pass on to String.format.
+        Object[] newArgs = new Object[args.length + 1];
+        newArgs[0] = getLineAndPosition(tree);
+        System.arraycopy(args, 0, newArgs, 1, args.length);
+
+        // note: mesg itself might contain other format specifiers...
+        return String.format("line %s " + mesg, newArgs);
+    } 
+
+    String getMsg()
+    {
+        return mesg;
+    } 
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/common/CqlCompiler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/common/CqlCompiler.java
index e69de29b..b5414736 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/common/CqlCompiler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/common/CqlCompiler.java
@@ -0,0 +1,149 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.compiler.common;
+
+
+import org.apache.cassandra.cql.common.*;
+import org.apache.cassandra.cql.compiler.parse.*;
+import org.apache.cassandra.cql.compiler.sem.*;
+
+import java.util.ArrayList;
+
+import org.antlr.runtime.*;
+import org.antlr.runtime.tree.*;
+import org.apache.cassandra.cql.common.Plan;
+import org.apache.cassandra.cql.compiler.parse.CqlLexer;
+import org.apache.cassandra.cql.compiler.parse.CqlParser;
+import org.apache.cassandra.cql.compiler.parse.ParseError;
+import org.apache.cassandra.cql.compiler.parse.ParseException;
+import org.apache.cassandra.cql.compiler.sem.SemanticException;
+import org.apache.cassandra.cql.compiler.sem.SemanticPhase;
+
+public class CqlCompiler
+{
+    // ANTLR does not provide case-insensitive tokenization support
+    // out of the box. So we override the LA (lookahead) function
+    // of the ANTLRStringStream class. Note: This doesn't change the
+    // token text-- but just relaxes the matching rules to match
+    // in upper case. [Logic borrowed from Hive code.]
+    // 
+    // Also see discussion on this topic in:
+    // http://www.antlr.org/wiki/pages/viewpage.action?pageId=1782.
+    public class ANTLRNoCaseStringStream  extends ANTLRStringStream
+    {
+        public ANTLRNoCaseStringStream(String input)
+        {
+            super(input);
+        }
+    
+        public int LA(int i)
+        {
+            int returnChar = super.LA(i);
+            if (returnChar == CharStream.EOF)
+            {
+                return returnChar; 
+            }
+            else if (returnChar == 0) 
+            {
+                return returnChar;
+            }
+
+            return Character.toUpperCase((char)returnChar);
+        }
+    }
+
+    // Override CQLParser. This gives flexibility in altering default error
+    // messages as well as accumulating multiple errors.
+    public class CqlParserX extends CqlParser
+    {
+        private ArrayList<ParseError> errors;
+
+        public CqlParserX(TokenStream input)
+        {
+            super(input);
+            errors = new ArrayList<ParseError>();
+        }
+
+        protected void mismatch(IntStream input, int ttype, BitSet follow) throws RecognitionException
+        {
+            throw new MismatchedTokenException(ttype, input);
+        }
+
+        public Object recoverFromMismatchedSet(IntStream input,
+                                             RecognitionException re,
+                                             BitSet follow) throws RecognitionException
+        {
+            throw re;
+        }
+
+        public void displayRecognitionError(String[] tokenNames,
+                                            RecognitionException e)
+        {
+            errors.add(new ParseError(this, e, tokenNames));
+        }
+
+        public ArrayList<ParseError> getErrors()
+        {
+            return errors;
+        }
+    }
+
+    // Compile a CQL query
+    public Plan compileQuery(String query) throws ParseException, SemanticException
+    {
+        CommonTree queryTree = null;
+        CqlLexer lexer = null;
+        CqlParserX parser = null;
+        CommonTokenStream tokens = null;
+
+        ANTLRStringStream input = new ANTLRNoCaseStringStream(query);
+
+        lexer = new CqlLexer(input);
+        tokens = new CommonTokenStream(lexer);
+        parser = new CqlParserX(tokens);
+
+        // built AST
+        try
+        {
+            queryTree = (CommonTree)(parser.root().getTree());
+        }
+        catch (RecognitionException e)
+        {
+            throw new ParseException(parser.getErrors());            
+        }
+        catch (RewriteEmptyStreamException e)
+        {
+            throw new ParseException(parser.getErrors());            
+        }
+
+        if (!parser.getErrors().isEmpty())
+        {
+            throw new ParseException(parser.getErrors());
+        }
+
+        if (!parser.errors.isEmpty())
+        {
+            throw new ParseException("parser error");
+        }
+
+        // Semantic analysis and code-gen.
+        // Eventually, I anticipate, I'll be forking these off into two separate phases.
+        return SemanticPhase.doSemanticAnalysis(queryTree);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/parse/ParseError.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/parse/ParseError.java
index e69de29b..d5baa642 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/parse/ParseError.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/parse/ParseError.java
@@ -0,0 +1,50 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.compiler.parse;
+
+import org.antlr.runtime.*;
+
+public class ParseError {
+  private BaseRecognizer br;
+  private RecognitionException re;
+  private String[] tokenNames;
+  
+  public ParseError(BaseRecognizer br, RecognitionException re, String[] tokenNames) {
+    this.br = br;
+    this.re = re;
+    this.tokenNames = tokenNames;
+    }
+  
+  public BaseRecognizer getBaseRecognizer() {
+    return br;
+  }
+
+  public RecognitionException getRecognitionException() {
+    return re;
+  }
+  
+  public String[] getTokenNames() {
+    return tokenNames;
+  }
+
+  public String getMessage() {
+    return br.getErrorHeader(re) + " " + br.getErrorMessage(re, tokenNames);
+  }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/parse/ParseException.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/parse/ParseException.java
index e69de29b..d89ce28d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/parse/ParseException.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/parse/ParseException.java
@@ -0,0 +1,57 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.compiler.parse;
+
+/**
+ * Exception from the CQL Parser
+ */
+
+import java.util.ArrayList;
+
+public class ParseException extends Exception {
+
+    private static final long serialVersionUID = 1L;
+    ArrayList<ParseError> errors = null;
+
+    public ParseException(ArrayList<ParseError> errors)
+    {
+      super();
+      this.errors = errors;
+    }
+
+    public ParseException(String message)
+    {
+        super(message);
+    }
+
+    public String getMessage() {
+
+      if (errors == null)
+          return super.getMessage();
+
+      StringBuilder sb = new StringBuilder();
+      for(ParseError err: errors) {
+        sb.append(err.getMessage());
+        sb.append("\n");
+      }
+
+      return sb.toString();
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/sem/SemanticException.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/sem/SemanticException.java
index e69de29b..72a17f64 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/sem/SemanticException.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/sem/SemanticException.java
@@ -0,0 +1,49 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.compiler.sem;
+
+
+/**
+ * Exception from the CQL SemanticAnalyzer
+ */
+
+public class SemanticException extends Exception
+{
+    private static final long serialVersionUID = 1L;
+
+    public SemanticException()
+    {
+        super();
+    }
+    
+    public SemanticException(String message)
+    {
+        super(message);
+    }
+    
+    public SemanticException(Throwable cause)
+    {
+        super(cause);
+    }
+    
+    public SemanticException(String message, Throwable cause)
+    {
+        super(message, cause);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/sem/SemanticPhase.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/sem/SemanticPhase.java
index e69de29b..a9d61383 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/sem/SemanticPhase.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/compiler/sem/SemanticPhase.java
@@ -0,0 +1,342 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql.compiler.sem;
+
+import java.util.Map;
+
+import org.antlr.runtime.tree.CommonTree;
+
+import org.apache.cassandra.cql.common.*;
+import org.apache.cassandra.cql.compiler.common.*;
+import org.apache.cassandra.cql.compiler.parse.*;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.cql.common.ColumnMapExpr;
+import org.apache.cassandra.cql.common.ColumnRangeQueryRSD;
+import org.apache.cassandra.cql.common.ConstantOperand;
+import org.apache.cassandra.cql.common.ExplainPlan;
+import org.apache.cassandra.cql.common.OperandDef;
+import org.apache.cassandra.cql.common.Pair;
+import org.apache.cassandra.cql.common.Plan;
+import org.apache.cassandra.cql.common.QueryPlan;
+import org.apache.cassandra.cql.common.RowSourceDef;
+import org.apache.cassandra.cql.common.SetColumnMap;
+import org.apache.cassandra.cql.common.SetSuperColumnMap;
+import org.apache.cassandra.cql.common.SetUniqueKey;
+import org.apache.cassandra.cql.common.SuperColumnMapExpr;
+import org.apache.cassandra.cql.common.SuperColumnRangeQueryRSD;
+import org.apache.cassandra.cql.common.UniqueKeyQueryRSD;
+import org.apache.cassandra.cql.common.Utils;
+import org.apache.cassandra.cql.compiler.common.CompilerErrorMsg;
+import org.apache.cassandra.cql.compiler.parse.CqlParser;
+import org.apache.log4j.Logger;
+
+//
+// Note: This class is CQL related work in progress.
+//
+// Currently, this phase combines both semantic analysis and code-gen.
+// I expect that as my ideas get refined/cleared up, I'll be drawing
+// a more clear distinction between semantic analysis phase and code-gen.
+//
+public class SemanticPhase
+{
+    private final static Logger logger_ = Logger.getLogger(SemanticPhase.class);    
+
+    // Current code-gen also happens in this phase!
+    public static Plan doSemanticAnalysis(CommonTree ast) throws SemanticException
+    {
+        Plan plan = null;
+
+        logger_.debug("AST: " + ast.toStringTree());
+
+        switch (ast.getType())
+        {
+        case CqlParser.A_GET:
+            plan = compileGet(ast);
+            break;
+        case CqlParser.A_SET:
+            plan = compileSet(ast);
+            break;
+        case CqlParser.A_DELETE:
+            compileDelete(ast);
+            break;
+        case CqlParser.A_SELECT:
+            compileSelect(ast);
+            break;
+        case CqlParser.A_EXPLAIN_PLAN:
+            // Case: EXPLAN PLAN <stmt>
+            // first, generate a plan for <stmt>
+            // and then, wrapper it with a special ExplainPlan plan
+            // whose execution will result in an explain plan rather
+            // than a normal execution of the statement.
+            plan = doSemanticAnalysis((CommonTree)(ast.getChild(0)));
+            plan = new ExplainPlan(plan);
+            break;
+        default:
+            // Unhandled AST node. Raise an internal error. 
+            throw new SemanticException(CompilerErrorMsg.INTERNAL_ERROR.getMsg(ast, "Unknown Node Type: " + ast.getType()));
+        }
+        return plan;
+    }
+
+    /** 
+     * Given a CommonTree AST node of type, A_COLUMN_ACCESS related functions, do semantic
+     * checking to ensure table name, column family name, and number of key dimensions
+     * specified are all valid. 
+     */
+    private static CFMetaData getColumnFamilyInfo(CommonTree ast) throws SemanticException
+    {
+        assert(ast.getType() == CqlParser.A_COLUMN_ACCESS);
+
+        CommonTree columnFamilyNode = (CommonTree)(ast.getChild(1)); 
+        CommonTree tableNode = (CommonTree)(ast.getChild(0));
+
+        String columnFamily = columnFamilyNode.getText();
+        String table = tableNode.getText();
+
+        Map<String, CFMetaData> columnFamilies = DatabaseDescriptor.getTableMetaData(table);
+        if (columnFamilies == null)
+        {
+            throw new SemanticException(CompilerErrorMsg.INVALID_TABLE.getMsg(ast, table));
+        }
+
+        CFMetaData cfMetaData = columnFamilies.get(columnFamily);
+        if (cfMetaData == null)
+        {
+            throw new SemanticException(CompilerErrorMsg.INVALID_COLUMN_FAMILY.getMsg(ast, columnFamily, table));
+        }
+
+        // Once you have drilled down to a row using a rowKey, a super column
+        // map can be indexed only 2 further levels deep; and a column map may
+        // be indexed up to 1 level deep.
+        int dimensions = numColumnDimensions(ast);
+        if (("Super".equals(cfMetaData.columnType) && (dimensions > 2)) ||
+            ("Standard".equals(cfMetaData.columnType) && dimensions > 1))
+        {
+            throw new SemanticException(CompilerErrorMsg.TOO_MANY_DIMENSIONS.getMsg(ast, cfMetaData.columnType));
+        }
+
+        return cfMetaData; 
+    }
+
+    private static String getRowKey(CommonTree ast)
+    {
+        assert(ast.getType() == CqlParser.A_COLUMN_ACCESS);
+        return Utils.unescapeSQLString(ast.getChild(2).getText());
+    }
+
+    private static int numColumnDimensions(CommonTree ast)
+    {
+        // Skip over table name, column family and rowKey
+        return ast.getChildCount() - 3;
+    }
+
+    // Returns the pos'th (0-based index) column specifier in the astNode
+    private static String getColumn(CommonTree ast, int pos)
+    {
+        // Skip over table name, column family and rowKey
+        return Utils.unescapeSQLString(ast.getChild(pos + 3).getText()); 
+    }
+
+    // Compile a GET statement
+    private static Plan compileGet(CommonTree ast) throws SemanticException
+    {
+        int childCount = ast.getChildCount();
+        assert(childCount == 1);
+
+        CommonTree columnFamilySpec = (CommonTree)ast.getChild(0);
+        assert(columnFamilySpec.getType() == CqlParser.A_COLUMN_ACCESS);
+
+        CFMetaData cfMetaData = getColumnFamilyInfo(columnFamilySpec);
+        ConstantOperand rowKey = new ConstantOperand(getRowKey(columnFamilySpec));
+        int dimensionCnt = numColumnDimensions(columnFamilySpec);
+
+        RowSourceDef rwsDef;
+        if ("Super".equals(cfMetaData.columnType))
+        {
+            if (dimensionCnt > 2)
+            {
+                // We don't expect this case to arise, since Cql.g grammar disallows this.
+                // therefore, raise this case as an "internal error".
+                throw new SemanticException(CompilerErrorMsg.INTERNAL_ERROR.getMsg(columnFamilySpec));
+            }
+
+            if (dimensionCnt == 2)
+            {
+                // Case: table.super_cf[<rowKey>][<superColumnKey>][<columnKey>]
+                ConstantOperand superColumnKey = new ConstantOperand(getColumn(columnFamilySpec, 0));                
+                ConstantOperand columnKey = new ConstantOperand(getColumn(columnFamilySpec, 1));
+                rwsDef = new UniqueKeyQueryRSD(cfMetaData, rowKey, superColumnKey, columnKey);
+            }
+            else if (dimensionCnt == 1)
+            {
+                // Case: table.super_cf[<rowKey>][<superColumnKey>]
+                ConstantOperand superColumnKey = new ConstantOperand(getColumn(columnFamilySpec, 0));                
+                rwsDef = new ColumnRangeQueryRSD(cfMetaData, rowKey, superColumnKey, -1, Integer.MAX_VALUE);
+            }
+            else
+            {
+                // Case: table.super_cf[<rowKey>]             
+                rwsDef = new SuperColumnRangeQueryRSD(cfMetaData, rowKey, -1, Integer.MAX_VALUE);
+            }
+        }
+        else  // Standard Column Family
+        {
+            if (dimensionCnt == 1)
+            {
+                // Case: table.standard_cf[<rowKey>][<columnKey>]
+                ConstantOperand columnKey = new ConstantOperand(getColumn(columnFamilySpec, 0));
+                rwsDef = new UniqueKeyQueryRSD(cfMetaData, rowKey, columnKey);
+            }
+            else
+            {
+                // Case: table.standard_cf[<rowKey>]
+                logger_.assertLog((dimensionCnt == 0), "invalid dimensionCnt: " + dimensionCnt);
+                rwsDef = new ColumnRangeQueryRSD(cfMetaData, rowKey, -1, Integer.MAX_VALUE);
+            }
+        }
+        return new QueryPlan(rwsDef);
+    }
+    
+    private static OperandDef  getSimpleExpr(CommonTree ast) throws SemanticException
+    {
+        int type = ast.getType();
+
+        // for now, the only simple expressions support are of string type
+        if (type != CqlParser.StringLiteral)
+        {
+            throw new SemanticException(CompilerErrorMsg.INVALID_TYPE.getMsg(ast));
+        }
+        return new ConstantOperand(Utils.unescapeSQLString(ast.getText()));
+    }
+
+    private static ColumnMapExpr getColumnMapExpr(CommonTree ast) throws SemanticException
+    {
+        int type = ast.getType();
+        if (type != CqlParser.A_COLUMN_MAP_VALUE)
+        {
+            throw new SemanticException(CompilerErrorMsg.INVALID_TYPE.getMsg(ast));
+        }
+        
+        int size = ast.getChildCount();
+        ColumnMapExpr result = new ColumnMapExpr();
+        for (int idx = 0; idx < size; idx++)
+        {
+            CommonTree entryNode = (CommonTree)(ast.getChild(idx));
+            OperandDef columnKey   = getSimpleExpr((CommonTree)(entryNode.getChild(0)));
+            OperandDef columnValue = getSimpleExpr((CommonTree)(entryNode.getChild(1)));            
+
+            Pair<OperandDef, OperandDef> entry = new Pair<OperandDef, OperandDef>(columnKey, columnValue);
+            result.add(entry);
+        }
+        return result;
+    }
+
+    private static SuperColumnMapExpr getSuperColumnMapExpr(CommonTree ast) throws SemanticException
+    {
+        int type = ast.getType();        
+        if (type != CqlParser.A_SUPERCOLUMN_MAP_VALUE)
+        {
+            throw new SemanticException(CompilerErrorMsg.INVALID_TYPE.getMsg(ast));
+        }
+        int size = ast.getChildCount();
+        SuperColumnMapExpr result = new SuperColumnMapExpr();
+        for (int idx = 0; idx < size; idx++)
+        {
+            CommonTree entryNode = (CommonTree)(ast.getChild(idx));
+            OperandDef    superColumnKey = getSimpleExpr((CommonTree)(entryNode.getChild(0)));
+            ColumnMapExpr columnMapExpr  = getColumnMapExpr((CommonTree)(entryNode.getChild(1)));            
+
+            Pair<OperandDef, ColumnMapExpr> entry = new Pair<OperandDef, ColumnMapExpr>(superColumnKey, columnMapExpr);
+            result.add(entry);
+        }
+        return result;
+    }
+
+    // compile a SET statement
+    private static Plan compileSet(CommonTree ast) throws SemanticException
+    {
+        int childCount = ast.getChildCount();
+        assert(childCount == 2);
+
+        CommonTree columnFamilySpec = (CommonTree)ast.getChild(0);
+        assert(columnFamilySpec.getType() == CqlParser.A_COLUMN_ACCESS);
+
+        CFMetaData cfMetaData = getColumnFamilyInfo(columnFamilySpec);
+        ConstantOperand rowKey = new ConstantOperand(getRowKey(columnFamilySpec));
+        int dimensionCnt = numColumnDimensions(columnFamilySpec);
+
+        CommonTree  valueNode = (CommonTree)(ast.getChild(1));
+
+        Plan plan = null;
+        if ("Super".equals(cfMetaData.columnType))
+        {
+            if (dimensionCnt == 2)
+            {
+                // Case: set table.super_cf['key']['supercolumn']['column'] = 'value'
+                OperandDef value = getSimpleExpr(valueNode);
+                ConstantOperand superColumnKey = new ConstantOperand(getColumn(columnFamilySpec, 0));
+                ConstantOperand columnKey = new ConstantOperand(getColumn(columnFamilySpec, 1));
+                plan = new SetUniqueKey(cfMetaData, rowKey, superColumnKey, columnKey, value);
+            }
+            else if (dimensionCnt == 1)
+            {
+                // Case: set table.super_cf['key']['supercolumn'] = <column_map>;
+                ColumnMapExpr columnMapExpr = getColumnMapExpr(valueNode);                
+                ConstantOperand superColumnKey = new ConstantOperand(getColumn(columnFamilySpec, 0));
+                plan = new SetColumnMap(cfMetaData, rowKey, superColumnKey, columnMapExpr);
+            }
+            else
+            {
+                // Case: set table.super_cf['key'] = <super_column_map>;
+                logger_.assertLog(dimensionCnt == 0, "invalid dimensionCnt: " + dimensionCnt);
+                SuperColumnMapExpr superColumnMapExpr = getSuperColumnMapExpr(valueNode);                
+                plan = new SetSuperColumnMap(cfMetaData, rowKey, superColumnMapExpr);
+            }
+        }
+        else  // Standard column family
+        {
+            if (dimensionCnt == 1)
+            {
+                // Case: set table.standard_cf['key']['column'] = 'value'
+                OperandDef value = getSimpleExpr(valueNode);                
+                ConstantOperand columnKey = new ConstantOperand(getColumn(columnFamilySpec, 0));
+                plan = new SetUniqueKey(cfMetaData, rowKey, columnKey, value);
+            } 
+            else
+            {
+                // Case: set table.standard_cf['key'] = <column_map>;
+                logger_.assertLog(dimensionCnt == 0, "invalid dimensionCnt: " + dimensionCnt);
+                ColumnMapExpr columnMapExpr = getColumnMapExpr(valueNode);                
+                plan = new SetColumnMap(cfMetaData, rowKey, columnMapExpr);
+            }
+        }
+        return plan;
+    }
+
+    private static void compileSelect(CommonTree ast) throws SemanticException
+    {
+        // stub; tbd.
+    }
+    private static void compileDelete(CommonTree ast) throws SemanticException
+    {
+        // stub; tbd.
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/driver/CqlDriver.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/driver/CqlDriver.java
index e69de29b..8ec2a8c8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/driver/CqlDriver.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/driver/CqlDriver.java
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.cql.driver;
+
+import org.apache.cassandra.cql.common.CqlResult;
+import org.apache.cassandra.cql.common.Plan;
+import org.apache.cassandra.cql.compiler.common.CqlCompiler;
+import org.apache.cassandra.cql.compiler.parse.ParseException;
+import org.apache.cassandra.cql.compiler.sem.SemanticException;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+// Server side driver class for CQL
+public class CqlDriver 
+{
+    private final static Logger logger_ = Logger.getLogger(CqlDriver.class);
+
+    // Execute a CQL Statement 
+    public static CqlResult executeQuery(String query)
+    {
+        CqlCompiler compiler = new CqlCompiler();
+
+        try
+        {
+            logger_.debug("Compiling CQL query ...");
+            Plan plan = compiler.compileQuery(query);
+            if (plan != null)
+            {
+                logger_.debug("Executing CQL query ...");            
+                return plan.execute();
+            }
+        }
+        catch (Exception e)
+        {
+            CqlResult result = new CqlResult(null);
+            result.errorTxt = e.getMessage();           
+
+            Class<? extends Exception> excpClass = e.getClass();
+            if ((excpClass != SemanticException.class)
+                && (excpClass != ParseException.class)
+                && (excpClass != RuntimeException.class))
+            {
+                result.errorTxt = "CQL Internal Error: " + result.errorTxt;
+                result.errorCode = 1; // failure
+                logger_.error(LogUtil.throwableToString(e));
+            }
+
+            return result;
+        }
+
+        return null;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/execution/RuntimeErrorMsg.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/execution/RuntimeErrorMsg.java
index e69de29b..8e066dc6 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/execution/RuntimeErrorMsg.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/cql/execution/RuntimeErrorMsg.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.cql.execution;
+
+/**
+ * List of error messages thrown by CQL's Execution Layer
+ **/
+public enum RuntimeErrorMsg
+{
+    // Error messages with String.format() style format specifiers
+    GENERIC_ERROR("CQL Execution Error"),
+    INTERNAL_ERROR("CQL Internal Error: %s"),
+    IMPLEMENTATION_RESTRICTION("Implementation Restriction: %s"),
+    NO_DATA_FOUND("No data found")
+    ;
+
+    private String mesg;
+    RuntimeErrorMsg(String mesg) 
+    {
+        this.mesg = mesg;
+    }
+
+    // Returns the formatted error message. 
+    public String getMsg(Object... args)
+    {
+        // note: mesg itself might contain other format specifiers...
+        return String.format(mesg, args);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/AbstractColumnFactory.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/AbstractColumnFactory.java
index e69de29b..8e65d998 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/AbstractColumnFactory.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/AbstractColumnFactory.java
@@ -0,0 +1,137 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.StringTokenizer;
+
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+abstract class AbstractColumnFactory
+{
+    private static Map<String, AbstractColumnFactory> columnFactory_ = new HashMap<String, AbstractColumnFactory>();
+
+	static
+	{
+		columnFactory_.put(ColumnFamily.getColumnType("Standard"),new ColumnFactory());
+		columnFactory_.put(ColumnFamily.getColumnType("Super"),new SuperColumnFactory());
+	}
+
+	static AbstractColumnFactory getColumnFactory(String columnType)
+	{
+		/* Create based on the type required. */
+		if ( columnType == null || columnType.equals("Standard") )
+			return columnFactory_.get("Standard");
+		else
+			return columnFactory_.get("Super");
+	}
+
+	public abstract IColumn createColumn(String name);
+	public abstract IColumn createColumn(String name, byte[] value);
+    public abstract IColumn createColumn(String name, byte[] value, long timestamp);
+    public abstract IColumn createColumn(String name, byte[] value, long timestamp, boolean deleted);
+    public abstract ICompactSerializer2<IColumn> createColumnSerializer();
+}
+
+class ColumnFactory extends AbstractColumnFactory
+{
+	public IColumn createColumn(String name)
+	{
+		return new Column(name);
+	}
+
+	public IColumn createColumn(String name, byte[] value)
+	{
+		return new Column(name, value);
+	}
+
+	public IColumn createColumn(String name, byte[] value, long timestamp)
+	{
+		return new Column(name, value, timestamp);
+	}
+
+    public IColumn createColumn(String name, byte[] value, long timestamp, boolean deleted) {
+        return new Column(name, value, timestamp, deleted);
+    }
+
+    public ICompactSerializer2<IColumn> createColumnSerializer()
+    {
+        return Column.serializer();
+    }
+}
+
+class SuperColumnFactory extends AbstractColumnFactory
+{
+    static String[] getSuperColumnAndColumn(String cName)
+    {
+        StringTokenizer st = new StringTokenizer(cName, ":");
+        String[] values = new String[st.countTokens()];
+        int i = 0;
+        while ( st.hasMoreElements() )
+        {
+            values[i++] = (String)st.nextElement();
+        }
+        return values;
+    }
+
+	public IColumn createColumn(String name)
+	{
+		String[] values = SuperColumnFactory.getSuperColumnAndColumn(name);
+        if ( values.length == 0 ||  values.length > 2 )
+            throw new IllegalArgumentException("Super Column " + name + " in invalid format. Must be in <super column name>:<column name> format.");
+        IColumn superColumn = new SuperColumn(values[0]);
+        if(values.length == 2)
+        {
+	        IColumn subColumn = new Column(values[1]);
+	        superColumn.addColumn(values[1], subColumn);
+        }
+		return superColumn;
+	}
+
+	public IColumn createColumn(String name, byte[] value)
+	{
+        return createColumn(name, value, 0);
+	}
+
+    public IColumn createColumn(String name, byte[] value, long timestamp)
+    {
+        return createColumn(name, value, timestamp, false);
+    }
+
+    public IColumn createColumn(String name, byte[] value, long timestamp, boolean deleted)
+	{
+		String[] values = SuperColumnFactory.getSuperColumnAndColumn(name);
+        if ( values.length != 2 )
+            throw new IllegalArgumentException("Super Column " + name + " in invalid format. Must be in <super column name>:<column name> format.");
+        IColumn superColumn = new SuperColumn(values[0]);
+        IColumn subColumn = new Column(values[1], value, timestamp, deleted);
+        superColumn.addColumn(values[1], subColumn);
+		return superColumn;
+	}
+
+    public ICompactSerializer2<IColumn> createColumnSerializer()
+    {
+        return SuperColumn.serializer();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtable.java
index e69de29b..3d093376 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtable.java
@@ -0,0 +1,167 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.service.StorageService;
+
+import org.apache.log4j.Logger;
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class BinaryMemtable
+{
+    private static Logger logger_ = Logger.getLogger( Memtable.class );
+    private int threshold_ = 512*1024*1024;
+    private AtomicInteger currentSize_ = new AtomicInteger(0);
+
+    /* Table and ColumnFamily name are used to determine the ColumnFamilyStore */
+    private String table_;
+    private String cfName_;
+    private boolean isFrozen_ = false;
+    private Map<String, byte[]> columnFamilies_ = new NonBlockingHashMap<String, byte[]>();
+    /* Lock and Condition for notifying new clients about Memtable switches */
+    Lock lock_ = new ReentrantLock();
+    Condition condition_;
+
+    BinaryMemtable(String table, String cfName) throws IOException
+    {
+        condition_ = lock_.newCondition();
+        table_ = table;
+        cfName_ = cfName;
+    }
+
+    public int getMemtableThreshold()
+    {
+        return currentSize_.get();
+    }
+
+    void resolveSize(int oldSize, int newSize)
+    {
+        currentSize_.addAndGet(newSize - oldSize);
+    }
+
+
+    boolean isThresholdViolated()
+    {
+        if (currentSize_.get() >= threshold_ || columnFamilies_.size() > 50000)
+        {
+            logger_.debug("CURRENT SIZE:" + currentSize_.get());
+        	return true;
+        }
+        return false;
+    }
+
+    String getColumnFamily()
+    {
+    	return cfName_;
+    }
+
+    /*
+     * This version is used by the external clients to put data into
+     * the memtable. This version will respect the threshold and flush
+     * the memtable to disk when the size exceeds the threshold.
+    */
+    void put(String key, byte[] buffer) throws IOException
+    {
+        if (isThresholdViolated() )
+        {
+            lock_.lock();
+            try
+            {
+                ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
+                if (!isFrozen_)
+                {
+                    isFrozen_ = true;
+                    BinaryMemtableManager.instance().submit(cfStore.getColumnFamilyName(), this);
+                    cfStore.switchBinaryMemtable(key, buffer);
+                }
+                else
+                {
+                    cfStore.applyBinary(key, buffer);
+                }
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        else
+        {
+            resolve(key, buffer);
+        }
+    }
+
+    private void resolve(String key, byte[] buffer)
+    {
+            columnFamilies_.put(key, buffer);
+            currentSize_.addAndGet(buffer.length + key.length());
+    }
+
+
+    /*
+     * 
+    */
+    void flush() throws IOException
+    {
+        if ( columnFamilies_.size() == 0 )
+            return;
+        ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
+        String directory = DatabaseDescriptor.getDataFileLocation();
+        String filename = cfStore.getNextFileName();
+
+        /*
+         * Use the SSTable to write the contents of the TreeMap
+         * to disk.
+        */
+        SSTable ssTable = new SSTable(directory, filename, StorageService.getPartitioner());
+        List<String> keys = new ArrayList<String>( columnFamilies_.keySet() );
+        Collections.sort(keys);        
+        /* Use this BloomFilter to decide if a key exists in a SSTable */
+        BloomFilter bf = new BloomFilter(keys.size(), 8);
+        for ( String key : keys )
+        {           
+            byte[] bytes = columnFamilies_.get(key);
+            if ( bytes.length > 0 )
+            {            	
+                /* Now write the key and value to disk */
+                ssTable.append(key, bytes);
+                bf.add(key);
+            }
+        }
+        ssTable.close(bf);
+        cfStore.storeLocation( ssTable.getDataFileLocation(), bf );
+        columnFamilies_.clear();       
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtableManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtableManager.java
index e69de29b..9af5b6ba 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtableManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtableManager.java
@@ -0,0 +1,97 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class BinaryMemtableManager
+{
+    private static BinaryMemtableManager instance_;
+    private static Lock lock_ = new ReentrantLock();
+    private static Logger logger_ = Logger.getLogger(BinaryMemtableManager.class);    
+
+    static BinaryMemtableManager instance() 
+    {
+        if ( instance_ == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                    instance_ = new BinaryMemtableManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return instance_;
+    }
+    
+    class BinaryMemtableFlusher implements Runnable
+    {
+        private BinaryMemtable memtable_;
+        
+        BinaryMemtableFlusher(BinaryMemtable memtable)
+        {
+            memtable_ = memtable;
+        }
+        
+        public void run()
+        {
+            try
+            {
+            	memtable_.flush();
+            }
+            catch (IOException e)
+            {
+                logger_.debug( LogUtil.throwableToString(e) );
+            }        	
+        }
+    }
+    
+    private ExecutorService flusher_ = new DebuggableThreadPoolExecutor( 1,
+            1,
+            Integer.MAX_VALUE,
+            TimeUnit.SECONDS,
+            new LinkedBlockingQueue<Runnable>(),
+            new ThreadFactoryImpl("BINARY-MEMTABLE-FLUSHER-POOL")
+            );  
+    
+    /* Submit memtables to be flushed to disk */
+    void submit(String cfName, BinaryMemtable memtbl)
+    {
+    	flusher_.submit( new BinaryMemtableFlusher(memtbl) );
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryVerbHandler.java
index e69de29b..e79f30f3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryVerbHandler.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import org.apache.cassandra.db.RowMutationVerbHandler.RowMutationContext;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class BinaryVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(BinaryVerbHandler.class);    
+    /* We use this so that we can reuse the same row mutation context for the mutation. */
+    private static ThreadLocal<RowMutationContext> tls_ = new InheritableThreadLocal<RowMutationContext>();
+    
+    public void doVerb(Message message)
+    { 
+        byte[] bytes = (byte[])message.getMessageBody()[0];
+        /* Obtain a Row Mutation Context from TLS */
+        RowMutationContext rowMutationCtx = tls_.get();
+        if ( rowMutationCtx == null )
+        {
+            rowMutationCtx = new RowMutationContext();
+            tls_.set(rowMutationCtx);
+        }                
+        rowMutationCtx.buffer_.reset(bytes, bytes.length);
+        
+	    try
+	    {
+            RowMutationMessage rmMsg = RowMutationMessage.serializer().deserialize(rowMutationCtx.buffer_);
+            RowMutation rm = rmMsg.getRowMutation();            	                
+            rowMutationCtx.row_.key(rm.key());
+            rm.load(rowMutationCtx.row_);
+	
+	    }        
+	    catch ( Exception e )
+	    {
+	        logger_.debug(LogUtil.throwableToString(e));            
+	    }        
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutDeployMessage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutDeployMessage.java
index e69de29b..4b2f89cc 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutDeployMessage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutDeployMessage.java
@@ -0,0 +1,89 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+
+
+public class CalloutDeployMessage
+{
+    private static ICompactSerializer<CalloutDeployMessage> serializer_;
+    
+    static
+    {
+        serializer_ = new CalloutDeployMessageSerializer();
+    }
+    
+    public static ICompactSerializer<CalloutDeployMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    public static Message getCalloutDeployMessage(CalloutDeployMessage cdMessage) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        serializer_.serialize(cdMessage, dos);
+        Message message = new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.calloutDeployVerbHandler_, new Object[]{bos.toByteArray()});
+        return message;
+    }
+    
+    /* Name of the callout */
+    private String callout_;
+    /* The actual procedure */
+    private String script_;
+    
+    public CalloutDeployMessage(String callout, String script)
+    {
+        callout_ = callout;
+        script_ = script;
+    }
+    
+    String getCallout()
+    {
+        return callout_;
+    }
+    
+    String getScript()
+    {
+        return script_;
+    }
+}
+
+class CalloutDeployMessageSerializer implements ICompactSerializer<CalloutDeployMessage>
+{
+    public void serialize(CalloutDeployMessage cdMessage, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(cdMessage.getCallout());
+        dos.writeUTF(cdMessage.getScript());
+    }
+    
+    public CalloutDeployMessage deserialize(DataInputStream dis) throws IOException
+    {
+        String callout = dis.readUTF();
+        String script = dis.readUTF();
+        return new CalloutDeployMessage(callout, script);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutDeployVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutDeployVerbHandler.java
index e69de29b..2fc8452c 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutDeployVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutDeployVerbHandler.java
@@ -0,0 +1,50 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+public class CalloutDeployVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(CalloutDeployVerbHandler.class);
+    
+    public void doVerb(Message message)
+    {
+        Object[] body = message.getMessageBody();
+        byte[] bytes = (byte[])body[0];
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(bytes, bytes.length);
+        try
+        {
+            CalloutDeployMessage cdMessage = CalloutDeployMessage.serializer().deserialize(bufIn);
+            /* save the callout to callout cache and to disk. */
+            CalloutManager.instance().addCallout( cdMessage.getCallout(), cdMessage.getScript() );
+        }
+        catch ( IOException ex )
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }        
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutManager.java
index e69de29b..b644cc03 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CalloutManager.java
@@ -0,0 +1,210 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.List;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import javax.script.Bindings;
+import javax.script.Invocable;
+import javax.script.ScriptEngine;
+import javax.script.ScriptEngineManager;
+import javax.script.Compilable;
+import javax.script.CompiledScript;
+import javax.script.ScriptException;
+import javax.script.SimpleBindings;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.procedures.GroovyScriptRunner;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.cassandra.utils.FileUtils;
+
+import org.apache.log4j.Logger;
+
+public class CalloutManager
+{
+    private final static Logger logger_ = Logger.getLogger(CalloutManager.class); 
+    private static final String extn_ = ".groovy";
+    /* Used to lock the factory for creation of CalloutManager instance */
+    private static Lock createLock_ = new ReentrantLock();
+    /* An instance of the CalloutManager  */
+    private static CalloutManager instance_;
+    
+    public static CalloutManager instance()
+    {
+        if ( instance_ == null )
+        {
+            CalloutManager.createLock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                {
+                    instance_ = new CalloutManager();
+                }
+            }
+            finally
+            {
+                CalloutManager.createLock_.unlock();
+            }
+        }
+        return instance_;
+    }
+    
+    /* Map containing the name of callout as key and the callout script as value */
+    private Map<String, CompiledScript> calloutCache_ = new HashMap<String, CompiledScript>();    
+    /* The Groovy Script compiler instance */
+    private Compilable compiler_;
+    /* The Groovy script invokable instance */
+    private Invocable invokable_;
+    
+    private CalloutManager()
+    {
+        ScriptEngineManager scriptManager = new ScriptEngineManager();
+        ScriptEngine groovyEngine = scriptManager.getEngineByName("groovy");
+        compiler_ = (Compilable)groovyEngine;
+        invokable_ = (Invocable)groovyEngine;
+    }
+    
+    /**
+     * Compile the script and cache the compiled script.
+     * @param script to be compiled
+     * @throws ScriptException
+     */
+    private void compileAndCache(String scriptId, String script) throws ScriptException
+    {
+        if ( compiler_ != null )
+        {
+            CompiledScript compiledScript = compiler_.compile(script);
+            calloutCache_.put(scriptId, compiledScript);
+        }
+    }
+    
+    /**
+     * Invoked on start up to load all the stored callouts, compile
+     * and cache them.
+     * 
+     * @throws IOException
+     */
+    void onStart() throws IOException
+    {
+    	String location = DatabaseDescriptor.getCalloutLocation();
+    	if ( location == null )
+    		return;
+    	
+        FileUtils.createDirectory(location);
+        
+        File[] files = new File(location).listFiles();
+        
+        for ( File file : files )
+        {
+            String f = file.getName();
+            /* Get the callout name from the file */
+            String callout = f.split(extn_)[0];
+            FileInputStream fis = new FileInputStream(file);
+            byte[] bytes = new byte[fis.available()];
+            fis.read(bytes);
+            fis.close();
+            /* cache the callout after compiling it */
+            try
+            {
+                compileAndCache(callout, new String(bytes));                    
+            }
+            catch ( ScriptException ex )
+            {
+                logger_.warn(LogUtil.throwableToString(ex));
+            }
+        }
+    }
+    
+    /**
+     * Store the callout in cache and write it out
+     * to disk.
+     * @param callout the name of the callout
+     * @param script actual implementation of the callout
+    */
+    public void addCallout(String callout, String script) throws IOException
+    {
+        /* cache the script */
+        /* cache the callout after compiling it */
+        try
+        {
+            compileAndCache(callout, script);                    
+        }
+        catch ( ScriptException ex )
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }
+        /* save the script to disk */
+        String scriptFile = DatabaseDescriptor.getCalloutLocation() + System.getProperty("file.separator") + callout + extn_;
+        File file = new File(scriptFile);
+        if ( file.exists() )
+        {
+            logger_.debug("Deleting the old script file ...");
+            file.delete();
+        }
+        FileOutputStream fos = new FileOutputStream(scriptFile);
+        fos.write(script.getBytes());
+        fos.close();
+    }
+    
+    /**
+     * Remove the registered callout and delete the
+     * script on the disk.
+     * @param callout to be removed
+     */
+    public void removeCallout(String callout)
+    {
+        /* remove the script from cache */
+        calloutCache_.remove(callout);
+        String scriptFile = DatabaseDescriptor.getCalloutLocation() + System.getProperty("file.separator") + callout + ".grv";
+        File file = new File(scriptFile);
+        file.delete();
+    }
+    
+    /**
+     * Execute the specified callout.
+     * @param callout to be executed.
+     * @param args arguments to be passed to the callouts.
+     */
+    public Object executeCallout(String callout, Object ... args)
+    {
+        Object result = null;
+        CompiledScript script = calloutCache_.get(callout);
+        if ( script != null )
+        {
+            try
+            {
+                Bindings binding = new SimpleBindings();
+                binding.put("args", args);
+                result = script.eval(binding);
+            }
+            catch(ScriptException ex)
+            {
+                logger_.warn(LogUtil.throwableToString(ex));
+            }
+        }
+        return result;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Column.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Column.java
index e69de29b..45e0ad42 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Column.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Column.java
@@ -0,0 +1,306 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.Collection;
+import java.nio.ByteBuffer;
+
+import org.apache.commons.lang.ArrayUtils;
+
+import org.apache.cassandra.utils.FBUtilities;
+
+
+/**
+ * Column is immutable, which prevents all kinds of confusion in a multithreaded environment.
+ * (TODO: look at making SuperColumn immutable too.  This is trickier but is probably doable
+ *  with something like PCollections -- http://code.google.com
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com ) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class Column implements IColumn
+{
+    private static ColumnSerializer serializer_ = new ColumnSerializer();
+
+    static ColumnSerializer serializer()
+    {
+        return serializer_;
+    }
+
+    private final String name;
+    private final byte[] value;
+    private final long timestamp;
+    private final boolean isMarkedForDelete;
+
+    Column(String name)
+    {
+        this(name, ArrayUtils.EMPTY_BYTE_ARRAY);
+    }
+
+    Column(String name, byte[] value)
+    {
+        this(name, value, 0);
+    }
+
+    Column(String name, byte[] value, long timestamp)
+    {
+        this(name, value, timestamp, false);
+    }
+
+    Column(String name, byte[] value, long timestamp, boolean isDeleted)
+    {
+        assert name != null;
+        assert value != null;
+        this.name = name;
+        this.value = value;
+        this.timestamp = timestamp;
+        isMarkedForDelete = isDeleted;
+    }
+
+    public String name()
+    {
+        return name;
+    }
+
+    public IColumn getSubColumn(String columnName)
+    {
+        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
+    }
+
+    public byte[] value()
+    {
+        return value;
+    }
+
+    public byte[] value(String key)
+    {
+        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
+    }
+
+    public Collection<IColumn> getSubColumns()
+    {
+        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
+    }
+
+    public int getObjectCount()
+    {
+        return 1;
+    }
+
+    public long timestamp()
+    {
+        return timestamp;
+    }
+
+    public long timestamp(String key)
+    {
+        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
+    }
+
+    public boolean isMarkedForDelete()
+    {
+        return isMarkedForDelete;
+    }
+
+    public long getMarkedForDeleteAt()
+    {
+        if (!isMarkedForDelete())
+        {
+            throw new IllegalStateException("column is not marked for delete");
+        }
+        return timestamp;
+    }
+
+    public int size()
+    {
+        /*
+         * Size of a column is =
+         *   size of a name (UtfPrefix + length of the string)
+         * + 1 byte to indicate if the column has been deleted
+         * + 8 bytes for timestamp
+         * + 4 bytes which basically indicates the size of the byte array
+         * + entire byte array.
+        */
+
+        /*
+           * We store the string as UTF-8 encoded, so when we calculate the length, it
+           * should be converted to UTF-8.
+           */
+        return IColumn.UtfPrefix_ + FBUtilities.getUTF8Length(name) + DBConstants.boolSize_ + DBConstants.tsSize_ + DBConstants.intSize_ + value.length;
+    }
+
+    /*
+     * This returns the size of the column when serialized.
+     * @see com.facebook.infrastructure.db.IColumn#serializedSize()
+    */
+    public int serializedSize()
+    {
+        return size();
+    }
+
+    public void addColumn(String name, IColumn column)
+    {
+        throw new UnsupportedOperationException("This operation is not supported for simple columns.");
+    }
+
+    public IColumn diff(IColumn column)
+    {
+        if (timestamp() < column.timestamp())
+        {
+            return column;
+        }
+        return null;
+    }
+
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder();
+        sb.append(name);
+        sb.append(":");
+        sb.append(isMarkedForDelete());
+        sb.append(":");
+        sb.append(value().length);
+        sb.append("@");
+        sb.append(timestamp());
+        return sb.toString();
+    }
+
+    public byte[] digest()
+    {
+        StringBuilder stringBuilder = new StringBuilder();
+        stringBuilder.append(name);
+        stringBuilder.append(":");
+        stringBuilder.append(timestamp);
+        return stringBuilder.toString().getBytes();
+    }
+
+    public int getLocalDeletionTime()
+    {
+        assert isMarkedForDelete;
+        return ByteBuffer.wrap(value).getInt();
+    }
+}
+
+class ColumnSerializer implements ICompactSerializer2<IColumn>
+{
+    public void serialize(IColumn column, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(column.name());
+        dos.writeBoolean(column.isMarkedForDelete());
+        dos.writeLong(column.timestamp());
+        dos.writeInt(column.value().length);
+        dos.write(column.value());
+    }
+
+    private IColumn defreeze(DataInputStream dis, String name) throws IOException
+    {
+        IColumn column = null;
+        boolean delete = dis.readBoolean();
+        long ts = dis.readLong();
+        int size = dis.readInt();
+        byte[] value = new byte[size];
+        dis.readFully(value);
+        column = new Column(name, value, ts, delete);
+        return column;
+    }
+
+    public IColumn deserialize(DataInputStream dis) throws IOException
+    {
+        String name = dis.readUTF();
+        return defreeze(dis, name);
+    }
+
+    /**
+     * Here we need to get the column and apply the filter.
+     */
+    public IColumn deserialize(DataInputStream dis, IFilter filter) throws IOException
+    {
+        if ( dis.available() == 0 )
+            return null;
+
+        String name = dis.readUTF();
+        IColumn column = new Column(name);
+        column = filter.filter(column, dis);
+        if ( column != null )
+        {
+            column = defreeze(dis, name);
+        }
+        else
+        {
+        	/* Skip a boolean and the timestamp */
+        	dis.skip(DBConstants.boolSize_ + DBConstants.tsSize_);
+            int size = dis.readInt();
+            dis.skip(size);
+        }
+        return column;
+    }
+
+    /**
+     * We know the name of the column here so just return it.
+     * Filter is pretty much useless in this call and is ignored.
+     */
+    public IColumn deserialize(DataInputStream dis, String columnName, IFilter filter) throws IOException
+    {
+        if ( dis.available() == 0 )
+            return null;
+        IColumn column = null;
+        String name = dis.readUTF();
+        if ( name.equals(columnName) )
+        {
+            column = defreeze(dis, name);
+            if( filter instanceof IdentityFilter )
+            {
+            	/*
+            	 * If this is being called with identity filter
+            	 * since a column name is passed in we know
+            	 * that this is a final call
+            	 * Hence if the column is found set the filter to done
+            	 * so that we do not look for the column in further files
+            	 */
+            	IdentityFilter f = (IdentityFilter)filter;
+            	f.setDone();
+            }
+        }
+        else
+        {
+        	/* Skip a boolean and the timestamp */
+        	dis.skip(DBConstants.boolSize_ + DBConstants.tsSize_);
+            int size = dis.readInt();
+            dis.skip(size);
+        }
+        return column;
+    }
+
+    public void skip(DataInputStream dis) throws IOException
+    {
+    	/* read the column name */
+        dis.readUTF();
+        /* boolean indicating if the column is deleted */
+        dis.readBoolean();
+        /* timestamp associated with the column */
+        dis.readLong();
+        /* size of the column */
+        int size = dis.readInt();
+        dis.skip(size);
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnComparatorFactory.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnComparatorFactory.java
index e69de29b..88e3d1d4 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnComparatorFactory.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnComparatorFactory.java
@@ -0,0 +1,150 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.Serializable;
+import java.util.Comparator;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ColumnComparatorFactory
+{
+    public static enum ComparatorType
+    {
+        NAME,
+        TIMESTAMP
+    }
+
+    private static Comparator<IColumn> nameComparator_ = new ColumnNameComparator();
+    private static Comparator<IColumn> timestampComparator_ = new ColumnTimestampComparator();
+
+    public static Comparator<IColumn> getComparator(ComparatorType comparatorType)
+    {
+        Comparator<IColumn> columnComparator = timestampComparator_;
+
+        switch (comparatorType)
+        {
+            case NAME:
+                columnComparator = nameComparator_;
+                break;
+
+            case TIMESTAMP:
+
+            default:
+                columnComparator = timestampComparator_;
+                break;
+        }
+
+        return columnComparator;
+    }
+
+    public static Comparator<IColumn> getComparator(int comparatorTypeInt)
+    {
+        ComparatorType comparatorType = ComparatorType.NAME;
+
+        if (comparatorTypeInt == ComparatorType.NAME.ordinal())
+        {
+            comparatorType = ComparatorType.NAME;
+        }
+        else if (comparatorTypeInt == ComparatorType.TIMESTAMP.ordinal())
+        {
+            comparatorType = ComparatorType.TIMESTAMP;
+        }
+        return getComparator(comparatorType);
+    }
+
+}
+
+abstract class AbstractColumnComparator implements Comparator<IColumn>, Serializable
+{
+    protected ColumnComparatorFactory.ComparatorType comparatorType_;
+
+    public AbstractColumnComparator(ColumnComparatorFactory.ComparatorType comparatorType)
+    {
+        comparatorType_ = comparatorType;
+    }
+
+    ColumnComparatorFactory.ComparatorType getComparatorType()
+    {
+        return comparatorType_;
+    }
+}
+
+class ColumnTimestampComparator extends AbstractColumnComparator
+{
+    ColumnTimestampComparator()
+    {
+        super(ColumnComparatorFactory.ComparatorType.TIMESTAMP);
+    }
+
+    /* if the time-stamps are the same then sort by names */
+    public int compare(IColumn column1, IColumn column2)
+    {
+        assert column1.getClass() == column2.getClass();
+        /* inverse sort by time to get hte latest first */
+        long result = column2.timestamp() - column1.timestamp();
+        int finalResult = 0;
+        if (result == 0)
+        {
+            result = column1.name().compareTo(column2.name());
+        }
+        if (result > 0)
+        {
+            finalResult = 1;
+        }
+        if (result < 0)
+        {
+            finalResult = -1;
+        }
+        return finalResult;
+    }
+}
+
+class ColumnNameComparator extends AbstractColumnComparator
+{
+    ColumnNameComparator()
+    {
+        super(ColumnComparatorFactory.ComparatorType.NAME);
+    }
+
+    /* if the names are the same then sort by time-stamps */
+    public int compare(IColumn column1, IColumn column2)
+    {
+        assert column1.getClass() == column2.getClass();
+        long result = column1.name().compareTo(column2.name());
+        int finalResult = 0;
+        if (result == 0 && (column1 instanceof Column))
+        {
+            /* inverse sort by time to get the latest first */
+            result = column2.timestamp() - column1.timestamp();
+        }
+        if (result > 0)
+        {
+            finalResult = 1;
+        }
+        if (result < 0)
+        {
+            finalResult = -1;
+        }
+        return finalResult;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamily.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamily.java
index e69de29b..8b6c592d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamily.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamily.java
@@ -0,0 +1,584 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.lang.reflect.Proxy;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedSet;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.FBUtilities;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public final class ColumnFamily
+{
+    /* The column serializer for this Column Family. Create based on config. */
+    private static ICompactSerializer2<ColumnFamily> serializer_;
+    public static final short utfPrefix_ = 2;   
+
+    private static Logger logger_ = Logger.getLogger( ColumnFamily.class );
+    private static Map<String, String> columnTypes_ = new HashMap<String, String>();
+    private static Map<String, String> indexTypes_ = new HashMap<String, String>();
+    private String type_;
+
+    static
+    {
+        serializer_ = new ColumnFamilySerializer();
+        /* TODO: These are the various column types. Hard coded for now. */
+        columnTypes_.put("Standard", "Standard");
+        columnTypes_.put("Super", "Super");
+
+        indexTypes_.put("Name", "Name");
+        indexTypes_.put("Time", "Time");
+    }
+
+    public static ICompactSerializer2<ColumnFamily> serializer()
+    {
+        return serializer_;
+    }
+
+    /*
+     * This method returns the serializer whose methods are
+     * preprocessed by a dynamic proxy.
+    */
+    public static ICompactSerializer2<ColumnFamily> serializerWithIndexes()
+    {
+        return (ICompactSerializer2<ColumnFamily>)Proxy.newProxyInstance( ColumnFamily.class.getClassLoader(), new Class[]{ICompactSerializer2.class}, new CompactSerializerInvocationHandler<ColumnFamily>(serializer_) );
+    }
+
+    public static String getColumnType(String key)
+    {
+    	if ( key == null )
+    		return columnTypes_.get("Standard");
+    	return columnTypes_.get(key);
+    }
+
+    public static String getColumnSortProperty(String columnIndexProperty)
+    {
+    	if ( columnIndexProperty == null )
+    		return indexTypes_.get("Time");
+        return indexTypes_.get(columnIndexProperty);
+    }
+
+    private transient AbstractColumnFactory columnFactory_;
+
+    private String name_;
+
+    private transient ICompactSerializer2<IColumn> columnSerializer_;
+    private long markedForDeleteAt = Long.MIN_VALUE;
+    private int localDeletionTime = Integer.MIN_VALUE;
+    private AtomicInteger size_ = new AtomicInteger(0);
+    private EfficientBidiMap columns_;
+
+    private Comparator<IColumn> columnComparator_;
+
+	private Comparator<IColumn> getColumnComparator(String cfName, String columnType)
+	{
+		if(columnComparator_ == null)
+		{
+			/*
+			 * if this columnfamily has supercolumns or there is an index on the column name,
+			 * then sort by name
+			*/
+			if("Super".equals(columnType) || DatabaseDescriptor.isNameSortingEnabled(cfName))
+			{
+				columnComparator_ = ColumnComparatorFactory.getComparator(ColumnComparatorFactory.ComparatorType.NAME);
+			}
+			/* if this columnfamily has simple columns, and no index on name sort by timestamp */
+			else
+			{
+				columnComparator_ = ColumnComparatorFactory.getComparator(ColumnComparatorFactory.ComparatorType.TIMESTAMP);
+			}
+		}
+
+		return columnComparator_;
+	}
+
+    public ColumnFamily(String cfName, String columnType)
+    {
+        name_ = cfName;
+        type_ = columnType;
+        createColumnFactoryAndColumnSerializer(columnType);
+    }
+
+    void createColumnFactoryAndColumnSerializer(String columnType)
+    {
+        if ( columnFactory_ == null )
+        {
+            columnFactory_ = AbstractColumnFactory.getColumnFactory(columnType);
+            columnSerializer_ = columnFactory_.createColumnSerializer();
+            if(columns_ == null)
+                columns_ = new EfficientBidiMap(getColumnComparator(name_, columnType));
+        }
+    }
+
+    void createColumnFactoryAndColumnSerializer()
+    {
+    	String columnType = DatabaseDescriptor.getColumnFamilyType(name_);
+        if ( columnType == null )
+        {
+        	List<String> tables = DatabaseDescriptor.getTables();
+        	if ( tables.size() > 0 )
+        	{
+        		String table = tables.get(0);
+        		columnType = Table.open(table).getColumnFamilyType(name_);
+        	}
+        }
+        createColumnFactoryAndColumnSerializer(columnType);
+    }
+
+    ColumnFamily cloneMeShallow()
+    {
+        ColumnFamily cf = new ColumnFamily(name_, type_);
+        cf.markedForDeleteAt = markedForDeleteAt;
+        cf.localDeletionTime = localDeletionTime;
+        return cf;
+    }
+
+    ColumnFamily cloneMe()
+    {
+        ColumnFamily cf = cloneMeShallow();
+        cf.columns_ = columns_.cloneMe();
+    	return cf;
+    }
+
+    public String name()
+    {
+        return name_;
+    }
+
+    /*
+     *  We need to go through each column
+     *  in the column family and resolve it before adding
+    */
+    void addColumns(ColumnFamily cf)
+    {
+        for (IColumn column : cf.getAllColumns())
+        {
+            addColumn(column);
+        }
+    }
+
+    public ICompactSerializer2<IColumn> getColumnSerializer()
+    {
+        createColumnFactoryAndColumnSerializer();
+    	return columnSerializer_;
+    }
+
+    public void addColumn(String name)
+    {
+    	addColumn(columnFactory_.createColumn(name));
+    }
+
+    int getColumnCount()
+    {
+    	int count = 0;
+    	Map<String, IColumn> columns = columns_.getColumns();
+    	if( columns != null )
+    	{
+    		if(!isSuper())
+    		{
+    			count = columns.size();
+    		}
+    		else
+    		{
+    			Collection<IColumn> values = columns.values();
+		    	for(IColumn column: values)
+		    	{
+		    		count += column.getObjectCount();
+		    	}
+    		}
+    	}
+    	return count;
+    }
+
+    public boolean isSuper()
+    {
+        return type_.equals("Super");
+    }
+
+    public void addColumn(String name, byte[] value)
+    {
+    	addColumn(name, value, 0);
+    }
+
+    public void addColumn(String name, byte[] value, long timestamp)
+    {
+        addColumn(name, value, timestamp, false);
+    }
+
+    public void addColumn(String name, byte[] value, long timestamp, boolean deleted)
+	{
+		IColumn column = columnFactory_.createColumn(name, value, timestamp, deleted);
+		addColumn(column);
+    }
+
+    void clear()
+    {
+    	columns_.clear();
+    }
+
+    /*
+     * If we find an old column that has the same name
+     * the ask it to resolve itself else add the new column .
+    */
+    void addColumn(IColumn column)
+    {
+        String name = column.name();
+        IColumn oldColumn = columns_.get(name);
+        if (oldColumn != null)
+        {
+            if (oldColumn instanceof SuperColumn)
+            {
+                int oldSize = oldColumn.size();
+                ((SuperColumn) oldColumn).putColumn(column);
+                size_.addAndGet(oldColumn.size() - oldSize);
+            }
+            else
+            {
+                if (oldColumn.timestamp() <= column.timestamp())
+                {
+                    columns_.put(name, column);
+                    size_.addAndGet(column.size());
+                }
+            }
+        }
+        else
+        {
+            size_.addAndGet(column.size());
+            columns_.put(name, column);
+        }
+    }
+
+    public IColumn getColumn(String name)
+    {
+        return columns_.get( name );
+    }
+
+    public SortedSet<IColumn> getAllColumns()
+    {
+        return columns_.getSortedColumns();
+    }
+
+    public Map<String, IColumn> getColumns()
+    {
+        return columns_.getColumns();
+    }
+
+    public void remove(String columnName)
+    {
+    	columns_.remove(columnName);
+    }
+
+    void delete(int localtime, long timestamp)
+    {
+        localDeletionTime = localtime;
+        markedForDeleteAt = timestamp;
+    }
+
+    public boolean isMarkedForDelete()
+    {
+        return markedForDeleteAt > Long.MIN_VALUE;
+    }
+
+    /*
+     * This function will calculate the differnce between 2 column families
+     * the external input is considered the superset of internal
+     * so there are no deletes in the diff.
+     */
+    ColumnFamily diff(ColumnFamily cfComposite)
+    {
+    	ColumnFamily cfDiff = new ColumnFamily(cfComposite.name(), cfComposite.type_);
+        if (cfComposite.getMarkedForDeleteAt() > getMarkedForDeleteAt())
+        {
+            cfDiff.delete(cfComposite.getLocalDeletionTime(), cfComposite.getMarkedForDeleteAt());
+        }
+
+        // (don't need to worry about cfNew containing IColumns that are shadowed by
+        // the delete tombstone, since cfNew was generated by CF.resolve, which
+        // takes care of those for us.)
+        Map<String, IColumn> columns = cfComposite.getColumns();
+        Set<String> cNames = columns.keySet();
+        for ( String cName : cNames )
+        {
+        	IColumn columnInternal = columns_.get(cName);
+        	IColumn columnExternal = columns.get(cName);
+        	if( columnInternal == null )
+        	{
+        		cfDiff.addColumn(columnExternal);
+        	}
+        	else
+        	{
+            	IColumn columnDiff = columnInternal.diff(columnExternal);
+        		if(columnDiff != null)
+        		{
+        			cfDiff.addColumn(columnDiff);
+        		}
+        	}
+        }
+
+        if (!cfDiff.getColumns().isEmpty() || cfDiff.isMarkedForDelete())
+        	return cfDiff;
+        else
+        	return null;
+    }
+
+    int size()
+    {
+        if ( size_.get() == 0 )
+        {
+            Set<String> cNames = columns_.getColumns().keySet();
+            for ( String cName : cNames )
+            {
+                size_.addAndGet(columns_.get(cName).size());
+            }
+        }
+        return size_.get();
+    }
+
+    public int hashCode()
+    {
+        return name().hashCode();
+    }
+
+    public boolean equals(Object o)
+    {
+        if ( !(o instanceof ColumnFamily) )
+            return false;
+        ColumnFamily cf = (ColumnFamily)o;
+        return name().equals(cf.name());
+    }
+
+    public String toString()
+    {
+    	StringBuilder sb = new StringBuilder();
+        sb.append("ColumnFamily(");
+    	sb.append(name_);
+
+        if (isMarkedForDelete()) {
+            sb.append(" -delete at " + getMarkedForDeleteAt() + "-");
+        }
+
+    	sb.append(" [");
+        sb.append(StringUtils.join(getAllColumns(), ", "));
+        sb.append("])");
+
+    	return sb.toString();
+    }
+
+    public byte[] digest()
+    {
+    	Set<IColumn> columns = columns_.getSortedColumns();
+    	byte[] xorHash = null;
+    	for(IColumn column : columns)
+    	{
+    		if(xorHash == null)
+    		{
+    			xorHash = column.digest();
+    		}
+    		else
+    		{
+                xorHash = FBUtilities.xor(xorHash, column.digest());
+    		}
+    	}
+    	return xorHash;
+    }
+
+    public long getMarkedForDeleteAt()
+    {
+        return markedForDeleteAt;
+    }
+
+    public int getLocalDeletionTime()
+    {
+        return localDeletionTime;
+    }
+
+    public String type()
+    {
+        return type_;
+    }
+
+    /** merge all columnFamilies into a single instance, with only the newest versions of columns preserved. */
+    static ColumnFamily resolve(List<ColumnFamily> columnFamilies)
+    {
+        int size = columnFamilies.size();
+        if (size == 0)
+            return null;
+
+        // start from nothing so that we don't include potential deleted columns from the first instance
+        ColumnFamily cf0 = columnFamilies.get(0);
+        ColumnFamily cf = cf0.cloneMeShallow();
+
+        // merge
+        for (ColumnFamily cf2 : columnFamilies)
+        {
+            assert cf.name().equals(cf2.name());
+            cf.addColumns(cf2);
+            cf.delete(Math.max(cf.getLocalDeletionTime(), cf2.getLocalDeletionTime()),
+                      Math.max(cf.getMarkedForDeleteAt(), cf2.getMarkedForDeleteAt()));
+        }
+        return cf;
+    }
+
+    public static class ColumnFamilySerializer implements ICompactSerializer2<ColumnFamily>
+    {
+        /*
+         * We are going to create indexes, and write out that information as well. The format
+         * of the data serialized is as follows.
+         *
+         * 1) Without indexes:
+         *  // written by the data
+         * 	<boolean false (index is not present)>
+         * 	<column family id>
+         * 	<is marked for delete>
+         * 	<total number of columns>
+         * 	<columns data>
+
+         * 	<boolean true (index is present)>
+         *
+         *  This part is written by the column indexer
+         * 	<size of index in bytes>
+         * 	<list of column names and their offsets relative to the first column>
+         *
+         *  <size of the cf in bytes>
+         * 	<column family id>
+         * 	<is marked for delete>
+         * 	<total number of columns>
+         * 	<columns data>
+        */
+        public void serialize(ColumnFamily columnFamily, DataOutputStream dos) throws IOException
+        {
+            Collection<IColumn> columns = columnFamily.getAllColumns();
+
+            dos.writeUTF(columnFamily.name());
+            dos.writeInt(columnFamily.localDeletionTime);
+            dos.writeLong(columnFamily.markedForDeleteAt);
+
+            dos.writeInt(columns.size());
+            for ( IColumn column : columns )
+            {
+                columnFamily.getColumnSerializer().serialize(column, dos);
+            }
+        }
+
+        /*
+         * Use this method to create a bare bones Column Family. This column family
+         * does not have any of the Column information.
+        */
+        private ColumnFamily defreezeColumnFamily(DataInputStream dis) throws IOException
+        {
+            String name = dis.readUTF();
+            ColumnFamily cf = new ColumnFamily(name, DatabaseDescriptor.getColumnFamilyType(name));
+            cf.delete(dis.readInt(), dis.readLong());
+            return cf;
+        }
+
+        public ColumnFamily deserialize(DataInputStream dis) throws IOException
+        {
+            ColumnFamily cf = defreezeColumnFamily(dis);
+            int size = dis.readInt();
+            IColumn column = null;
+            for ( int i = 0; i < size; ++i )
+            {
+                column = cf.getColumnSerializer().deserialize(dis);
+                if(column != null)
+                {
+                    cf.addColumn(column);
+                }
+            }
+            return cf;
+        }
+
+        /*
+         * This version of deserialize is used when we need a specific set if columns for
+         * a column family specified in the name cfName parameter.
+        */
+        public ColumnFamily deserialize(DataInputStream dis, IFilter filter) throws IOException
+        {
+            ColumnFamily cf = defreezeColumnFamily(dis);
+            int size = dis.readInt();
+            IColumn column = null;
+            for ( int i = 0; i < size; ++i )
+            {
+                column = cf.getColumnSerializer().deserialize(dis, filter);
+                if(column != null)
+                {
+                    cf.addColumn(column);
+                    column = null;
+                    if(filter.isDone())
+                    {
+                        break;
+                    }
+                }
+            }
+            return cf;
+        }
+
+        /*
+         * Deserialize a particular column or super column or the entire columnfamily given a : seprated name
+         * name could be of the form cf:superColumn:column  or cf:column or cf
+         */
+        public ColumnFamily deserialize(DataInputStream dis, String name, IFilter filter) throws IOException
+        {
+            String[] names = RowMutation.getColumnAndColumnFamily(name);
+            String columnName = "";
+            if ( names.length == 1 )
+                return deserialize(dis, filter);
+            if( names.length == 2 )
+                columnName = names[1];
+            if( names.length == 3 )
+                columnName = names[1]+ ":" + names[2];
+
+            ColumnFamily cf = defreezeColumnFamily(dis);
+            /* read the number of columns */
+            int size = dis.readInt();
+            for ( int i = 0; i < size; ++i )
+            {
+                IColumn column = cf.getColumnSerializer().deserialize(dis, columnName, filter);
+                if ( column != null )
+                {
+                    cf.addColumn(column);
+                    break;
+                }
+            }
+            return cf;
+        }
+
+        public void skip(DataInputStream dis) throws IOException
+        {
+            throw new UnsupportedOperationException("This operation is not yet supported.");
+        }
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyNotDefinedException.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyNotDefinedException.java
index e69de29b..2675a940 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyNotDefinedException.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyNotDefinedException.java
@@ -0,0 +1,34 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import org.apache.cassandra.service.InvalidRequestException;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ColumnFamilyNotDefinedException extends InvalidRequestException
+{
+    public ColumnFamilyNotDefinedException(String message)
+    {
+        super(message);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
index e69de29b..1ce5a195 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
@@ -0,0 +1,1457 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.File;
+import java.io.IOException;
+import java.lang.management.ManagementFactory;
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.Set;
+import java.util.StringTokenizer;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IndexHelper;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.SequenceFile;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ColumnFamilyStore implements ColumnFamilyStoreMBean
+{
+    private static int threshHold_ = 4;
+    private static final int bufSize_ = 128*1024*1024;
+    private static int compactionMemoryThreshold_ = 1 << 30;
+    private static Logger logger_ = Logger.getLogger(ColumnFamilyStore.class);
+
+    private final String table_;
+    public final String columnFamily_;
+    private final boolean isSuper_;
+    
+    private volatile Integer memtableSwitchCount = 0;
+
+    /* This is used to generate the next index for a SSTable */
+    private AtomicInteger fileIndexGenerator_ = new AtomicInteger(0);
+
+    /* memtable associated with this ColumnFamilyStore. */
+    private AtomicReference<Memtable> memtable_;
+    private AtomicReference<BinaryMemtable> binaryMemtable_;
+
+    /* SSTables on disk for this column family */
+    private Set<String> ssTables_ = new HashSet<String>();
+
+    /* Modification lock used for protecting reads from compactions. */
+    private ReentrantReadWriteLock lock_ = new ReentrantReadWriteLock(true);
+
+    /* Flag indicates if a compaction is in process */
+    private AtomicBoolean isCompacting_ = new AtomicBoolean(false);
+
+    ColumnFamilyStore(String table, String columnFamily, boolean isSuper, int indexValue) throws IOException
+    {
+        table_ = table;
+        columnFamily_ = columnFamily;
+        isSuper_ = isSuper;
+        fileIndexGenerator_.set(indexValue);
+        memtable_ = new AtomicReference<Memtable>(new Memtable(table_, columnFamily_));
+        binaryMemtable_ = new AtomicReference<BinaryMemtable>(new BinaryMemtable(table_, columnFamily_));
+    }
+
+    public static ColumnFamilyStore getColumnFamilyStore(String table, String columnFamily) throws IOException
+    {
+        /*
+         * Get all data files associated with old Memtables for this table.
+         * These files are named as follows <Table>-1.db, ..., <Table>-n.db. Get
+         * the max which in this case is n and increment it to use it for next
+         * index.
+         */
+        List<Integer> indices = new ArrayList<Integer>();
+        String[] dataFileDirectories = DatabaseDescriptor.getAllDataFileLocations();
+        for ( String directory : dataFileDirectories )
+        {
+            File fileDir = new File(directory);
+            File[] files = fileDir.listFiles();
+            for (File file : files)
+            {
+                String filename = file.getName();
+                String[] tblCfName = getTableAndColumnFamilyName(filename);
+
+                if (tblCfName[0].equals(table)
+                        && tblCfName[1].equals(columnFamily))
+                {
+                    int index = getIndexFromFileName(filename);
+                    indices.add(index);
+                }
+            }
+        }
+        Collections.sort(indices);
+        int value = (indices.size() > 0) ? (indices.get(indices.size() - 1)) : 0;
+
+        ColumnFamilyStore cfs = new ColumnFamilyStore(table, columnFamily, "Super".equals(DatabaseDescriptor.getColumnType(columnFamily)), value);
+
+        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
+        try
+        {
+            mbs.registerMBean(cfs, new ObjectName(
+                    "org.apache.cassandra.db:type=ColumnFamilyStore-" + table + "." + columnFamily));
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+
+        return cfs;
+    }
+
+    void onStart() throws IOException
+    {
+        /* Do major compaction */
+        List<File> ssTables = new ArrayList<File>();
+        String[] dataFileDirectories = DatabaseDescriptor.getAllDataFileLocations();
+        for ( String directory : dataFileDirectories )
+        {
+            File fileDir = new File(directory);
+            File[] files = fileDir.listFiles();
+            for (File file : files)
+            {
+                String filename = file.getName();
+                if(((file.length() == 0) || (filename.contains("-" + SSTable.temporaryFile_)) ) && (filename.contains(columnFamily_)))
+                {
+                	file.delete();
+                	continue;
+                }
+                
+                String[] tblCfName = getTableAndColumnFamilyName(filename);
+                if (tblCfName[0].equals(table_)
+                        && tblCfName[1].equals(columnFamily_)
+                        && filename.contains("-Data.db"))
+                {
+                    ssTables.add(file.getAbsoluteFile());
+                }
+            }
+        }
+        Collections.sort(ssTables, new FileUtils.FileComparator());
+        List<String> filenames = new ArrayList<String>();
+        for (File ssTable : ssTables)
+        {
+            filenames.add(ssTable.getAbsolutePath());
+        }
+
+        /* There are no files to compact just add to the list of SSTables */
+        ssTables_.addAll(filenames);
+        /* Load the index files and the Bloom Filters associated with them. */
+        SSTable.onStart(filenames);
+        logger_.debug("Submitting a major compaction task ...");
+        MinorCompactionManager.instance().submit(ColumnFamilyStore.this);
+        if(columnFamily_.equals(Table.hints_))
+        {
+        	HintedHandOffManager.instance().submit(this);
+        }
+        // TODO this seems unnecessary -- each memtable flush checks to see if it needs to compact, too
+        MinorCompactionManager.instance().submitPeriodicCompaction(this);
+    }
+
+    List<String> getAllSSTablesOnDisk()
+    {
+        return new ArrayList<String>(ssTables_);
+    }
+
+    /*
+     * This method is called to obtain statistics about
+     * the Column Family represented by this Column Family
+     * Store. It will report the total number of files on
+     * disk and the total space oocupied by the data files
+     * associated with this Column Family.
+    */
+    public String cfStats(String newLineSeparator)
+    {
+        StringBuilder sb = new StringBuilder();
+        /*
+         * We want to do this so that if there are
+         * no files on disk we do not want to display
+         * something ugly on the admin page.
+        */
+        if ( ssTables_.size() == 0 )
+        {
+            return sb.toString();
+        }
+        sb.append(columnFamily_ + " statistics :");
+        sb.append(newLineSeparator);
+        sb.append("Number of files on disk : " + ssTables_.size());
+        sb.append(newLineSeparator);
+        double totalSpace = 0d;
+        for ( String file : ssTables_ )
+        {
+            File f = new File(file);
+            totalSpace += f.length();
+        }
+        String diskSpace = FileUtils.stringifyFileSize(totalSpace);
+        sb.append("Total disk space : " + diskSpace);
+        sb.append(newLineSeparator);
+        sb.append("--------------------------------------");
+        sb.append(newLineSeparator);
+        return sb.toString();
+    }
+
+    /*
+     * This is called after bootstrap to add the files
+     * to the list of files maintained.
+    */
+    void addToList(String file)
+    {
+    	lock_.writeLock().lock();
+        try
+        {
+            ssTables_.add(file);
+        }
+        finally
+        {
+        	lock_.writeLock().unlock();
+        }
+    }
+
+    void touch(String key, boolean fData) throws IOException
+    {
+        /* Scan the SSTables on disk first */
+        lock_.readLock().lock();
+        try
+        {
+            List<String> files = new ArrayList<String>(ssTables_);
+            for (String file : files)
+            {
+                /*
+                 * Get the BloomFilter associated with this file. Check if the key
+                 * is present in the BloomFilter. If not continue to the next file.
+                */
+                boolean bVal = SSTable.isKeyInFile(key, file);
+                if ( !bVal )
+                    continue;
+                SSTable ssTable = new SSTable(file, StorageService.getPartitioner());
+                ssTable.touch(key, fData);
+            }
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    }
+
+    /*
+     * This method forces a compaction of the SSTables on disk. We wait
+     * for the process to complete by waiting on a future pointer.
+    */
+    boolean forceCompaction(List<Range> ranges, EndPoint target, long skip, List<String> fileList)
+    {        
+    	Future<Boolean> futurePtr = null;
+    	if( ranges != null)
+    		futurePtr = MinorCompactionManager.instance().submit(ColumnFamilyStore.this, ranges, target, fileList);
+    	else
+    		MinorCompactionManager.instance().submitMajor(ColumnFamilyStore.this, skip);
+    	
+        boolean result = true;
+        try
+        {
+            /* Waiting for the compaction to complete. */
+        	if(futurePtr != null)
+        		result = futurePtr.get();
+            logger_.debug("Done forcing compaction ...");
+        }
+        catch (ExecutionException ex)
+        {
+            logger_.debug(LogUtil.throwableToString(ex));
+        }
+        catch ( InterruptedException ex2 )
+        {
+            logger_.debug(LogUtil.throwableToString(ex2));
+        }
+        return result;
+    }
+
+    String getColumnFamilyName()
+    {
+        return columnFamily_;
+    }
+
+    private static String[] getTableAndColumnFamilyName(String filename)
+    {
+        StringTokenizer st = new StringTokenizer(filename, "-");
+        String[] values = new String[2];
+        int i = 0;
+        while (st.hasMoreElements())
+        {
+            if (i == 0)
+                values[i] = (String) st.nextElement();
+            else if (i == 1)
+            {
+                values[i] = (String) st.nextElement();
+                break;
+            }
+            ++i;
+        }
+        return values;
+    }
+
+    protected static int getIndexFromFileName(String filename)
+    {
+        /*
+         * File name is of the form <table>-<column family>-<index>-Data.db.
+         * This tokenizer will strip the .db portion.
+         */
+        StringTokenizer st = new StringTokenizer(filename, "-");
+        /*
+         * Now I want to get the index portion of the filename. We accumulate
+         * the indices and then sort them to get the max index.
+         */
+        int count = st.countTokens();
+        int i = 0;
+        String index = null;
+        while (st.hasMoreElements())
+        {
+            index = (String) st.nextElement();
+            if (i == (count - 2))
+                break;
+            ++i;
+        }
+        return Integer.parseInt(index);
+    }
+
+    String getNextFileName()
+    {
+    	// Psuedo increment so that we do not generate consecutive numbers 
+    	fileIndexGenerator_.incrementAndGet();
+        return table_ + "-" + columnFamily_ + "-" + fileIndexGenerator_.incrementAndGet();
+    }
+
+    /*
+     * Return a temporary file name.
+     */
+    String getTempFileName()
+    {
+    	// Psuedo increment so that we do not generate consecutive numbers 
+    	fileIndexGenerator_.incrementAndGet();
+        return table_ + "-" + columnFamily_ + "-" + SSTable.temporaryFile_ + "-" + fileIndexGenerator_.incrementAndGet();
+    }
+
+    /*
+     * Return a temporary file name. Based on the list of files input 
+     * This fn sorts the list and generates a number between he 2 lowest filenames 
+     * ensuring uniqueness.
+     * Since we do not generate consecutive numbers hence the lowest file number
+     * can just be incremented to generate the next file. 
+     */
+    String getTempFileName( List<String> files)
+    {
+    	int lowestIndex;
+    	int index;
+    	Collections.sort(files, new FileNameComparator(FileNameComparator.Ascending));
+    	
+    	if( files.size() <= 1)
+    		return null;
+    	lowestIndex = getIndexFromFileName(files.get(0));
+   		
+   		index = lowestIndex + 1 ;
+
+        return table_ + "-" + columnFamily_ + "-" + SSTable.temporaryFile_ + "-" + index;
+    }
+
+    
+    /*
+     * This version is used only on start up when we are recovering from logs.
+     * In the future we may want to parellelize the log processing for a table
+     * by having a thread per log file present for recovery. Re-visit at that
+     * time.
+     */
+    void switchMemtable(String key, ColumnFamily columnFamily, CommitLog.CommitLogContext cLogCtx) throws IOException
+    {
+        memtable_.set( new Memtable(table_, columnFamily_) );
+        if(!key.equals(Memtable.flushKey_))
+        	memtable_.get().put(key, columnFamily, cLogCtx);
+        
+        if (memtableSwitchCount == Integer.MAX_VALUE)
+        {
+            memtableSwitchCount = 0;
+        }
+        memtableSwitchCount++;
+    }
+
+    /*
+     * This version is used only on start up when we are recovering from logs.
+     * In the future we may want to parellelize the log processing for a table
+     * by having a thread per log file present for recovery. Re-visit at that
+     * time.
+     */
+    void switchBinaryMemtable(String key, byte[] buffer) throws IOException
+    {
+        binaryMemtable_.set( new BinaryMemtable(table_, columnFamily_) );
+        binaryMemtable_.get().put(key, buffer);
+    }
+
+    void forceFlush() throws IOException
+    {
+        memtable_.get().forceflush(this);
+    }
+
+    void forceBlockingFlush() throws IOException, ExecutionException, InterruptedException
+    {
+        forceFlush();
+        // block for flush to finish by adding a no-op action to the flush executorservice
+        // and waiting for that to finish.  (this works since flush ES is single-threaded.)
+        Future f = MemtableManager.instance().flusher_.submit(new Runnable()
+        {
+            public void run()
+            {
+            }
+        });
+        f.get();
+    }
+
+    void forceFlushBinary()
+    {
+        BinaryMemtableManager.instance().submit(getColumnFamilyName(), binaryMemtable_.get());
+        //binaryMemtable_.get().flush(true);
+    }
+
+    /**
+     * Insert/Update the column family for this key. 
+     * param @ lock - lock that needs to be used. 
+     * param @ key - key for update/insert 
+     * param @ columnFamily - columnFamily changes
+    */
+    void apply(String key, ColumnFamily columnFamily, CommitLog.CommitLogContext cLogCtx)
+            throws IOException
+    {
+        memtable_.get().put(key, columnFamily, cLogCtx);
+    }
+
+    /*
+     * Insert/Update the column family for this key. param @ lock - lock that
+     * needs to be used. param @ key - key for update/insert param @
+     * columnFamily - columnFamily changes
+     */
+    void applyBinary(String key, byte[] buffer)
+            throws IOException
+    {
+        binaryMemtable_.get().put(key, buffer);
+    }
+
+    public ColumnFamily getColumnFamily(String key, String columnFamilyColumn, IFilter filter) throws IOException
+    {
+        List<ColumnFamily> columnFamilies = getColumnFamilies(key, columnFamilyColumn, filter);
+        return resolveAndRemoveDeleted(columnFamilies);
+    }
+
+    public ColumnFamily getColumnFamily(String key, String columnFamilyColumn, IFilter filter, int gcBefore) throws IOException
+    {
+        List<ColumnFamily> columnFamilies = getColumnFamilies(key, columnFamilyColumn, filter);
+        ColumnFamily cf = ColumnFamily.resolve(columnFamilies);
+        return removeDeleted(cf, gcBefore);
+    }
+
+    /**
+     *
+     * Get the column family in the most efficient order.
+     * 1. Memtable
+     * 2. Sorted list of files
+     */
+    List<ColumnFamily> getColumnFamilies(String key, String columnFamilyColumn, IFilter filter) throws IOException
+    {
+        List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();
+        /* Get the ColumnFamily from Memtable */
+        getColumnFamilyFromCurrentMemtable(key, columnFamilyColumn, filter, columnFamilies);
+        if (columnFamilies.size() == 0 || !filter.isDone())
+        {
+            /* Check if MemtableManager has any historical information */
+            MemtableManager.instance().getColumnFamily(key, columnFamily_, columnFamilyColumn, filter, columnFamilies);
+        }
+        if (columnFamilies.size() == 0 || !filter.isDone())
+        {
+            long start = System.currentTimeMillis();
+            getColumnFamilyFromDisk(key, columnFamilyColumn, columnFamilies, filter);
+            logger_.debug("DISK TIME: " + (System.currentTimeMillis() - start) + " ms.");
+        }
+        return columnFamilies;
+    }
+
+    /**
+     * Fetch from disk files and go in sorted order  to be efficient
+     * This fn exits as soon as the required data is found.
+     * @param key
+     * @param cf
+     * @param columnFamilies
+     * @param filter
+     * @throws IOException
+     */
+    private void getColumnFamilyFromDisk(String key, String cf, List<ColumnFamily> columnFamilies, IFilter filter) throws IOException
+    {
+        /* Scan the SSTables on disk first */
+        List<String> files = new ArrayList<String>();        
+    	lock_.readLock().lock();
+        try
+        {
+            files.addAll(ssTables_);
+            Collections.sort(files, new FileNameComparator(FileNameComparator.Descending));
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    		        	        
+        for (String file : files)
+        {
+            /*
+             * Get the BloomFilter associated with this file. Check if the key
+             * is present in the BloomFilter. If not continue to the next file.
+            */
+            boolean bVal = SSTable.isKeyInFile(key, file);
+            if ( !bVal )
+                continue;
+            ColumnFamily columnFamily = fetchColumnFamily(key, cf, filter, file);
+            long start = System.currentTimeMillis();
+            if (columnFamily != null)
+            {
+                columnFamilies.add(columnFamily);
+                if(filter.isDone())
+                {
+                	break;
+                }
+            }
+            logger_.debug("DISK Data structure population  TIME: " + (System.currentTimeMillis() - start) + " ms.");
+        }
+    }
+
+
+    private ColumnFamily fetchColumnFamily(String key, String cf, IFilter filter, String ssTableFile) throws IOException
+	{
+		SSTable ssTable = new SSTable(ssTableFile, StorageService.getPartitioner());
+		long start = System.currentTimeMillis();
+		DataInputBuffer bufIn;
+		bufIn = filter.next(key, cf, ssTable);
+		logger_.debug("DISK ssTable.next TIME: " + (System.currentTimeMillis() - start) + " ms.");
+		if (bufIn.getLength() == 0)
+			return null;
+        start = System.currentTimeMillis();
+        ColumnFamily columnFamily = ColumnFamily.serializer().deserialize(bufIn, cf, filter);
+		logger_.debug("DISK Deserialize TIME: " + (System.currentTimeMillis() - start) + " ms.");
+		if (columnFamily == null)
+			return null;
+		return columnFamily;
+	}
+
+    private void getColumnFamilyFromCurrentMemtable(String key, String cf, IFilter filter, List<ColumnFamily> columnFamilies)
+    {
+        /* Get the ColumnFamily from Memtable */
+        ColumnFamily columnFamily = memtable_.get().get(key, cf, filter);
+        if (columnFamily != null)
+        {
+            columnFamilies.add(columnFamily);
+        }
+    }
+
+    /** like resolve, but leaves the resolved CF as the only item in the list */
+    private static void merge(List<ColumnFamily> columnFamilies)
+    {
+        ColumnFamily cf = ColumnFamily.resolve(columnFamilies);
+        columnFamilies.clear();
+        columnFamilies.add(cf);
+    }
+
+    private static ColumnFamily resolveAndRemoveDeleted(List<ColumnFamily> columnFamilies) {
+        ColumnFamily cf = ColumnFamily.resolve(columnFamilies);
+        return removeDeleted(cf);
+    }
+
+    /*
+     This is complicated because we need to preserve deleted columns, supercolumns, and columnfamilies
+     until they have been deleted for at least GC_GRACE_IN_SECONDS.  But, we do not need to preserve
+     their contents; just the object itself as a "tombstone" that can be used to repair other
+     replicas that do not know about the deletion.
+     */
+    static ColumnFamily removeDeleted(ColumnFamily cf)
+    {
+        return removeDeleted(cf, (int)(System.currentTimeMillis() / 1000) - DatabaseDescriptor.getGcGraceInSeconds());
+    }
+
+    static ColumnFamily removeDeleted(ColumnFamily cf, int gcBefore)
+    {
+        if (cf == null)
+            return null;
+
+        for (String cname : new ArrayList<String>(cf.getColumns().keySet()))
+        {
+            IColumn c = cf.getColumns().get(cname);
+            if (c instanceof SuperColumn)
+            {
+                long minTimestamp = Math.max(c.getMarkedForDeleteAt(), cf.getMarkedForDeleteAt());
+                // don't operate directly on the supercolumn, it could be the one in the memtable
+                cf.remove(cname);
+                SuperColumn sc = new SuperColumn(cname);
+                sc.markForDeleteAt(c.getLocalDeletionTime(), c.getMarkedForDeleteAt());
+                for (IColumn subColumn : c.getSubColumns())
+                {
+                    if (subColumn.timestamp() >= minTimestamp)
+                    {
+                        if (!subColumn.isMarkedForDelete() || subColumn.getLocalDeletionTime() > gcBefore)
+                        {
+                            sc.addColumn(subColumn.name(), subColumn);
+                        }
+                    }
+                }
+                if (sc.getSubColumns().size() > 0 || sc.getLocalDeletionTime() > gcBefore)
+                {
+                    cf.addColumn(sc);
+                }
+            }
+            else if ((c.isMarkedForDelete() && c.getLocalDeletionTime() <= gcBefore)
+                     || c.timestamp() < cf.getMarkedForDeleteAt())
+            {
+                cf.remove(cname);
+            }
+        }
+
+        if (cf.getColumnCount() == 0 && cf.getLocalDeletionTime() <= gcBefore)
+        {
+            return null;
+        }
+        return cf;
+    }
+
+    /*
+     * This version is used only on start up when we are recovering from logs.
+     * Hence no locking is required since we process logs on the main thread. In
+     * the future we may want to parellelize the log processing for a table by
+     * having a thread per log file present for recovery. Re-visit at that time.
+     */
+    void applyNow(String key, ColumnFamily columnFamily) throws IOException
+    {
+         memtable_.get().putOnRecovery(key, columnFamily);
+    }
+
+    /*
+     * This method is called when the Memtable is frozen and ready to be flushed
+     * to disk. This method informs the CommitLog that a particular ColumnFamily
+     * is being flushed to disk.
+     */
+    void onMemtableFlush(CommitLog.CommitLogContext cLogCtx) throws IOException
+    {
+        if ( cLogCtx.isValidContext() )
+            CommitLog.open(table_).onMemtableFlush(columnFamily_, cLogCtx);
+    }
+
+    /*
+     * Called after the Memtable flushes its in-memory data. This information is
+     * cached in the ColumnFamilyStore. This is useful for reads because the
+     * ColumnFamilyStore first looks in the in-memory store and the into the
+     * disk to find the key. If invoked during recoveryMode the
+     * onMemtableFlush() need not be invoked.
+     *
+     * param @ filename - filename just flushed to disk
+     * param @ bf - bloom filter which indicates the keys that are in this file.
+    */
+    void storeLocation(String filename, BloomFilter bf)
+    {
+        int ssTableSize = 0;
+    	lock_.writeLock().lock();
+        try
+        {
+            ssTables_.add(filename);
+            SSTable.storeBloomFilter(filename, bf);
+            ssTableSize = ssTables_.size();
+        }
+        finally
+        {
+        	lock_.writeLock().unlock();
+        }
+
+        if ((ssTableSize >= threshHold_ && !isCompacting_.get())
+            || (isCompacting_.get() && ssTableSize % threshHold_ == 0))
+        {
+            logger_.debug("Submitting for  compaction ...");
+            MinorCompactionManager.instance().submit(ColumnFamilyStore.this);
+            logger_.debug("Submitted for compaction ...");
+        }
+    }
+
+    PriorityQueue<FileStruct> initializePriorityQueue(List<String> files, List<Range> ranges, int minBufferSize)
+    {
+        PriorityQueue<FileStruct> pq = new PriorityQueue<FileStruct>();
+        if (files.size() > 1 || (ranges != null &&  files.size() > 0))
+        {
+            int bufferSize = Math.min( (ColumnFamilyStore.compactionMemoryThreshold_ / files.size()), minBufferSize ) ;
+            FileStruct fs = null;
+            for (String file : files)
+            {
+            	try
+            	{
+            		fs = new FileStruct(SequenceFile.bufferedReader(file, bufferSize), StorageService.getPartitioner());
+	                fs.advance();
+	                if(fs.isExhausted())
+	                	continue;
+	                pq.add(fs);
+            	}
+            	catch ( Exception ex)
+            	{
+                    logger_.warn("corrupt file?  or are you just blowing away data files manually out from under me?", ex);
+            		try
+            		{
+            			if (fs != null)
+            			{
+            				fs.close();
+            			}
+            		}
+            		catch(Exception e)
+            		{
+            			logger_.warn("Unable to close file :" + file);
+            		}
+                }
+            }
+        }
+        return pq;
+    }
+
+    /*
+     * Group files of similar size into buckets.
+     */
+    static Set<List<String>> getCompactionBuckets(List<String> files, long min)
+    {
+    	Map<List<String>, Long> buckets = new ConcurrentHashMap<List<String>, Long>();
+    	for(String fname : files)
+    	{
+    		File f = new File(fname);
+    		long size = f.length();
+
+    		boolean bFound = false;
+            // look for a bucket containing similar-sized files:
+            // group in the same bucket if it's w/in 50% of the average for this bucket,
+            // or this file and the bucket are all considered "small" (less than `min`)
+            for (List<String> bucket : buckets.keySet())
+    		{
+                long averageSize = buckets.get(bucket);
+                if ((size > averageSize/2 && size < 3*averageSize/2)
+                    || ( size < min && averageSize < min))
+    			{
+                    // remove and re-add because adding changes the hash
+                    buckets.remove(bucket);
+    				averageSize = (averageSize + size) / 2 ;
+                    bucket.add(fname);
+                    buckets.put(bucket, averageSize);
+    				bFound = true;
+    				break;
+    			}
+    		}
+            // no similar bucket found; put it in a new one
+    		if(!bFound)
+    		{
+                ArrayList<String> bucket = new ArrayList<String>();
+                bucket.add(fname);
+                buckets.put(bucket, size);
+    		}
+    	}
+
+        return buckets.keySet();
+    }
+
+    /*
+     * Break the files into buckets and then compact.
+     */
+    void doCompaction() throws IOException
+    {
+        isCompacting_.set(true);
+        List<String> files = new ArrayList<String>(ssTables_);
+        try
+        {
+	        int count;
+	    	for(List<String> fileList : getCompactionBuckets(files, 50L*1024L*1024L))
+            {
+	    		Collections.sort( fileList , new FileNameComparator( FileNameComparator.Ascending));
+	    		if(fileList.size() >= threshHold_ )
+	    		{
+	    			files.clear();
+	    			count = 0;
+	    			for(String file : fileList)
+	    			{
+	    				files.add(file);
+	    				count++;
+	    				if( count == threshHold_ )
+	    					break;
+	    			}
+                    // For each bucket if it has crossed the threshhold do the compaction
+                    // In case of range  compaction merge the counting bloom filters also.
+                    if( count == threshHold_)
+                        doFileCompaction(files, bufSize_);
+	    		}
+	    	}
+        }
+        finally
+        {
+        	isCompacting_.set(false);
+        }
+    }
+
+    void doMajorCompaction(long skip)
+    {
+    	doMajorCompactionInternal( skip );
+    }
+
+    /*
+     * Compact all the files irrespective of the size.
+     * skip : is the ammount in Gb of the files to be skipped
+     * all files greater than skip GB are skipped for this compaction.
+     * Except if skip is 0 , in that case this is ignored and all files are taken.
+     */
+    void doMajorCompactionInternal(long skip)
+    {
+        isCompacting_.set(true);
+        List<String> filesInternal = new ArrayList<String>(ssTables_);
+        List<String> files;
+        try
+        {
+        	 if( skip > 0L )
+        	 {
+        		 files = new ArrayList<String>();
+	        	 for ( String file : filesInternal )
+	        	 {
+	        		 File f = new File(file);
+	        		 if( f.length() < skip*1024L*1024L*1024L )
+	        		 {
+	        			 files.add(file);
+	        		 }
+	        	 }
+        	 }
+        	 else
+        	 {
+        		 files = filesInternal;
+        	 }
+        	 doFileCompaction(files, bufSize_);
+        }
+        catch ( Exception ex)
+        {
+        	ex.printStackTrace();
+        }
+        finally
+        {
+        	isCompacting_.set(false);
+        }
+    }
+
+    /*
+     * Add up all the files sizes this is the worst case file
+     * size for compaction of all the list of files given.
+     */
+    long getExpectedCompactedFileSize(List<String> files)
+    {
+    	long expectedFileSize = 0;
+    	for(String file : files)
+    	{
+    		File f = new File(file);
+    		long size = f.length();
+    		expectedFileSize = expectedFileSize + size;
+    	}
+    	return expectedFileSize;
+    }
+
+    /*
+     *  Find the maximum size file in the list .
+     */
+    String getMaxSizeFile( List<String> files )
+    {
+    	long maxSize = 0L;
+    	String maxFile = null;
+    	for ( String file : files )
+    	{
+    		File f = new File(file);
+    		if(f.length() > maxSize )
+    		{
+    			maxSize = f.length();
+    			maxFile = file;
+    		}
+    	}
+    	return maxFile;
+    }
+
+    boolean doAntiCompaction(List<Range> ranges, EndPoint target, List<String> fileList)
+    {
+        isCompacting_.set(true);
+        List<String> files = new ArrayList<String>(ssTables_);
+        boolean result = true;
+        try
+        {
+        	 result = doFileAntiCompaction(files, ranges, target, fileList, null);
+        }
+        catch ( Exception ex)
+        {
+        	ex.printStackTrace();
+        }
+        finally
+        {
+        	isCompacting_.set(false);
+        }
+        return result;
+
+    }
+
+    void forceCleanup()
+    {
+    	MinorCompactionManager.instance().submitCleanup(ColumnFamilyStore.this);
+    }
+    
+    /**
+     * This function goes over each file and removes the keys that the node is not responsible for 
+     * and only keeps keys that this node is responsible for.
+     * @throws IOException
+     */
+    void doCleanupCompaction()
+    {
+        isCompacting_.set(true);
+        List<String> files = new ArrayList<String>(ssTables_);
+        for(String file: files)
+        {
+	        try
+	        {
+	        	doCleanup(file);
+	        }
+	        catch ( Exception ex)
+	        {
+	        	ex.printStackTrace();
+	        }
+        }
+    	isCompacting_.set(false);
+    }
+    /**
+     * cleans up one particular file by removing keys that this node is not responsible for.
+     * @param file
+     * @throws IOException
+     */
+    /* TODO: Take care of the comments later. */
+    void doCleanup(String file)
+    {
+    	if(file == null )
+    		return;
+        List<Range> myRanges;
+    	List<String> files = new ArrayList<String>();
+    	files.add(file);
+    	List<String> newFiles = new ArrayList<String>();
+    	Map<EndPoint, List<Range>> endPointtoRangeMap = StorageService.instance().constructEndPointToRangesMap();
+    	myRanges = endPointtoRangeMap.get(StorageService.getLocalStorageEndPoint());
+    	List<BloomFilter> compactedBloomFilters = new ArrayList<BloomFilter>();
+        doFileAntiCompaction(files, myRanges, null, newFiles, compactedBloomFilters);
+        logger_.debug("Original file : " + file + " of size " + new File(file).length());
+        lock_.writeLock().lock();
+        try
+        {
+            ssTables_.remove(file);
+            SSTable.removeAssociatedBloomFilter(file);
+            for (String newfile : newFiles)
+            {                            	
+                logger_.debug("New file : " + newfile + " of size " + new File(newfile).length());
+                if ( newfile != null )
+                {
+                    ssTables_.add(newfile);
+                    logger_.debug("Inserting bloom filter for file " + newfile);
+                    SSTable.storeBloomFilter(newfile, compactedBloomFilters.get(0));
+                }
+            }
+            SSTable.delete(file);
+        }
+        finally
+        {
+            lock_.writeLock().unlock();
+        }
+    }
+    
+    /**
+     * This function is used to do the anti compaction process , it spits out the file which has keys that belong to a given range
+     * If the target is not specified it spits out the file as a compacted file with the unecessary ranges wiped out.
+     * @param files
+     * @param ranges
+     * @param target
+     * @param fileList
+     * @return
+     * @throws IOException
+     */
+    boolean doFileAntiCompaction(List<String> files, List<Range> ranges, EndPoint target, List<String> fileList, List<BloomFilter> compactedBloomFilters)
+    {
+    	boolean result = false;
+        long startTime = System.currentTimeMillis();
+        long totalBytesRead = 0;
+        long totalBytesWritten = 0;
+        long totalkeysRead = 0;
+        long totalkeysWritten = 0;
+        String rangeFileLocation;
+        String mergedFileName;
+        IPartitioner p = StorageService.getPartitioner();
+        try
+        {
+	        // Calculate the expected compacted filesize
+	    	long expectedRangeFileSize = getExpectedCompactedFileSize(files);
+	    	/* in the worst case a node will be giving out alf of its data so we take a chance */
+	    	expectedRangeFileSize = expectedRangeFileSize / 2;
+	        rangeFileLocation = DatabaseDescriptor.getCompactionFileLocation(expectedRangeFileSize);
+//	        boolean isLoop = isLoopAround( ranges );
+//	        Range maxRange = getMaxRange( ranges );
+	        // If the compaction file path is null that means we have no space left for this compaction.
+	        if( rangeFileLocation == null )
+	        {
+	            logger_.warn("Total bytes to be written for range compaction  ..."
+	                    + expectedRangeFileSize + "   is greater than the safe limit of the disk space available.");
+	            return result;
+	        }
+	        PriorityQueue<FileStruct> pq = initializePriorityQueue(files, ranges, ColumnFamilyStore.bufSize_);
+	        if (pq.size() > 0)
+	        {
+	            mergedFileName = getTempFileName();
+	            SSTable ssTableRange = null ;
+	            String lastkey = null;
+	            List<FileStruct> lfs = new ArrayList<FileStruct>();
+	            DataOutputBuffer bufOut = new DataOutputBuffer();
+	            int expectedBloomFilterSize = SSTable.getApproximateKeyCount(files);
+	            expectedBloomFilterSize = (expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTable.indexInterval();
+	            logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
+	            /* Create the bloom filter for the compacted file. */
+	            BloomFilter compactedRangeBloomFilter = new BloomFilter(expectedBloomFilterSize, 15);
+	            List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();
+
+	            while (pq.size() > 0 || lfs.size() > 0)
+	            {
+	                FileStruct fs = null;
+	                if (pq.size() > 0)
+	                {
+	                    fs = pq.poll();
+	                }
+	                if (fs != null
+	                        && (lastkey == null || lastkey.equals(fs.getKey())))
+	                {
+	                    // The keys are the same so we need to add this to the
+	                    // ldfs list
+	                    lastkey = fs.getKey();
+	                    lfs.add(fs);
+	                }
+	                else
+	                {
+	                    Collections.sort(lfs, new FileStructComparator());
+	                    ColumnFamily columnFamily;
+	                    bufOut.reset();
+	                    if(lfs.size() > 1)
+	                    {
+		                    for (FileStruct filestruct : lfs)
+		                    {
+		                    	try
+		                    	{
+	                                /* read the length although we don't need it */
+	                                filestruct.getBufIn().readInt();
+	                                // Skip the Index
+                                    IndexHelper.skipBloomFilterAndIndex(filestruct.getBufIn());
+	                                // We want to add only 2 and resolve them right there in order to save on memory footprint
+	                                if(columnFamilies.size() > 1)
+	                                {
+	    		                        // Now merge the 2 column families
+                                        merge(columnFamilies);
+	                                }
+			                        // deserialize into column families
+			                        columnFamilies.add(ColumnFamily.serializer().deserialize(filestruct.getBufIn()));
+		                    	}
+		                    	catch ( Exception ex)
+		                    	{
+                                    logger_.warn(LogUtil.throwableToString(ex));
+                                }
+		                    }
+		                    // Now after merging all crap append to the sstable
+		                    columnFamily = resolveAndRemoveDeleted(columnFamilies);
+		                    columnFamilies.clear();
+		                    if( columnFamily != null )
+		                    {
+			                	/* serialize the cf with column indexes */
+			                    ColumnFamily.serializerWithIndexes().serialize(columnFamily, bufOut);
+		                    }
+	                    }
+	                    else
+	                    {
+		                    FileStruct filestruct = lfs.get(0);
+	                    	try
+	                    	{
+		                        /* read the length although we don't need it */
+		                        int size = filestruct.getBufIn().readInt();
+		                        bufOut.write(filestruct.getBufIn(), size);
+	                    	}
+	                    	catch ( Exception ex)
+	                    	{
+	                    		logger_.warn(LogUtil.throwableToString(ex));
+	                            filestruct.close();
+	                            continue;
+	                    	}
+	                    }
+                        if (Range.isTokenInRanges(StorageService.getPartitioner().getInitialToken(lastkey), ranges))
+	                    {
+	                        if(ssTableRange == null )
+	                        {
+	                        	if( target != null )
+	                        		rangeFileLocation = rangeFileLocation + System.getProperty("file.separator") + "bootstrap";
+	                	        FileUtils.createDirectory(rangeFileLocation);
+	                            ssTableRange = new SSTable(rangeFileLocation, mergedFileName, StorageService.getPartitioner());
+	                        }	                        
+	                        try
+	                        {
+		                        ssTableRange.append(lastkey, bufOut);
+		                        compactedRangeBloomFilter.add(lastkey);
+	                        }
+	                        catch(Exception ex)
+	                        {
+	                            logger_.warn( LogUtil.throwableToString(ex) );
+	                        }
+	                    }
+	                    totalkeysWritten++;
+	                    for (FileStruct filestruct : lfs)
+	                    {
+	                    	try
+	                    	{
+                                filestruct.advance();
+	                    		if (filestruct.isExhausted())
+	                    		{
+	                    			continue;
+	                    		}
+	                    		/* keep on looping until we find a key in the range */
+                                while (!Range.isTokenInRanges(StorageService.getPartitioner().getInitialToken(filestruct.getKey()), ranges))
+	                            {
+                                    filestruct.advance();
+                                    if (filestruct.isExhausted())
+		                    		{
+		                    			break;
+		                    		}
+	        	                    /* check if we need to continue , if we are done with ranges empty the queue and close all file handles and exit */
+	        	                    //if( !isLoop && StorageService.token(filestruct.key).compareTo(maxRange.right()) > 0 && !filestruct.key.equals(""))
+	        	                    //{
+	                                    //filestruct.reader.close();
+	                                    //filestruct = null;
+	                                    //break;
+	        	                    //}
+	                            }
+	                            if (!filestruct.isExhausted())
+	                            {
+	                            	pq.add(filestruct);
+	                            }
+		                        totalkeysRead++;
+	                    	}
+	                    	catch ( Exception ex )
+	                    	{
+	                    		// Ignore the exception as it might be a corrupted file
+	                    		// in any case we have read as far as possible from it
+	                    		// and it will be deleted after compaction.
+                                logger_.warn(LogUtil.throwableToString(ex));
+	                            filestruct.close();
+                            }
+	                    }
+	                    lfs.clear();
+	                    lastkey = null;
+	                    if (fs != null)
+	                    {
+	                        // Add back the fs since we processed the rest of
+	                        // filestructs
+	                        pq.add(fs);
+	                    }
+	                }
+	            }
+	            if( ssTableRange != null )
+	            {
+                    if ( fileList == null )
+                        fileList = new ArrayList<String>();
+                    ssTableRange.closeRename(compactedRangeBloomFilter, fileList);
+                    if(compactedBloomFilters != null)
+                    	compactedBloomFilters.add(compactedRangeBloomFilter);
+	            }
+	        }
+        }
+        catch ( Exception ex)
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+        logger_.debug("Total time taken for range split   ..."
+                + (System.currentTimeMillis() - startTime));
+        logger_.debug("Total bytes Read for range split  ..." + totalBytesRead);
+        logger_.debug("Total bytes written for range split  ..."
+                + totalBytesWritten + "   Total keys read ..." + totalkeysRead);
+        return result;
+    }
+
+    private void doFill(BloomFilter bf, String decoratedKey)
+    {
+        bf.add(StorageService.getPartitioner().undecorateKey(decoratedKey));
+    }
+    
+    /*
+     * This function does the actual compaction for files.
+     * It maintains a priority queue of with the first key from each file
+     * and then removes the top of the queue and adds it to the SStable and
+     * repeats this process while reading the next from each file until its
+     * done with all the files . The SStable to which the keys are written
+     * represents the new compacted file. Before writing if there are keys
+     * that occur in multiple files and are the same then a resolution is done
+     * to get the latest data.
+     *
+     */
+    void  doFileCompaction(List<String> files,  int minBufferSize) throws IOException
+    {
+    	String newfile = null;
+        long startTime = System.currentTimeMillis();
+        long totalBytesRead = 0;
+        long totalBytesWritten = 0;
+        long totalkeysRead = 0;
+        long totalkeysWritten = 0;
+        // Calculate the expected compacted filesize
+        long expectedCompactedFileSize = getExpectedCompactedFileSize(files);
+        String compactionFileLocation = DatabaseDescriptor.getCompactionFileLocation(expectedCompactedFileSize);
+        // If the compaction file path is null that means we have no space left for this compaction.
+        if( compactionFileLocation == null )
+        {
+            String maxFile = getMaxSizeFile( files );
+            files.remove( maxFile );
+            doFileCompaction(files , minBufferSize);
+            return;
+        }
+        PriorityQueue<FileStruct> pq = initializePriorityQueue(files, null, minBufferSize);
+        if (pq.size() > 0)
+        {
+            String mergedFileName = getTempFileName( files );
+            SSTable ssTable = null;
+            String lastkey = null;
+            List<FileStruct> lfs = new ArrayList<FileStruct>();
+            DataOutputBuffer bufOut = new DataOutputBuffer();
+            int expectedBloomFilterSize = SSTable.getApproximateKeyCount(files);
+            expectedBloomFilterSize = (expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTable.indexInterval();
+            logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
+            /* Create the bloom filter for the compacted file. */
+            BloomFilter compactedBloomFilter = new BloomFilter(expectedBloomFilterSize, 15);
+            List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();
+
+            while (pq.size() > 0 || lfs.size() > 0)
+            {
+                FileStruct fs = null;
+                if (pq.size() > 0)
+                {
+                    fs = pq.poll();
+                }
+                if (fs != null
+                        && (lastkey == null || lastkey.equals(fs.getKey())))
+                {
+                    // The keys are the same so we need to add this to the
+                    // ldfs list
+                    lastkey = fs.getKey();
+                    lfs.add(fs);
+                }
+                else
+                {
+                    Collections.sort(lfs, new FileStructComparator());
+                    ColumnFamily columnFamily;
+                    bufOut.reset();
+                    if(lfs.size() > 1)
+                    {
+                        for (FileStruct filestruct : lfs)
+                        {
+                            try
+                            {
+                                /* read the length although we don't need it */
+                                filestruct.getBufIn().readInt();
+                                // Skip the Index
+                                IndexHelper.skipBloomFilterAndIndex(filestruct.getBufIn());
+                                // We want to add only 2 and resolve them right there in order to save on memory footprint
+                                if(columnFamilies.size() > 1)
+                                {
+                                    merge(columnFamilies);
+                                }
+                                // deserialize into column families
+                                columnFamilies.add(ColumnFamily.serializer().deserialize(filestruct.getBufIn()));
+                            }
+                            catch ( Exception ex)
+                            {
+                                logger_.warn("error in filecompaction", ex);
+                            }
+                        }
+                        // Now after merging all crap append to the sstable
+                        columnFamily = resolveAndRemoveDeleted(columnFamilies);
+                        columnFamilies.clear();
+                        if( columnFamily != null )
+                        {
+                            /* serialize the cf with column indexes */
+                            ColumnFamily.serializerWithIndexes().serialize(columnFamily, bufOut);
+                        }
+                    }
+                    else
+                    {
+                        FileStruct filestruct = lfs.get(0);
+                        try
+                        {
+                            /* read the length although we don't need it */
+                            int size = filestruct.getBufIn().readInt();
+                            bufOut.write(filestruct.getBufIn(), size);
+                        }
+                        catch ( Exception ex)
+                        {
+                            ex.printStackTrace();
+                            filestruct.close();
+                            continue;
+                        }
+                    }
+
+                    if ( ssTable == null )
+                    {
+                        ssTable = new SSTable(compactionFileLocation, mergedFileName, StorageService.getPartitioner());
+                    }
+                    ssTable.append(lastkey, bufOut);
+
+                    /* Fill the bloom filter with the key */
+                    doFill(compactedBloomFilter, lastkey);
+                    totalkeysWritten++;
+                    for (FileStruct filestruct : lfs)
+                    {
+                        try
+                        {
+                            filestruct.advance();
+                            if (filestruct.isExhausted())
+                            {
+                                continue;
+                            }
+                            pq.add(filestruct);
+                            totalkeysRead++;
+                        }
+                        catch ( Throwable ex )
+                        {
+                            // Ignore the exception as it might be a corrupted file
+                            // in any case we have read as far as possible from it
+                            // and it will be deleted after compaction.
+                            filestruct.close();
+                        }
+                    }
+                    lfs.clear();
+                    lastkey = null;
+                    if (fs != null)
+                    {
+                        /* Add back the fs since we processed the rest of filestructs */
+                        pq.add(fs);
+                    }
+                }
+            }
+            if ( ssTable != null )
+            {
+                ssTable.closeRename(compactedBloomFilter);
+                newfile = ssTable.getDataFileLocation();
+            }
+            lock_.writeLock().lock();
+            try
+            {
+                for (String file : files)
+                {
+                    ssTables_.remove(file);
+                    SSTable.removeAssociatedBloomFilter(file);
+                }
+                if ( newfile != null )
+                {
+                    ssTables_.add(newfile);
+                    logger_.debug("Inserting bloom filter for file " + newfile);
+                    SSTable.storeBloomFilter(newfile, compactedBloomFilter);
+                    totalBytesWritten = (new File(newfile)).length();
+                }
+            }
+            finally
+            {
+                lock_.writeLock().unlock();
+            }
+            for (String file : files)
+            {
+                SSTable.delete(file);
+            }
+        }
+        logger_.debug("Total time taken for compaction  ..."
+                + (System.currentTimeMillis() - startTime));
+        logger_.debug("Total bytes Read for compaction  ..." + totalBytesRead);
+        logger_.debug("Total bytes written for compaction  ..."
+                + totalBytesWritten + "   Total keys read ..." + totalkeysRead);
+    }
+
+    public boolean isSuper()
+    {
+        return isSuper_;
+    }
+
+    public void flushMemtableOnRecovery() throws IOException
+    {
+        memtable_.get().flushOnRecovery();
+    }
+
+    public int getMemtableColumnsCount()
+    {
+        return memtable_.get().getCurrentObjectCount();
+    }
+
+    public int getMemtableDataSize()
+    {
+        return memtable_.get().getCurrentSize();
+    }
+
+    public int getMemtableSwitchCount()
+    {
+        return memtableSwitchCount;
+    }
+
+    /**
+     * clears out all data associated with this ColumnFamily.
+     * For use in testing.
+     */
+    public void reset() throws IOException, ExecutionException, InterruptedException
+    {
+        forceBlockingFlush();
+        for (String fName : ssTables_)
+        {
+            new File(fName).delete();
+        }
+        ssTables_.clear();
+    }
+
+    public Object getMemtable()
+    {
+        return memtable_.get();
+    }
+
+    public Set<String> getSSTableFilenames()
+    {
+        return Collections.unmodifiableSet(ssTables_);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java
index e69de29b..33ece8e5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java
@@ -0,0 +1,51 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+/**
+ * The MBean interface for ColumnFamilyStore
+ * 
+ * @author Eric Evans
+ *
+ */
+public interface ColumnFamilyStoreMBean
+{
+    /**
+     * Returns the total amount of data stored in the memtable, including
+     * column related overhead.
+     * 
+     * @return The size in bytes.
+     */
+    public int getMemtableDataSize();
+    
+    /**
+     * Returns the total number of columns present in the memtable.
+     * 
+     * @return The number of columns.
+     */
+    public int getMemtableColumnsCount();
+    
+    /**
+     * Returns the number of times that a flush has resulted in the 
+     * memtable being switched out.
+     * 
+     * @return the number of memtable switches
+     */
+    public int getMemtableSwitchCount();
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnIndexer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnIndexer.java
index e69de29b..d446bc65 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnIndexer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnIndexer.java
@@ -0,0 +1,181 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IndexHelper;
+import org.apache.cassandra.utils.BloomFilter;
+
+
+/**
+ * Help to create an index for a column family based on size of columns
+ * Author : Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+
+public class ColumnIndexer
+{
+	/**
+	 * Given a column family this, function creates an in-memory structure that represents the
+	 * column index for the column family, and subsequently writes it to disk.
+	 * @param columnFamily Column family to create index for
+	 * @param dos data output stream
+	 * @throws IOException
+	 */
+    public static void serialize(ColumnFamily columnFamily, DataOutputStream dos) throws IOException
+	{
+        Collection<IColumn> columns = columnFamily.getAllColumns();
+        BloomFilter bf = createColumnBloomFilter(columns);                    
+        /* Write out the bloom filter. */
+        DataOutputBuffer bufOut = new DataOutputBuffer(); 
+        BloomFilter.serializer().serialize(bf, bufOut);
+        /* write the length of the serialized bloom filter. */
+        dos.writeInt(bufOut.getLength());
+        /* write out the serialized bytes. */
+        dos.write(bufOut.getData(), 0, bufOut.getLength());
+
+        /* Do the indexing */
+        TypeInfo typeInfo = DatabaseDescriptor.getTypeInfo(columnFamily.name());        
+        doIndexing(typeInfo, columns, dos);        
+	}
+    
+    /**
+     * Create a bloom filter that contains the subcolumns and the columns that
+     * make up this Column Family.
+     * @param columns columns of the ColumnFamily
+     * @return BloomFilter with the summarized information.
+     */
+    private static BloomFilter createColumnBloomFilter(Collection<IColumn> columns)
+    {
+        int columnCount = 0;
+        for ( IColumn column : columns )
+        {
+            columnCount += column.getObjectCount();
+        }
+        
+        BloomFilter bf = new BloomFilter(columnCount, 4);
+        for ( IColumn column : columns )
+        {
+            bf.add(column.name());
+            /* If this is SuperColumn type Column Family we need to get the subColumns too. */
+            if ( column instanceof SuperColumn )
+            {
+                Collection<IColumn> subColumns = column.getSubColumns();
+                for ( IColumn subColumn : subColumns )
+                {
+                    bf.add(subColumn.name());
+                }
+            }
+        }
+        return bf;
+    }
+    
+    private static IndexHelper.ColumnIndexInfo getColumnIndexInfo(TypeInfo typeInfo, IColumn column)
+    {
+        IndexHelper.ColumnIndexInfo cIndexInfo = null;
+        
+        if ( column instanceof SuperColumn )
+        {
+            cIndexInfo = IndexHelper.ColumnIndexFactory.instance(TypeInfo.STRING);            
+            cIndexInfo.set(column.name());
+        }
+        else
+        {
+            cIndexInfo = IndexHelper.ColumnIndexFactory.instance(typeInfo);                        
+            switch(typeInfo)
+            {
+                case STRING:
+                    cIndexInfo.set(column.name());                        
+                    break;
+                    
+                case LONG:
+                    cIndexInfo.set(column.timestamp());                        
+                    break;
+            }
+        }
+        
+        return cIndexInfo;
+    }
+
+    /**
+     * Given the collection of columns in the Column Family,
+     * the name index is generated and written into the provided
+     * stream
+     * @param columns for whom the name index needs to be generated
+     * @param bf bloom filter that summarizes the columns that make
+     *           up the column family.
+     * @param dos stream into which the serialized name index needs
+     *            to be written.
+     * @throws IOException
+     */
+    private static void doIndexing(TypeInfo typeInfo, Collection<IColumn> columns, DataOutputStream dos) throws IOException
+    {
+        /* we are going to write column indexes */
+        int numColumns = 0;
+        int position = 0;
+        int indexSizeInBytes = 0;
+        int sizeSummarized = 0;
+        
+        /*
+         * Maintains a list of KeyPositionInfo objects for the columns in this
+         * column family. The key is the column name and the position is the
+         * relative offset of that column name from the start of the list.
+         * We do this so that we don't read all the columns into memory.
+        */
+        
+        List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();        
+        
+        /* column offsets at the right thresholds into the index map. */
+        for ( IColumn column : columns )
+        {
+            /* if we hit the column index size that we have to index after, go ahead and index it */
+            if(position - sizeSummarized >= DatabaseDescriptor.getColumnIndexSize())
+            {      
+                /*
+                 * ColumnSort applies only to columns. So in case of 
+                 * SuperColumn always use the name indexing scheme for
+                 * the SuperColumns. We will fix this later.
+                 */
+                IndexHelper.ColumnIndexInfo cIndexInfo = getColumnIndexInfo(typeInfo, column);                
+                cIndexInfo.position(position);
+                cIndexInfo.count(numColumns);                
+                columnIndexList.add(cIndexInfo);
+                /*
+                 * we will be writing this object as a UTF8 string and two ints,
+                 * so calculate the size accordingly. Note that we store the string
+                 * as UTF-8 encoded, so when we calculate the length, it should be
+                 * converted to UTF-8.
+                 */
+                indexSizeInBytes += cIndexInfo.size();
+                sizeSummarized = position;
+                numColumns = 0;
+            }
+            position += column.serializedSize();
+            ++numColumns;
+        }
+        /* write the column index list */
+        IndexHelper.serialize(indexSizeInBytes, columnIndexList, dos);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnReadCommand.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnReadCommand.java
index e69de29b..e16b0dbc 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnReadCommand.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnReadCommand.java
@@ -0,0 +1,90 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+public class ColumnReadCommand extends ReadCommand
+{
+    public final String columnFamilyColumn;
+
+    public ColumnReadCommand(String table, String key, String columnFamilyColumn)
+    {
+        super(table, key, CMD_TYPE_GET_COLUMN);
+        this.columnFamilyColumn = columnFamilyColumn;
+    }
+
+    @Override
+    public String getColumnFamilyName()
+    {
+        String[] values = RowMutation.getColumnAndColumnFamily(columnFamilyColumn);
+        return values[0];
+    }
+
+    @Override
+    public ReadCommand copy()
+    {
+        ReadCommand readCommand= new ColumnReadCommand(table, key, columnFamilyColumn);
+        readCommand.setDigestQuery(isDigestQuery());
+        return readCommand;
+    }
+
+    @Override
+    public Row getRow(Table table) throws IOException    
+    {
+        return table.getRow(key, columnFamilyColumn);
+    }
+
+    @Override
+    public String toString()
+    {
+        return "GetColumnReadMessage(" +
+               "table='" + table + '\'' +
+               ", key='" + key + '\'' +
+               ", columnFamilyColumn='" + columnFamilyColumn + '\'' +
+               ')';
+    }
+}
+
+class ColumnReadCommandSerializer extends ReadCommandSerializer
+{
+    @Override
+    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
+    { 
+        ColumnReadCommand realRM = (ColumnReadCommand)rm;
+        dos.writeBoolean(realRM.isDigestQuery());
+        dos.writeUTF(realRM.table);
+        dos.writeUTF(realRM.key);
+        dos.writeUTF(realRM.columnFamilyColumn);
+    }
+
+    @Override
+    public ReadCommand deserialize(DataInputStream dis) throws IOException
+    {
+        boolean isDigest = dis.readBoolean();
+        String table = dis.readUTF();
+        String key = dis.readUTF();
+        String columnFamily_column = dis.readUTF();
+        ColumnReadCommand rm = new ColumnReadCommand(table, key, columnFamily_column);
+        rm.setDigestQuery(isDigest);
+        return rm;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnsSinceReadCommand.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnsSinceReadCommand.java
index e69de29b..df670c21 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnsSinceReadCommand.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnsSinceReadCommand.java
@@ -0,0 +1,95 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+public class ColumnsSinceReadCommand extends ReadCommand
+{
+    public final String columnFamily;
+    public final long sinceTimestamp;
+
+    public ColumnsSinceReadCommand(String table, String key, String columnFamily, long sinceTimestamp)
+    {
+        super(table, key, CMD_TYPE_GET_COLUMNS_SINCE);
+        this.columnFamily = columnFamily;
+        this.sinceTimestamp = sinceTimestamp;
+    }
+
+    @Override
+    public String getColumnFamilyName()
+    {
+        return columnFamily;
+    }
+
+    @Override
+    public ReadCommand copy()
+    {
+        ReadCommand readCommand= new ColumnsSinceReadCommand(table, key, columnFamily, sinceTimestamp);
+        readCommand.setDigestQuery(isDigestQuery());
+        return readCommand;
+    }
+
+    @Override
+    public Row getRow(Table table) throws IOException
+    {        
+        return table.getRow(key, columnFamily, sinceTimestamp);
+    }
+
+    @Override
+    public String toString()
+    {
+        return "GetColumnsSinceMessage(" +
+               "table='" + table + '\'' +
+               ", key='" + key + '\'' +
+               ", columnFamily='" + columnFamily + '\'' +
+               ", sinceTimestamp='" + sinceTimestamp + '\'' +
+               ')';
+    }
+
+}
+
+class ColumnsSinceReadCommandSerializer extends ReadCommandSerializer
+{
+    @Override
+    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
+    {
+        ColumnsSinceReadCommand realRM = (ColumnsSinceReadCommand)rm;
+        dos.writeBoolean(realRM.isDigestQuery());
+        dos.writeUTF(realRM.table);
+        dos.writeUTF(realRM.key);
+        dos.writeUTF(realRM.columnFamily);
+        dos.writeLong(realRM.sinceTimestamp);
+    }
+
+    @Override
+    public ReadCommand deserialize(DataInputStream dis) throws IOException
+    {
+        boolean isDigest = dis.readBoolean();
+        String table = dis.readUTF();
+        String key = dis.readUTF();
+        String columnFamily = dis.readUTF();
+        long sinceTimestamp = dis.readLong();
+
+        ColumnsSinceReadCommand rm = new ColumnsSinceReadCommand(table, key, columnFamily, sinceTimestamp);
+        rm.setDigestQuery(isDigest);
+        return rm;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLog.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLog.java
index e69de29b..380143e2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLog.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLog.java
@@ -0,0 +1,658 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.*;
+import java.util.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.IFileWriter;
+import org.apache.cassandra.io.SequenceFile;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+/*
+ * Commit Log tracks every write operation into the system. The aim
+ * of the commit log is to be able to successfully recover data that was
+ * not stored to disk via the Memtable. Every Commit Log maintains a
+ * header represented by the abstraction CommitLogHeader. The header
+ * contains a bit array and an array of longs and both the arrays are
+ * of size, #column families for the Table, the Commit Log represents.
+ * Whenever a ColumnFamily is written to, for the first time its bit flag
+ * is set to one in the CommitLogHeader. When it is flushed to disk by the
+ * Memtable its corresponding bit in the header is set to zero. This helps
+ * track which CommitLogs can be thrown away as a result of Memtable flushes.
+ * However if a ColumnFamily is flushed and again written to disk then its
+ * entry in the array of longs is updated with the offset in the Commit Log
+ * file where it was written. This helps speed up recovery since we can seek
+ * to these offsets and start processing the commit log.
+ * Every Commit Log is rolled over everytime it reaches its threshold in size.
+ * Over time there could be a number of commit logs that would be generated.
+ * Hovever whenever we flush a column family disk and update its bit flag we
+ * take this bit array and bitwise & it with the headers of the other commit
+ * logs that are older.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class CommitLog
+{
+    private static final int bufSize_ = 128*1024*1024;
+    private static Map<String, CommitLog> instances_ = new HashMap<String, CommitLog>();
+    private static Lock lock_ = new ReentrantLock();
+    private static Logger logger_ = Logger.getLogger(CommitLog.class);
+    private static Map<String, CommitLogHeader> clHeaders_ = new HashMap<String, CommitLogHeader>();
+    
+    public static final class CommitLogContext
+    {
+        static CommitLogContext NULL = new CommitLogContext(null, -1L);
+        /* Commit Log associated with this operation */
+        private String file_;
+        /* Offset within the Commit Log where this row as added */
+        private long position_;
+
+        public CommitLogContext(String file, long position)
+        {
+            file_ = file;
+            position_ = position;
+        }
+
+        boolean isValidContext()
+        {
+            return (position_ != -1L);
+        }
+
+        String file()
+        {
+            return file_;
+        }
+
+        long position()
+        {
+            return position_;
+        }
+    }
+
+    public static class CommitLogFileComparator implements Comparator<String>
+    {
+        public int compare(String f, String f2)
+        {
+            return (int)(getCreationTime(f) - getCreationTime(f2));
+        }
+
+        public boolean equals(Object o)
+        {
+            if ( !(o instanceof CommitLogFileComparator) )
+                return false;
+            return true;
+        }
+    }
+
+    static long getCreationTime(String file)
+    {
+        String[] entries = FBUtilities.strip(file, "-.");
+        return Long.parseLong(entries[entries.length - 2]);
+    }
+
+    /*
+     * Write the serialized commit log header into the specified commit log.
+    */
+    private static void writeCommitLogHeader(String commitLogFileName, byte[] bytes) throws IOException
+    {     
+        IFileWriter logWriter = CommitLog.createWriter(commitLogFileName);
+        logWriter.seek(0L);
+        /* write the commit log header */
+        logWriter.writeDirect(bytes);
+        logWriter.close();
+    }
+
+    private static IFileWriter createWriter(String file) throws IOException
+    {        
+        if ( DatabaseDescriptor.isFastSync() )
+        {
+            /* Add this to the threshold */
+            int bufSize = 4*1024*1024;
+            return SequenceFile.fastWriter(file, CommitLog.bufSize_ + bufSize);
+        }
+        else
+            return SequenceFile.writer(file);
+    }
+
+    static CommitLog open(String table) throws IOException
+    {
+        CommitLog commitLog = instances_.get(table);
+        if ( commitLog == null )
+        {
+            CommitLog.lock_.lock();
+            try
+            {
+                commitLog = instances_.get(table);
+                if ( commitLog == null )
+                {
+                    commitLog = new CommitLog(table, false);
+                    instances_.put(table, commitLog);
+                }
+            }
+            finally
+            {
+                CommitLog.lock_.unlock();
+            }
+        }
+        return commitLog;
+    }
+
+    static String getTableName(String file)
+    {
+        String[] values = file.split("-");
+        return values[1];
+    }
+
+    private String table_;
+    /* Current commit log file */
+    private String logFile_;
+    /* header for current commit log */
+    private CommitLogHeader clHeader_;
+    private IFileWriter logWriter_;
+    private long commitHeaderStartPos_;
+    /* Force rollover the commit log on the next insert */
+    private boolean forcedRollOver_ = false;
+
+
+    /*
+     * Generates a file name of the format CommitLog-<table>-<timestamp>.log in the
+     * directory specified by the Database Descriptor.
+    */
+    private void setNextFileName()
+    {
+        logFile_ = DatabaseDescriptor.getLogFileLocation() +
+                            System.getProperty("file.separator") +
+                            "CommitLog-" +
+                            table_ +
+                            "-" +
+                            System.currentTimeMillis() +
+                            ".log";
+    }
+
+    /*
+     * param @ table - name of table for which we are maintaining
+     *                 this commit log.
+     * param @ recoverymode - is commit log being instantiated in
+     *                        in recovery mode.
+    */
+    CommitLog(String table, boolean recoveryMode) throws IOException
+    {
+        table_ = table;
+        if ( !recoveryMode )
+        {
+            setNextFileName();            
+            logWriter_ = CommitLog.createWriter(logFile_);
+            writeCommitLogHeader();
+        }
+    }
+
+    /*
+     * This ctor is currently used only for debugging. We
+     * are now using it to modify the header so that recovery
+     * can be tested in as many scenarios as we could imagine.
+     *
+     * param @ logFile - logfile which we wish to modify.
+    */
+    CommitLog(File logFile) throws IOException
+    {
+        table_ = CommitLog.getTableName(logFile.getName());
+        logFile_ = logFile.getAbsolutePath();        
+        logWriter_ = CommitLog.createWriter(logFile_);
+        commitHeaderStartPos_ = 0L;
+    }
+
+    String getLogFile()
+    {
+        return logFile_;
+    }
+
+    void readCommitLogHeader(String logFile, byte[] bytes) throws IOException
+    {
+        IFileReader logReader = SequenceFile.reader(logFile);
+        try
+        {
+            logReader.readDirect(bytes);
+        }
+        finally
+        {
+            logReader.close();
+        }
+    }
+
+    /*
+     * This is invoked on startup via the ctor. It basically
+     * writes a header with all bits set to zero.
+    */
+    private void writeCommitLogHeader() throws IOException
+    {
+        Table table = Table.open(table_);
+        int cfSize = table.getNumberOfColumnFamilies();
+        /* record the beginning of the commit header */
+        commitHeaderStartPos_ = logWriter_.getCurrentPosition();
+        /* write the commit log header */
+        clHeader_ = new CommitLogHeader(cfSize);
+        writeCommitLogHeader(clHeader_.toByteArray(), false);
+    }
+
+    private void writeCommitLogHeader(byte[] bytes, boolean reset) throws IOException
+    {
+        /* record the current position */
+        long currentPos = logWriter_.getCurrentPosition();
+        logWriter_.seek(commitHeaderStartPos_);
+        /* write the commit log header */
+        logWriter_.writeDirect(bytes);
+        if ( reset )
+        {
+            /* seek back to the old position */
+            logWriter_.seek(currentPos);
+        }
+    }
+
+    void recover(List<File> clogs) throws IOException
+    {
+        Table table = Table.open(table_);
+        int cfSize = table.getNumberOfColumnFamilies();
+        int size = CommitLogHeader.size(cfSize);
+        byte[] header = new byte[size];
+        byte[] header2 = new byte[size];
+        int index = clogs.size() - 1;
+
+        File file = clogs.get(index);
+        readCommitLogHeader(file.getAbsolutePath(), header);
+
+        Stack<File> filesNeeded = new Stack<File>();
+        filesNeeded.push(file);
+
+        /*
+         * Identify files that we need for processing. This can be done
+         * using the information in the header of each file. Simply and
+         * the byte[] (which are the headers) and stop at the file where
+         * the result is a zero.
+        */
+        for ( int i = (index - 1); i >= 0; --i )
+        {
+            file = clogs.get(i);
+            readCommitLogHeader(file.getAbsolutePath(), header2);
+            byte[] result = CommitLogHeader.and(header, header2);
+            if ( !CommitLogHeader.isZero(result) )
+            {
+                filesNeeded.push(file);
+            }
+            else
+            {
+                break;
+            }
+        }
+
+        doRecovery(filesNeeded, header);
+    }
+
+    private void printHeader(byte[] header)
+    {
+        StringBuilder sb = new StringBuilder("");
+        for ( byte b : header )
+        {
+            sb.append(b);
+            sb.append(" ");
+        }
+        logger_.debug(sb.toString());
+    }
+
+    private void doRecovery(Stack<File> filesNeeded, byte[] header) throws IOException
+    {
+        Table table = Table.open(table_);
+
+        DataInputBuffer bufIn = new DataInputBuffer();
+        DataOutputBuffer bufOut = new DataOutputBuffer();        
+
+        while ( !filesNeeded.isEmpty() )
+        {
+            File file = filesNeeded.pop();
+            // IFileReader reader = SequenceFile.bufferedReader(file.getAbsolutePath(), DatabaseDescriptor.getLogFileSizeThreshold());
+            IFileReader reader = SequenceFile.reader(file.getAbsolutePath());
+            try
+            {
+                reader.readDirect(header);
+                /* deserialize the commit log header */
+                bufIn.reset(header, 0, header.length);
+                CommitLogHeader clHeader = CommitLogHeader.serializer().deserialize(bufIn);
+                /* seek to the lowest position */
+                int lowPos = CommitLogHeader.getLowestPosition(clHeader);
+                /*
+                 * If lowPos == 0 then we need to skip the processing of this
+                 * file.
+                */
+                if (lowPos == 0)
+                    break;
+                else
+                    reader.seek(lowPos);
+
+                /* read the logs populate RowMutation and apply */
+                while ( !reader.isEOF() )
+                {
+                    bufOut.reset();
+                    long bytesRead = reader.next(bufOut);
+                    if ( bytesRead == -1 )
+                        break;
+
+                    bufIn.reset(bufOut.getData(), bufOut.getLength());
+                    /* Skip over the commit log key portion */
+                    bufIn.readUTF();
+                    /* Skip over data size */
+                    bufIn.readInt();
+                    
+                    /* read the commit log entry */
+                    try
+                    {                        
+                        Row row = Row.serializer().deserialize(bufIn);
+                        Map<String, ColumnFamily> columnFamilies = new HashMap<String, ColumnFamily>(row.getColumnFamilyMap());
+                        /* remove column families that have already been flushed */
+                    	Set<String> cNames = columnFamilies.keySet();
+
+                        for ( String cName : cNames )
+                        {
+                        	ColumnFamily columnFamily = columnFamilies.get(cName);
+                        	/* TODO: Remove this to not process Hints */
+                        	if ( !DatabaseDescriptor.isApplicationColumnFamily(cName) )
+                        	{
+                        		row.removeColumnFamily(columnFamily);
+                        		continue;
+                        	}	
+                            int id = table.getColumnFamilyId(columnFamily.name());
+                            if ( clHeader.get(id) == 0 || reader.getCurrentPosition() < clHeader.getPosition(id) )
+                                row.removeColumnFamily(columnFamily);
+                        }
+                        if ( !row.isEmpty() )
+                        {                            
+                        	table.applyNow(row);
+                        }
+                    }
+                    catch ( IOException e )
+                    {
+                        logger_.debug( LogUtil.throwableToString(e) );
+                    }
+                }
+                reader.close();
+                /* apply the rows read */
+                table.flush(true);
+            }
+            catch ( Throwable th )
+            {
+                logger_.info( LogUtil.throwableToString(th) );
+                /* close the reader and delete this commit log. */
+                reader.close();
+                FileUtils.delete( new File[]{file} );
+            }
+        }
+    }
+
+    /*
+     * Update the header of the commit log if a new column family
+     * is encountered for the first time.
+    */
+    private void updateHeader(Row row) throws IOException
+    {
+    	Map<String, ColumnFamily> columnFamilies = row.getColumnFamilyMap();
+        Table table = Table.open(table_);
+        Set<String> cNames = columnFamilies.keySet();
+        for ( String cName : cNames )
+        {
+        	ColumnFamily columnFamily = columnFamilies.get(cName);
+        	int id = table.getColumnFamilyId(columnFamily.name());
+        	if ( clHeader_.get(id) == 0 || ( clHeader_.get(id) == 1 && clHeader_.getPosition(id) == 0 ) )
+        	{
+            	if ( clHeader_.get(id) == 0 || ( clHeader_.get(id) == 1 && clHeader_.getPosition(id) == 0 ) )
+            	{
+	        		clHeader_.turnOn( id, logWriter_.getCurrentPosition() );
+	        		writeCommitLogHeader(clHeader_.toByteArray(), true);
+            	}
+        	}
+        }
+    }
+
+    /*
+     * Adds the specified row to the commit log. This method will reset the
+     * file offset to what it is before the start of the operation in case
+     * of any problems. This way we can assume that the subsequent commit log
+     * entry will override the garbage left over by the previous write.
+    */
+    synchronized CommitLogContext add(Row row) throws IOException
+    {
+        long currentPosition = -1L;
+        CommitLogContext cLogCtx = null;
+        DataOutputBuffer cfBuffer = new DataOutputBuffer();
+        long fileSize = 0L;
+        
+        try
+        {
+            /* serialize the row */
+            cfBuffer.reset();
+            Row.serializer().serialize(row, cfBuffer);
+            currentPosition = logWriter_.getCurrentPosition();
+            cLogCtx = new CommitLogContext(logFile_, currentPosition);
+            /* Update the header */
+            updateHeader(row);
+            logWriter_.append(table_, cfBuffer);
+            fileSize = logWriter_.getFileSize();                       
+            checkThresholdAndRollLog(fileSize);            
+        }
+        catch (IOException e)
+        {
+            if ( currentPosition != -1 )
+                logWriter_.seek(currentPosition);
+            throw e;
+        }
+        finally
+        {                  	
+            cfBuffer.close();            
+        }
+        return cLogCtx;
+    }
+
+    /*
+     * This is called on Memtable flush to add to the commit log
+     * a token indicating that this column family has been flushed.
+     * The bit flag associated with this column family is set in the
+     * header and this is used to decide if the log file can be deleted.
+    */
+    synchronized void onMemtableFlush(String cf, CommitLog.CommitLogContext cLogCtx) throws IOException
+    {
+        Table table = Table.open(table_);
+        int id = table.getColumnFamilyId(cf);
+        /* trying discarding old commit log files */
+        discard(cLogCtx, id);
+    }
+
+
+    /*
+     * Check if old commit logs can be deleted. However we cannot
+     * do this anymore in the Fast Sync mode and hence I think we
+     * should get rid of Fast Sync mode altogether. If there is
+     * a pathological event where few CF's are rarely being updated
+     * then their Memtable never gets flushed.
+     * This will prevent commit logs from being deleted. WE NEED to
+     * fix this using some hueristic and force flushing such Memtables.
+     *
+     * param @ cLogCtx The commitLog context .
+     * param @ id id of the columnFamily being flushed to disk.
+     *
+    */
+    private void discard(CommitLog.CommitLogContext cLogCtx, int id) throws IOException
+    {
+        /* retrieve the commit log header associated with the file in the context */
+        CommitLogHeader commitLogHeader = clHeaders_.get(cLogCtx.file());
+        if(commitLogHeader == null )
+        {
+            if( logFile_.equals(cLogCtx.file()) )
+            {
+                /* this means we are dealing with the current commit log. */
+                commitLogHeader = clHeader_;
+                clHeaders_.put(cLogCtx.file(), clHeader_);
+            }
+            else
+                return;
+        }
+        /*
+         * We do any processing only if there is a change in the position in the context.
+         * This can happen if an older Memtable's flush comes in after a newer Memtable's
+         * flush. Right now this cannot happen since Memtables are flushed on a single
+         * thread.
+        */
+        if ( cLogCtx.position() < commitLogHeader.getPosition(id) )
+            return;
+        commitLogHeader.turnOff(id);
+        /* Sort the commit logs based on creation time */
+        List<String> oldFiles = new ArrayList<String>(clHeaders_.keySet());
+        Collections.sort(oldFiles, new CommitLogFileComparator());
+        List<String> listOfDeletedFiles = new ArrayList<String>();
+        /*
+         * Loop through all the commit log files in the history. Now process
+         * all files that are older than the one in the context. For each of
+         * these files the header needs to modified by performing a bitwise &
+         * of the header with the header of the file in the context. If we
+         * encounter the file in the context in our list of old commit log files
+         * then we update the header and write it back to the commit log.
+        */
+        for(String oldFile : oldFiles)
+        {
+            if(oldFile.equals(cLogCtx.file()))
+            {
+                /*
+                 * We need to turn on again. This is because we always keep
+                 * the bit turned on and the position indicates from where the
+                 * commit log needs to be read. When a flush occurs we turn off
+                 * perform & operation and then turn on with the new position.
+                */
+                commitLogHeader.turnOn(id, cLogCtx.position());
+                writeCommitLogHeader(cLogCtx.file(), commitLogHeader.toByteArray());
+                break;
+            }
+            else
+            {
+                CommitLogHeader oldCommitLogHeader = clHeaders_.get(oldFile);
+                oldCommitLogHeader.and(commitLogHeader);
+                if(oldCommitLogHeader.isSafeToDelete())
+                {
+                	logger_.debug("Deleting commit log:"+ oldFile);
+                    FileUtils.deleteAsync(oldFile);
+                    listOfDeletedFiles.add(oldFile);
+                }
+                else
+                {
+                    writeCommitLogHeader(oldFile, oldCommitLogHeader.toByteArray());
+                }
+            }
+        }
+
+        for ( String deletedFile : listOfDeletedFiles)
+        {
+            clHeaders_.remove(deletedFile);
+        }
+    }
+
+    private void checkThresholdAndRollLog( long fileSize )
+    {
+        try
+        {
+            if ( fileSize >= DatabaseDescriptor.getLogFileSizeThreshold() || forcedRollOver_ )
+            {
+                if ( logWriter_.getFileSize() >= DatabaseDescriptor.getLogFileSizeThreshold() || forcedRollOver_ )
+                {
+	                /* Rolls the current log file over to a new one. */
+	                setNextFileName();
+	                String oldLogFile = logWriter_.getFileName();
+	                //history_.add(oldLogFile);
+	                logWriter_.close();
+	
+	                /* point reader/writer to a new commit log file. */
+	                // logWriter_ = SequenceFile.writer(logFile_);
+	                logWriter_ = CommitLog.createWriter(logFile_);
+	                /* squirrel away the old commit log header */
+	                clHeaders_.put(oldLogFile, new CommitLogHeader( clHeader_ ));
+	                /*
+	                 * We need to zero out positions because the positions in
+	                 * the old file do not make sense in the new one.
+	                */
+	                clHeader_.zeroPositions();
+	                writeCommitLogHeader(clHeader_.toByteArray(), false);
+	                // Get the list of files in commit log directory if it is greater than a certain number  
+	                // Force flush all the column families that way we ensure that a slowly populated column family is not screwing up 
+	                // by accumulating the commit logs .
+                }
+            }
+        }
+        catch ( IOException e )
+        {
+            logger_.info(LogUtil.throwableToString(e));
+        }
+        finally
+        {
+        	forcedRollOver_ = false;
+        }
+    }
+
+    public void setForcedRollOver()
+    {
+    	forcedRollOver_ = true;
+    }
+
+    public static void reset()
+    {
+        CommitLog.instances_.clear();
+    }
+
+    public static void main(String[] args) throws Throwable
+    {
+        LogUtil.init();
+        
+        File logDir = new File(DatabaseDescriptor.getLogFileLocation());
+        File[] files = logDir.listFiles();
+        Arrays.sort( files, new FileUtils.FileComparator() );
+
+        byte[] bytes = new byte[CommitLogHeader.size(Integer.parseInt(args[0]))];
+        for ( File file : files )
+        {
+            CommitLog clog = new CommitLog( file );
+            clog.readCommitLogHeader(file.getAbsolutePath(), bytes);
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(bytes, 0, bytes.length);
+            CommitLogHeader clHeader = CommitLogHeader.serializer().deserialize(bufIn);
+            /*
+            StringBuilder sb = new StringBuilder("");
+            for ( byte b : bytes )
+            {
+                sb.append(b);
+                sb.append(" ");
+            }
+            */
+            System.out.println("FILE:" + file);
+            System.out.println(clHeader.toString());
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLogEntry.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLogEntry.java
index e69de29b..ba90dc99 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLogEntry.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLogEntry.java
@@ -0,0 +1,103 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+/*
+ * An instance of this class represents an update to a table. 
+ * This is written to the CommitLog to be replayed on recovery. It
+ * contains enough information to be written to a SSTable to 
+ * capture events that happened before some catastrophe.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+class CommitLogEntry
+{    
+    private static ICompactSerializer<CommitLogEntry> serializer_;
+    static
+    {
+        serializer_ = new CommitLogEntrySerializer();
+    }
+    
+    static ICompactSerializer<CommitLogEntry> serializer()
+    {
+        return serializer_;
+    }    
+    
+    private int length_;
+    private byte[] value_ = new byte[0];
+    
+    CommitLogEntry()
+    {
+    }
+    
+    CommitLogEntry(byte[] value)
+    {
+        this(value, 0);
+    }
+    
+    CommitLogEntry(byte[] value, int length)
+    {
+        value_ = value;   
+        length_ = length;
+    }
+    
+    void value(byte[] bytes)
+    {
+        value_ = bytes;
+    }
+    
+    byte[] value()
+    {
+        return value_;
+    }
+    
+    void length(int size)
+    {
+        length_ = size;
+    }
+    
+    int length()
+    {
+        return length_;
+    }
+}
+
+class CommitLogEntrySerializer implements ICompactSerializer<CommitLogEntry>
+{
+    public void serialize(CommitLogEntry logEntry, DataOutputStream dos) throws IOException
+    {    
+        int length = logEntry.length();
+        dos.writeInt(length);
+        dos.write(logEntry.value(), 0, length);           
+    }
+    
+    public CommitLogEntry deserialize(DataInputStream dis) throws IOException
+    {        
+        byte[] value = new byte[dis.readInt()];
+        dis.readFully(value);        
+        return new CommitLogEntry(value);
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLogHeader.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLogHeader.java
index e69de29b..3ebb85b9 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLogHeader.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CommitLogHeader.java
@@ -0,0 +1,277 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.*;
+import java.nio.ByteBuffer;
+import java.util.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.io.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class CommitLogHeader
+{
+    private static ICompactSerializer<CommitLogHeader> serializer_;
+    
+    static
+    {
+        serializer_ = new CommitLogHeaderSerializer();
+    }
+    
+    static ICompactSerializer<CommitLogHeader> serializer()
+    {
+        return serializer_;
+    }
+    
+    static int size(int size)
+    {
+        /* 
+         * We serialize the CommitLogHeader as a byte[] and write it
+         * to disk. So we first write an "int" to specify the length 
+         * of the byte[] which is why we first have a 4 in the sum.
+         * We then have size which is the number of bits to track who
+         * has been flushed and then the rest is the position[]
+         * size = #of column families 
+         *        + 
+         *        size of the bitset 
+         *        + 
+         *        size of position array 
+         */
+        return 4 + size + (4 * size); 
+    }
+    
+    static int getLowestPosition(CommitLogHeader clHeader)
+    {
+        int[] positions = clHeader.getPositions();
+        int minPosition = Integer.MAX_VALUE;
+        for ( int position : positions )
+        {
+            if ( position < minPosition && position > 0)
+            {
+                minPosition = position;
+            }
+        }
+        
+        if(minPosition == Integer.MAX_VALUE)
+            minPosition = 0;
+        return minPosition;
+    }
+    
+    /* 
+     * Bitwise & of each byte in the two arrays.
+     * Both arrays are of same length. In order
+     * to be memory efficient the result is in
+     * the third parameter.
+    */
+    static byte[] and(byte[] bytes, byte[] bytes2) throws IOException
+    { 
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(bytes, 0, bytes.length);
+        CommitLogHeader clHeader = CommitLogHeader.serializer().deserialize(bufIn);
+        byte[] clh = clHeader.getBitSet();
+        
+        bufIn.reset(bytes2, 0, bytes2.length);
+        CommitLogHeader clHeader2 = CommitLogHeader.serializer().deserialize(bufIn);
+        byte[] clh2 = clHeader2.getBitSet();
+        
+        byte[] result = new byte[clh.length];
+        for ( int i = 0; i < clh.length; ++i )
+        {            
+            result[i] = (byte)(clh[i] & clh2[i]);
+        }
+        
+        return result;
+    }
+    
+    static boolean isZero(byte[] bytes)
+    {
+        for ( byte b : bytes )
+        {
+            if ( b == 1 )
+                return false;
+        }
+        return true;
+    }
+    
+    private byte[] header_ = new byte[0];
+    private int[] position_ = new int[0];
+    
+    CommitLogHeader(int size)
+    {
+        header_ = new byte[size];
+        position_ = new int[size];
+    }
+    
+    /*
+     * This ctor is used while deserializing. This ctor
+     * also builds an index of position to column family
+     * Id.
+    */
+    CommitLogHeader(byte[] header, int[] position)
+    {
+        header_ = header;
+        position_ = position;
+    }
+    
+    CommitLogHeader(CommitLogHeader clHeader)
+    {
+        header_ = new byte[clHeader.header_.length];
+        System.arraycopy(clHeader.header_, 0, header_, 0, header_.length);
+        position_ = new int[clHeader.position_.length];
+        System.arraycopy(clHeader.position_, 0, position_, 0, position_.length);
+    }
+    
+    byte get(int index)
+    {
+        return header_[index];
+    } 
+    
+    int getPosition(int index)
+    {
+        return position_[index];
+    }
+    
+    void turnOn(int index, long position)
+    {
+        turnOn(header_, index, position);
+    }
+    
+    void turnOn(byte[] bytes, int index, long position)
+    {
+        bytes[index] = (byte)1;
+        position_[index] = (int)position;
+    }
+    
+    void turnOff(int index)
+    {
+        turnOff(header_, index);
+    }
+    
+    void turnOff(byte[] bytes, int index)
+    {
+        bytes[index] = (byte)0;
+        position_[index] = 0; 
+    }
+    
+    boolean isSafeToDelete() throws IOException
+    {
+        return isSafeToDelete(header_);
+    }
+    
+    boolean isSafeToDelete(byte[] bytes) throws IOException
+    {        
+        for ( byte b : bytes )
+        {
+            if ( b == 1 )
+                return false;
+        }
+        return true;
+    }
+    
+    byte[] getBitSet()
+    {
+        return header_;
+    }
+    
+    int[] getPositions()
+    {
+        return position_;
+    }
+    
+    void zeroPositions()
+    {
+        int size = position_.length;
+        position_ = new int[size];
+    }
+    
+    void and (CommitLogHeader commitLogHeader)
+    {        
+        byte[] clh2 = commitLogHeader.header_;
+        for ( int i = 0; i < header_.length; ++i )
+        {            
+            header_[i] = (byte)(header_[i] & clh2[i]);
+        }
+    }
+    
+    byte[] toByteArray() throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);        
+        CommitLogHeader.serializer().serialize(this, dos);
+        return bos.toByteArray();
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder("");        
+        for ( int i = 0; i < header_.length; ++i )
+        {
+            sb.append(header_[i]);
+            sb.append(":");
+            Table table = Table.open( DatabaseDescriptor.getTables().get(0));
+            sb.append(table.getColumnFamilyName(i));
+            sb.append(" ");
+        }        
+        sb.append(" | " );        
+        for ( int position : position_ )
+        {
+            sb.append(position);
+            sb.append(" ");
+        }        
+        return sb.toString();
+    }
+}
+
+class CommitLogHeaderSerializer implements ICompactSerializer<CommitLogHeader>
+{
+    public void serialize(CommitLogHeader clHeader, DataOutputStream dos) throws IOException
+    {        
+        dos.writeInt(clHeader.getBitSet().length);
+        dos.write(clHeader.getBitSet());
+        int[] positions = clHeader.getPositions();        
+        
+        for ( int position : positions )
+        {
+            dos.writeInt(position);
+        }
+    }
+    
+    public CommitLogHeader deserialize(DataInputStream dis) throws IOException
+    {
+        int size = dis.readInt();
+        byte[] bitFlags = new byte[size];
+        dis.readFully(bitFlags);
+        
+        int[] position = new int[size];
+        for ( int i = 0; i < size; ++i )
+        {
+            position[i] = dis.readInt();
+        }
+                                                 
+        return new CommitLogHeader(bitFlags, position);
+    }
+}
+
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CompactSerializerInvocationHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CompactSerializerInvocationHandler.java
index e69de29b..9fda1069 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CompactSerializerInvocationHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CompactSerializerInvocationHandler.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.lang.reflect.InvocationHandler;
+import java.lang.reflect.Method;
+
+import org.apache.cassandra.io.DataOutputBuffer;
+
+
+/*
+ * This is the abstraction that pre-processes calls to implmentations
+ * of the ICompactSerializer2 serialize() via dynamic proxies.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class CompactSerializerInvocationHandler<T> implements InvocationHandler
+{
+    private ICompactSerializer2<T> serializer_;
+
+    public CompactSerializerInvocationHandler(ICompactSerializer2<T> serializer)
+    {
+        serializer_ = serializer;
+    }
+
+    /*
+     * This dynamic runtime proxy adds the indexes before the actual coumns are serialized.
+    */
+    public Object invoke(Object proxy, Method m, Object[] args) throws Throwable
+    {
+        /* Do the preprocessing here. */
+    	ColumnFamily cf = (ColumnFamily)args[0];
+    	DataOutputBuffer bufOut = (DataOutputBuffer)args[1];
+    	ColumnIndexer.serialize(cf, bufOut);
+        return m.invoke(serializer_, args);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CountFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CountFilter.java
index e69de29b..174b425c 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CountFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CountFilter.java
@@ -0,0 +1,139 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.SSTable;
+
+
+/**
+ * This class provides a filter for fitering out columns
+ * greater than a certain count.
+ * 
+ * @author pmalik
+ *
+ */
+public class CountFilter implements IFilter
+{
+	private long countLimit_;
+	private boolean isDone_;
+	private int offset_;
+
+	CountFilter(int countLimit)
+	{
+		countLimit_ = countLimit;
+		isDone_ = false;
+		offset_ = 0;
+	}
+	
+	CountFilter(int countLimit, int offset)
+    {
+        this(countLimit);
+        offset_ = offset;
+    }
+
+	public ColumnFamily filter(String cfNameParam, ColumnFamily columnFamily)
+	{
+    	String[] values = RowMutation.getColumnAndColumnFamily(cfNameParam);
+        if ( columnFamily == null )
+            return columnFamily;
+
+        ColumnFamily filteredCf = new ColumnFamily(columnFamily.name(), columnFamily.type());
+		if( countLimit_ <= 0 )
+		{
+			isDone_ = true;
+			return filteredCf;
+		}
+		if( values.length == 1)
+		{
+    		Collection<IColumn> columns = columnFamily.getAllColumns();
+    		for(IColumn column : columns)
+    		{
+    			if (offset_ <= 0) {
+    				filteredCf.addColumn(column);
+    				countLimit_--;
+    			} else
+    				offset_ --;
+    			if( countLimit_ <= 0 )
+    			{
+    				isDone_ = true;
+    				return filteredCf;
+    			}
+    		}
+		}
+		else if(values.length == 2 && columnFamily.isSuper())
+		{
+    		Collection<IColumn> columns = columnFamily.getAllColumns();
+    		for(IColumn column : columns)
+    		{
+    			SuperColumn superColumn = (SuperColumn)column;
+    			SuperColumn filteredSuperColumn = new SuperColumn(superColumn.name());
+				filteredCf.addColumn(filteredSuperColumn);
+        		Collection<IColumn> subColumns = superColumn.getSubColumns();
+        		for(IColumn subColumn : subColumns)
+        		{
+        			if (offset_ <=0 ){
+        				filteredSuperColumn.addColumn(subColumn.name(), subColumn);
+        				countLimit_--;
+        			} else
+        				offset_--;
+        			
+	    			if( countLimit_ <= 0 )
+	    			{
+	    				isDone_ = true;
+	    				return filteredCf;
+	    			}
+        		}
+    		}
+		}
+    	else
+    	{
+    		throw new UnsupportedOperationException();
+    	}
+		return filteredCf;
+	}
+
+    public IColumn filter(IColumn column, DataInputStream dis) throws IOException
+    {
+		countLimit_--;
+		if( countLimit_ <= 0 )
+		{
+			isDone_ = true;
+		}
+		return column;
+    }
+
+	public boolean isDone()
+	{
+		return isDone_;
+	}
+
+	public void setDone()
+	{
+		isDone_ = true;
+	}
+
+    public DataInputBuffer next(String key, String cf, SSTable ssTable) throws IOException
+    {
+    	return ssTable.next(key, cf);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DBConstants.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DBConstants.java
index e69de29b..59274f84 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DBConstants.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DBConstants.java
@@ -0,0 +1,9 @@
+package org.apache.cassandra.db;
+
+final class DBConstants
+{
+	public static final int boolSize_ = 1;
+	public static final int intSize_ = 4;
+	public static final int longSize_ = 8;
+	public static final int tsSize_ = 8;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DBManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DBManager.java
index e69de29b..ece3918b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DBManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DBManager.java
@@ -0,0 +1,162 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.BasicUtilities;
+import org.apache.cassandra.utils.FBUtilities;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class DBManager
+{
+    private static DBManager dbMgr_;
+    private static Lock lock_ = new ReentrantLock();
+
+    public static DBManager instance() throws IOException
+    {
+        if ( dbMgr_ == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( dbMgr_ == null )
+                    dbMgr_ = new DBManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return dbMgr_;
+    }
+
+    public static class StorageMetadata
+    {
+        private Token myToken;
+        private int generation_;
+
+        StorageMetadata(Token storageId, int generation)
+        {
+            myToken = storageId;
+            generation_ = generation;
+        }
+
+        public Token getStorageId()
+        {
+            return myToken;
+        }
+
+        public void setStorageId(Token storageId)
+        {
+            myToken = storageId;
+        }
+
+        public int getGeneration()
+        {
+            return generation_;
+        }
+    }
+
+    public DBManager() throws IOException
+    {
+        Set<String> tables = DatabaseDescriptor.getTableToColumnFamilyMap().keySet();
+        
+        for (String table : tables)
+        {
+            Table tbl = Table.open(table);
+            tbl.onStart();
+        }
+        /* Do recovery if need be. */
+        RecoveryManager recoveryMgr = RecoveryManager.instance();
+        recoveryMgr.doRecovery();
+    }
+
+    /*
+     * This method reads the system table and retrieves the metadata
+     * associated with this storage instance. Currently we store the
+     * metadata in a Column Family called LocatioInfo which has two
+     * columns namely "Token" and "Generation". This is the token that
+     * gets gossiped around and the generation info is used for FD.
+    */
+    public DBManager.StorageMetadata start() throws IOException
+    {
+        StorageMetadata storageMetadata = null;
+        /* Read the system table to retrieve the storage ID and the generation */
+        SystemTable sysTable = SystemTable.openSystemTable(SystemTable.name_);
+        Row row = sysTable.get(FBUtilities.getHostAddress());
+
+        IPartitioner p = StorageService.getPartitioner();
+        if ( row == null )
+        {
+            Token token = p.getDefaultToken();
+            int generation = 1;
+
+            String key = FBUtilities.getHostAddress();
+            row = new Row(key);
+            ColumnFamily cf = new ColumnFamily(SystemTable.cfName_, "Standard");
+            cf.addColumn(new Column(SystemTable.token_, p.getTokenFactory().toByteArray(token)));
+            cf.addColumn(new Column(SystemTable.generation_, BasicUtilities.intToByteArray(generation)) );
+            row.addColumnFamily(cf);
+            sysTable.apply(row);
+            storageMetadata = new StorageMetadata( token, generation);
+        }
+        else
+        {
+            /* we crashed and came back up need to bump generation # */
+        	Map<String, ColumnFamily> columnFamilies = row.getColumnFamilyMap();
+        	Set<String> cfNames = columnFamilies.keySet();
+
+            for ( String cfName : cfNames )
+            {
+            	ColumnFamily columnFamily = columnFamilies.get(cfName);
+
+                IColumn tokenColumn = columnFamily.getColumn(SystemTable.token_);
+                Token token = p.getTokenFactory().fromByteArray(tokenColumn.value());
+
+                IColumn generation = columnFamily.getColumn(SystemTable.generation_);
+                int gen = BasicUtilities.byteArrayToInt(generation.value()) + 1;
+
+                Column generation2 = new Column("Generation", BasicUtilities.intToByteArray(gen), generation.timestamp() + 1);
+                columnFamily.addColumn(generation2);
+                storageMetadata = new StorageMetadata(token, gen);
+                break;
+            }
+            sysTable.reset(row);
+        }
+        return storageMetadata;
+    }
+
+    public static void main(String[] args) throws Throwable
+    {
+        DBManager.instance().start();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DataFileVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DataFileVerbHandler.java
index e69de29b..666f4a16 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DataFileVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/DataFileVerbHandler.java
@@ -0,0 +1,46 @@
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+public class DataFileVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger( DataFileVerbHandler.class );
+    
+    public void doVerb(Message message)
+    {        
+        Object[] body = message.getMessageBody();
+        byte[] bytes = (byte[])body[0];
+        String table = new String(bytes);
+        logger_.info("**** Received a request from " + message.getFrom());
+        
+        List<String> allFiles = Table.open(table).getAllSSTablesOnDisk();        
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        try
+        {
+            dos.writeInt(allFiles.size());
+            for ( String file : allFiles )
+            {
+                dos.writeUTF(file);
+            }
+            Message response = message.getReply( StorageService.getLocalStorageEndPoint(), new Object[]{bos.toByteArray()});
+            MessagingService.getMessagingInstance().sendOneWay(response, message.getFrom());
+        }
+        catch ( IOException ex )
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/EfficientBidiMap.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/EfficientBidiMap.java
index e69de29b..5b6bf2be 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/EfficientBidiMap.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/EfficientBidiMap.java
@@ -0,0 +1,107 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.Serializable;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.Map;
+import java.util.SortedSet;
+import java.util.Comparator;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class EfficientBidiMap implements Serializable
+{
+    private NonBlockingHashMap<String, IColumn> map_;
+    private ConcurrentSkipListSet<IColumn> sortedSet_;
+    private Comparator<IColumn> columnComparator_;
+
+    EfficientBidiMap(Comparator<IColumn> columnComparator)
+    {
+        this(new NonBlockingHashMap<String, IColumn>(), new ConcurrentSkipListSet<IColumn>(columnComparator), columnComparator);
+    }
+
+    private EfficientBidiMap(NonBlockingHashMap<String, IColumn> map, ConcurrentSkipListSet<IColumn> set, Comparator<IColumn> comparator)
+    {
+    	map_ = map;
+    	sortedSet_ = set;
+    	columnComparator_ = comparator;
+    }
+
+    public Comparator<IColumn> getComparator()
+    {
+    	return columnComparator_;
+    }
+
+    public void put(String key, IColumn column)
+    {
+        IColumn oldColumn = map_.put(key, column);
+        if (oldColumn != null)
+            sortedSet_.remove(oldColumn);
+        sortedSet_.add(column);
+    }
+
+    public IColumn get(String key)
+    {
+        return map_.get(key);
+    }
+
+    public SortedSet<IColumn> getSortedColumns()
+    {
+    	return sortedSet_;
+    }
+
+    public Map<String, IColumn> getColumns()
+    {
+        return map_;
+    }
+
+    public int size()
+    {
+    	return map_.size();
+    }
+
+    public void remove (String columnName)
+    {
+    	sortedSet_.remove(map_.get(columnName));
+    	map_.remove(columnName);
+    }
+    void clear()
+    {
+    	map_.clear();
+    	sortedSet_.clear();
+    }
+
+    ColumnComparatorFactory.ComparatorType getComparatorType()
+	{
+		return ((AbstractColumnComparator)columnComparator_).getComparatorType();
+	}
+
+    EfficientBidiMap cloneMe()
+    {
+    	return new EfficientBidiMap((NonBlockingHashMap<String, IColumn>) map_.clone(), sortedSet_.clone(), columnComparator_);
+    }
+}
+
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileNameComparator.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileNameComparator.java
index e69de29b..23dde11f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileNameComparator.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileNameComparator.java
@@ -0,0 +1,32 @@
+package org.apache.cassandra.db;
+
+import java.util.Comparator;
+
+class FileNameComparator implements Comparator<String>
+{
+	// 0 - ascending , 1- descending
+	private int order_ = 1 ;
+	
+	public static final int  Ascending = 0 ;
+	public static final int  Descending = 1 ;
+	
+	FileNameComparator( int order )
+	{
+		order_ = order;
+	}
+	
+    public int compare(String f, String f2)
+    {
+    	if( order_ == 1 )
+    		return ColumnFamilyStore.getIndexFromFileName(f2) - ColumnFamilyStore.getIndexFromFileName(f);
+    	else
+    		return ColumnFamilyStore.getIndexFromFileName(f) - ColumnFamilyStore.getIndexFromFileName(f2);
+    }
+
+    public boolean equals(Object o)
+    {
+        if (!(o instanceof FileNameComparator))
+            return false;
+        return true;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileStruct.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileStruct.java
index e69de29b..201cba96 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileStruct.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileStruct.java
@@ -0,0 +1,210 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.Iterator;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.Coordinate;
+import org.apache.cassandra.dht.IPartitioner;
+
+
+public class FileStruct implements Comparable<FileStruct>, Iterator<String>
+{
+    private String key = null; // decorated!
+    private boolean exhausted = false;
+    private IFileReader reader;
+    private DataInputBuffer bufIn;
+    private DataOutputBuffer bufOut;
+    private IPartitioner partitioner;
+    private FileStructIterator iterator = new FileStructIterator();
+
+    public FileStruct(IFileReader reader, IPartitioner partitioner)
+    {
+        this.reader = reader;
+        this.partitioner = partitioner;
+        bufIn = new DataInputBuffer();
+        bufOut = new DataOutputBuffer();
+    }
+
+    public String getFileName()
+    {
+        return reader.getFileName();
+    }
+
+    public void close() throws IOException
+    {
+        reader.close();
+    }
+
+    public boolean isExhausted()
+    {
+        return exhausted;
+    }
+
+    public DataInputBuffer getBufIn()
+    {
+        return bufIn;
+    }
+
+    public String getKey()
+    {
+        return key;
+    }
+
+    public int compareTo(FileStruct f)
+    {
+        return partitioner.getDecoratedKeyComparator().compare(key, f.key);
+    }    
+
+    public void seekTo(String seekKey)
+    {
+        try
+        {
+            Coordinate range = SSTable.getCoordinates(seekKey, reader, partitioner);
+            reader.seek(range.end_);
+            long position = reader.getPositionFromBlockIndex(seekKey);
+            if (position == -1)
+            {
+                reader.seek(range.start_);
+            }
+            else
+            {
+                reader.seek(position);
+            }
+
+            while (!exhausted)
+            {
+                advance();
+                if (key.compareTo(seekKey) >= 0)
+                {
+                    break;
+                }
+            }
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException("corrupt sstable", e);
+        }
+    }
+
+    /*
+     * Read the next key from the data file, skipping block indexes.
+     * Caller must check isExhausted after each call to see if further
+     * reads are valid.
+     * Do not mix with calls to the iterator interface (next/hasnext).
+     * @deprecated -- prefer the iterator interface.
+     */
+    public void advance() throws IOException
+    {
+        if (exhausted)
+        {
+            throw new IndexOutOfBoundsException();
+        }
+
+        bufOut.reset();
+        if (reader.isEOF())
+        {
+            reader.close();
+            exhausted = true;
+            return;
+        }
+
+        long bytesread = reader.next(bufOut);
+        if (bytesread == -1)
+        {
+            reader.close();
+            exhausted = true;
+            return;
+        }
+
+        bufIn.reset(bufOut.getData(), bufOut.getLength());
+        key = bufIn.readUTF();
+        /* If the key we read is the Block Index Key then omit and read the next key. */
+        if (key.equals(SSTable.blockIndexKey_))
+        {
+            reader.close();
+            exhausted = true;
+        }
+    }
+
+    public boolean hasNext()
+    {
+        return iterator.hasNext();
+    }
+
+    /** do not mix with manual calls to advance(). */
+    public String next()
+    {
+        return iterator.next();
+    }
+
+    public void remove()
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    private class FileStructIterator
+    {
+        String saved;
+
+        private void forward()
+        {
+            try
+            {
+                advance();
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+            saved = isExhausted() ? null : key;
+        }
+
+        private void maybeInit()
+        {
+            if (key == null && !isExhausted())
+            {
+                forward();
+            }
+        }
+
+        public boolean hasNext()
+        {
+            maybeInit();
+            return saved != null;
+        }
+
+        public String next()
+        {
+            maybeInit();
+            if (saved == null)
+            {
+                throw new IndexOutOfBoundsException();
+            }
+            String key = saved;
+            forward();
+            return key;
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileStructComparator.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileStructComparator.java
index e69de29b..d7bd9aeb 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileStructComparator.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/FileStructComparator.java
@@ -0,0 +1,11 @@
+package org.apache.cassandra.db;
+
+import java.util.Comparator;
+
+class FileStructComparator implements Comparator<FileStruct>
+{
+    public int compare(FileStruct f, FileStruct f2)
+    {
+        return f.getFileName().compareTo(f2.getFileName());
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/HintedHandOffManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/HintedHandOffManager.java
index e69de29b..338c1588 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/HintedHandOffManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/HintedHandOffManager.java
@@ -0,0 +1,282 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.Collection;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.IComponentShutdown;
+import org.apache.cassandra.service.StorageService;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class HintedHandOffManager implements IComponentShutdown
+{
+    private static HintedHandOffManager instance_;
+    private static Lock lock_ = new ReentrantLock();
+    private static Logger logger_ = Logger.getLogger(HintedHandOffManager.class);
+    public static final String key_ = "HintedHandOffKey";
+    final static long intervalInMins_ = 20;
+    private ScheduledExecutorService executor_ = new DebuggableScheduledThreadPoolExecutor(1, new ThreadFactoryImpl("HINTED-HANDOFF-POOL"));
+
+
+    public static HintedHandOffManager instance()
+    {
+        if ( instance_ == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                    instance_ = new HintedHandOffManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return instance_;
+    }
+
+    class HintedHandOff implements Runnable
+    {
+        private ColumnFamilyStore columnFamilyStore_ = null;
+        private EndPoint endPoint_ = null;
+
+        HintedHandOff(ColumnFamilyStore columnFamilyStore)
+        {
+        	columnFamilyStore_ = columnFamilyStore;
+        }
+        HintedHandOff(EndPoint endPoint)
+        {
+        	endPoint_ = endPoint;
+        }
+
+        private boolean sendMessage(String endpointAddress, String key) throws Exception
+        {
+        	boolean success = false; // TODO : fix the hack we need to make sure the data is written on the other end.
+        	if(FailureDetector.instance().isAlive(new EndPoint(endpointAddress, DatabaseDescriptor.getControlPort())))
+        	{
+        		success = true;
+        	}
+        	else
+        	{
+        		return success;
+        	}
+        	Table table = Table.open(DatabaseDescriptor.getTables().get(0));
+        	Row row = null;
+        	row = table.get(key);
+        	RowMutation rm = new RowMutation(DatabaseDescriptor.getTables().get(0), row);
+			RowMutationMessage rmMsg = new RowMutationMessage(rm);
+			Message message = RowMutationMessage.makeRowMutationMessage( rmMsg );
+			EndPoint endPoint = new EndPoint(endpointAddress, DatabaseDescriptor.getStoragePort());
+			MessagingService.getMessagingInstance().sendOneWay(message, endPoint);
+			return success;
+        }
+
+        private void deleteEndPoint(String endpointAddress, String key) throws Exception
+        {
+        	RowMutation rm = new RowMutation(DatabaseDescriptor.getTables().get(0), key_);
+        	rm.delete(Table.hints_ + ":" + key + ":" + endpointAddress, System.currentTimeMillis());
+        	rm.apply();
+        }
+
+        private void deleteKey(String key) throws Exception
+        {
+        	RowMutation rm = new RowMutation(DatabaseDescriptor.getTables().get(0), key_);
+        	rm.delete(Table.hints_ + ":" + key, System.currentTimeMillis());
+        	rm.apply();
+        }
+
+        private void runHints()
+        {
+            logger_.debug("Started  hinted handoff " + columnFamilyStore_.columnFamily_);
+
+            // 1. Scan through all the keys that we need to handoff
+            // 2. For each key read the list of recepients and send
+            // 3. Delete that recepient from the key if write was successful
+            // 4. If all writes were success for a given key we can even delete the key .
+            // 5. Now force a flush
+            // 6. Do major compaction to clean up all deletes etc.
+            // 7. I guess we r done
+            Table table =  Table.open(DatabaseDescriptor.getTables().get(0));
+            ColumnFamily hintedColumnFamily = null;
+            boolean success = false;
+            boolean allsuccess = true;
+            try
+            {
+            	hintedColumnFamily = table.get(key_, Table.hints_);
+            	if(hintedColumnFamily == null)
+            	{
+                    // Force flush now
+                    columnFamilyStore_.forceFlush();
+            		return;
+            	}
+            	Collection<IColumn> keys = hintedColumnFamily.getAllColumns();
+            	if(keys != null)
+            	{
+                	for(IColumn key : keys)
+                	{
+                		// Get all the endpoints for teh key
+                		Collection<IColumn> endpoints =  key.getSubColumns();
+                		allsuccess = true;
+                		if ( endpoints != null )
+                		{
+                			for(IColumn endpoint : endpoints )
+                			{
+                				success = sendMessage(endpoint.name(), key.name());
+                				if(success)
+                				{
+                					// Delete the endpoint from the list
+                					deleteEndPoint(endpoint.name(), key.name());
+                				}
+                				else
+                				{
+                					allsuccess = false;
+                				}
+                			}
+                		}
+                		if(endpoints == null || allsuccess)
+                		{
+                			// Delete the key itself.
+                			deleteKey(key.name());
+                		}
+                	}
+            	}
+                // Force flush now
+                columnFamilyStore_.forceFlush();
+
+                // Now do a major compaction
+                columnFamilyStore_.forceCompaction(null, null, 0, null);
+            }
+            catch ( Exception ex)
+            {
+            	logger_.warn(ex.getMessage());
+            }
+            logger_.debug("Finished hinted handoff ..."+columnFamilyStore_.columnFamily_);
+        }
+
+        private void runDeliverHints(EndPoint to)
+        {
+            logger_.debug("Started  hinted handoff for endPoint " + endPoint_.getHost());
+
+            // 1. Scan through all the keys that we need to handoff
+            // 2. For each key read the list of recepients if teh endpoint matches send
+            // 3. Delete that recepient from the key if write was successful
+
+            Table table =  Table.open(DatabaseDescriptor.getTables().get(0));
+            ColumnFamily hintedColumnFamily = null;
+            boolean success = false;
+            try
+            {
+            	hintedColumnFamily = table.get(key_, Table.hints_);
+            	if(hintedColumnFamily == null)
+            		return;
+            	Collection<IColumn> keys = hintedColumnFamily.getAllColumns();
+            	if(keys != null)
+            	{
+                	for(IColumn key : keys)
+                	{
+                		// Get all the endpoints for teh key
+                		Collection<IColumn> endpoints =  key.getSubColumns();
+                		if ( endpoints != null )
+                		{
+                			for(IColumn endpoint : endpoints )
+                			{
+                				if(endpoint.name().equals(endPoint_.getHost()))
+                				{
+	                				success = sendMessage(endpoint.name(), key.name());
+	                				if(success)
+	                				{
+	                					// Delete the endpoint from the list
+	                					deleteEndPoint(endpoint.name(), key.name());
+	                				}
+                				}
+                			}
+                		}
+                		if(endpoints == null)
+                		{
+                			// Delete the key itself.
+                			deleteKey(key.name());
+                		}
+                	}
+            	}
+            }
+            catch ( Exception ex)
+            {
+            	logger_.warn(ex.getMessage());
+            }
+            logger_.debug("Finished hinted handoff for endpoint ..." + endPoint_.getHost());
+        }
+
+        public void run()
+        {
+        	if(endPoint_ == null)
+        	{
+        		runHints();
+        	}
+        	else
+        	{
+        		runDeliverHints(endPoint_);
+        	}
+
+        }
+    }
+
+    public HintedHandOffManager()
+    {
+    	StorageService.instance().registerComponentForShutdown(this);
+    }
+
+    public void submit(ColumnFamilyStore columnFamilyStore)
+    {
+    	executor_.scheduleWithFixedDelay(new HintedHandOff(columnFamilyStore), HintedHandOffManager.intervalInMins_,
+    			HintedHandOffManager.intervalInMins_, TimeUnit.MINUTES);
+    }
+
+    /*
+     * This method is used to deliver hints to a particular endpoint.
+     * When we learn that some endpoint is back up we deliver the data
+     * to him via an event driven mechanism.
+    */
+    public void deliverHints(EndPoint to)
+    {
+    	executor_.submit(new HintedHandOff(to));
+    }
+
+    public void shutdown()
+    {
+    	executor_.shutdownNow();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IColumn.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IColumn.java
index e69de29b..3378bcfd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IColumn.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IColumn.java
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.Collection;
+import java.util.Map;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IColumn
+{
+    public static short UtfPrefix_ = 2;
+    public boolean isMarkedForDelete();
+    public long getMarkedForDeleteAt();
+    public String name();
+    public int size();
+    public int serializedSize();
+    public long timestamp();
+    public long timestamp(String key);
+    public byte[] value();
+    public byte[] value(String key);
+    public Collection<IColumn> getSubColumns();
+    public IColumn getSubColumn(String columnName);
+    public void addColumn(String name, IColumn column);
+    public IColumn diff(IColumn column);
+    public int getObjectCount();
+    public byte[] digest();
+    public int getLocalDeletionTime(); // for tombstone GC, so int is sufficient granularity
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ICompactSerializer2.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ICompactSerializer2.java
index e69de29b..f7808e5f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ICompactSerializer2.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ICompactSerializer2.java
@@ -0,0 +1,65 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.utils.BloomFilter;
+
+
+/**
+ * This interface is an extension of the ICompactSerializer which allows for partial deserialization
+ * of a type.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface ICompactSerializer2<T> extends ICompactSerializer<T>
+{   
+	/**
+     * Returns an instance of an IColumn which contains only the 
+     * columns that are required. This is specified in the <i>columnNames</i>
+     * argument.
+     * 
+     * @param dis DataInput from which we need to deserialize.
+     * @throws IOException
+     * @return type which contains the specified items.
+	*/
+	public T deserialize(DataInputStream dis, IFilter filter) throws IOException;
+    
+    /**
+     * This method is used to deserialize just the specified field from 
+     * the serialized stream.
+     * 
+     * @param dis DataInput from which we need to deserialize.
+     * @param name name of the desired field.
+     * @throws IOException
+     * @return the deserialized type.
+    */
+	public T deserialize(DataInputStream dis, String name, IFilter filter) throws IOException;
+    
+    /**
+     * 
+     * @param dis
+     * @throws IOException
+     */
+    public void skip(DataInputStream dis) throws IOException;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IFilter.java
index e69de29b..fb7ab522 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IFilter.java
@@ -0,0 +1,16 @@
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.SSTable;
+
+
+public interface IFilter
+{
+	public boolean isDone();
+	public ColumnFamily filter(String cfName, ColumnFamily cf);
+    public IColumn filter(IColumn column, DataInputStream dis) throws IOException;
+    public DataInputBuffer next(String key, String cf, SSTable ssTable) throws IOException;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IScanner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IScanner.java
index e69de29b..bbb79474 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IScanner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IScanner.java
@@ -0,0 +1,11 @@
+package org.apache.cassandra.db;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+public interface IScanner<T> extends Closeable
+{
+    public boolean hasNext() throws IOException;
+    public T next() throws IOException;
+    public void fetch(String key, String cf) throws IOException;    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IdentityFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IdentityFilter.java
index e69de29b..968c2d06 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IdentityFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/IdentityFilter.java
@@ -0,0 +1,72 @@
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.continuations.Suspendable;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.SSTable;
+
+
+public class IdentityFilter implements IFilter
+{
+    private boolean isDone_ = false;
+    
+	public boolean isDone()
+	{
+		return isDone_;
+	}
+
+	public ColumnFamily filter(String cfString, ColumnFamily columnFamily)
+	{
+    	String[] values = RowMutation.getColumnAndColumnFamily(cfString);
+    	if( columnFamily == null )
+    		return columnFamily;
+
+		if (values.length == 2 && !columnFamily.isSuper())
+		{
+			Collection<IColumn> columns = columnFamily.getAllColumns();
+			if(columns.size() >= 1)
+				isDone_ = true;
+		}
+		if (values.length == 3 && columnFamily.isSuper())
+		{
+    		Collection<IColumn> columns = columnFamily.getAllColumns();
+    		for(IColumn column : columns)
+    		{
+    			SuperColumn superColumn = (SuperColumn)column;
+        		Collection<IColumn> subColumns = superColumn.getSubColumns();
+        		if( subColumns.size() >= 1 )
+        			isDone_ = true;
+    		}
+		}
+		return columnFamily;
+	}
+
+	public IColumn filter(IColumn column, DataInputStream dis) throws IOException
+	{
+		// TODO Auto-generated method stub
+		return column;
+	}
+
+	public DataInputBuffer next(String key, String cf, SSTable ssTable) throws IOException
+	{
+		return ssTable.next(key, cf);
+	}
+
+	public void setDone()
+	{
+		isDone_ = true;
+	}
+	/**
+	 * @param args
+	 */
+	public static void main(String[] args)
+	{
+		// TODO Auto-generated method stub
+
+	}
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/LoadVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/LoadVerbHandler.java
index e69de29b..3943c395 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/LoadVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/LoadVerbHandler.java
@@ -0,0 +1,67 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class LoadVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(LoadVerbHandler.class);    
+    
+    public void doVerb(Message message)
+    { 
+        try
+        {
+	        Object[] body = message.getMessageBody();
+	        RowMutationMessage rmMsg = (RowMutationMessage)body[0];
+	        RowMutation rm = rmMsg.getRowMutation();
+	
+			EndPoint[] endpoints = StorageService.instance().getNStorageEndPoint(rm.key());
+	
+			Message messageInternal = new Message(StorageService.getLocalStorageEndPoint(), 
+	                StorageService.mutationStage_,
+					StorageService.mutationVerbHandler_, 
+	                new Object[]{ rmMsg }
+	        );
+            
+            StringBuilder sb = new StringBuilder();
+			for(EndPoint endPoint : endpoints)
+			{                
+                sb.append(endPoint);
+				MessagingService.getMessagingInstance().sendOneWay(messageInternal, endPoint);
+			}
+            logger_.debug("Sent data to " + sb.toString());            
+        }        
+        catch ( Exception e )
+        {
+            logger_.debug(LogUtil.throwableToString(e));            
+        }        
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
index e69de29b..c4f49bd3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
@@ -0,0 +1,461 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.PriorityQueue;
+import java.util.Arrays;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.FutureTask;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.service.StorageService;
+import org.cliffc.high_scale_lib.NonBlockingHashSet;
+import org.apache.cassandra.utils.DestructivePQIterator;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Memtable implements Comparable<Memtable>
+{
+	private static Logger logger_ = Logger.getLogger( Memtable.class );
+    private static Set<ExecutorService> runningExecutorServices_ = new NonBlockingHashSet<ExecutorService>();
+    public static final String flushKey_ = "FlushKey";
+
+    public static void shutdown()
+    {
+        for (ExecutorService exs : runningExecutorServices_)
+        {
+            exs.shutdownNow();
+        }
+    }
+
+    private MemtableThreadPoolExecutor executor_;
+
+    private int threshold_ = DatabaseDescriptor.getMemtableSize()*1024*1024;
+    private int thresholdCount_ = DatabaseDescriptor.getMemtableObjectCount()*1024*1024;
+    private AtomicInteger currentSize_ = new AtomicInteger(0);
+    private AtomicInteger currentObjectCount_ = new AtomicInteger(0);
+
+    /* Table and ColumnFamily name are used to determine the ColumnFamilyStore */
+    private String table_;
+    private String cfName_;
+    /* Creation time of this Memtable */
+    private long creationTime_;
+    private volatile boolean isFrozen_ = false;
+    private Map<String, ColumnFamily> columnFamilies_ = new HashMap<String, ColumnFamily>();
+    /* Lock and Condition for notifying new clients about Memtable switches */
+    Lock lock_ = new ReentrantLock();
+
+    Memtable(String table, String cfName)
+    {
+        executor_ = new MemtableThreadPoolExecutor();
+        runningExecutorServices_.add(executor_);
+
+        table_ = table;
+        cfName_ = cfName;
+        creationTime_ = System.currentTimeMillis();
+    }
+
+    class Putter implements Runnable
+    {
+        private String key_;
+        private ColumnFamily columnFamily_;
+
+        Putter(String key, ColumnFamily cf)
+        {
+            key_ = key;
+            columnFamily_ = cf;
+        }
+
+        public void run()
+        {
+        	resolve(key_, columnFamily_);
+        }
+    }
+
+    class Getter implements Callable<ColumnFamily>
+    {
+        private String key_;
+        private String columnFamilyName_;
+        private IFilter filter_;
+
+        Getter(String key, String cfName)
+        {
+            key_ = key;
+            columnFamilyName_ = cfName;
+        }
+
+        Getter(String key, String cfName, IFilter filter)
+        {
+            this(key, cfName);
+            filter_ = filter;
+        }
+
+        public ColumnFamily call()
+        {
+        	ColumnFamily cf = getLocalCopy(key_, columnFamilyName_, filter_);
+            return cf;
+        }
+    }
+
+    /**
+     * Compares two Memtable based on creation time.
+     * @param rhs Memtable to compare to.
+     * @return a negative integer, zero, or a positive integer as this object
+     * is less than, equal to, or greater than the specified object.
+     */
+    public int compareTo(Memtable rhs)
+    {
+    	long diff = creationTime_ - rhs.creationTime_;
+    	if ( diff > 0 )
+    		return 1;
+    	else if ( diff < 0 )
+    		return -1;
+    	else
+    		return 0;
+    }
+
+    public int getCurrentSize()
+    {
+        return currentSize_.get();
+    }
+    
+    public int getCurrentObjectCount()
+    {
+        return currentObjectCount_.get();
+    }
+
+    void resolveSize(int oldSize, int newSize)
+    {
+        currentSize_.addAndGet(newSize - oldSize);
+    }
+
+    void resolveCount(int oldCount, int newCount)
+    {
+        currentObjectCount_.addAndGet(newCount - oldCount);
+    }
+
+    boolean isThresholdViolated(String key)
+    {
+    	boolean bVal = false;//isLifetimeViolated();
+        if (currentSize_.get() >= threshold_ ||  currentObjectCount_.get() >= thresholdCount_ || bVal || key.equals(flushKey_))
+        {
+        	if ( bVal )
+        		logger_.info("Memtable's lifetime for " + cfName_ + " has been violated.");
+        	return true;
+        }
+        return false;
+    }
+
+    String getColumnFamily()
+    {
+    	return cfName_;
+    }
+
+    void printExecutorStats()
+    {
+    	long taskCount = (executor_.getTaskCount() - executor_.getCompletedTaskCount());
+    	logger_.debug("MEMTABLE TASKS : " + taskCount);
+    }
+
+    /*
+     * This version is used by the external clients to put data into
+     * the memtable. This version will respect the threshold and flush
+     * the memtable to disk when the size exceeds the threshold.
+    */
+    void put(String key, ColumnFamily columnFamily, final CommitLog.CommitLogContext cLogCtx) throws IOException
+    {
+        if (isThresholdViolated(key) )
+        {
+            lock_.lock();
+            try
+            {
+                final ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
+                if (!isFrozen_)
+                {
+                    isFrozen_ = true;
+                    cfStore.switchMemtable(key, columnFamily, cLogCtx);
+                    executor_.flushWhenTerminated(cLogCtx);
+                    executor_.shutdown();
+                }
+                else
+                {
+                    // retry the put on the new memtable
+                    cfStore.apply(key, columnFamily, cLogCtx);
+                }
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        else
+        {
+        	printExecutorStats();
+        	Runnable putter = new Putter(key, columnFamily);
+        	executor_.submit(putter);
+        }
+    }
+
+    /*
+     * This version is used to switch memtable and force flush.
+     * Flushing is still done in a separate executor -- forceFlush only blocks
+     * until the flush runnable is queued.
+    */
+    public void forceflush(ColumnFamilyStore cfStore) throws IOException
+    {
+        RowMutation rm = new RowMutation(DatabaseDescriptor.getTables().get(0), flushKey_);
+
+        try
+        {
+            if (cfStore.isSuper())
+            {
+                rm.add(cfStore.getColumnFamilyName() + ":SC1:Column", "0".getBytes(), 0);
+            } else {
+                rm.add(cfStore.getColumnFamilyName() + ":Column", "0".getBytes(), 0);
+            }
+            rm.apply();
+            executor_.flushQueuer.get();
+        }
+        catch (Exception ex)
+        {
+            throw new RuntimeException(ex);
+        }
+    }
+
+    void flushOnRecovery() throws IOException {
+        flush(CommitLog.CommitLogContext.NULL);
+    }
+
+    private void resolve(String key, ColumnFamily columnFamily)
+    {
+    	ColumnFamily oldCf = columnFamilies_.get(key);
+        if ( oldCf != null )
+        {
+            int oldSize = oldCf.size();
+            int oldObjectCount = oldCf.getColumnCount();
+            oldCf.addColumns(columnFamily);
+            int newSize = oldCf.size();
+            int newObjectCount = oldCf.getColumnCount();
+            resolveSize(oldSize, newSize);
+            resolveCount(oldObjectCount, newObjectCount);
+            oldCf.delete(Math.max(oldCf.getLocalDeletionTime(), columnFamily.getLocalDeletionTime()),
+                         Math.max(oldCf.getMarkedForDeleteAt(), columnFamily.getMarkedForDeleteAt()));
+        }
+        else
+        {
+            columnFamilies_.put(key, columnFamily);
+            currentSize_.addAndGet(columnFamily.size() + key.length());
+            currentObjectCount_.addAndGet(columnFamily.getColumnCount());
+        }
+    }
+
+    /*
+     * This version is called on commit log recovery. The threshold
+     * is not respected and a forceFlush() needs to be invoked to flush
+     * the contents to disk.
+    */
+    void putOnRecovery(String key, ColumnFamily columnFamily)
+    {
+        if(!key.equals(Memtable.flushKey_))
+        	resolve(key, columnFamily);
+    }
+
+    ColumnFamily getLocalCopy(String key, String columnFamilyColumn, IFilter filter)
+    {
+    	String[] values = RowMutation.getColumnAndColumnFamily(columnFamilyColumn);
+    	ColumnFamily columnFamily = null;
+        if(values.length == 1 )
+        {
+        	columnFamily = columnFamilies_.get(key);
+        }
+        else
+        {
+        	ColumnFamily cFamily = columnFamilies_.get(key);
+        	if (cFamily == null) return null;
+
+        	if (values.length == 2) {
+                IColumn column = cFamily.getColumn(values[1]); // super or normal column
+                if (column != null )
+                {
+                    columnFamily = cFamily.cloneMeShallow();
+                    columnFamily.addColumn(column);
+                }
+        	}
+            else
+            {
+                assert values.length == 3;
+                SuperColumn superColumn = (SuperColumn)cFamily.getColumn(values[1]);
+                if (superColumn != null)
+                {
+                    IColumn subColumn = superColumn.getSubColumn(values[2]);
+                    if (subColumn != null)
+                    {
+                        columnFamily = cFamily.cloneMeShallow();
+                        SuperColumn container = new SuperColumn(superColumn.name());
+                        container.markForDeleteAt(superColumn.getLocalDeletionTime(), superColumn.getMarkedForDeleteAt());
+                        container.addColumn(subColumn.name(), subColumn);
+                        columnFamily.addColumn(container);
+                    }
+                }
+        	}
+        }
+        /* Filter unnecessary data from the column based on the provided filter */
+        return filter.filter(columnFamilyColumn, columnFamily);
+    }
+
+    ColumnFamily get(String key, String cfName, IFilter filter)
+    {
+    	printExecutorStats();
+    	Callable<ColumnFamily> call = new Getter(key, cfName, filter);
+    	ColumnFamily cf = null;
+    	try
+    	{
+    		cf = executor_.submit(call).get();
+    	}
+    	catch ( ExecutionException ex )
+    	{
+    		logger_.debug(LogUtil.throwableToString(ex));
+    	}
+    	catch ( InterruptedException ex2 )
+    	{
+    		logger_.debug(LogUtil.throwableToString(ex2));
+    	}
+    	return cf;
+    }
+
+    void flush(CommitLog.CommitLogContext cLogCtx) throws IOException
+    {
+        ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
+        if ( columnFamilies_.size() == 0 )
+        {
+        	// This should be called even if size is 0
+        	// This is because we should try to delete the useless commitlogs
+        	// even though there is nothing to flush in memtables for a given family like Hints etc.
+            cfStore.onMemtableFlush(cLogCtx);
+            return;
+        }
+
+        String directory = DatabaseDescriptor.getDataFileLocation();
+        String filename = cfStore.getNextFileName();
+        SSTable ssTable = new SSTable(directory, filename, StorageService.getPartitioner());
+
+        // sort keys in the order they would be in when decorated
+        final IPartitioner partitioner = StorageService.getPartitioner();
+        final Comparator<String> dc = partitioner.getDecoratedKeyComparator();
+        ArrayList<String> orderedKeys = new ArrayList<String>(columnFamilies_.keySet());
+        Collections.sort(orderedKeys, new Comparator<String>()
+        {
+            public int compare(String o1, String o2)
+            {
+                return dc.compare(partitioner.decorateKey(o1), partitioner.decorateKey(o2));
+            }
+        });
+        DataOutputBuffer buffer = new DataOutputBuffer();
+        /* Use this BloomFilter to decide if a key exists in a SSTable */
+        BloomFilter bf = new BloomFilter(columnFamilies_.size(), 15);
+        for (String key : orderedKeys)
+        {
+            buffer.reset();
+            ColumnFamily columnFamily = columnFamilies_.get(key);
+            if ( columnFamily != null )
+            {
+                /* serialize the cf with column indexes */
+                ColumnFamily.serializerWithIndexes().serialize( columnFamily, buffer );
+                /* Now write the key and value to disk */
+                ssTable.append(partitioner.decorateKey(key), buffer);
+                bf.add(key);
+            }
+        }
+        ssTable.close(bf);
+        cfStore.onMemtableFlush(cLogCtx);
+        cfStore.storeLocation( ssTable.getDataFileLocation(), bf );
+        buffer.close();
+    }
+
+    private class MemtableThreadPoolExecutor extends DebuggableThreadPoolExecutor
+    {
+        FutureTask flushQueuer;
+
+        public MemtableThreadPoolExecutor()
+        {
+            super(1, 1, Integer.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(), new ThreadFactoryImpl("FAST-MEMTABLE-POOL"));
+        }
+
+        protected void terminated()
+        {
+            super.terminated();
+            runningExecutorServices_.remove(this);
+            if (flushQueuer != null)
+            {
+                flushQueuer.run();
+            }
+        }
+
+        public void flushWhenTerminated(final CommitLog.CommitLogContext cLogCtx)
+        {
+            Runnable runnable = new Runnable()
+            {
+                public void run()
+                {
+                    MemtableManager.instance().submit(cfName_, Memtable.this, cLogCtx);
+                }
+            };
+            flushQueuer = new FutureTask(runnable, null);
+        }
+    }
+
+    public Iterator<String> sortedKeyIterator()
+    {
+        Set<String> keys = columnFamilies_.keySet();
+        if (keys.size() == 0)
+        {
+            // cannot create a PQ of size zero (wtf?)
+            return Arrays.asList(new String[0]).iterator();
+        }
+        PriorityQueue<String> pq = new PriorityQueue<String>(keys.size(), StorageService.getPartitioner().getDecoratedKeyComparator());
+        pq.addAll(keys);
+        return new DestructivePQIterator<String>(pq);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/MemtableManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/MemtableManager.java
index e69de29b..179e757d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/MemtableManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/MemtableManager.java
@@ -0,0 +1,178 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.*;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MemtableManager
+{
+    private static MemtableManager instance_;
+    private static Lock lock_ = new ReentrantLock();
+    private static Logger logger_ = Logger.getLogger(MemtableManager.class);
+    private ReentrantReadWriteLock rwLock_ = new ReentrantReadWriteLock(true);
+    public static MemtableManager instance()
+    {
+        if ( instance_ == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                    instance_ = new MemtableManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return instance_;
+    }
+    
+    class MemtableFlusher implements Runnable
+    {
+        private Memtable memtable_;
+        private CommitLog.CommitLogContext cLogCtx_;
+        
+        MemtableFlusher(Memtable memtable, CommitLog.CommitLogContext cLogCtx)
+        {
+            memtable_ = memtable;
+            cLogCtx_ = cLogCtx;
+        }
+        
+        public void run()
+        {
+            try
+            {
+            	memtable_.flush(cLogCtx_);
+            }
+            catch (IOException e)
+            {
+                logger_.debug( LogUtil.throwableToString(e) );
+            }
+        	rwLock_.writeLock().lock();
+            try
+            {
+            	List<Memtable> memtables = history_.get(memtable_.getColumnFamily());
+                memtables.remove(memtable_);                	
+            }
+        	finally
+        	{
+            	rwLock_.writeLock().unlock();
+        	}
+        }
+    }
+    
+    private Map<String, List<Memtable>> history_ = new HashMap<String, List<Memtable>>();
+    ExecutorService flusher_ = new DebuggableThreadPoolExecutor( 1,
+            1,
+            Integer.MAX_VALUE,
+            TimeUnit.SECONDS,
+            new LinkedBlockingQueue<Runnable>(),
+            new ThreadFactoryImpl("MEMTABLE-FLUSHER-POOL")
+            );  
+    
+    /* Submit memtables to be flushed to disk */
+    void submit(String cfName, Memtable memtbl, CommitLog.CommitLogContext cLogCtx)
+    {
+    	rwLock_.writeLock().lock();
+    	try
+    	{
+	        List<Memtable> memtables = history_.get(cfName);
+	        if ( memtables == null )
+	        {
+	            memtables = new ArrayList<Memtable>();
+	            history_.put(cfName, memtables);
+	        }
+	        memtables.add(memtbl);	        
+	        flusher_.submit( new MemtableFlusher(memtbl, cLogCtx) );
+    	}
+    	finally
+    	{
+        	rwLock_.writeLock().unlock();
+    	}
+    }
+    
+
+    /*
+     * Retrieve column family from the list of Memtables that have been
+     * submitted for flush but have not yet been flushed.
+     * It also filters out unneccesary columns based on the passed in filter.
+    */
+    void getColumnFamily(String key, String cfName, String cf, IFilter filter, List<ColumnFamily> columnFamilies)
+    {
+    	rwLock_.readLock().lock();
+    	try
+    	{
+	        /* Get all memtables associated with this column family */
+	        List<Memtable> memtables = history_.get(cfName);
+	        if ( memtables != null )
+	        {
+		        Collections.sort(memtables);
+	        	int size = memtables.size();
+	            for ( int i = size - 1; i >= 0; --i  )
+	            {
+	                ColumnFamily columnFamily = memtables.get(i).getLocalCopy(key, cf, filter);
+	                if ( columnFamily != null )
+	                {
+	                    columnFamilies.add(columnFamily);
+	                    if( filter.isDone())
+	                    	break;
+	                }
+	            }
+	        }        
+    	}
+    	finally
+    	{
+        	rwLock_.readLock().unlock();
+    	}
+    }
+
+    public List<Memtable> getUnflushedMemtables(String cfName)
+    {
+        rwLock_.readLock().lock();
+        try
+        {
+            List<Memtable> memtables = history_.get(cfName);
+            if (memtables != null)
+            {
+                return new ArrayList<Memtable>(memtables);
+            }
+            return Arrays.asList(new Memtable[0]);
+        }
+        finally
+        {
+            rwLock_.readLock().unlock();
+        }
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/MinorCompactionManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/MinorCompactionManager.java
index e69de29b..ae31a25e 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/MinorCompactionManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/MinorCompactionManager.java
@@ -0,0 +1,196 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.db.HintedHandOffManager.HintedHandOff;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.IComponentShutdown;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.concurrent.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class MinorCompactionManager implements IComponentShutdown
+{
+    private static MinorCompactionManager instance_;
+    private static Lock lock_ = new ReentrantLock();
+    private static Logger logger_ = Logger.getLogger(MinorCompactionManager.class);
+    final static long intervalInMins_ = 5;
+
+    public static MinorCompactionManager instance()
+    {
+        if ( instance_ == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                    instance_ = new MinorCompactionManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return instance_;
+    }
+
+    class FileCompactor implements Runnable
+    {
+        private ColumnFamilyStore columnFamilyStore_;
+
+        FileCompactor(ColumnFamilyStore columnFamilyStore)
+        {
+        	columnFamilyStore_ = columnFamilyStore;
+        }
+
+        public void run()
+        {
+                logger_.debug("Started  compaction ..."+columnFamilyStore_.columnFamily_);
+            try
+            {
+                columnFamilyStore_.doCompaction();
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+            logger_.debug("Finished compaction ..."+columnFamilyStore_.columnFamily_);
+        }
+    }
+
+    class FileCompactor2 implements Callable<Boolean>
+    {
+        private ColumnFamilyStore columnFamilyStore_;
+        private List<Range> ranges_;
+        private EndPoint target_;
+        private List<String> fileList_;
+
+        FileCompactor2(ColumnFamilyStore columnFamilyStore, List<Range> ranges, EndPoint target,List<String> fileList)
+        {
+            columnFamilyStore_ = columnFamilyStore;
+            ranges_ = ranges;
+            target_ = target;
+            fileList_ = fileList;
+        }
+
+        public Boolean call()
+        {
+        	boolean result;
+            logger_.debug("Started  compaction ..."+columnFamilyStore_.columnFamily_);
+            result = columnFamilyStore_.doAntiCompaction(ranges_, target_,fileList_);
+            logger_.debug("Finished compaction ..."+columnFamilyStore_.columnFamily_);
+            return result;
+        }
+    }
+
+    class OnDemandCompactor implements Runnable
+    {
+        private ColumnFamilyStore columnFamilyStore_;
+        private long skip_ = 0L;
+
+        OnDemandCompactor(ColumnFamilyStore columnFamilyStore, long skip)
+        {
+            columnFamilyStore_ = columnFamilyStore;
+            skip_ = skip;
+        }
+
+        public void run()
+        {
+            logger_.debug("Started  Major compaction ..."+columnFamilyStore_.columnFamily_);
+            columnFamilyStore_.doMajorCompaction(skip_);
+            logger_.debug("Finished Major compaction ..."+columnFamilyStore_.columnFamily_);
+        }
+    }
+
+    class CleanupCompactor implements Runnable
+    {
+        private ColumnFamilyStore columnFamilyStore_;
+
+        CleanupCompactor(ColumnFamilyStore columnFamilyStore)
+        {
+        	columnFamilyStore_ = columnFamilyStore;
+        }
+
+        public void run()
+        {
+            logger_.debug("Started  compaction ..."+columnFamilyStore_.columnFamily_);
+            columnFamilyStore_.doCleanupCompaction();
+            logger_.debug("Finished compaction ..."+columnFamilyStore_.columnFamily_);
+        }
+    }
+    
+    
+    private ScheduledExecutorService compactor_ = new DebuggableScheduledThreadPoolExecutor(1, new ThreadFactoryImpl("MINOR-COMPACTION-POOL"));
+
+    public MinorCompactionManager()
+    {
+    	StorageService.instance().registerComponentForShutdown(this);
+	}
+
+    public void shutdown()
+    {
+    	compactor_.shutdownNow();
+    }
+
+    public void submitPeriodicCompaction(ColumnFamilyStore columnFamilyStore)
+    {        
+    	compactor_.scheduleWithFixedDelay(new FileCompactor(columnFamilyStore), MinorCompactionManager.intervalInMins_,
+    			MinorCompactionManager.intervalInMins_, TimeUnit.MINUTES);       
+    }
+
+    public Future submit(ColumnFamilyStore columnFamilyStore)
+    {
+        return compactor_.submit(new FileCompactor(columnFamilyStore));
+    }
+    
+    public void submitCleanup(ColumnFamilyStore columnFamilyStore)
+    {
+        compactor_.submit(new CleanupCompactor(columnFamilyStore));
+    }
+
+    public Future<Boolean> submit(ColumnFamilyStore columnFamilyStore, List<Range> ranges, EndPoint target, List<String> fileList)
+    {
+        return compactor_.submit( new FileCompactor2(columnFamilyStore, ranges, target, fileList) );
+    }
+
+    public void  submitMajor(ColumnFamilyStore columnFamilyStore, long skip)
+    {
+        compactor_.submit( new OnDemandCompactor(columnFamilyStore, skip) );
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/NamesFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/NamesFilter.java
index e69de29b..887ab6d4 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/NamesFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/NamesFilter.java
@@ -0,0 +1,121 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.SSTable;
+
+
+
+public class NamesFilter implements IFilter
+{
+    /* list of column names to filter against. */
+    private List<String> names_;
+
+    NamesFilter(List<String> names)
+    {
+        names_ = new ArrayList<String>(names);
+    }
+
+    public ColumnFamily filter(String cf, ColumnFamily columnFamily)
+    {
+        if ( columnFamily == null )
+        {
+            return columnFamily;
+        }
+    	String[] values = RowMutation.getColumnAndColumnFamily(cf);
+        ColumnFamily filteredCf = new ColumnFamily(columnFamily.name(), columnFamily.type());
+		if( values.length == 1 )
+		{
+			Collection<IColumn> columns = columnFamily.getAllColumns();
+			for(IColumn column : columns)
+			{
+		        if ( names_.contains(column.name()) )
+		        {
+		            names_.remove(column.name());
+					filteredCf.addColumn(column);
+		        }
+				if( isDone() )
+				{
+					return filteredCf;
+				}
+			}
+		}
+		else if (values.length == 2 && columnFamily.isSuper())
+		{
+    		Collection<IColumn> columns = columnFamily.getAllColumns();
+    		for(IColumn column : columns)
+    		{
+    			SuperColumn superColumn = (SuperColumn)column;
+    			SuperColumn filteredSuperColumn = new SuperColumn(superColumn.name());
+				filteredCf.addColumn(filteredSuperColumn);
+        		Collection<IColumn> subColumns = superColumn.getSubColumns();
+        		for(IColumn subColumn : subColumns)
+        		{
+    		        if ( names_.contains(subColumn.name()) )
+    		        {
+    		            names_.remove(subColumn.name());
+    		            filteredSuperColumn.addColumn(subColumn.name(), subColumn);
+    		        }
+    				if( isDone() )
+    				{
+    					return filteredCf;
+    				}
+    			}
+    		}
+		}
+    	else
+    	{
+    		throw new UnsupportedOperationException();
+    	}
+		return filteredCf;
+    }
+
+    public IColumn filter(IColumn column, DataInputStream dis) throws IOException
+    {
+        String columnName = column.name();
+        if ( names_.contains(columnName) )
+        {
+            names_.remove(columnName);
+        }
+        else
+        {
+            column = null;
+        }
+
+        return column;
+    }
+
+    public boolean isDone()
+    {
+        return names_.isEmpty();
+    }
+
+    public DataInputBuffer next(String key, String cf, SSTable ssTable) throws IOException
+    {
+    	return ssTable.next(key, cf, names_, null);
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeCommand.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeCommand.java
index e69de29b..3609d855 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeCommand.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeCommand.java
@@ -0,0 +1,64 @@
+package org.apache.cassandra.db;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.DataInputStream;
+import java.util.Arrays;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+
+public class RangeCommand
+{
+    private static RangeCommandSerializer serializer = new RangeCommandSerializer();
+
+    public final String table;
+    public final String startWith;
+    public final String stopAt;
+    public final int maxResults;
+
+    public RangeCommand(String table, String startWith, String stopAt, int maxResults)
+    {
+        this.table = table;
+        this.startWith = startWith;
+        this.stopAt = stopAt;
+        this.maxResults = maxResults;
+    }
+
+    public Message getMessage() throws IOException
+    {
+        DataOutputBuffer dob = new DataOutputBuffer();
+        serializer.serialize(this, dob);
+        return new Message(StorageService.getLocalStorageEndPoint(),
+                           StorageService.readStage_,
+                           StorageService.rangeVerbHandler_,
+                           Arrays.copyOf(dob.getData(), dob.getLength()));
+    }
+
+    public static RangeCommand read(Message message) throws IOException
+    {
+        byte[] bytes = (byte[]) message.getMessageBody()[0];
+        DataInputBuffer dib = new DataInputBuffer();
+        dib.reset(bytes, bytes.length);
+        return serializer.deserialize(new DataInputStream(dib));
+    }
+}
+
+class RangeCommandSerializer implements ICompactSerializer<RangeCommand>
+{
+    public void serialize(RangeCommand command, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(command.table);
+        dos.writeUTF(command.startWith);
+        dos.writeUTF(command.stopAt);
+        dos.writeInt(command.maxResults);
+    }
+
+    public RangeCommand deserialize(DataInputStream dis) throws IOException
+    {
+        return new RangeCommand(dis.readUTF(), dis.readUTF(), dis.readUTF(), dis.readInt());
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeFilter.java
index e69de29b..43fa8d29 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeFilter.java
@@ -0,0 +1,118 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.SSTable;
+
+/**
+ * Filters columns to satisfy colmin <= colname <= colmax
+ *
+ */
+public class RangeFilter implements IFilter
+{
+    private final String colMin_;
+    private final String colMax_;
+    private boolean isDone_;
+    int count_;
+
+    RangeFilter(String colMin, String colMax)
+    {
+        colMin_ = colMin;
+        colMax_ = colMax;
+        isDone_ = false;
+        count_ = -1;
+    }
+    
+    RangeFilter(String colMin, String colMax, int count)
+    {
+        colMin_ = colMin;
+        colMax_ = colMax;
+        isDone_ = false;
+        count_ = count;
+    }
+
+    public ColumnFamily filter(String cfName, ColumnFamily cf)
+    {
+        if (cf == null)
+            return null;
+
+        if (count_ == 0)
+        {
+            isDone_ = true;
+            return null;
+        }
+
+        ColumnFamily filteredColumnFamily = new ColumnFamily(cfName, cf.type());
+
+        Collection<IColumn> columns = cf.getAllColumns();
+        for (IColumn c : columns)
+        {
+            if (c.name().compareTo(colMin_) >= 0
+                    && c.name().compareTo(colMax_) <= 0)
+            {
+                filteredColumnFamily.addColumn(c);
+                if (count_ > 0)
+                    count_--;
+                if (count_==0)
+                {
+                    isDone_ = true;
+                    break;
+                }
+            }
+        }
+        return filteredColumnFamily;
+    }
+
+    public IColumn filter(IColumn column, DataInputStream dis)
+            throws IOException
+    {
+        if (column == null || isDone_)
+            return null;
+
+        if (column.name().compareTo(colMin_) >= 0
+                && column.name().compareTo(colMax_) <= 0)
+        {
+            if (count_ > 0)
+                count_--;
+            if (count_ == 0)
+                isDone_ = true;
+            return column;
+        } else
+        {
+            return null;
+        }
+    }
+
+    public boolean isDone()
+    {
+        return isDone_;
+    }
+
+    public DataInputBuffer next(String key, String cf, SSTable ssTable)
+            throws IOException
+    {
+        return ssTable.next(key, cf);
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeReply.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeReply.java
index e69de29b..7237a68c 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeReply.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RangeReply.java
@@ -0,0 +1,55 @@
+package org.apache.cassandra.db;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Arrays;
+import java.io.IOException;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+
+public class RangeReply
+{
+    public final List<String> keys;
+
+    public RangeReply(List<String> keys)
+    {
+        this.keys = Collections.unmodifiableList(keys);
+    }
+
+    public Message getReply(Message originalMessage)
+    {
+        DataOutputBuffer dob = new DataOutputBuffer();
+        for (String key : keys)
+        {
+            try
+            {
+                dob.writeUTF(key);
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+        byte[] data = Arrays.copyOf(dob.getData(), dob.getLength());
+        return originalMessage.getReply(StorageService.getLocalStorageEndPoint(), data);
+    }
+
+    public static RangeReply read(byte[] body) throws IOException
+    {
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(body, body.length);
+
+        List<String> keys = new ArrayList<String>();
+        while (bufIn.getPosition() < body.length)
+        {
+            keys.add(bufIn.readUTF());
+        }
+
+        return new RangeReply(keys);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadCommand.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadCommand.java
index e69de29b..3d49eadd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadCommand.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadCommand.java
@@ -0,0 +1,113 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+
+
+public abstract class ReadCommand
+{
+    public static final String DO_REPAIR = "READ-REPAIR";
+    public static final byte CMD_TYPE_GET_ROW=1;
+    public static final byte CMD_TYPE_GET_COLUMN=2;
+    public static final byte CMD_TYPE_GET_SLICE_BY_NAMES=3;
+    public static final byte CMD_TYPE_GET_COLUMNS_SINCE=4;
+    public static final byte CMD_TYPE_GET_SLICE=5;
+    public static final String EMPTY_CF = "";
+    
+    private static ReadCommandSerializer serializer = new ReadCommandSerializer();
+
+    public static ReadCommandSerializer serializer()
+    {
+        return serializer;
+    }
+
+    public Message makeReadMessage() throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        ReadCommand.serializer().serialize(this, dos);
+        return new Message(StorageService.getLocalStorageEndPoint(), StorageService.readStage_, StorageService.readVerbHandler_, bos.toByteArray());
+    }
+
+    public final String table;
+    public final String key;
+    private boolean isDigestQuery = false;    
+    protected final byte commandType;
+
+    protected ReadCommand(String table, String key, byte cmdType)
+    {
+        this.table = table;
+        this.key = key;
+        this.commandType = cmdType;
+    }
+    
+    public boolean isDigestQuery()
+    {
+        return isDigestQuery;
+    }
+
+    public void setDigestQuery(boolean isDigestQuery)
+    {
+        this.isDigestQuery = isDigestQuery;
+    }
+
+    public abstract String getColumnFamilyName();
+    
+    public abstract ReadCommand copy();
+
+    public abstract Row getRow(Table table) throws IOException;
+}
+
+class ReadCommandSerializer implements ICompactSerializer<ReadCommand>
+{
+    private static final Map<Byte, ReadCommandSerializer> CMD_SERIALIZER_MAP = new HashMap<Byte, ReadCommandSerializer>(); 
+    static 
+    {
+        CMD_SERIALIZER_MAP.put(ReadCommand.CMD_TYPE_GET_ROW, new RowReadCommandSerializer());
+        CMD_SERIALIZER_MAP.put(ReadCommand.CMD_TYPE_GET_COLUMN, new ColumnReadCommandSerializer());
+        CMD_SERIALIZER_MAP.put(ReadCommand.CMD_TYPE_GET_SLICE_BY_NAMES, new SliceByNamesReadCommandSerializer());
+        CMD_SERIALIZER_MAP.put(ReadCommand.CMD_TYPE_GET_COLUMNS_SINCE, new ColumnsSinceReadCommandSerializer());
+        CMD_SERIALIZER_MAP.put(ReadCommand.CMD_TYPE_GET_SLICE, new SliceReadCommandSerializer());
+    }
+
+
+    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
+    {
+        dos.writeByte(rm.commandType);
+        ReadCommandSerializer ser = CMD_SERIALIZER_MAP.get(rm.commandType);
+        ser.serialize(rm, dos);
+    }
+
+    public ReadCommand deserialize(DataInputStream dis) throws IOException
+    {
+        byte msgType = dis.readByte();
+        return CMD_SERIALIZER_MAP.get(msgType).deserialize(dis);
+    }
+        
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadRepairVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadRepairVerbHandler.java
index e69de29b..6f96a339 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadRepairVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadRepairVerbHandler.java
@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.*;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.service.*;
+import org.apache.cassandra.utils.*;
+import org.apache.cassandra.concurrent.*;
+import org.apache.cassandra.net.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ReadRepairVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(ReadRepairVerbHandler.class);    
+    
+    public void doVerb(Message message)
+    {          
+        byte[] body = (byte[])message.getMessageBody()[0];
+        DataInputBuffer buffer = new DataInputBuffer();
+        buffer.reset(body, body.length);        
+        
+        try
+        {
+            RowMutationMessage rmMsg = RowMutationMessage.serializer().deserialize(buffer);
+            RowMutation rm = rmMsg.getRowMutation();
+            rm.apply();                                   
+        }
+        catch ( IOException e )
+        {
+            logger_.debug(LogUtil.throwableToString(e));            
+        }        
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadResponse.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadResponse.java
index e69de29b..a3f7d7f5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadResponse.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadResponse.java
@@ -0,0 +1,145 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+
+
+/*
+ * The read response message is sent by the server when reading data 
+ * this encapsulates the tablename and teh row that has been read.
+ * The table name is needed so that we can use it to create repairs.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class ReadResponse implements Serializable 
+{
+private static ICompactSerializer<ReadResponse> serializer_;
+
+    static
+    {
+        serializer_ = new ReadResponseSerializer();
+    }
+
+    public static ICompactSerializer<ReadResponse> serializer()
+    {
+        return serializer_;
+    }
+    
+	public static Message makeReadResponseMessage(ReadResponse readResponse) throws IOException
+    {
+    	ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        ReadResponse.serializer().serialize(readResponse, dos);
+        Message message = new Message(StorageService.getLocalStorageEndPoint(), MessagingService.responseStage_, MessagingService.responseVerbHandler_, new Object[]{bos.toByteArray()});         
+        return message;
+    }
+	
+	private String table_;
+	private Row row_;
+	private byte[] digest_ = new byte[0];
+    private boolean isDigestQuery_ = false;
+
+	public ReadResponse(String table, byte[] digest )
+    {
+		table_ = table;
+		digest_= digest;
+	}
+
+	public ReadResponse(String table, Row row)
+    {
+		table_ = table;
+		row_ = row;
+	}
+
+	public String table() 
+    {
+		return table_;
+	}
+
+	public Row row() 
+    {
+		return row_;
+    }
+        
+	public byte[] digest() 
+    {
+		return digest_;
+	}
+
+	public boolean isDigestQuery()
+    {
+    	return isDigestQuery_;
+    }
+    
+    public void setIsDigestQuery(boolean isDigestQuery)
+    {
+    	isDigestQuery_ = isDigestQuery;
+    }
+}
+
+class ReadResponseSerializer implements ICompactSerializer<ReadResponse>
+{
+	public void serialize(ReadResponse rm, DataOutputStream dos) throws IOException
+	{
+		dos.writeUTF(rm.table());
+        dos.writeInt(rm.digest().length);
+        dos.write(rm.digest());
+        dos.writeBoolean(rm.isDigestQuery());
+        
+        if( !rm.isDigestQuery() && rm.row() != null )
+        {            
+            Row.serializer().serialize(rm.row(), dos);
+        }				
+	}
+	
+    public ReadResponse deserialize(DataInputStream dis) throws IOException
+    {
+    	String table = dis.readUTF();
+        int digestSize = dis.readInt();
+        byte[] digest = new byte[digestSize];
+        dis.read(digest, 0 , digestSize);
+        boolean isDigest = dis.readBoolean();
+        
+        Row row = null;
+        if ( !isDigest )
+        {
+            row = Row.serializer().deserialize(dis);
+        }
+		
+		ReadResponse rmsg = null;
+    	if( isDigest  )
+        {
+    		rmsg =  new ReadResponse(table, digest);
+        }
+    	else
+        {
+    		rmsg =  new ReadResponse(table, row);
+        }
+        rmsg.setIsDigestQuery(isDigest);
+    	return rmsg;
+    } 
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadVerbHandler.java
index e69de29b..fc4e1f53 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ReadVerbHandler.java
@@ -0,0 +1,129 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ReadVerbHandler implements IVerbHandler
+{
+    protected static class ReadContext
+    {
+        protected DataInputBuffer bufIn_ = new DataInputBuffer();
+        protected DataOutputBuffer bufOut_ = new DataOutputBuffer();
+    }
+
+    private static Logger logger_ = Logger.getLogger( ReadVerbHandler.class );
+    /* We use this so that we can reuse the same row mutation context for the mutation. */
+    private static ThreadLocal<ReadVerbHandler.ReadContext> tls_ = new InheritableThreadLocal<ReadVerbHandler.ReadContext>();
+    
+    protected static ReadVerbHandler.ReadContext getCurrentReadContext()
+    {
+        return tls_.get();
+    }
+    
+    protected static void setCurrentReadContext(ReadVerbHandler.ReadContext readContext)
+    {
+        tls_.set(readContext);
+    }
+
+    public void doVerb(Message message)
+    {
+        byte[] body = (byte[])message.getMessageBody()[0];
+        /* Obtain a Read Context from TLS */
+        ReadContext readCtx = tls_.get();
+        if ( readCtx == null )
+        {
+            readCtx = new ReadContext();
+            tls_.set(readCtx);
+        }
+        readCtx.bufIn_.reset(body, body.length);
+
+        try
+        {
+            ReadCommand readCommand = ReadCommand.serializer().deserialize(readCtx.bufIn_);
+            Table table = Table.open(readCommand.table);
+            Row row = null;
+            long start = System.currentTimeMillis();
+            row = readCommand.getRow(table);
+            logger_.info("getRow()  TIME: " + (System.currentTimeMillis() - start) + " ms.");
+            start = System.currentTimeMillis();
+            ReadResponse readResponse = null;
+            if(readCommand.isDigestQuery())
+            {
+                readResponse = new ReadResponse(table.getTableName(), row.digest());
+            }
+            else
+            {
+                readResponse = new ReadResponse(table.getTableName(), row);
+            }
+            readResponse.setIsDigestQuery(readCommand.isDigestQuery());
+            /* serialize the ReadResponseMessage. */
+            readCtx.bufOut_.reset();
+
+            start = System.currentTimeMillis();
+            ReadResponse.serializer().serialize(readResponse, readCtx.bufOut_);
+            logger_.info("serialize  TIME: " + (System.currentTimeMillis() - start) + " ms.");
+
+            byte[] bytes = new byte[readCtx.bufOut_.getLength()];
+            start = System.currentTimeMillis();
+            System.arraycopy(readCtx.bufOut_.getData(), 0, bytes, 0, bytes.length);
+            logger_.info("copy  TIME: " + (System.currentTimeMillis() - start) + " ms.");
+
+            Message response = message.getReply( StorageService.getLocalStorageEndPoint(), new Object[]{bytes} );
+            MessagingService.getMessagingInstance().sendOneWay(response, message.getFrom());
+            logger_.info("ReadVerbHandler  TIME 2: " + (System.currentTimeMillis() - start) + " ms.");
+            
+            /* Do read repair if header of the message says so */
+            if (message.getHeader(ReadCommand.DO_REPAIR) != null)
+            {
+                doReadRepair(row, readCommand);
+            }
+        }
+        catch ( IOException ex)
+        {
+            logger_.info( LogUtil.throwableToString(ex) );
+        }
+    }
+    
+    private void doReadRepair(Row row, ReadCommand readCommand)
+    {
+        List<EndPoint> endpoints = StorageService.instance().getNLiveStorageEndPoint(readCommand.key);
+        /* Remove the local storage endpoint from the list. */ 
+        endpoints.remove( StorageService.getLocalStorageEndPoint() );
+            
+        if (endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
+            StorageService.instance().doConsistencyCheck(row, endpoints, readCommand);
+    }     
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RecoveryManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RecoveryManager.java
index e69de29b..513b1dcf 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RecoveryManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RecoveryManager.java
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.*;
+import java.io.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.log4j.Logger;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class RecoveryManager
+{
+    private static RecoveryManager instance_;
+    private static Logger logger_ = Logger.getLogger(RecoveryManager.class);
+    
+    synchronized static RecoveryManager instance() throws IOException
+    {
+        if ( instance_ == null )
+            instance_ = new RecoveryManager();
+        return instance_;
+    }
+
+    public static File[] getListofCommitLogs()
+    {
+        String directory = DatabaseDescriptor.getLogFileLocation();
+        File file = new File(directory);
+        File[] files = file.listFiles();
+        return files;
+    }
+    
+    public static Map<String, List<File>> getListOFCommitLogsPerTable()
+    {
+        File[] files = getListofCommitLogs();
+        /* Maintains a mapping of table name to a list of commit log files */
+        Map<String, List<File>> tableToCommitLogs = new HashMap<String, List<File>>();
+        
+        for (File f : files)
+        {
+            String table = CommitLog.getTableName(f.getName());
+            List<File> clogs = tableToCommitLogs.get(table);
+            if ( clogs == null )
+            {
+                clogs = new ArrayList<File>();
+                tableToCommitLogs.put(table, clogs);
+            }
+            clogs.add(f);
+        }
+        return tableToCommitLogs;
+    }
+    
+    public void doRecovery() throws IOException
+    {
+        File[] files = getListofCommitLogs();
+        Map<String, List<File>> tableToCommitLogs = getListOFCommitLogsPerTable();
+        recoverEachTable(tableToCommitLogs);
+        FileUtils.delete(files);
+    }
+    
+    private void recoverEachTable(Map<String, List<File>> tableToCommitLogs) throws IOException
+    {
+        Comparator<File> fCmp = new FileUtils.FileComparator();
+        Set<String> tables = tableToCommitLogs.keySet();
+        for ( String table : tables )
+        {
+            List<File> clogs = tableToCommitLogs.get(table);
+            Collections.sort(clogs, fCmp);
+            CommitLog clog = new CommitLog(table, true);
+            clog.recover(clogs);
+        }
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {
+        long start = System.currentTimeMillis();
+        RecoveryManager rm = RecoveryManager.instance();
+        rm.doRecovery();  
+        logger_.debug( "Time taken : " + (System.currentTimeMillis() - start) + " ms.");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Row.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Row.java
index e69de29b..b84dee9f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Row.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Row.java
@@ -0,0 +1,236 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Hashtable;
+import java.util.Map;
+import java.util.Set;
+import java.util.Arrays;
+
+import org.apache.commons.lang.ArrayUtils;
+import org.apache.commons.lang.StringUtils;
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class Row
+{
+    private static RowSerializer serializer_ = new RowSerializer();
+    private static Logger logger_ = Logger.getLogger(Row.class);
+
+    static RowSerializer serializer()
+    {
+        return serializer_;
+    }
+
+    private String key_;
+
+    private Map<String, ColumnFamily> columnFamilies_ = new Hashtable<String, ColumnFamily>();
+
+    protected Row()
+    {
+    }
+
+    public Row(String key)
+    {
+        key_ = key;
+    }
+
+    public String key()
+    {
+        return key_;
+    }
+
+    void key(String key)
+    {
+        key_ = key;
+    }
+
+    public Set<String> getColumnFamilyNames()
+    {
+        return columnFamilies_.keySet();
+    }
+
+    public Collection<ColumnFamily> getColumnFamilies()
+    {
+        return columnFamilies_.values();
+    }
+
+    @Deprecated
+    // (use getColumnFamilies or getColumnFamilyNames)
+    public Map<String, ColumnFamily> getColumnFamilyMap()
+    {
+        return columnFamilies_;
+    }
+
+    public ColumnFamily getColumnFamily(String cfName)
+    {
+        return columnFamilies_.get(cfName);
+    }
+
+    void addColumnFamily(ColumnFamily columnFamily)
+    {
+        columnFamilies_.put(columnFamily.name(), columnFamily);
+    }
+
+    void removeColumnFamily(ColumnFamily columnFamily)
+    {
+        columnFamilies_.remove(columnFamily.name());
+        int delta = (-1) * columnFamily.size();
+    }
+
+    public boolean isEmpty()
+    {
+        return (columnFamilies_.size() == 0);
+    }
+
+    /*
+     * This function will repair the current row with the input row
+     * what that means is that if there are any differences between the 2 rows then
+     * this fn will make the current row take the latest changes .
+     */
+    public void repair(Row rowOther)
+    {
+        for (ColumnFamily cfOld : rowOther.getColumnFamilies())
+        {
+            ColumnFamily cf = columnFamilies_.get(cfOld.name());
+            if (cf == null)
+            {
+                addColumnFamily(cfOld);
+            }
+            else
+            {
+                columnFamilies_.remove(cf.name());
+                addColumnFamily(ColumnFamily.resolve(Arrays.asList(cfOld, cf)));
+            }
+        }
+    }
+
+    /*
+     * This function will calculate the difference between 2 rows
+     * and return the resultant row. This assumes that the row that
+     * is being submitted is a super set of the current row so
+     * it only calculates additional
+     * difference and does not take care of what needs to be removed from the current row to make
+     * it same as the input row.
+     */
+    public Row diff(Row rowComposite)
+    {
+        Row rowDiff = new Row(key_);
+
+        for (ColumnFamily cfComposite : rowComposite.getColumnFamilies())
+        {
+            ColumnFamily cf = columnFamilies_.get(cfComposite.name());
+            if (cf == null)
+                rowDiff.addColumnFamily(cfComposite);
+            else
+            {
+                ColumnFamily cfDiff = cf.diff(cfComposite);
+                if (cfDiff != null)
+                    rowDiff.addColumnFamily(cfDiff);
+            }
+        }
+        if (rowDiff.getColumnFamilies().isEmpty())
+            return null;
+        else
+            return rowDiff;
+    }
+
+    public Row cloneMe()
+    {
+        Row row = new Row(key_);
+        row.columnFamilies_ = new HashMap<String, ColumnFamily>(columnFamilies_);
+        return row;
+    }
+
+    public byte[] digest()
+    {
+        long start = System.currentTimeMillis();
+        Set<String> cfamilies = columnFamilies_.keySet();
+        byte[] xorHash = ArrayUtils.EMPTY_BYTE_ARRAY;
+        for (String cFamily : cfamilies)
+        {
+            if (xorHash.length == 0)
+            {
+                xorHash = columnFamilies_.get(cFamily).digest();
+            }
+            else
+            {
+                byte[] tmpHash = columnFamilies_.get(cFamily).digest();
+                xorHash = FBUtilities.xor(xorHash, tmpHash);
+            }
+        }
+        logger_.info("DIGEST TIME: " + (System.currentTimeMillis() - start)
+                     + " ms.");
+        return xorHash;
+    }
+
+    void clear()
+    {
+        columnFamilies_.clear();
+    }
+
+    public String toString()
+    {
+        return "Row(" + key_ + " [" + StringUtils.join(columnFamilies_.values(), ", ") + ")]";
+    }
+}
+
+class RowSerializer implements ICompactSerializer<Row>
+{
+    public void serialize(Row row, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(row.key());
+        Map<String, ColumnFamily> columnFamilies = row.getColumnFamilyMap();
+        int size = columnFamilies.size();
+        dos.writeInt(size);
+
+        if (size > 0)
+        {
+            Set<String> cNames = columnFamilies.keySet();
+            for (String cName : cNames)
+            {
+                ColumnFamily.serializer().serialize(columnFamilies.get(cName), dos);
+            }
+        }
+    }
+
+    public Row deserialize(DataInputStream dis) throws IOException
+    {
+        String key = dis.readUTF();
+        Row row = new Row(key);
+        int size = dis.readInt();
+
+        if (size > 0)
+        {
+            for (int i = 0; i < size; ++i)
+            {
+                ColumnFamily cf = ColumnFamily.serializer().deserialize(dis);
+                row.addColumnFamily(cf);
+            }
+        }
+        return row;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutation.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutation.java
index e69de29b..7c9167a2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutation.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutation.java
@@ -0,0 +1,387 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ExecutionException;
+import java.nio.ByteBuffer;
+
+import org.apache.commons.lang.ArrayUtils;
+import org.apache.commons.lang.StringUtils;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.service.batch_mutation_super_t;
+import org.apache.cassandra.service.batch_mutation_t;
+import org.apache.cassandra.service.column_t;
+import org.apache.cassandra.service.superColumn_t;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class RowMutation implements Serializable
+{
+    private static ICompactSerializer<RowMutation> serializer_;
+    public static final String HINT = "HINT";
+
+    static
+    {
+        serializer_ = new RowMutationSerializer();
+    }   
+
+    static ICompactSerializer<RowMutation> serializer()
+    {
+        return serializer_;
+    }
+
+    private String table_;
+    private String key_;
+    protected Map<String, ColumnFamily> modifications_ = new HashMap<String, ColumnFamily>();
+
+    /* Ctor for JAXB */
+    private RowMutation()
+    {
+    }
+
+    public RowMutation(String table, String key)
+    {
+        table_ = table;
+        key_ = key;
+    }
+
+    public RowMutation(String table, Row row)
+    {
+        table_ = table;
+        key_ = row.key();
+        for (ColumnFamily cf : row.getColumnFamilies())
+        {
+            add(cf);
+        }
+    }
+
+    protected RowMutation(String table, String key, Map<String, ColumnFamily> modifications)
+    {
+        table_ = table;
+        key_ = key;
+        modifications_ = modifications;
+    }
+
+    public static String[] getColumnAndColumnFamily(String cf)
+    {
+        return cf.split(":");
+    }
+
+    public String table()
+    {
+        return table_;
+    }
+
+    public String key()
+    {
+        return key_;
+    }
+
+    public Set<String> columnFamilyNames()
+    {
+        return modifications_.keySet();
+    }
+
+    void addHints(String hint) throws IOException
+    {
+        String cfName = Table.hints_ + ":" + hint;
+        add(cfName, ArrayUtils.EMPTY_BYTE_ARRAY, 0);
+    }
+
+    /*
+     * Specify a column family name and the corresponding column
+     * family object.
+     * param @ cf - column family name
+     * param @ columnFamily - the column family.
+    */
+    public void add(ColumnFamily columnFamily)
+    {
+        if (modifications_.containsKey(columnFamily.name()))
+        {
+            throw new IllegalArgumentException("ColumnFamily " + columnFamily.name() + " is already being modified");
+        }
+        modifications_.put(columnFamily.name(), columnFamily);
+    }
+
+    /*
+     * Specify a column name and a corresponding value for
+     * the column. Column name is specified as <column family>:column.
+     * This will result in a ColumnFamily associated with
+     * <column family> as name and a Column with <column>
+     * as name. The columan can be further broken up
+     * as super column name : columnname  in case of super columns
+     *
+     * param @ cf - column name as <column family>:<column>
+     * param @ value - value associated with the column
+     * param @ timestamp - ts associated with this data.
+    */
+    public void add(String cf, byte[] value, long timestamp)
+    {
+        String[] values = RowMutation.getColumnAndColumnFamily(cf);
+
+        if ( values.length == 0 || values.length == 1 || values.length > 3 )
+            throw new IllegalArgumentException("Column Family " + cf + " in invalid format. Must be in <column family>:<column> format.");
+
+        ColumnFamily columnFamily = modifications_.get(values[0]);
+        if( values.length == 2 )
+        {
+            if ( columnFamily == null )
+            {
+            	columnFamily = new ColumnFamily(values[0], ColumnFamily.getColumnType("Standard"));
+            }
+        	columnFamily.addColumn(values[1], value, timestamp);
+        }
+        if( values.length == 3 )
+        {
+            if ( columnFamily == null )
+            {
+            	columnFamily = new ColumnFamily(values[0], ColumnFamily.getColumnType("Super"));
+            }
+        	columnFamily.addColumn(values[1]+ ":" + values[2], value, timestamp);
+        }
+        modifications_.put(values[0], columnFamily);
+    }
+
+    public void delete(String columnFamilyColumn, long timestamp)
+    {
+        String[] values = RowMutation.getColumnAndColumnFamily(columnFamilyColumn);
+        String cfName = values[0];
+
+        if (modifications_.containsKey(cfName))
+        {
+            throw new IllegalArgumentException("ColumnFamily " + cfName + " is already being modified");
+        }
+        if (values.length == 0 || values.length > 3)
+            throw new IllegalArgumentException("Column Family " + columnFamilyColumn + " in invalid format. Must be in <column family>:<column> format.");
+
+        int localDeleteTime = (int) (System.currentTimeMillis() / 1000);
+
+        ColumnFamily columnFamily = modifications_.get(cfName);
+        if (columnFamily == null)
+            columnFamily = new ColumnFamily(cfName, DatabaseDescriptor.getColumnType(cfName));
+        if (values.length == 2)
+        {
+            if (columnFamily.isSuper())
+            {
+                SuperColumn sc = new SuperColumn(values[1]);
+                sc.markForDeleteAt(localDeleteTime, timestamp);
+                columnFamily.addColumn(sc);
+            }
+            else
+            {
+                ByteBuffer bytes = ByteBuffer.allocate(4);
+                bytes.putInt(localDeleteTime);
+                columnFamily.addColumn(values[1], bytes.array(), timestamp, true);
+            }
+        }
+        else if (values.length == 3)
+        {
+            ByteBuffer bytes = ByteBuffer.allocate(4);
+            bytes.putInt(localDeleteTime);
+            columnFamily.addColumn(values[1] + ":" + values[2], bytes.array(), timestamp, true);
+        }
+        else
+        {
+            assert values.length == 1;
+            columnFamily.delete(localDeleteTime, timestamp);
+        }
+        modifications_.put(cfName, columnFamily);
+    }
+
+    /*
+     * This is equivalent to calling commit. Applies the changes to
+     * to the table that is obtained by calling Table.open().
+    */
+    public void apply() throws IOException
+    {
+        Row row = new Row(key_);
+        apply(row);
+    }
+
+    /*
+     * Allows RowMutationVerbHandler to optimize by re-using a single Row object.
+    */
+    void apply(Row emptyRow) throws IOException
+    {
+        assert emptyRow.getColumnFamilyMap().size() == 0;
+        Table table = Table.open(table_);
+        for (String cfName : modifications_.keySet())
+        {
+            assert table.isValidColumnFamily(cfName);
+            emptyRow.addColumnFamily(modifications_.get(cfName));
+        }
+        table.apply(emptyRow);
+    }
+
+    /*
+     * This is equivalent to calling commit. Applies the changes to
+     * to the table that is obtained by calling Table.open().
+    */
+    void load(Row row) throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open(table_);
+        Set<String> cfNames = modifications_.keySet();
+        for (String cfName : cfNames)
+        {
+            assert table.isValidColumnFamily(cfName);
+            row.addColumnFamily(modifications_.get(cfName));
+        }
+        table.load(row);
+    }
+
+    public Message makeRowMutationMessage() throws IOException
+    {
+        return makeRowMutationMessage(StorageService.mutationVerbHandler_);
+    }
+
+    public Message makeRowMutationMessage(String verbHandlerName) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        serializer().serialize(this, dos);
+        EndPoint local = StorageService.getLocalStorageEndPoint();
+        EndPoint from = (local != null) ? local : new EndPoint(FBUtilities.getHostAddress(), 7000);
+        return new Message(from, StorageService.mutationStage_, verbHandlerName, bos.toByteArray());
+    }
+
+    public static RowMutation getRowMutation(batch_mutation_t batchMutation)
+    {
+        RowMutation rm = new RowMutation(batchMutation.table,
+                                         batchMutation.key.trim());
+        for (String cfname : batchMutation.cfmap.keySet())
+        {
+            List<column_t> list = batchMutation.cfmap.get(cfname);
+            for (column_t columnData : list)
+            {
+                rm.add(cfname + ":" + columnData.columnName,
+                       columnData.value, columnData.timestamp);
+
+            }
+        }
+        return rm;
+    }
+
+    public static RowMutation getRowMutation(batch_mutation_super_t batchMutationSuper)
+    {
+        RowMutation rm = new RowMutation(batchMutationSuper.table,
+                                         batchMutationSuper.key.trim());
+        Set keys = batchMutationSuper.cfmap.keySet();
+        Iterator keyIter = keys.iterator();
+        while (keyIter.hasNext())
+        {
+            Object key = keyIter.next(); // Get the next key.
+            List<superColumn_t> list = batchMutationSuper.cfmap.get(key);
+            for (superColumn_t superColumnData : list)
+            {
+                if (superColumnData.columns.size() != 0)
+                {
+                    for (column_t columnData : superColumnData.columns)
+                    {
+                        rm.add(key.toString() + ":" + superColumnData.name + ":" + columnData.columnName,
+                               columnData.value, columnData.timestamp);
+                    }
+                }
+                else
+                {
+                    rm.add(key.toString() + ":" + superColumnData.name, ArrayUtils.EMPTY_BYTE_ARRAY, 0);
+                }
+            }
+        }
+        return rm;
+    }
+
+    public String toString()
+    {
+        return "RowMutation(" +
+               "key='" + key_ + '\'' +
+               ", modifications=[" + StringUtils.join(modifications_.values(), ", ") + "]" +
+               ')';
+    }
+}
+
+class RowMutationSerializer implements ICompactSerializer<RowMutation>
+{
+    private void freezeTheMaps(Map<String, ColumnFamily> map, DataOutputStream dos) throws IOException
+    {
+        int size = map.size();
+        dos.writeInt(size);
+        if (size > 0)
+        {
+            Set<String> keys = map.keySet();
+            for (String key : keys)
+            {
+                dos.writeUTF(key);
+                ColumnFamily cf = map.get(key);
+                if (cf != null)
+                {
+                    ColumnFamily.serializer().serialize(cf, dos);
+                }
+            }
+        }
+    }
+
+    public void serialize(RowMutation rm, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(rm.table());
+        dos.writeUTF(rm.key());
+
+        /* serialize the modifications_ in the mutation */
+        freezeTheMaps(rm.modifications_, dos);
+    }
+
+    private Map<String, ColumnFamily> defreezeTheMaps(DataInputStream dis) throws IOException
+    {
+        Map<String, ColumnFamily> map = new HashMap<String, ColumnFamily>();
+        int size = dis.readInt();
+        for (int i = 0; i < size; ++i)
+        {
+            String key = dis.readUTF();
+            ColumnFamily cf = ColumnFamily.serializer().deserialize(dis);
+            map.put(key, cf);
+        }
+        return map;
+    }
+
+    public RowMutation deserialize(DataInputStream dis) throws IOException
+    {
+        String table = dis.readUTF();
+        String key = dis.readUTF();
+        Map<String, ColumnFamily> modifications = defreezeTheMaps(dis);
+        return new RowMutation(table, key, modifications);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationMessage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationMessage.java
index e69de29b..3abb22a1 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationMessage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationMessage.java
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+
+import javax.xml.bind.annotation.XmlElement;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class RowMutationMessage implements Serializable
+{   
+    public static final String hint_ = "HINT";
+    private static ICompactSerializer<RowMutationMessage> serializer_;	
+	
+    static
+    {
+        serializer_ = new RowMutationMessageSerializer();
+    }
+
+    static ICompactSerializer<RowMutationMessage> serializer()
+    {
+        return serializer_;
+    }
+
+    public static Message makeRowMutationMessage(RowMutationMessage rowMutationMessage) throws IOException
+    {         
+        return makeRowMutationMessage(rowMutationMessage, StorageService.mutationVerbHandler_);
+    }
+    
+    public static Message makeRowMutationMessage(RowMutationMessage rowMutationMessage, String verbHandlerName) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        RowMutationMessage.serializer().serialize(rowMutationMessage, dos);
+        EndPoint local = StorageService.getLocalStorageEndPoint();
+        EndPoint from = ( local != null ) ? local : new EndPoint(FBUtilities.getHostAddress(), 7000);
+        Message message = new Message(from, StorageService.mutationStage_, verbHandlerName, new Object[]{bos.toByteArray()});         
+        return message;
+    }
+    
+    @XmlElement(name="RowMutation")
+    private RowMutation rowMutation_;
+    
+    private RowMutationMessage()
+    {}
+    
+    public RowMutationMessage(RowMutation rowMutation)
+    {
+        rowMutation_ = rowMutation;
+    }
+    
+   public RowMutation getRowMutation()
+   {
+       return rowMutation_;
+   }
+}
+
+class RowMutationMessageSerializer implements ICompactSerializer<RowMutationMessage>
+{
+	public void serialize(RowMutationMessage rm, DataOutputStream dos) throws IOException
+	{
+		RowMutation.serializer().serialize(rm.getRowMutation(), dos);
+	}
+	
+    public RowMutationMessage deserialize(DataInputStream dis) throws IOException
+    {
+    	RowMutation rm = RowMutation.serializer().deserialize(dis);
+    	return new RowMutationMessage(rm);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
index e69de29b..361afae3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
@@ -0,0 +1,102 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.*;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.io.*;
+
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.service.*;
+import org.apache.cassandra.utils.*;
+import org.apache.cassandra.concurrent.*;
+import org.apache.cassandra.net.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class RowMutationVerbHandler implements IVerbHandler
+{
+    protected static class RowMutationContext
+    {
+        protected Row row_ = new Row();
+        protected DataInputBuffer buffer_ = new DataInputBuffer();
+    }
+
+    private static Logger logger_ = Logger.getLogger(RowMutationVerbHandler.class);
+    /* We use this so that we can reuse the same row mutation context for the mutation. */
+    private static ThreadLocal<RowMutationContext> tls_ = new InheritableThreadLocal<RowMutationContext>();
+
+    public void doVerb(Message message)
+    {
+        byte[] bytes = (byte[]) message.getMessageBody()[0];
+        /* Obtain a Row Mutation Context from TLS */
+        RowMutationContext rowMutationCtx = tls_.get();
+        if ( rowMutationCtx == null )
+        {
+            rowMutationCtx = new RowMutationContext();
+            tls_.set(rowMutationCtx);
+        }
+
+        rowMutationCtx.buffer_.reset(bytes, bytes.length);
+
+        try
+        {
+            RowMutation rm = RowMutation.serializer().deserialize(rowMutationCtx.buffer_);
+            logger_.debug("Applying " + rm);
+
+            /* Check if there were any hints in this message */
+            byte[] hintedBytes = message.getHeader(RowMutation.HINT);
+            if ( hintedBytes != null && hintedBytes.length > 0 )
+            {
+            	EndPoint hint = EndPoint.fromBytes(hintedBytes);
+                logger_.debug("Adding hint for " + hint);
+                /* add necessary hints to this mutation */
+                RowMutation hintedMutation = new RowMutation(rm.table(), HintedHandOffManager.key_);
+                hintedMutation.addHints(rm.key() + ":" + hint.getHost());
+                hintedMutation.apply();
+            }
+
+            long start = System.currentTimeMillis();
+
+            rowMutationCtx.row_.clear();
+            rowMutationCtx.row_.key(rm.key());
+            rm.apply(rowMutationCtx.row_);
+
+            long end = System.currentTimeMillis();
+
+            WriteResponse response = new WriteResponse(rm.table(), rm.key(), true);
+            Message responseMessage = WriteResponse.makeWriteResponseMessage(message, response);
+            logger_.debug("Mutation applied in " + (end - start) + "ms.  Sending response to " +  message.getFrom() + " for key :" + rm.key());
+            MessagingService.getMessagingInstance().sendOneWay(responseMessage, message.getFrom());
+        }
+        catch (IOException e)
+        {
+            logger_.error("Error in row mutation", e);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowReadCommand.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowReadCommand.java
index e69de29b..2b560c34 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowReadCommand.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/RowReadCommand.java
@@ -0,0 +1,84 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+public class RowReadCommand extends ReadCommand
+{
+    public RowReadCommand(String table, String key)
+    {
+        super(table, key, CMD_TYPE_GET_ROW);
+    }
+
+    @Override
+    public String getColumnFamilyName()
+    {
+        return null;
+    }
+
+    @Override
+    public ReadCommand copy()
+    {
+        ReadCommand readCommand= new RowReadCommand(table, key);
+        readCommand.setDigestQuery(isDigestQuery());
+        return readCommand;
+    }
+
+    @Override
+    public Row getRow(Table table) throws IOException    
+    {
+        return table.get(key);
+    }
+
+    @Override
+    public String toString()
+    {
+        return "GetColumnReadMessage(" +
+               "table='" + table + '\'' +
+               ", key='" + key + '\'' +
+               ')';
+    }
+
+}
+
+class RowReadCommandSerializer extends ReadCommandSerializer
+{
+    @Override
+    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
+    { 
+        RowReadCommand realRM = (RowReadCommand)rm;
+        dos.writeBoolean(realRM.isDigestQuery());
+        dos.writeUTF(realRM.table);
+        dos.writeUTF(realRM.key);
+    }
+
+    @Override
+    public ReadCommand deserialize(DataInputStream dis) throws IOException
+    {
+        boolean isDigest = dis.readBoolean();
+        String table = dis.readUTF();
+        String key = dis.readUTF();
+        RowReadCommand rm = new RowReadCommand(table, key);
+        rm.setDigestQuery(isDigest);
+        return rm;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Scanner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Scanner.java
index e69de29b..9cb09e9a 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Scanner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Scanner.java
@@ -0,0 +1,89 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.*;
+import java.io.IOException;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+
+/**
+ * This class is used to loop through a retrieved column family
+ * to get all columns in Iterator style. Usage is as follows:
+ * Scanner scanner = new Scanner("table");
+ * scanner.fetchColumnfamily(key, "column-family");
+ * 
+ * while ( scanner.hasNext() )
+ * {
+ *     Column column = scanner.next();
+ *     // Do something with the column
+ * }
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Scanner implements IScanner<IColumn>
+{
+    /* Table over which we are scanning. */
+    private String table_; 
+    /* Iterator when iterating over the columns of a given key in a column family */
+    private Iterator<IColumn> columnIt_;
+        
+    public Scanner(String table)
+    {
+        table_ = table;
+    }
+    
+    /**
+     * Fetch the columns associated with this key for the specified column family.
+     * This method basically sets up an iterator internally and then provides an 
+     * iterator like interface to iterate over the columns.
+     * @param key key we are interested in.
+     * @param cf column family we are interested in.
+     * @throws IOException
+     */
+    public void fetch(String key, String cf) throws IOException
+    {        
+        if ( cf != null )
+        {
+            Table table = Table.open(table_);
+            ColumnFamily columnFamily = table.get(key, cf);
+            if ( columnFamily != null )
+            {
+                Collection<IColumn> columns = columnFamily.getAllColumns();            
+                columnIt_ = columns.iterator();
+            }
+        }
+    }        
+    
+    public boolean hasNext() throws IOException
+    {
+        return columnIt_.hasNext();
+    }
+    
+    public IColumn next()
+    {
+        return columnIt_.next();
+    }
+    
+    public void close() throws IOException
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the Scanner");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SliceByNamesReadCommand.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SliceByNamesReadCommand.java
index e69de29b..a9acdf23 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SliceByNamesReadCommand.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SliceByNamesReadCommand.java
@@ -0,0 +1,115 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.commons.lang.StringUtils;
+
+public class SliceByNamesReadCommand extends ReadCommand
+{
+    public final String columnFamily;
+    public final List<String> columnNames;
+
+    public SliceByNamesReadCommand(String table, String key, String columnFamily, List<String> columnNames)
+    {
+        super(table, key, CMD_TYPE_GET_SLICE_BY_NAMES);
+        this.columnFamily = columnFamily;
+        this.columnNames = Collections.unmodifiableList(columnNames);
+    }
+
+    @Override
+    public String getColumnFamilyName()
+    {
+        return columnFamily;
+    }
+
+    @Override
+    public ReadCommand copy()
+    {
+        ReadCommand readCommand= new SliceByNamesReadCommand(table, key, columnFamily, columnNames);
+        readCommand.setDigestQuery(isDigestQuery());
+        return readCommand;
+    }
+    
+    @Override
+    public Row getRow(Table table) throws IOException
+    {        
+        return table.getRow(key, columnFamily, columnNames);
+    }
+
+    @Override
+    public String toString()
+    {
+        return "GetSliceByNamesReadMessage(" +
+               "table='" + table + '\'' +
+               ", key='" + key + '\'' +
+               ", columnFamily='" + columnFamily + '\'' +
+               ", columns=[" + StringUtils.join(columnNames, ", ") + "]" +
+               ')';
+    }
+
+}
+
+class SliceByNamesReadCommandSerializer extends ReadCommandSerializer
+{
+    @Override
+    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
+    {
+        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
+        dos.writeBoolean(realRM.isDigestQuery());
+        dos.writeUTF(realRM.table);
+        dos.writeUTF(realRM.key);
+        dos.writeUTF(realRM.columnFamily);
+        dos.writeInt(realRM.columnNames.size());
+        if (realRM.columnNames.size() > 0)
+        {
+            for (String cName : realRM.columnNames)
+            {
+                dos.writeInt(cName.getBytes().length);
+                dos.write(cName.getBytes());
+            }
+        }
+    }
+
+    @Override
+    public ReadCommand deserialize(DataInputStream dis) throws IOException
+    {
+        boolean isDigest = dis.readBoolean();
+        String table = dis.readUTF();
+        String key = dis.readUTF();
+        String columnFamily = dis.readUTF();
+
+        int size = dis.readInt();
+        List<String> columns = new ArrayList<String>();
+        for (int i = 0; i < size; ++i)
+        {
+            byte[] bytes = new byte[dis.readInt()];
+            dis.readFully(bytes);
+            columns.add(new String(bytes));
+        }
+        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnFamily, columns);
+        rm.setDigestQuery(isDigest);
+        return rm;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SliceReadCommand.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SliceReadCommand.java
index e69de29b..e8c28620 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SliceReadCommand.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SliceReadCommand.java
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+public class SliceReadCommand extends ReadCommand
+{
+    /* for a slice of a standard column, cFC should only be the CF name.
+       for a supercolumn slice, it will be CF:supercolumn. */
+    public final String columnFamilyColumn;
+    public final int start;
+    public final int count;
+
+    public SliceReadCommand(String table, String key, String columnFamilyColumn, int start, int count)
+    {
+        super(table, key, CMD_TYPE_GET_SLICE);
+        this.columnFamilyColumn = columnFamilyColumn;
+        this.start = start;
+        this.count = count;
+    }
+
+    @Override
+    public String getColumnFamilyName()
+    {
+        return RowMutation.getColumnAndColumnFamily(columnFamilyColumn)[0];
+    }
+
+    @Override
+    public ReadCommand copy()
+    {
+        ReadCommand readCommand = new SliceReadCommand(table, key, columnFamilyColumn, start, count);
+        readCommand.setDigestQuery(isDigestQuery());
+        return readCommand;
+    }
+    
+    @Override
+    public Row getRow(Table table) throws IOException
+    {
+        return table.getRow(key, columnFamilyColumn, start, count);
+    }
+
+    @Override
+    public String toString()
+    {
+        return "GetSliceReadMessage(" +
+               "table='" + table + '\'' +
+               ", key='" + key + '\'' +
+               ", columnFamily='" + columnFamilyColumn + '\'' +
+               ", start='" + start + '\'' +
+               ", count='" + count + '\'' +
+               ')';
+    }
+}
+
+class SliceReadCommandSerializer extends ReadCommandSerializer
+{
+    @Override
+    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
+    {
+        SliceReadCommand realRM = (SliceReadCommand)rm;
+        dos.writeBoolean(realRM.isDigestQuery());
+        dos.writeUTF(realRM.table);
+        dos.writeUTF(realRM.key);
+        dos.writeUTF(realRM.columnFamilyColumn);
+        dos.writeInt(realRM.start);
+        dos.writeInt(realRM.count);
+    }
+
+    @Override
+    public ReadCommand deserialize(DataInputStream dis) throws IOException
+    {
+        boolean isDigest = dis.readBoolean();
+        String table = dis.readUTF();
+        String key = dis.readUTF();
+        String columnFamily = dis.readUTF();
+        int start = dis.readInt();
+        int count = dis.readInt();
+        
+        SliceReadCommand rm = new SliceReadCommand(table, key, columnFamily, start, count);
+        rm.setDigestQuery(isDigest);
+        return rm;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SuperColumn.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SuperColumn.java
index e69de29b..2ea76dad 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SuperColumn.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SuperColumn.java
@@ -0,0 +1,468 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.Collection;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.commons.lang.ArrayUtils;
+import org.apache.commons.lang.StringUtils;
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.utils.FBUtilities;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class SuperColumn implements IColumn, Serializable
+{
+	private static Logger logger_ = Logger.getLogger(SuperColumn.class);
+	private static SuperColumnSerializer serializer_ = new SuperColumnSerializer();
+	private final static String seperator_ = ":";
+
+    static SuperColumnSerializer serializer()
+    {
+        return serializer_;
+    }
+
+	private String name_;
+    private EfficientBidiMap columns_ = new EfficientBidiMap(ColumnComparatorFactory.getComparator(ColumnComparatorFactory.ComparatorType.TIMESTAMP));
+    private int localDeletionTime = Integer.MIN_VALUE;
+	private long markedForDeleteAt = Long.MIN_VALUE;
+    private AtomicInteger size_ = new AtomicInteger(0);
+
+    SuperColumn()
+    {
+    }
+
+    SuperColumn(String name)
+    {
+    	name_ = name;
+    }
+
+	public boolean isMarkedForDelete()
+	{
+		return markedForDeleteAt > Long.MIN_VALUE;
+	}
+
+    public String name()
+    {
+    	return name_;
+    }
+
+    public Collection<IColumn> getSubColumns()
+    {
+    	return columns_.getSortedColumns();
+    }
+
+    public IColumn getSubColumn(String columnName)
+    {
+        IColumn column = columns_.get(columnName);
+        assert column == null || column instanceof Column;
+        return column;
+    }
+
+    public int compareTo(IColumn superColumn)
+    {
+        return (name_.compareTo(superColumn.name()));
+    }
+
+
+    public int size()
+    {
+        /*
+         * return the size of the individual columns
+         * that make up the super column. This is an
+         * APPROXIMATION of the size used only from the
+         * Memtable.
+        */
+        return size_.get();
+    }
+
+    /**
+     * This returns the size of the super-column when serialized.
+     * @see org.apache.cassandra.db.IColumn#serializedSize()
+    */
+    public int serializedSize()
+    {
+        /*
+         * Size of a super-column is =
+         *   size of a name (UtfPrefix + length of the string)
+         * + 1 byte to indicate if the super-column has been deleted
+         * + 4 bytes for size of the sub-columns
+         * + 4 bytes for the number of sub-columns
+         * + size of all the sub-columns.
+        */
+
+    	/*
+    	 * We store the string as UTF-8 encoded, so when we calculate the length, it
+    	 * should be converted to UTF-8.
+    	 */
+    	/*
+    	 * We need to keep the way we are calculating the column size in sync with the
+    	 * way we are calculating the size for the column family serializer.
+    	 */
+    	return IColumn.UtfPrefix_ + FBUtilities.getUTF8Length(name_) + DBConstants.boolSize_ + DBConstants.intSize_ + DBConstants.intSize_ + getSizeOfAllColumns();
+    }
+
+    /**
+     * This calculates the exact size of the sub columns on the fly
+     */
+    int getSizeOfAllColumns()
+    {
+        int size = 0;
+        Collection<IColumn> subColumns = getSubColumns();
+        for ( IColumn subColumn : subColumns )
+        {
+            size += subColumn.serializedSize();
+        }
+        return size;
+    }
+
+    public void remove(String columnName)
+    {
+    	columns_.remove(columnName);
+    }
+
+    public long timestamp()
+    {
+    	throw new UnsupportedOperationException("This operation is not supported for Super Columns.");
+    }
+
+    public long timestamp(String key)
+    {
+    	IColumn column = columns_.get(key);
+    	if ( column instanceof SuperColumn )
+    		throw new UnsupportedOperationException("A super column cannot hold other super columns.");
+    	if ( column != null )
+    		return column.timestamp();
+    	throw new IllegalArgumentException("Timestamp was requested for a column that does not exist.");
+    }
+
+    public byte[] value()
+    {
+    	throw new UnsupportedOperationException("This operation is not supported for Super Columns.");
+    }
+
+    public byte[] value(String key)
+    {
+    	IColumn column = columns_.get(key);
+    	if ( column != null )
+    		return column.value();
+    	throw new IllegalArgumentException("Value was requested for a column that does not exist.");
+    }
+
+    public void addColumn(String name, IColumn column)
+    {
+    	if ( column instanceof SuperColumn )
+    		throw new UnsupportedOperationException("A super column cannot hold other super columns.");
+    	IColumn oldColumn = columns_.get(name);
+    	if ( oldColumn == null )
+        {
+    		columns_.put(name, column);
+            size_.addAndGet(column.size());
+        }
+    	else
+    	{
+    		if ( oldColumn.timestamp() <= column.timestamp() )
+            {
+    			columns_.put(name, column);
+                int delta = (-1)*oldColumn.size();
+                /* subtract the size of the oldColumn */
+                size_.addAndGet(delta);
+                /* add the size of the new column */
+                size_.addAndGet(column.size());
+            }
+    	}
+    }
+
+    /*
+     * Go through each sub column if it exists then as it to resolve itself
+     * if the column does not exist then create it.
+     */
+    public void putColumn(IColumn column)
+    {
+    	if ( !(column instanceof SuperColumn))
+    		throw new UnsupportedOperationException("Only Super column objects should be put here");
+    	if( !name_.equals(column.name()))
+    		throw new IllegalArgumentException("The name should match the name of the current column or super column");
+
+        for (IColumn subColumn : column.getSubColumns())
+        {
+        	addColumn(subColumn.name(), subColumn);
+        }
+        if (column.getMarkedForDeleteAt() > markedForDeleteAt)
+        {
+            markForDeleteAt(column.getLocalDeletionTime(),  column.getMarkedForDeleteAt());
+        }
+    }
+
+    public int getObjectCount()
+    {
+    	return 1 + columns_.size();
+    }
+
+    public long getMarkedForDeleteAt() {
+        return markedForDeleteAt;
+    }
+
+    int getColumnCount()
+    {
+    	return columns_.size();
+    }
+
+    public IColumn diff(IColumn columnNew)
+    {
+    	IColumn columnDiff = new SuperColumn(columnNew.name());
+        if (columnNew.getMarkedForDeleteAt() > getMarkedForDeleteAt())
+        {
+            ((SuperColumn)columnDiff).markForDeleteAt(columnNew.getLocalDeletionTime(), columnNew.getMarkedForDeleteAt());
+        }
+
+        // (don't need to worry about columnNew containing subColumns that are shadowed by
+        // the delete tombstone, since columnNew was generated by CF.resolve, which
+        // takes care of those for us.)
+        for (IColumn subColumn : columnNew.getSubColumns())
+        {
+        	IColumn columnInternal = columns_.get(subColumn.name());
+        	if(columnInternal == null )
+        	{
+        		columnDiff.addColumn(subColumn.name(), subColumn);
+        	}
+        	else
+        	{
+            	IColumn subColumnDiff = columnInternal.diff(subColumn);
+        		if(subColumnDiff != null)
+        		{
+            		columnDiff.addColumn(subColumn.name(), subColumnDiff);
+        		}
+        	}
+        }
+
+        if (!columnDiff.getSubColumns().isEmpty() || columnNew.isMarkedForDelete())
+        	return columnDiff;
+        else
+        	return null;
+    }
+
+    public byte[] digest()
+    {
+    	Set<IColumn> columns = columns_.getSortedColumns();
+    	byte[] xorHash = ArrayUtils.EMPTY_BYTE_ARRAY;
+    	if(name_ == null)
+    		return xorHash;
+    	xorHash = name_.getBytes();
+    	for(IColumn column : columns)
+    	{
+			xorHash = FBUtilities.xor(xorHash, column.digest());
+    	}
+    	return xorHash;
+    }
+
+
+    public String toString()
+    {
+    	StringBuilder sb = new StringBuilder();
+        sb.append("SuperColumn(");
+    	sb.append(name_);
+
+        if (isMarkedForDelete()) {
+            sb.append(" -delete at " + getMarkedForDeleteAt() + "-");
+        }
+
+        sb.append(" [");
+        sb.append(StringUtils.join(getSubColumns(), ", "));
+        sb.append("])");
+
+        return sb.toString();
+    }
+
+    public int getLocalDeletionTime()
+    {
+        return localDeletionTime;
+    }
+
+    public void markForDeleteAt(int localDeleteTime, long timestamp)
+    {
+        this.localDeletionTime = localDeleteTime;
+        this.markedForDeleteAt = timestamp;
+    }
+}
+
+class SuperColumnSerializer implements ICompactSerializer2<IColumn>
+{
+    public void serialize(IColumn column, DataOutputStream dos) throws IOException
+    {
+    	SuperColumn superColumn = (SuperColumn)column;
+        dos.writeUTF(superColumn.name());
+        dos.writeInt(superColumn.getLocalDeletionTime());
+        dos.writeLong(superColumn.getMarkedForDeleteAt());
+
+        Collection<IColumn> columns  = column.getSubColumns();
+        int size = columns.size();
+        dos.writeInt(size);
+
+        dos.writeInt(superColumn.getSizeOfAllColumns());
+        for ( IColumn subColumn : columns )
+        {
+            Column.serializer().serialize(subColumn, dos);
+        }
+    }
+
+    /*
+     * Use this method to create a bare bones Super Column. This super column
+     * does not have any of the Column information.
+    */
+    private SuperColumn defreezeSuperColumn(DataInputStream dis) throws IOException
+    {
+        String name = dis.readUTF();
+        SuperColumn superColumn = new SuperColumn(name);
+        superColumn.markForDeleteAt(dis.readInt(), dis.readLong());
+        return superColumn;
+    }
+
+    public IColumn deserialize(DataInputStream dis) throws IOException
+    {
+        SuperColumn superColumn = defreezeSuperColumn(dis);
+        fillSuperColumn(superColumn, dis);
+        return superColumn;
+    }
+
+    public void skip(DataInputStream dis) throws IOException
+    {
+        defreezeSuperColumn(dis);
+        /* read the number of columns stored */
+        dis.readInt();
+        /* read the size of all columns to skip */
+        int size = dis.readInt();
+        dis.skip(size);
+    }
+
+    private void fillSuperColumn(IColumn superColumn, DataInputStream dis) throws IOException
+    {
+        assert dis.available() != 0;
+
+        /* read the number of columns */
+        int size = dis.readInt();
+        /* read the size of all columns */
+        dis.readInt();
+        for ( int i = 0; i < size; ++i )
+        {
+            IColumn subColumn = Column.serializer().deserialize(dis);
+            superColumn.addColumn(subColumn.name(), subColumn);
+        }
+    }
+
+    public IColumn deserialize(DataInputStream dis, IFilter filter) throws IOException
+    {
+        if ( dis.available() == 0 )
+            return null;
+
+        IColumn superColumn = defreezeSuperColumn(dis);
+        superColumn = filter.filter(superColumn, dis);
+        if(superColumn != null)
+        {
+            fillSuperColumn(superColumn, dis);
+            return superColumn;
+        }
+        else
+        {
+            /* read the number of columns stored */
+            dis.readInt();
+            /* read the size of all columns to skip */
+            int size = dis.readInt();
+            dis.skip(size);
+        	return null;
+        }
+    }
+
+    /*
+     * Deserialize a particular column since the name is in the form of
+     * superColumn:column.
+    */
+    public IColumn deserialize(DataInputStream dis, String name, IFilter filter) throws IOException
+    {
+        if ( dis.available() == 0 )
+            return null;
+
+        String[] names = RowMutation.getColumnAndColumnFamily(name);
+        if ( names.length == 1 )
+        {
+            IColumn superColumn = defreezeSuperColumn(dis);
+            if(name.equals(superColumn.name()))
+            {
+                /* read the number of columns stored */
+                int size = dis.readInt();
+                /* read the size of all columns */
+                dis.readInt();
+                IColumn column = null;
+                for ( int i = 0; i < size; ++i )
+                {
+                    column = Column.serializer().deserialize(dis, filter);
+                    if(column != null)
+                    {
+                        superColumn.addColumn(column.name(), column);
+                        column = null;
+                        if(filter.isDone())
+                        {
+                            break;
+                        }
+                    }
+                }
+                return superColumn;
+            }
+            else
+            {
+                /* read the number of columns stored */
+                dis.readInt();
+                /* read the size of all columns to skip */
+                int size = dis.readInt();
+                dis.skip(size);
+            	return null;
+            }
+        }
+
+        SuperColumn superColumn = defreezeSuperColumn(dis);
+        if ( !superColumn.isMarkedForDelete() )
+        {
+            int size = dis.readInt();
+            /* skip the size of the columns */
+            dis.readInt();
+            if ( size > 0 )
+            {
+                for ( int i = 0; i < size; ++i )
+                {
+                    IColumn subColumn = Column.serializer().deserialize(dis, names[1], filter);
+                    if ( subColumn != null )
+                    {
+                        superColumn.addColumn(subColumn.name(), subColumn);
+                        break;
+                    }
+                }
+            }
+        }
+        return superColumn;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SystemTable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SystemTable.java
index e69de29b..6d483e68 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SystemTable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/SystemTable.java
@@ -0,0 +1,180 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.IFileWriter;
+import org.apache.cassandra.io.SequenceFile;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.dht.IPartitioner;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SystemTable
+{
+    private static Logger logger_ = Logger.getLogger(SystemTable.class);
+    private static Map<String, SystemTable> instances_ = new HashMap<String, SystemTable>();
+
+    /* Name of the SystemTable */
+    public static final String name_ = "System";
+    /* Name of the only column family in the Table */
+    public static final String cfName_ = "LocationInfo";
+    /* Name of columns in this table */
+    static final String generation_ = "Generation";
+    static final String token_ = "Token";
+
+    /* The ID associated with this column family */
+    static final int cfId_ = -1;
+
+    /* Table name. */
+    private String table_;
+    /* after the header position */
+    private long startPosition_ = 0L;
+    /* Cache the SystemRow that we read. */
+    private Row systemRow_;
+
+    /* Use the following writer/reader to write/read to System table */
+    private IFileWriter writer_;
+    private IFileReader reader_;
+
+    public static SystemTable openSystemTable(String tableName) throws IOException
+    {
+        SystemTable table = instances_.get("System");
+        if ( table == null )
+        {
+            table = new SystemTable(tableName);
+            instances_.put(tableName, table);
+        }
+        return table;
+    }
+
+    SystemTable(String table) throws IOException
+    {
+        table_ = table;
+        String systemTable = getFileName();
+        writer_ = SequenceFile.writer(systemTable);
+        reader_ = SequenceFile.reader(systemTable);
+    }
+
+    private String getFileName()
+    {
+        return DatabaseDescriptor.getMetadataDirectory() + System.getProperty("file.separator") + table_ + ".db";
+    }
+
+    /*
+     * Selects the row associated with the given key.
+    */
+    public Row get(String key) throws IOException
+    {
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        reader_.next(bufOut);
+
+        if ( bufOut.getLength() > 0 )
+        {
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(bufOut.getData(), bufOut.getLength());
+            /*
+             * This buffer contains key and value so we need to strip
+             * certain parts
+           */
+            // read the key
+            bufIn.readUTF();
+            // read the data length and then deserialize
+            bufIn.readInt();
+            try
+            {
+                systemRow_ = Row.serializer().deserialize(bufIn);
+            }
+            catch ( IOException e )
+            {
+                logger_.debug( LogUtil.throwableToString(e) );
+            }
+        }
+        return systemRow_;
+    }
+
+    /*
+     * This is a one time thing and hence we do not need
+     * any commit log related activity. Just write in an
+     * atomic fashion to the underlying SequenceFile.
+    */
+    void apply(Row row) throws IOException
+    {
+        systemRow_ = row;
+        String file = getFileName();
+        long currentPos = writer_.getCurrentPosition();
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        Row.serializer().serialize(row, bufOut);
+        try
+        {
+            writer_.append(row.key(), bufOut);
+        }
+        catch ( IOException e )
+        {
+            writer_.seek(currentPos);
+            throw e;
+        }
+    }
+
+    /*
+     * This method is used to update the SystemTable with the
+     * new token.
+    */
+    public void updateToken(Token token) throws IOException
+    {
+        IPartitioner p = StorageService.getPartitioner();
+        if ( systemRow_ != null )
+        {
+            Map<String, ColumnFamily> columnFamilies = systemRow_.getColumnFamilyMap();
+            /* Retrieve the "LocationInfo" column family */
+            ColumnFamily columnFamily = columnFamilies.get(SystemTable.cfName_);
+            long oldTokenColumnTimestamp = columnFamily.getColumn(SystemTable.token_).timestamp();
+            /* create the "Token" whose value is the new token. */
+            IColumn tokenColumn = new Column(SystemTable.token_, p.getTokenFactory().toByteArray(token), oldTokenColumnTimestamp + 1);
+            /* replace the old "Token" column with this new one. */
+            logger_.debug("Replacing old token " + p.getTokenFactory().fromByteArray(columnFamily.getColumn(SystemTable.token_).value()) + " with " + token);
+            columnFamily.addColumn(tokenColumn);
+            reset(systemRow_);
+        }
+    }
+
+    public void reset(Row row) throws IOException
+    {
+        writer_.seek(startPosition_);
+        apply(row);
+    }
+
+    void delete(Row row) throws IOException
+    {
+        throw new UnsupportedOperationException("This operation is not supported for System tables");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Table.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Table.java
index e69de29b..6aa4d4f2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Table.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Table.java
@@ -0,0 +1,946 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.*;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.commons.collections.IteratorUtils;
+import org.apache.commons.collections.Predicate;
+
+import org.apache.cassandra.analytics.DBAnalyticsSource;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.BootstrapInitiateMessage;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.IFileWriter;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.SequenceFile;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.io.IStreamComplete;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.service.CassandraServer;
+import org.apache.cassandra.utils.BasicUtilities;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+*/
+
+public class Table
+{
+    /*
+     * This class represents the metadata of this Table. The metadata
+     * is basically the column family name and the ID associated with
+     * this column family. We use this ID in the Commit Log header to
+     * determine when a log file that has been rolled can be deleted.
+    */
+    public static class TableMetadata
+    {
+        /* Name of the column family */
+        public final static String cfName_ = "TableMetadata";
+        private static ICompactSerializer<TableMetadata> serializer_;
+        static
+        {
+            serializer_ = new TableMetadataSerializer();
+        }
+        
+        private static TableMetadata tableMetadata_;
+        /* Use the following writer/reader to write/read to Metadata table */
+        private static IFileWriter writer_;
+        private static IFileReader reader_;
+        
+        public static Table.TableMetadata instance() throws IOException
+        {
+            if ( tableMetadata_ == null )
+            {
+                String file = getFileName();
+                writer_ = SequenceFile.writer(file);        
+                reader_ = SequenceFile.reader(file);
+                Table.TableMetadata.load();
+                if ( tableMetadata_ == null )
+                    tableMetadata_ = new Table.TableMetadata();
+            }
+            return tableMetadata_;
+        }
+
+        static ICompactSerializer<TableMetadata> serializer()
+        {
+            return serializer_;
+        }
+        
+        private static void load() throws IOException
+        {            
+            String file = Table.TableMetadata.getFileName();
+            File f = new File(file);
+            if ( f.exists() )
+            {
+                DataOutputBuffer bufOut = new DataOutputBuffer();
+                DataInputBuffer bufIn = new DataInputBuffer();
+                
+                if ( reader_ == null )
+                {
+                    reader_ = SequenceFile.reader(file);
+                }
+                
+                while ( !reader_.isEOF() )
+                {
+                    /* Read the metadata info. */
+                    reader_.next(bufOut);
+                    bufIn.reset(bufOut.getData(), bufOut.getLength());
+
+                    /* The key is the table name */
+                    bufIn.readUTF();
+                    /* read the size of the data we ignore this value */
+                    bufIn.readInt();
+                    tableMetadata_ = Table.TableMetadata.serializer().deserialize(bufIn);
+                    break;
+                }        
+            }            
+        }
+        
+        /* The mapping between column family and the column type. */
+        private Map<String, String> cfTypeMap_ = new HashMap<String, String>();
+        private Map<String, Integer> cfIdMap_ = new HashMap<String, Integer>();
+        private Map<Integer, String> idCfMap_ = new HashMap<Integer, String>();        
+        
+        private static String getFileName()
+        {
+            String table = DatabaseDescriptor.getTables().get(0);
+            return DatabaseDescriptor.getMetadataDirectory() + System.getProperty("file.separator") + table + "-Metadata.db";
+        }
+
+        public void add(String cf, int id)
+        {
+            add(cf, id, "Standard");
+        }
+        
+        public void add(String cf, int id, String type)
+        {
+            cfIdMap_.put(cf, id);
+            idCfMap_.put(id, cf);
+            cfTypeMap_.put(cf, type);
+        }
+        
+        public boolean isEmpty()
+        {
+            return cfIdMap_.isEmpty();
+        }
+
+        int getColumnFamilyId(String columnFamily)
+        {
+            return cfIdMap_.get(columnFamily);
+        }
+
+        String getColumnFamilyName(int id)
+        {
+            return idCfMap_.get(id);
+        }
+        
+        String getColumnFamilyType(String cfName)
+        {
+            return cfTypeMap_.get(cfName);
+        }
+
+        Set<String> getColumnFamilies()
+        {
+            return cfIdMap_.keySet();
+        }
+        
+        int size()
+        {
+            return cfIdMap_.size();
+        }
+        
+        boolean isValidColumnFamily(String cfName)
+        {
+            return cfIdMap_.containsKey(cfName);
+        }
+        
+        public void apply() throws IOException
+        {
+            String table = DatabaseDescriptor.getTables().get(0);
+            DataOutputBuffer bufOut = new DataOutputBuffer();
+            Table.TableMetadata.serializer_.serialize(this, bufOut);
+            try
+            {
+                writer_.append(table, bufOut);
+            }
+            catch ( IOException ex )
+            {
+                writer_.seek(0L);
+                logger_.debug(LogUtil.throwableToString(ex));
+            }
+        }
+
+        public String toString()
+        {
+            StringBuilder sb = new StringBuilder("");
+            Set<String> cfNames = cfIdMap_.keySet();
+            
+            for ( String cfName : cfNames )
+            {
+                sb.append(cfName);
+                sb.append("---->");
+                sb.append(cfIdMap_.get(cfName));
+                sb.append(System.getProperty("line.separator"));
+            }
+            
+            return sb.toString();
+        }
+    }
+
+    static class TableMetadataSerializer implements ICompactSerializer<TableMetadata>
+    {
+        public void serialize(TableMetadata tmetadata, DataOutputStream dos) throws IOException
+        {
+            int size = tmetadata.cfIdMap_.size();
+            dos.writeInt(size);
+            Set<String> cfNames = tmetadata.cfIdMap_.keySet();
+
+            for ( String cfName : cfNames )
+            {
+                dos.writeUTF(cfName);
+                dos.writeInt( tmetadata.cfIdMap_.get(cfName).intValue() );
+                dos.writeUTF(tmetadata.getColumnFamilyType(cfName));
+            }            
+        }
+
+        public TableMetadata deserialize(DataInputStream dis) throws IOException
+        {
+            TableMetadata tmetadata = new TableMetadata();
+            int size = dis.readInt();
+            for( int i = 0; i < size; ++i )
+            {
+                String cfName = dis.readUTF();
+                int id = dis.readInt();
+                String type = dis.readUTF();
+                tmetadata.add(cfName, id, type);
+            }            
+            return tmetadata;
+        }
+    }
+
+    /**
+     * This is the callback handler that is invoked when we have
+     * completely been bootstrapped for a single file by a remote host.
+    */
+    public static class BootstrapCompletionHandler implements IStreamComplete
+    {                
+        public void onStreamCompletion(String host, StreamContextManager.StreamContext streamContext, StreamContextManager.StreamStatus streamStatus) throws IOException
+        {                        
+            /* Parse the stream context and the file to the list of SSTables in the associated Column Family Store. */            
+            if (streamContext.getTargetFile().contains("-Data.db"))
+            {
+                File file = new File( streamContext.getTargetFile() );
+                String fileName = file.getName();
+                /*
+                 * If the file is a Data File we need to load the indicies associated
+                 * with this file. We also need to cache the file name in the SSTables
+                 * list of the associated Column Family. Also merge the CBF into the
+                 * sampler.
+                */                
+                SSTable ssTable = new SSTable(streamContext.getTargetFile(), StorageService.getPartitioner());
+                ssTable.close();
+                logger_.debug("Merging the counting bloom filter in the sampler ...");                
+                String[] peices = FBUtilities.strip(fileName, "-");
+                Table.open(peices[0]).getColumnFamilyStore(peices[1]).addToList(streamContext.getTargetFile());                
+            }
+            
+            EndPoint to = new EndPoint(host, DatabaseDescriptor.getStoragePort());
+            logger_.debug("Sending a bootstrap terminate message with " + streamStatus + " to " + to);
+            /* Send a StreamStatusMessage object which may require the source node to re-stream certain files. */
+            StreamContextManager.StreamStatusMessage streamStatusMessage = new StreamContextManager.StreamStatusMessage(streamStatus);
+            Message message = StreamContextManager.StreamStatusMessage.makeStreamStatusMessage(streamStatusMessage);
+            MessagingService.getMessagingInstance().sendOneWay(message, to);           
+        }
+    }
+
+    public static class BootStrapInitiateVerbHandler implements IVerbHandler
+    {
+        /*
+         * Here we handle the BootstrapInitiateMessage. Here we get the
+         * array of StreamContexts. We get file names for the column
+         * families associated with the files and replace them with the
+         * file names as obtained from the column family store on the
+         * receiving end.
+        */
+        public void doVerb(Message message)
+        {
+            byte[] body = (byte[])message.getMessageBody()[0];
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length); 
+            
+            try
+            {
+                BootstrapInitiateMessage biMsg = BootstrapInitiateMessage.serializer().deserialize(bufIn);
+                StreamContextManager.StreamContext[] streamContexts = biMsg.getStreamContext();                
+                
+                Map<String, String> fileNames = getNewNames(streamContexts);
+                /*
+                 * For each of stream context's in the incoming message
+                 * generate the new file names and store the new file names
+                 * in the StreamContextManager.
+                */
+                for (StreamContextManager.StreamContext streamContext : streamContexts )
+                {                    
+                    StreamContextManager.StreamStatus streamStatus = new StreamContextManager.StreamStatus(streamContext.getTargetFile(), streamContext.getExpectedBytes() );
+                    File sourceFile = new File( streamContext.getTargetFile() );
+                    String[] peices = FBUtilities.strip(sourceFile.getName(), "-");
+                    String newFileName = fileNames.get( peices[1] + "-" + peices[2] );
+                    
+                    String file = new String(DatabaseDescriptor.getDataFileLocation() + System.getProperty("file.separator") + newFileName + "-Data.db");
+                    logger_.debug("Received Data from  : " + message.getFrom() + " " + streamContext.getTargetFile() + " " + file);
+                    streamContext.setTargetFile(file);
+                    addStreamContext(message.getFrom().getHost(), streamContext, streamStatus);                                            
+                }    
+                                             
+                StreamContextManager.registerStreamCompletionHandler(message.getFrom().getHost(), new Table.BootstrapCompletionHandler());
+                /* Send a bootstrap initiation done message to execute on default stage. */
+                logger_.debug("Sending a bootstrap initiate done message ...");                
+                Message doneMessage = new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapInitiateDoneVerbHandler_, new Object[]{new byte[0]} );
+                MessagingService.getMessagingInstance().sendOneWay(doneMessage, message.getFrom());
+            }
+            catch ( IOException ex )
+            {
+                logger_.info(LogUtil.throwableToString(ex));
+            }
+        }
+        
+        private Map<String, String> getNewNames(StreamContextManager.StreamContext[] streamContexts)
+        {
+            /* 
+             * Mapping for each file with unique CF-i ---> new file name. For eg.
+             * for a file with name <Table>-<CF>-<i>-Data.db there is a corresponding
+             * <Table>-<CF>-<i>-Index.db. We maintain a mapping from <CF>-<i> to a newly
+             * generated file name.
+            */
+            Map<String, String> fileNames = new HashMap<String, String>();
+            /* Get the distinct entries from StreamContexts i.e have one entry per Data/Index file combination */
+            Set<String> distinctEntries = new HashSet<String>();
+            for ( StreamContextManager.StreamContext streamContext : streamContexts )
+            {
+                String[] peices = FBUtilities.strip(streamContext.getTargetFile(), "-");
+                distinctEntries.add(peices[1] + "-" + peices[2]);
+            }
+            
+            /* Generate unique file names per entry */
+            Table table = Table.open( DatabaseDescriptor.getTables().get(0) );
+            Map<String, ColumnFamilyStore> columnFamilyStores = table.getColumnFamilyStores();
+            
+            for ( String distinctEntry : distinctEntries )
+            {
+                String[] peices = FBUtilities.strip(distinctEntry, "-");
+                ColumnFamilyStore cfStore = columnFamilyStores.get(peices[0]);
+                logger_.debug("Generating file name for " + distinctEntry + " ...");
+                fileNames.put(distinctEntry, cfStore.getNextFileName());
+            }
+            
+            return fileNames;
+        }
+
+        private void addStreamContext(String host, StreamContextManager.StreamContext streamContext, StreamContextManager.StreamStatus streamStatus)
+        {
+            logger_.debug("Adding stream context " + streamContext + " for " + host + " ...");
+            StreamContextManager.addStreamContext(host, streamContext, streamStatus);
+        }
+    }
+    
+    private static Logger logger_ = Logger.getLogger(Table.class);
+    public static final String recycleBin_ = "RecycleColumnFamily";
+    public static final String hints_ = "HintsColumnFamily";
+    
+    /* Used to lock the factory for creation of Table instance */
+    private static Lock createLock_ = new ReentrantLock();
+    private static Map<String, Table> instances_ = new HashMap<String, Table>();
+    /* Table name. */
+    private String table_;
+    /* Handle to the Table Metadata */
+    private Table.TableMetadata tableMetadata_;
+    /* ColumnFamilyStore per column family */
+    private Map<String, ColumnFamilyStore> columnFamilyStores_ = new HashMap<String, ColumnFamilyStore>();
+    /* The AnalyticsSource instance which keeps track of statistics reported to Ganglia. */
+    private DBAnalyticsSource dbAnalyticsSource_;
+    // cache application CFs since Range queries ask for them a _lot_
+    private Set<String> applicationColumnFamilies_;
+
+    public static Table open(String table)
+    {
+        Table tableInstance = instances_.get(table);
+        /*
+         * Read the config and figure the column families for this table.
+         * Set the isConfigured flag so that we do not read config all the
+         * time.
+        */
+        if ( tableInstance == null )
+        {
+            Table.createLock_.lock();
+            try
+            {
+                if ( tableInstance == null )
+                {
+                    tableInstance = new Table(table);
+                    instances_.put(table, tableInstance);
+                }
+            }
+            finally
+            {
+                createLock_.unlock();
+            }
+        }
+        return tableInstance;
+    }
+        
+    public Set<String> getColumnFamilies()
+    {
+        return tableMetadata_.getColumnFamilies();
+    }
+
+    Map<String, ColumnFamilyStore> getColumnFamilyStores()
+    {
+        return columnFamilyStores_;
+    }
+
+    public ColumnFamilyStore getColumnFamilyStore(String cfName)
+    {
+        return columnFamilyStores_.get(cfName);
+    }
+    
+    String getColumnFamilyType(String cfName)
+    {
+        String cfType = null;
+        if ( tableMetadata_ != null )
+          cfType = tableMetadata_.getColumnFamilyType(cfName);
+        return cfType;
+    }
+
+    /*
+     * This method is called to obtain statistics about
+     * the table. It will return statistics about all
+     * the column families that make up this table. 
+    */
+    public String tableStats(String newLineSeparator, java.text.DecimalFormat df)
+    {
+        StringBuilder sb = new StringBuilder();
+        sb.append(table_ + " statistics :");
+        sb.append(newLineSeparator);
+        int oldLength = sb.toString().length();
+        
+        Set<String> cfNames = columnFamilyStores_.keySet();
+        for ( String cfName : cfNames )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get(cfName);
+            sb.append(cfStore.cfStats(newLineSeparator));
+        }
+        int newLength = sb.toString().length();
+        
+        /* Don't show anything if there is nothing to show. */
+        if ( newLength == oldLength )
+            return "";
+        
+        return sb.toString();
+    }
+
+    void onStart() throws IOException
+    {
+        /* Cache the callouts if any */
+        CalloutManager.instance().onStart();
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+            if ( cfStore != null )
+                cfStore.onStart();
+        }         
+    }
+    
+    /** 
+     * Do a cleanup of keys that do not belong locally.
+     */
+    public void doGC()
+    {
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+            if ( cfStore != null )
+                cfStore.forceCleanup();
+        }   
+    }
+    
+    
+    /*
+     * This method is used to ensure that all keys
+     * prior to the specified key, as dtermined by
+     * the SSTable index bucket it falls in, are in
+     * buffer cache.  
+    */
+    public void touch(String key, boolean fData) throws IOException
+    {
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            if ( DatabaseDescriptor.isApplicationColumnFamily(columnFamily) )
+            {
+                ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+                if ( cfStore != null )
+                    cfStore.touch(key, fData);
+            }
+        }
+    }
+
+    /*
+     * Clear the existing snapshots in the system
+     */
+    public void clearSnapshot()
+    {
+    	String snapshotDir = DatabaseDescriptor.getSnapshotDirectory();
+    	File snapshot = new File(snapshotDir);
+    	FileUtils.deleteDir(snapshot);
+    }
+    
+    /*
+     * This method is invoked only during a bootstrap process. We basically
+     * do a complete compaction since we can figure out based on the ranges
+     * whether the files need to be split.
+    */
+    public boolean forceCompaction(List<Range> ranges, EndPoint target, List<String> fileList)
+    {
+        boolean result = true;
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            if ( !isApplicationColumnFamily(columnFamily) )
+                continue;
+            
+            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+            if ( cfStore != null )
+            {
+                /* Counting Bloom Filter for the Column Family */
+                cfStore.forceCompaction(ranges, target, 0, fileList);                
+            }
+        }
+        return result;
+    }
+    
+    /*
+     * This method is an ADMIN operation to force compaction
+     * of all SSTables on disk. 
+    */
+    public void forceCompaction()
+    {
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+            if ( cfStore != null )
+                MinorCompactionManager.instance().submitMajor(cfStore, 0);
+        }
+    }
+
+    /*
+     * Get the list of all SSTables on disk. 
+    */
+    public List<String> getAllSSTablesOnDisk()
+    {
+        List<String> list = new ArrayList<String>();
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+            if ( cfStore != null )
+                list.addAll( cfStore.getAllSSTablesOnDisk() );
+        }
+        return list;
+    }
+
+    private Table(String table)
+    {
+        table_ = table;
+        dbAnalyticsSource_ = new DBAnalyticsSource();
+        try
+        {
+            tableMetadata_ = Table.TableMetadata.instance();
+            Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+            for ( String columnFamily : columnFamilies )
+            {
+                columnFamilyStores_.put(columnFamily, ColumnFamilyStore.getColumnFamilyStore(table, columnFamily));
+            }
+        }
+        catch ( IOException ex )
+        {
+            logger_.info(LogUtil.throwableToString(ex));
+        }
+    }
+
+    String getTableName()
+    {
+        return table_;
+    }
+    
+    boolean isApplicationColumnFamily(String columnFamily)
+    {
+        return DatabaseDescriptor.isApplicationColumnFamily(columnFamily);
+    }
+
+    int getNumberOfColumnFamilies()
+    {
+        return tableMetadata_.size();
+    }
+
+    int getColumnFamilyId(String columnFamily)
+    {
+        return tableMetadata_.getColumnFamilyId(columnFamily);
+    }
+
+    String getColumnFamilyName(int id)
+    {
+        return tableMetadata_.getColumnFamilyName(id);
+    }
+
+    boolean isValidColumnFamily(String columnFamily)
+    {
+        return tableMetadata_.isValidColumnFamily(columnFamily);
+    }
+
+    /**
+     * Selects the row associated with the given key.
+    */
+    public Row get(String key) throws IOException
+    {        
+        Row row = new Row(key);
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        long start = System.currentTimeMillis();
+        for ( String columnFamily : columnFamilies )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get(columnFamily);
+            if ( cfStore != null )
+            {                
+                ColumnFamily cf = cfStore.getColumnFamily(key, columnFamily, new IdentityFilter());                
+                if ( cf != null )
+                    row.addColumnFamily(cf);
+            }
+        }
+        
+        long timeTaken = System.currentTimeMillis() - start;
+        dbAnalyticsSource_.updateReadStatistics(timeTaken);
+        return row;
+    }
+
+
+    /**
+     * Selects the specified column family for the specified key.
+    */
+    public ColumnFamily get(String key, String cf) throws IOException
+    {
+        String[] values = RowMutation.getColumnAndColumnFamily(cf);
+        long start = System.currentTimeMillis();
+        ColumnFamilyStore cfStore = columnFamilyStores_.get(values[0]);
+        assert cfStore != null : "Column family " + cf + " has not been defined";
+        ColumnFamily columnFamily = cfStore.getColumnFamily(key, cf, new IdentityFilter());
+        long timeTaken = System.currentTimeMillis() - start;
+        dbAnalyticsSource_.updateReadStatistics(timeTaken);
+        return columnFamily;
+    }
+
+    /**
+     * Selects only the specified column family for the specified key.
+    */
+    public Row getRow(String key, String cf) throws IOException
+    {
+        Row row = new Row(key);
+        ColumnFamily columnFamily = get(key, cf);
+        if ( columnFamily != null )
+        	row.addColumnFamily(columnFamily);
+        return row;
+    }
+  
+    /**
+     * Selects only the specified column family for the specified key.
+    */
+    public Row getRow(String key, String cf, int start, int count) throws IOException
+    {
+        Row row = new Row(key);
+        String[] values = RowMutation.getColumnAndColumnFamily(cf);
+        ColumnFamilyStore cfStore = columnFamilyStores_.get(values[0]);
+        long start1 = System.currentTimeMillis();
+        assert cfStore != null : "Column family " + cf + " has not been defined";
+        ColumnFamily columnFamily = cfStore.getColumnFamily(key, cf, new IdentityFilter());
+        if ( columnFamily != null )
+        {
+
+            ColumnFamily filteredCf = null;
+            if ((count <=0 || count == Integer.MAX_VALUE) && start <= 0) //Don't need to filter
+            {
+                filteredCf = columnFamily;
+            }
+            else
+            {
+                filteredCf = new CountFilter(count, start).filter(cf, columnFamily);
+            }
+            row.addColumnFamily(filteredCf);
+        }
+        long timeTaken = System.currentTimeMillis() - start1;
+        dbAnalyticsSource_.updateReadStatistics(timeTaken);
+        return row;
+    }
+    
+    public Row getRow(String key, String cf, long sinceTimeStamp) throws IOException
+    {
+        Row row = new Row(key);
+        String[] values = RowMutation.getColumnAndColumnFamily(cf);
+        ColumnFamilyStore cfStore = columnFamilyStores_.get(values[0]);
+        long start1 = System.currentTimeMillis();
+        assert cfStore != null : "Column family " + cf + " has not been defined";
+        ColumnFamily columnFamily = cfStore.getColumnFamily(key, cf, new TimeFilter(sinceTimeStamp));
+        if ( columnFamily != null )
+            row.addColumnFamily(columnFamily);
+        long timeTaken = System.currentTimeMillis() - start1;
+        dbAnalyticsSource_.updateReadStatistics(timeTaken);
+        return row;
+    }
+
+    /**
+     * This method returns the specified columns for the specified
+     * column family.
+     * 
+     *  param @ key - key for which data is requested.
+     *  param @ cf - column family we are interested in.
+     *  param @ columns - columns that are part of the above column family.
+    */
+    public Row getRow(String key, String cf, List<String> columns) throws IOException
+    {
+    	Row row = new Row(key);
+        String[] values = RowMutation.getColumnAndColumnFamily(cf);
+        ColumnFamilyStore cfStore = columnFamilyStores_.get(values[0]);
+
+        if ( cfStore != null )
+        {
+        	ColumnFamily columnFamily = cfStore.getColumnFamily(key, cf, new NamesFilter(new ArrayList<String>(columns)));
+        	if ( columnFamily != null )
+        		row.addColumnFamily(columnFamily);
+        }
+    	return row;
+    }
+
+    /**
+     * This method adds the row to the Commit Log associated with this table.
+     * Once this happens the data associated with the individual column families
+     * is also written to the column family store's memtable.
+    */
+    void apply(Row row) throws IOException
+    {
+        /* Add row to the commit log. */
+        long start = System.currentTimeMillis();
+        CommitLog.CommitLogContext cLogCtx = CommitLog.open(table_).add(row);
+
+        for (ColumnFamily columnFamily : row.getColumnFamilies())
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get(columnFamily.name());
+            cfStore.apply(row.key(), columnFamily, cLogCtx);
+        }
+
+        long timeTaken = System.currentTimeMillis() - start;
+        dbAnalyticsSource_.updateWriteStatistics(timeTaken);
+    }
+
+    void applyNow(Row row) throws IOException
+    {
+        String key = row.key();
+        Map<String, ColumnFamily> columnFamilies = row.getColumnFamilyMap();
+
+        Set<String> cNames = columnFamilies.keySet();
+        for ( String cName : cNames )
+        {
+            ColumnFamily columnFamily = columnFamilies.get(cName);
+            ColumnFamilyStore cfStore = columnFamilyStores_.get(columnFamily.name());
+            cfStore.applyNow( key, columnFamily );
+        }
+    }
+
+    public void flush(boolean fRecovery) throws IOException
+    {
+        Set<String> cfNames = columnFamilyStores_.keySet();
+        for ( String cfName : cfNames )
+        {
+            if (fRecovery) {
+                columnFamilyStores_.get(cfName).flushMemtableOnRecovery();
+            } else {
+                columnFamilyStores_.get(cfName).forceFlush();
+            }
+        }
+    }
+
+    void load(Row row) throws IOException
+    {
+        String key = row.key();
+        /* Add row to the commit log. */
+        long start = System.currentTimeMillis();
+                
+        Map<String, ColumnFamily> columnFamilies = row.getColumnFamilyMap();
+        Set<String> cNames = columnFamilies.keySet();
+        for ( String cName : cNames )
+        {
+        	if( cName.equals(Table.recycleBin_))
+        	{
+	        	ColumnFamily columnFamily = columnFamilies.get(cName);
+	        	Collection<IColumn> columns = columnFamily.getAllColumns();
+        		for(IColumn column : columns)
+        		{
+    	            ColumnFamilyStore cfStore = columnFamilyStores_.get(column.name());
+    	            if(column.timestamp() == 1)
+    	            {
+    	            	cfStore.forceFlushBinary();
+    	            }
+    	            else if(column.timestamp() == 2)
+    	            {
+    	            	cfStore.forceCompaction(null, null, BasicUtilities.byteArrayToLong(column.value()), null);
+    	            }
+    	            else if(column.timestamp() == 3)
+    	            {
+    	            	cfStore.forceFlush();
+    	            }
+    	            else if(column.timestamp() == 4)
+    	            {
+    	            	cfStore.forceCleanup();
+    	            }    	            
+    	            else
+    	            {
+    	            	cfStore.applyBinary(key, column.value());
+    	            }
+        		}
+        	}
+        }
+        row.clear();
+        long timeTaken = System.currentTimeMillis() - start;
+        dbAnalyticsSource_.updateWriteStatistics(timeTaken);
+    }
+
+    public Set<String> getApplicationColumnFamilies()
+    {
+        if (applicationColumnFamilies_ == null)
+        {
+            applicationColumnFamilies_ = new HashSet<String>();
+            for (String cfName : getColumnFamilies())
+            {
+                if (DatabaseDescriptor.isApplicationColumnFamily(cfName))
+                {
+                    applicationColumnFamilies_.add(cfName);
+                }
+            }
+        }
+        return applicationColumnFamilies_;
+    }
+
+    /**
+     * @param startWith key to start with, inclusive.  empty string = start at beginning.
+     * @param stopAt key to stop at, inclusive.  empty string = stop only when keys are exhausted.
+     * @param maxResults
+     * @return list of keys between startWith and stopAt
+     */
+    public List<String> getKeyRange(final String startWith, final String stopAt, int maxResults) throws IOException
+    {
+        // (OPP key decoration is a no-op so using the "decorated" comparator against raw keys is fine)
+        final Comparator<String> comparator = StorageService.getPartitioner().getDecoratedKeyComparator();
+
+        // create a CollatedIterator that will return unique keys from different sources
+        // (current memtable, historical memtables, and SSTables) in the correct order.
+        List<Iterator<String>> iterators = new ArrayList<Iterator<String>>();
+        for (String cfName : getApplicationColumnFamilies())
+        {
+            ColumnFamilyStore cfs = getColumnFamilyStore(cfName);
+
+            // memtable keys: current and historical
+            Iterator<Memtable> memtables = (Iterator<Memtable>) IteratorUtils.chainedIterator(
+                    IteratorUtils.singletonIterator(cfs.getMemtable()),
+                    MemtableManager.instance().getUnflushedMemtables(cfName).iterator());
+            while (memtables.hasNext())
+            {
+                iterators.add(IteratorUtils.filteredIterator(memtables.next().sortedKeyIterator(), new Predicate()
+                {
+                    public boolean evaluate(Object key)
+                    {
+                        String st = (String)key;
+                        return comparator.compare(startWith, st) <= 0 && (stopAt.isEmpty() || comparator.compare(st, stopAt) <= 0);
+                    }
+                }));
+            }
+
+            // sstables
+            for (String filename : cfs.getSSTableFilenames())
+            {
+                FileStruct fs = new FileStruct(SequenceFile.reader(filename), StorageService.getPartitioner());
+                fs.seekTo(startWith);
+                iterators.add(fs);
+            }
+        }
+        Iterator<String> iter = IteratorUtils.collatedIterator(comparator, iterators);
+
+        // pull keys out of the CollatedIterator.  checking tombstone status is expensive,
+        // so we set an arbitrary limit on how many we'll do at once.
+        List<String> keys = new ArrayList<String>();
+        String last = null, current = null;
+        while (keys.size() < maxResults)
+        {
+            if (!iter.hasNext())
+            {
+                break;
+            }
+            current = iter.next();
+            if (!current.equals(last))
+            {
+                if (!stopAt.isEmpty() && comparator.compare(stopAt, current) < 0)
+                {
+                    break;
+                }
+                last = current;
+                // make sure there is actually non-tombstone content associated w/ this key
+                // TODO record the key source(s) somehow and only check that source (e.g., memtable or sstable)
+                for (String cfName : getApplicationColumnFamilies())
+                {
+                    ColumnFamilyStore cfs = getColumnFamilyStore(cfName);
+                    ColumnFamily cf = cfs.getColumnFamily(current, cfName, new IdentityFilter(), Integer.MAX_VALUE);
+                    if (cf != null && cf.getColumns().size() > 0)
+                    {
+                        keys.add(current);
+                        break;
+                    }
+                }
+            }
+        }
+
+        return keys;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TableNotDefinedException.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TableNotDefinedException.java
index e69de29b..e1b6437c 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TableNotDefinedException.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TableNotDefinedException.java
@@ -0,0 +1,11 @@
+package org.apache.cassandra.db;
+
+import org.apache.cassandra.service.InvalidRequestException;
+
+public class TableNotDefinedException extends InvalidRequestException
+{
+    public TableNotDefinedException(String why)
+    {
+        super(why);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TimeFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TimeFilter.java
index e69de29b..a6f1ac21 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TimeFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TimeFilter.java
@@ -0,0 +1,150 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.IndexHelper;
+import org.apache.cassandra.io.SSTable;
+
+
+/**
+ * This class provides a filter for fitering out columns
+ * that are older than a specific time.
+ * 
+ * @author pmalik
+ *
+ */
+class TimeFilter implements IFilter
+{
+	private long timeLimit_;
+	private boolean isDone_;
+	
+	TimeFilter(long timeLimit)
+	{
+		timeLimit_ = timeLimit;		
+		isDone_ = false;
+	}
+
+	public ColumnFamily filter(String cf, ColumnFamily columnFamily)
+	{
+    	if (columnFamily == null)
+    		return columnFamily;
+
+        String[] values = RowMutation.getColumnAndColumnFamily(cf);
+        ColumnFamily filteredCf = new ColumnFamily(columnFamily.name(), columnFamily.type());
+		if (values.length == 1 && !columnFamily.isSuper())
+		{
+    		Collection<IColumn> columns = columnFamily.getAllColumns();
+    		int i =0; 
+    		for(IColumn column : columns)
+    		{
+    			if ( column.timestamp() >=  timeLimit_ )
+    			{
+    				filteredCf.addColumn(column);
+    				++i;
+    			}
+    			else
+    			{
+    				break;
+    			}
+    		}
+    		if( i < columns.size() )
+    		{
+    			isDone_ = true;
+    		}
+		}    	
+    	else if (values.length == 2 && columnFamily.isSuper())
+    	{
+    		/* 
+    		 * TODO : For super columns we need to re-visit this issue.
+    		 * For now this fn will set done to true if we are done with
+    		 * atleast one super column
+    		 */
+    		Collection<IColumn> columns = columnFamily.getAllColumns();
+    		for(IColumn column : columns)
+    		{
+    			SuperColumn superColumn = (SuperColumn)column;
+       			SuperColumn filteredSuperColumn = new SuperColumn(superColumn.name());
+				filteredCf.addColumn(filteredSuperColumn);
+        		Collection<IColumn> subColumns = superColumn.getSubColumns();
+        		int i = 0;
+        		for(IColumn subColumn : subColumns)
+        		{
+	    			if (  subColumn.timestamp()  >=  timeLimit_ )
+	    			{
+			            filteredSuperColumn.addColumn(subColumn.name(), subColumn);
+	    				++i;
+	    			}
+	    			else
+	    			{
+	    				break;
+	    			}
+        		}
+        		if( i < filteredSuperColumn.getColumnCount() )
+        		{
+        			isDone_ = true;
+        		}
+    		}
+    	}
+    	else 
+    	{
+    		throw new UnsupportedOperationException();
+    	}
+		return filteredCf;
+	}
+    
+    public IColumn filter(IColumn column, DataInputStream dis) throws IOException
+    {
+    	long timeStamp = 0;
+    	/*
+    	 * If its a column instance we need the timestamp to verify if 
+    	 * it should be filtered , but at this instance the timestamp is not read
+    	 * so we read the timestamp and set the buffer back so that the rest of desrialization
+    	 * logic does not change.
+    	 */
+    	if(column instanceof Column)
+    	{
+	    	dis.mark(1000);
+	        dis.readBoolean();
+	        timeStamp = dis.readLong();
+		    dis.reset();
+	    	if( timeStamp < timeLimit_ )
+	    	{
+	    		isDone_ = true;
+	    		return null;
+	    	}
+    	}
+    	return column;
+    }
+    
+	
+	public boolean isDone()
+	{
+		return isDone_;
+	}
+
+	public DataInputBuffer next(String key, String cfName, SSTable ssTable) throws IOException
+    {
+    	return ssTable.next( key, cfName, null, new IndexHelper.TimeRange( timeLimit_, Long.MAX_VALUE ) );
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TouchMessage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TouchMessage.java
index e69de29b..52f6d9bd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TouchMessage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TouchMessage.java
@@ -0,0 +1,109 @@
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import javax.xml.bind.annotation.XmlElement;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+
+
+public class TouchMessage
+{
+
+private static ICompactSerializer<TouchMessage> serializer_;	
+	
+    static
+    {
+        serializer_ = new TouchMessageSerializer();
+    }
+
+    static ICompactSerializer<TouchMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    public static Message makeTouchMessage(TouchMessage touchMessage) throws IOException
+    {
+    	ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        TouchMessage.serializer().serialize(touchMessage, dos);
+        Message message = new Message(StorageService.getLocalStorageEndPoint(), StorageService.readStage_, StorageService.touchVerbHandler_, new Object[]{bos.toByteArray()});         
+        return message;
+    }
+    
+    @XmlElement(name="Table")
+    private String table_;
+    
+    @XmlElement(name="Key")
+    private String key_;
+    
+    @XmlElement(name="fData")
+    private boolean fData_ = true;
+        
+    private TouchMessage()
+    {
+    }
+    
+    public TouchMessage(String table, String key)
+    {
+        table_ = table;
+        key_ = key;
+    }
+
+    public TouchMessage(String table, String key, boolean fData)
+    {
+        table_ = table;
+        key_ = key;
+        fData_ = fData;
+    }
+    
+
+    String table()
+    {
+        return table_;
+    }
+    
+    String key()
+    {
+        return key_;
+    }
+
+    public boolean isData()
+    {
+    	return fData_;
+    }
+}
+
+class TouchMessageSerializer implements ICompactSerializer<TouchMessage>
+{
+	public void serialize(TouchMessage tm, DataOutputStream dos) throws IOException
+	{
+		dos.writeUTF(tm.table());
+		dos.writeUTF(tm.key());
+		dos.writeBoolean(tm.isData());
+	}
+	
+    public TouchMessage deserialize(DataInputStream dis) throws IOException
+    {
+		String table = dis.readUTF();
+		String key = dis.readUTF();
+		boolean fData = dis.readBoolean();
+		TouchMessage tm = new TouchMessage( table, key, fData);
+    	return tm;
+    }
+	
+	/**
+	 * @param args
+	 */
+	public static void main(String[] args)
+	{
+		// TODO Auto-generated method stub
+
+	}
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TouchVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TouchVerbHandler.java
index e69de29b..5a2fe15e 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TouchVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TouchVerbHandler.java
@@ -0,0 +1,58 @@
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+public class TouchVerbHandler implements IVerbHandler
+{
+    private static class ReadContext
+    {
+        protected DataInputBuffer bufIn_ = new DataInputBuffer();
+    }
+
+	
+    private static Logger logger_ = Logger.getLogger( ReadVerbHandler.class );
+    /* We use this so that we can reuse the same row mutation context for the mutation. */
+    private static ThreadLocal<ReadContext> tls_ = new InheritableThreadLocal<ReadContext>();
+
+    public void doVerb(Message message)
+    {
+        byte[] body = (byte[])message.getMessageBody()[0];
+        /* Obtain a Read Context from TLS */
+        ReadContext readCtx = tls_.get();
+        if ( readCtx == null )
+        {
+            readCtx = new ReadContext();
+            tls_.set(readCtx);
+        }
+        readCtx.bufIn_.reset(body, body.length);
+
+        try
+        {
+            TouchMessage touchMessage = TouchMessage.serializer().deserialize(readCtx.bufIn_);
+            Table table = Table.open(touchMessage.table());
+   			table.touch(touchMessage.key(), touchMessage.isData());
+        }
+        catch ( IOException ex)
+        {
+            logger_.info( LogUtil.throwableToString(ex) );
+        }
+    }
+	
+	
+	/**
+	 * @param args
+	 */
+	public static void main(String[] args)
+	{
+		// TODO Auto-generated method stub
+
+	}
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TypeInfo.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TypeInfo.java
index e69de29b..d80e9c8a 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TypeInfo.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/TypeInfo.java
@@ -0,0 +1,120 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+public enum TypeInfo
+{
+    BYTE,
+    CHAR,
+    SHORT,
+    INT,
+    LONG,
+    DOUBLE,
+    FLOAT,
+    STRING,
+    BLOB;
+    
+    public static byte toByte(TypeInfo ti)
+    {
+        byte value = 0;
+        switch(ti)
+        {
+            case BYTE:
+                value = 1;
+                break;
+            
+            case CHAR:
+                value = 2;
+                break;
+                
+            case SHORT:
+                value = 3;
+                break;
+                
+            case INT:
+                value = 4;
+                break;
+                
+            case LONG:
+                value = 5;
+                break;
+                
+            case DOUBLE:
+                value = 6;
+                break;
+                
+            case FLOAT:
+                value = 7;
+                break;
+                
+            case STRING:
+                value = 8;
+                break;
+                
+            case BLOB:
+                value = 9;
+                break;
+        }
+        
+        return value;
+    }
+    
+    public static TypeInfo fromByte(byte b)
+    {
+        TypeInfo ti = null;
+        switch(b)
+        {
+            case 1:
+                ti = TypeInfo.BYTE;                
+                break;
+                
+            case 2:
+                ti = TypeInfo.CHAR;
+                break;
+                
+            case 3:
+                ti = TypeInfo.SHORT;
+                break;
+                
+            case 4:
+                ti = TypeInfo.INT;
+                break;
+                
+            case 5:
+                ti = TypeInfo.LONG;
+                break;
+                
+            case 6:
+                ti = TypeInfo.DOUBLE;
+                break;
+                
+            case 7:
+                ti = TypeInfo.FLOAT;
+                break;
+                
+            case 8:
+                ti = TypeInfo.STRING;
+                break;
+                
+            case 9:
+                ti = TypeInfo.BLOB;
+                break;               
+        }
+        return ti;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/WriteResponse.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/WriteResponse.java
index e69de29b..66433389 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/WriteResponse.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/WriteResponse.java
@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+
+import javax.xml.bind.annotation.XmlElement;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+
+
+/*
+ * This message is sent back the row mutation verb handler 
+ * and basically specifes if the write succeeded or not for a particular 
+ * key in a table
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class WriteResponse 
+{
+    private static WriteResponseSerializer serializer_ = new WriteResponseSerializer();
+
+    public static WriteResponseSerializer serializer()
+    {
+        return serializer_;
+    }
+
+    public static Message makeWriteResponseMessage(Message original, WriteResponse writeResponseMessage) throws IOException
+    {
+    	ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        WriteResponse.serializer().serialize(writeResponseMessage, dos);
+        return original.getReply(StorageService.getLocalStorageEndPoint(), bos.toByteArray());
+    }
+
+	private final String table_;
+	private final String key_;
+	private final boolean status_;
+
+	public WriteResponse(String table, String key, boolean bVal) {
+		table_ = table;
+		key_ = key;
+		status_ = bVal;
+	}
+
+	public String table()
+	{
+		return table_;
+	}
+
+	public String key()
+	{
+		return key_;
+	}
+
+	public boolean isSuccess()
+	{
+		return status_;
+	}
+
+    public static class WriteResponseSerializer implements ICompactSerializer<WriteResponse>
+    {
+        public void serialize(WriteResponse wm, DataOutputStream dos) throws IOException
+        {
+            dos.writeUTF(wm.table());
+            dos.writeUTF(wm.key());
+            dos.writeBoolean(wm.isSuccess());
+        }
+
+        public WriteResponse deserialize(DataInputStream dis) throws IOException
+        {
+            String table = dis.readUTF();
+            String key = dis.readUTF();
+            boolean status = dis.readBoolean();
+            return new WriteResponse(table, key, status);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BigIntegerToken.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BigIntegerToken.java
index e69de29b..2615b2d9 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BigIntegerToken.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BigIntegerToken.java
@@ -0,0 +1,16 @@
+package org.apache.cassandra.dht;
+
+import java.math.BigInteger;
+
+public class BigIntegerToken extends Token<BigInteger>
+{
+    public BigIntegerToken(BigInteger token)
+    {
+        super(token);
+    }
+
+    // convenience method for testing
+    public BigIntegerToken(String token) {
+        this(new BigInteger(token));
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootStrapper.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootStrapper.java
index e69de29b..ea3c0cbd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootStrapper.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootStrapper.java
@@ -0,0 +1,130 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+ import java.util.ArrayList;
+ import java.util.Collections;
+ import java.util.HashMap;
+ import java.util.HashSet;
+ import java.util.List;
+ import java.util.Map;
+ import java.util.Set;
+
+ import org.apache.log4j.Logger;
+
+ import org.apache.cassandra.locator.TokenMetadata;
+ import org.apache.cassandra.net.EndPoint;
+ import org.apache.cassandra.service.StorageService;
+ import org.apache.cassandra.utils.LogUtil;
+
+
+/**
+ * This class handles the boostrapping responsibilities for
+ * any new endpoint.
+*/
+public class BootStrapper implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger(BootStrapper.class);
+    /* endpoints that need to be bootstrapped */
+    protected EndPoint[] targets_ = new EndPoint[0];
+    /* tokens of the nodes being bootstapped. */
+    protected final Token[] tokens_;
+    protected TokenMetadata tokenMetadata_ = null;
+    private List<EndPoint> filters_ = new ArrayList<EndPoint>();
+
+    public BootStrapper(EndPoint[] target, Token... token)
+    {
+        targets_ = target;
+        tokens_ = token;
+        tokenMetadata_ = StorageService.instance().getTokenMetadata();
+    }
+    
+    public BootStrapper(EndPoint[] target, Token[] token, EndPoint[] filters)
+    {
+        this(target, token);
+        Collections.addAll(filters_, filters);
+    }
+
+    public void run()
+    {
+        try
+        {
+            logger_.debug("Beginning bootstrap process for " + targets_ + " ...");                                                               
+            /* copy the token to endpoint map */
+            Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+            /* remove the tokens associated with the endpoints being bootstrapped */                
+            for (Token token : tokens_)
+            {
+                tokenToEndPointMap.remove(token);                    
+            }
+
+            Set<Token> oldTokens = new HashSet<Token>( tokenToEndPointMap.keySet() );
+            Range[] oldRanges = StorageService.instance().getAllRanges(oldTokens);
+            logger_.debug("Total number of old ranges " + oldRanges.length);
+            /* 
+             * Find the ranges that are split. Maintain a mapping between
+             * the range being split and the list of subranges.
+            */                
+            Map<Range, List<Range>> splitRanges = LeaveJoinProtocolHelper.getRangeSplitRangeMapping(oldRanges, tokens_);                                                      
+            /* Calculate the list of nodes that handle the old ranges */
+            Map<Range, List<EndPoint>> oldRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(oldRanges, tokenToEndPointMap);
+            /* Mapping of split ranges to the list of endpoints responsible for the range */                
+            Map<Range, List<EndPoint>> replicasForSplitRanges = new HashMap<Range, List<EndPoint>>();                                
+            Set<Range> rangesSplit = splitRanges.keySet();                
+            for ( Range splitRange : rangesSplit )
+            {
+                replicasForSplitRanges.put( splitRange, oldRangeToEndPointMap.get(splitRange) );
+            }                
+            /* Remove the ranges that are split. */
+            for ( Range splitRange : rangesSplit )
+            {
+                oldRangeToEndPointMap.remove(splitRange);
+            }
+            
+            /* Add the subranges of the split range to the map with the same replica set. */
+            for ( Range splitRange : rangesSplit )
+            {
+                List<Range> subRanges = splitRanges.get(splitRange);
+                List<EndPoint> replicas = replicasForSplitRanges.get(splitRange);
+                for ( Range subRange : subRanges )
+                {
+                    /* Make sure we clone or else we are hammered. */
+                    oldRangeToEndPointMap.put(subRange, new ArrayList<EndPoint>(replicas));
+                }
+            }                
+            
+            /* Add the new token and re-calculate the range assignments */
+            Collections.addAll( oldTokens, tokens_ );
+            Range[] newRanges = StorageService.instance().getAllRanges(oldTokens);
+
+            logger_.debug("Total number of new ranges " + newRanges.length);
+            /* Calculate the list of nodes that handle the new ranges */
+            Map<Range, List<EndPoint>> newRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(newRanges);
+            /* Calculate ranges that need to be sent and from whom to where */
+            Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget = LeaveJoinProtocolHelper.getRangeSourceTargetInfo(oldRangeToEndPointMap, newRangeToEndPointMap);
+            /* Send messages to respective folks to stream data over to the new nodes being bootstrapped */
+            LeaveJoinProtocolHelper.assignWork(rangesWithSourceTarget, filters_);                
+        }
+        catch ( Throwable th )
+        {
+            logger_.debug( LogUtil.throwableToString(th) );
+        }
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapInitiateMessage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapInitiateMessage.java
index e69de29b..47383db4 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapInitiateMessage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapInitiateMessage.java
@@ -0,0 +1,99 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.net.io.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class BootstrapInitiateMessage implements Serializable
+{
+    private static ICompactSerializer<BootstrapInitiateMessage> serializer_;
+    
+    static
+    {
+        serializer_ = new BootstrapInitiateMessageSerializer();
+    }
+    
+    public static ICompactSerializer<BootstrapInitiateMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    public static Message makeBootstrapInitiateMessage(BootstrapInitiateMessage biMessage) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        BootstrapInitiateMessage.serializer().serialize(biMessage, dos);
+        return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapInitiateVerbHandler_, new Object[]{bos.toByteArray()} );
+    }
+    
+    protected StreamContextManager.StreamContext[] streamContexts_ = new StreamContextManager.StreamContext[0];
+   
+    public BootstrapInitiateMessage(StreamContextManager.StreamContext[] streamContexts)
+    {
+        streamContexts_ = streamContexts;
+    }
+    
+    public StreamContextManager.StreamContext[] getStreamContext()
+    {
+        return streamContexts_;
+    }
+}
+
+class BootstrapInitiateMessageSerializer implements ICompactSerializer<BootstrapInitiateMessage>
+{
+    public void serialize(BootstrapInitiateMessage bim, DataOutputStream dos) throws IOException
+    {
+        dos.writeInt(bim.streamContexts_.length);
+        for ( StreamContextManager.StreamContext streamContext : bim.streamContexts_ )
+        {
+            StreamContextManager.StreamContext.serializer().serialize(streamContext, dos);
+        }
+    }
+    
+    public BootstrapInitiateMessage deserialize(DataInputStream dis) throws IOException
+    {
+        int size = dis.readInt();
+        StreamContextManager.StreamContext[] streamContexts = new StreamContextManager.StreamContext[0];
+        if ( size > 0 )
+        {
+            streamContexts = new StreamContextManager.StreamContext[size];
+            for ( int i = 0; i < size; ++i )
+            {
+                streamContexts[i] = StreamContextManager.StreamContext.serializer().deserialize(dis);
+            }
+        }
+        return new BootstrapInitiateMessage(streamContexts);
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadata.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadata.java
index e69de29b..34126b24 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadata.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadata.java
@@ -0,0 +1,102 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.CompactEndPointSerializationHelper;
+import org.apache.cassandra.net.EndPoint;
+
+
+
+/**
+ * This encapsulates information of the list of 
+ * ranges that a target node requires in order to 
+ * be bootstrapped. This will be bundled in a 
+ * BootstrapMetadataMessage and sent to nodes that
+ * are going to handoff the data.
+*/
+class BootstrapMetadata
+{
+    private static ICompactSerializer<BootstrapMetadata> serializer_;
+    static
+    {
+        serializer_ = new BootstrapMetadataSerializer();
+    }
+    
+    protected static ICompactSerializer<BootstrapMetadata> serializer()
+    {
+        return serializer_;
+    }
+    
+    protected EndPoint target_;
+    protected List<Range> ranges_;
+    
+    BootstrapMetadata(EndPoint target, List<Range> ranges)
+    {
+        target_ = target;
+        ranges_ = ranges;
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder("");
+        sb.append(target_);
+        sb.append("------->");
+        for ( Range range : ranges_ )
+        {
+            sb.append(range);
+            sb.append(" ");
+        }
+        return sb.toString();
+    }
+}
+
+class BootstrapMetadataSerializer implements ICompactSerializer<BootstrapMetadata>
+{
+    public void serialize(BootstrapMetadata bsMetadata, DataOutputStream dos) throws IOException
+    {
+        CompactEndPointSerializationHelper.serialize(bsMetadata.target_, dos);
+        int size = (bsMetadata.ranges_ == null) ? 0 : bsMetadata.ranges_.size();            
+        dos.writeInt(size);
+        
+        for ( Range range : bsMetadata.ranges_ )
+        {
+            Range.serializer().serialize(range, dos);
+        }            
+    }
+
+    public BootstrapMetadata deserialize(DataInputStream dis) throws IOException
+    {            
+        EndPoint target = CompactEndPointSerializationHelper.deserialize(dis);
+        int size = dis.readInt();
+        List<Range> ranges = (size == 0) ? null : new ArrayList<Range>();
+        for( int i = 0; i < size; ++i )
+        {
+            ranges.add(Range.serializer().deserialize(dis));
+        }            
+        return new BootstrapMetadata( target, ranges );
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadataMessage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadataMessage.java
index e69de29b..8530d65b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadataMessage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadataMessage.java
@@ -0,0 +1,90 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+
+
+
+/**
+ * This class encapsulates the message that needs to be sent
+ * to nodes that handoff data. The message contains information
+ * about the node to be bootstrapped and the ranges with which
+ * it needs to be bootstrapped.
+*/
+class BootstrapMetadataMessage
+{
+    private static ICompactSerializer<BootstrapMetadataMessage> serializer_;
+    static
+    {
+        serializer_ = new BootstrapMetadataMessageSerializer();
+    }
+    
+    protected static ICompactSerializer<BootstrapMetadataMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    protected static Message makeBootstrapMetadataMessage(BootstrapMetadataMessage bsMetadataMessage) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        BootstrapMetadataMessage.serializer().serialize(bsMetadataMessage, dos);
+        return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bsMetadataVerbHandler_, new Object[]{bos.toByteArray()} );            
+    }        
+    
+    protected BootstrapMetadata[] bsMetadata_ = new BootstrapMetadata[0];
+    
+    BootstrapMetadataMessage(BootstrapMetadata[] bsMetadata)
+    {
+        bsMetadata_ = bsMetadata;
+    }
+}
+
+class BootstrapMetadataMessageSerializer implements ICompactSerializer<BootstrapMetadataMessage>
+{
+    public void serialize(BootstrapMetadataMessage bsMetadataMessage, DataOutputStream dos) throws IOException
+    {
+        BootstrapMetadata[] bsMetadata = bsMetadataMessage.bsMetadata_;
+        int size = (bsMetadata == null) ? 0 : bsMetadata.length;
+        dos.writeInt(size);
+        for ( BootstrapMetadata bsmd : bsMetadata )
+        {
+            BootstrapMetadata.serializer().serialize(bsmd, dos);
+        }
+    }
+
+    public BootstrapMetadataMessage deserialize(DataInputStream dis) throws IOException
+    {            
+        int size = dis.readInt();
+        BootstrapMetadata[] bsMetadata = new BootstrapMetadata[size];
+        for ( int i = 0; i < size; ++i )
+        {
+            bsMetadata[i] = BootstrapMetadata.serializer().deserialize(dis);
+        }
+        return new BootstrapMetadataMessage(bsMetadata);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadataVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadataVerbHandler.java
index e69de29b..fafe4655 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadataVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapMetadataVerbHandler.java
@@ -0,0 +1,163 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.service.StreamManager;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * This verb handler handles the BootstrapMetadataMessage that is sent
+ * by the leader to the nodes that are responsible for handing off data. 
+*/
+public class BootstrapMetadataVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(BootstrapMetadataVerbHandler.class);
+    
+    public void doVerb(Message message)
+    {
+        logger_.debug("Received a BootstrapMetadataMessage from " + message.getFrom());
+        byte[] body = (byte[])message.getMessageBody()[0];
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(body, body.length);
+        try
+        {
+            BootstrapMetadataMessage bsMetadataMessage = BootstrapMetadataMessage.serializer().deserialize(bufIn);
+            BootstrapMetadata[] bsMetadata = bsMetadataMessage.bsMetadata_;
+            
+            /*
+             * This is for debugging purposes. Remove later.
+            */
+            for ( BootstrapMetadata bsmd : bsMetadata )
+            {
+                logger_.debug(bsmd.toString());                                      
+            }
+            
+            for ( BootstrapMetadata bsmd : bsMetadata )
+            {
+                long startTime = System.currentTimeMillis();
+                doTransfer(bsmd.target_, bsmd.ranges_);     
+                logger_.debug("Time taken to boostrap " + 
+                        bsmd.target_ + 
+                        " is " + 
+                        (System.currentTimeMillis() - startTime) +
+                        " msecs.");
+            }
+        }
+        catch ( IOException ex )
+        {
+            logger_.info(LogUtil.throwableToString(ex));
+        }
+    }
+    
+    /*
+     * This method needs to figure out the files on disk
+     * locally for each range and then stream them using
+     * the Bootstrap protocol to the target endpoint.
+    */
+    private void doTransfer(EndPoint target, List<Range> ranges) throws IOException
+    {
+        if ( ranges.size() == 0 )
+        {
+            logger_.debug("No ranges to give scram ...");
+            return;
+        }
+        
+        /* Just for debugging process - remove later */            
+        for ( Range range : ranges )
+        {
+            StringBuilder sb = new StringBuilder("");                
+            sb.append(range.toString());
+            sb.append(" ");            
+            logger_.debug("Beginning transfer process to " + target + " for ranges " + sb.toString());                
+        }
+      
+        /*
+         * (1) First we dump all the memtables to disk.
+         * (2) Run a version of compaction which will basically
+         *     put the keys in the range specified into a directory
+         *     named as per the endpoint it is destined for inside the
+         *     bootstrap directory.
+         * (3) Handoff the data.
+        */
+        List<String> tables = DatabaseDescriptor.getTables();
+        for ( String tName : tables )
+        {
+            Table table = Table.open(tName);
+            logger_.debug("Flushing memtables ...");
+            table.flush(false);
+            logger_.debug("Forcing compaction ...");
+            /* Get the counting bloom filter for each endpoint and the list of files that need to be streamed */
+            List<String> fileList = new ArrayList<String>();
+            boolean bVal = table.forceCompaction(ranges, target, fileList);                
+            doHandoff(target, fileList);
+        }
+    }
+
+    /**
+     * Stream the files in the bootstrap directory over to the
+     * node being bootstrapped.
+    */
+    private void doHandoff(EndPoint target, List<String> fileList) throws IOException
+    {
+        List<File> filesList = new ArrayList<File>();
+        for(String file : fileList)
+        {
+            filesList.add(new File(file));
+        }
+        File[] files = filesList.toArray(new File[0]);
+        StreamContextManager.StreamContext[] streamContexts = new StreamContextManager.StreamContext[files.length];
+        int i = 0;
+        for ( File file : files )
+        {
+            streamContexts[i] = new StreamContextManager.StreamContext(file.getAbsolutePath(), file.length());
+            logger_.debug("Stream context metadata " + streamContexts[i]);
+            ++i;
+        }
+        
+        if ( files.length > 0 )
+        {
+            /* Set up the stream manager with the files that need to streamed */
+            StreamManager.instance(target).addFilesToStream(streamContexts);
+            /* Send the bootstrap initiate message */
+            BootstrapInitiateMessage biMessage = new BootstrapInitiateMessage(streamContexts);
+            Message message = BootstrapInitiateMessage.makeBootstrapInitiateMessage(biMessage);
+            logger_.debug("Sending a bootstrap initiate message to " + target + " ...");
+            MessagingService.getMessagingInstance().sendOneWay(message, target);                
+            logger_.debug("Waiting for transfer to " + target + " to complete");
+            StreamManager.instance(target).waitForStreamCompletion();
+            logger_.debug("Done with transfer to " + target);  
+        }
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapSourceTarget.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapSourceTarget.java
index e69de29b..4f70c9fe 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapSourceTarget.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/BootstrapSourceTarget.java
@@ -0,0 +1,49 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * This class encapsulates who is the source and the
+ * target of a bootstrap for a particular range.
+ */
+class BootstrapSourceTarget
+{
+    protected EndPoint source_;
+    protected EndPoint target_;
+    
+    BootstrapSourceTarget(EndPoint source, EndPoint target)
+    {
+        source_ = source;
+        target_ = target;
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder("");
+        sb.append("SOURCE: ");
+        sb.append(source_);
+        sb.append(" ----> ");
+        sb.append("TARGET: ");
+        sb.append(target_);
+        sb.append(" ");
+        return sb.toString();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/IPartitioner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/IPartitioner.java
index e69de29b..5154b6d5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/IPartitioner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/IPartitioner.java
@@ -0,0 +1,48 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.util.Comparator;
+
+public interface IPartitioner
+{
+    /**
+     * transform key to on-disk format s.t. keys are stored in node comparison order.
+     * this lets bootstrap rip out parts of the sstable sequentially instead of doing random seeks.
+     *
+     * @param key the raw, client-facing key
+     * @return decorated on-disk version of key
+     */
+    public String decorateKey(String key);
+
+    public String undecorateKey(String decoratedKey);
+
+    public Comparator<String> getDecoratedKeyComparator();
+
+    public Comparator<String> getReverseDecoratedKeyComparator();
+
+    /**
+     * @return the token to use for this node if none was saved
+     */
+    public Token getInitialToken(String key);
+
+    public Token getDefaultToken();
+
+    public Token.TokenFactory getTokenFactory();
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/LeaveJoinProtocolHelper.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/LeaveJoinProtocolHelper.java
index e69de29b..cbd5ecbb 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/LeaveJoinProtocolHelper.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/LeaveJoinProtocolHelper.java
@@ -0,0 +1,225 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+ import java.io.IOException;
+ import java.util.ArrayList;
+ import java.util.Arrays;
+ import java.util.HashMap;
+ import java.util.List;
+ import java.util.Map;
+ import java.util.Set;
+
+ import org.apache.log4j.Logger;
+
+ import org.apache.cassandra.net.EndPoint;
+ import org.apache.cassandra.net.Message;
+ import org.apache.cassandra.net.MessagingService;
+
+
+class LeaveJoinProtocolHelper
+{
+    private static Logger logger_ = Logger.getLogger(LeaveJoinProtocolHelper.class);
+    
+    /**
+     * Give a range a-------------b which is being split as
+     * a-----x-----y-----b then we want a mapping from 
+     * (a, b] --> (a, x], (x, y], (y, b] 
+    */
+    protected static Map<Range, List<Range>> getRangeSplitRangeMapping(Range[] oldRanges, Token[] allTokens)
+    {
+        Map<Range, List<Range>> splitRanges = new HashMap<Range, List<Range>>();
+        Token[] tokens = new Token[allTokens.length];
+        System.arraycopy(allTokens, 0, tokens, 0, tokens.length);
+        Arrays.sort(tokens);
+        
+        Range prevRange = null;
+        Token prevToken = null;
+        boolean bVal = false;
+        
+        for ( Range oldRange : oldRanges )
+        {
+            if (bVal)
+            {
+                bVal = false; 
+                List<Range> subRanges = splitRanges.get(prevRange);
+                if ( subRanges != null )
+                    subRanges.add( new Range(prevToken, prevRange.right()) );     
+            }
+            
+            prevRange = oldRange;
+            prevToken = oldRange.left();                
+            for (Token token : tokens)
+            {     
+                List<Range> subRanges = splitRanges.get(oldRange);
+                if ( oldRange.contains(token) )
+                {                        
+                    if ( subRanges == null )
+                    {
+                        subRanges = new ArrayList<Range>();
+                        splitRanges.put(oldRange, subRanges);
+                    }                            
+                    subRanges.add( new Range(prevToken, token) );
+                    prevToken = token;
+                    bVal = true;
+                }
+                else
+                {
+                    if ( bVal )
+                    {
+                        bVal = false;                                                                                
+                        subRanges.add( new Range(prevToken, oldRange.right()) );                            
+                    }
+                }
+            }
+        }
+        /* This is to handle the last range being processed. */
+        if ( bVal )
+        {
+            bVal = false; 
+            List<Range> subRanges = splitRanges.get(prevRange);
+            subRanges.add( new Range(prevToken, prevRange.right()) );                            
+        }
+        return splitRanges;
+    }
+    
+    protected static Map<Range, List<BootstrapSourceTarget>> getRangeSourceTargetInfo(Map<Range, List<EndPoint>> oldRangeToEndPointMap, Map<Range, List<EndPoint>> newRangeToEndPointMap)
+    {
+        Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget = new HashMap<Range, List<BootstrapSourceTarget>>();
+        /*
+         * Basically calculate for each range the endpoints handling the
+         * range in the old token set and in the new token set. Whoever
+         * gets bumped out of the top N will have to hand off that range
+         * to the new dude.
+        */
+        Set<Range> oldRangeSet = oldRangeToEndPointMap.keySet();
+        for(Range range : oldRangeSet)
+        {
+            logger_.debug("Attempting to figure out the dudes who are bumped out for " + range + " ...");
+            List<EndPoint> oldEndPoints = oldRangeToEndPointMap.get(range);
+            List<EndPoint> newEndPoints = newRangeToEndPointMap.get(range);
+            if ( newEndPoints != null )
+            {                        
+                List<EndPoint> newEndPoints2 = new ArrayList<EndPoint>(newEndPoints);
+                for ( EndPoint newEndPoint : newEndPoints2 )
+                {
+                    if ( oldEndPoints.contains(newEndPoint) )
+                    {
+                        oldEndPoints.remove(newEndPoint);
+                        newEndPoints.remove(newEndPoint);
+                    }
+                }                        
+            }
+            else
+            {
+                logger_.warn("Trespassing - scram");
+            }
+            logger_.debug("Done figuring out the dudes who are bumped out for range " + range + " ...");
+        }
+        for ( Range range : oldRangeSet )
+        {                    
+            List<EndPoint> oldEndPoints = oldRangeToEndPointMap.get(range);
+            List<EndPoint> newEndPoints = newRangeToEndPointMap.get(range);
+            List<BootstrapSourceTarget> srcTarget = rangesWithSourceTarget.get(range);
+            if ( srcTarget == null )
+            {
+                srcTarget = new ArrayList<BootstrapSourceTarget>();
+                rangesWithSourceTarget.put(range, srcTarget);
+            }
+            int i = 0;
+            for ( EndPoint oldEndPoint : oldEndPoints )
+            {                        
+                srcTarget.add( new BootstrapSourceTarget(oldEndPoint, newEndPoints.get(i++)) );
+            }
+        }
+        return rangesWithSourceTarget;
+    }
+    
+    /**
+     * This method sends messages out to nodes instructing them 
+     * to stream the specified ranges to specified target nodes. 
+    */
+    protected static void assignWork(Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget) throws IOException
+    {
+        assignWork(rangesWithSourceTarget, null);
+    }
+    
+    /**
+     * This method sends messages out to nodes instructing them 
+     * to stream the specified ranges to specified target nodes. 
+    */
+    protected static void assignWork(Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget, List<EndPoint> filters) throws IOException
+    {
+        /*
+         * Map whose key is the source node and the value is a map whose key is the
+         * target and value is the list of ranges to be sent to it. 
+        */
+        Map<EndPoint, Map<EndPoint, List<Range>>> rangeInfo = new HashMap<EndPoint, Map<EndPoint, List<Range>>>();
+        Set<Range> ranges = rangesWithSourceTarget.keySet();
+        
+        for ( Range range : ranges )
+        {
+            List<BootstrapSourceTarget> rangeSourceTargets = rangesWithSourceTarget.get(range);
+            for ( BootstrapSourceTarget rangeSourceTarget : rangeSourceTargets )
+            {
+                Map<EndPoint, List<Range>> targetRangeMap = rangeInfo.get(rangeSourceTarget.source_);
+                if ( targetRangeMap == null )
+                {
+                    targetRangeMap = new HashMap<EndPoint, List<Range>>();
+                    rangeInfo.put(rangeSourceTarget.source_, targetRangeMap);
+                }
+                List<Range> rangesToGive = targetRangeMap.get(rangeSourceTarget.target_);
+                if ( rangesToGive == null )
+                {
+                    rangesToGive = new ArrayList<Range>();
+                    targetRangeMap.put(rangeSourceTarget.target_, rangesToGive);
+                }
+                rangesToGive.add(range);
+            }
+        }
+        
+        Set<EndPoint> sources = rangeInfo.keySet();
+        for ( EndPoint source : sources )
+        {
+            /* only send the message to the nodes that are in the filter. */
+            if ( filters != null && filters.size() > 0 && !filters.contains(source) )
+            {
+                logger_.debug("Filtering endpoint " + source + " as source ...");
+                continue;
+            }
+            
+            Map<EndPoint, List<Range>> targetRangesMap = rangeInfo.get(source);
+            Set<EndPoint> targets = targetRangesMap.keySet();
+            List<BootstrapMetadata> bsmdList = new ArrayList<BootstrapMetadata>();
+            
+            for ( EndPoint target : targets )
+            {
+                List<Range> rangeForTarget = targetRangesMap.get(target);
+                BootstrapMetadata bsMetadata = new BootstrapMetadata(target, rangeForTarget);
+                bsmdList.add(bsMetadata);
+            }
+            
+            BootstrapMetadataMessage bsMetadataMessage = new BootstrapMetadataMessage(bsmdList.toArray( new BootstrapMetadata[0] ) );
+            /* Send this message to the source to do his shit. */
+            Message message = BootstrapMetadataMessage.makeBootstrapMetadataMessage(bsMetadataMessage); 
+            logger_.debug("Sending the BootstrapMetadataMessage to " + source);
+            MessagingService.getMessagingInstance().sendOneWay(message, source);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/LeaveJoinProtocolImpl.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/LeaveJoinProtocolImpl.java
index e69de29b..97e06527 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/LeaveJoinProtocolImpl.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/LeaveJoinProtocolImpl.java
@@ -0,0 +1,290 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+ import java.util.ArrayList;
+ import java.util.Collections;
+ import java.util.HashMap;
+ import java.util.HashSet;
+ import java.util.List;
+ import java.util.Map;
+ import java.util.Set;
+
+ import org.apache.log4j.Logger;
+
+ import org.apache.cassandra.locator.TokenMetadata;
+ import org.apache.cassandra.net.EndPoint;
+ import org.apache.cassandra.service.StorageService;
+ import org.apache.cassandra.utils.LogUtil;
+
+
+/**
+ * This class performs the exact opposite of the
+ * operations of the Bootstrapper class. Given 
+ * a bunch of nodes that need to move it determines 
+ * who they need to hand off data in terms of ranges.
+*/
+public class LeaveJoinProtocolImpl implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger(LeaveJoinProtocolImpl.class);    
+    
+    /* endpoints that are to be moved. */
+    protected EndPoint[] targets_ = new EndPoint[0];
+    /* position where they need to be moved */
+    protected final Token[] tokens_;
+    /* token metadata information */
+    protected TokenMetadata tokenMetadata_ = null;
+
+    public LeaveJoinProtocolImpl(EndPoint[] targets, Token[] tokens)
+    {
+        targets_ = targets;
+        tokens_ = tokens;
+        tokenMetadata_ = StorageService.instance().getTokenMetadata();
+    }
+
+    public void run()
+    {  
+        try
+        {
+            logger_.debug("Beginning leave/join process for ...");                                                               
+            /* copy the token to endpoint map */
+            Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+            /* copy the endpoint to token map */
+            Map<EndPoint, Token> endpointToTokenMap = tokenMetadata_.cloneEndPointTokenMap();
+            
+            Set<Token> oldTokens = new HashSet<Token>( tokenToEndPointMap.keySet() );
+            Range[] oldRanges = StorageService.instance().getAllRanges(oldTokens);
+            logger_.debug("Total number of old ranges " + oldRanges.length);
+            /* Calculate the list of nodes that handle the old ranges */
+            Map<Range, List<EndPoint>> oldRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(oldRanges);
+            
+            /* Remove the tokens of the nodes leaving the ring */
+            Set<Token> tokens = getTokensForLeavingNodes();
+            oldTokens.removeAll(tokens);
+            Range[] rangesAfterNodesLeave = StorageService.instance().getAllRanges(oldTokens);
+            /* Get expanded range to initial range mapping */
+            Map<Range, List<Range>> expandedRangeToOldRangeMap = getExpandedRangeToOldRangeMapping(oldRanges, rangesAfterNodesLeave);
+            /* add the new token positions to the old tokens set */
+            for (Token token : tokens_)
+                oldTokens.add(token);
+            Range[] rangesAfterNodesJoin = StorageService.instance().getAllRanges(oldTokens);
+            /* replace the ranges that were split with the split ranges in the old configuration */
+            addSplitRangesToOldConfiguration(oldRangeToEndPointMap, rangesAfterNodesJoin);
+            
+            /* Re-calculate the new ranges after the new token positions are added */
+            Range[] newRanges = StorageService.instance().getAllRanges(oldTokens);
+            /* Remove the old locations from tokenToEndPointMap and add the new locations they are moving to */
+            for ( int i = 0; i < targets_.length; ++i )
+            {
+                tokenToEndPointMap.remove( endpointToTokenMap.get(targets_[i]) );
+                tokenToEndPointMap.put(tokens_[i], targets_[i]);
+            }            
+            /* Calculate the list of nodes that handle the new ranges */            
+            Map<Range, List<EndPoint>> newRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(newRanges, tokenToEndPointMap);
+            /* Remove any expanded ranges and replace them with ranges whose aggregate is the expanded range in the new configuration. */
+            removeExpandedRangesFromNewConfiguration(newRangeToEndPointMap, expandedRangeToOldRangeMap);
+            /* Calculate ranges that need to be sent and from whom to where */
+            Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget = LeaveJoinProtocolHelper.getRangeSourceTargetInfo(oldRangeToEndPointMap, newRangeToEndPointMap);
+            /* For debug purposes only */
+            Set<Range> ranges = rangesWithSourceTarget.keySet();
+            for ( Range range : ranges )
+            {
+                System.out.print("RANGE: " + range + ":: ");
+                List<BootstrapSourceTarget> infos = rangesWithSourceTarget.get(range);
+                for ( BootstrapSourceTarget info : infos )
+                {
+                    System.out.print(info);
+                    System.out.print(" ");
+                }
+                System.out.println(System.getProperty("line.separator"));
+            }
+            /* Send messages to respective folks to stream data over to the new nodes being bootstrapped */
+            LeaveJoinProtocolHelper.assignWork(rangesWithSourceTarget);
+        }
+        catch ( Throwable th )
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }
+    }
+    
+    /**
+     * This method figures out the ranges that have been split and
+     * replaces them with the split range.
+     * @param oldRangeToEndPointMap old range mapped to their replicas.
+     * @param rangesAfterNodesJoin ranges after the nodes have joined at
+     *        their respective position.
+     */
+    private void addSplitRangesToOldConfiguration(Map<Range, List<EndPoint>> oldRangeToEndPointMap, Range[] rangesAfterNodesJoin)
+    {
+        /* 
+         * Find the ranges that are split. Maintain a mapping between
+         * the range being split and the list of subranges.
+        */                
+        Map<Range, List<Range>> splitRanges = LeaveJoinProtocolHelper.getRangeSplitRangeMapping(oldRangeToEndPointMap.keySet().toArray( new Range[0] ), tokens_);
+        /* Mapping of split ranges to the list of endpoints responsible for the range */                
+        Map<Range, List<EndPoint>> replicasForSplitRanges = new HashMap<Range, List<EndPoint>>();                                
+        Set<Range> rangesSplit = splitRanges.keySet();                
+        for ( Range splitRange : rangesSplit )
+        {
+            replicasForSplitRanges.put( splitRange, oldRangeToEndPointMap.get(splitRange) );
+        }
+        /* Remove the ranges that are split. */
+        for ( Range splitRange : rangesSplit )
+        {
+            oldRangeToEndPointMap.remove(splitRange);
+        }
+        
+        /* Add the subranges of the split range to the map with the same replica set. */
+        for ( Range splitRange : rangesSplit )
+        {
+            List<Range> subRanges = splitRanges.get(splitRange);
+            List<EndPoint> replicas = replicasForSplitRanges.get(splitRange);
+            for ( Range subRange : subRanges )
+            {
+                /* Make sure we clone or else we are hammered. */
+                oldRangeToEndPointMap.put(subRange, new ArrayList<EndPoint>(replicas));
+            }
+        }
+    }
+    
+    /**
+     * Reset the newRangeToEndPointMap and replace the expanded range
+     * with the ranges whose aggregate is the expanded range. This happens
+     * only when nodes leave the ring to migrate to a different position.
+     * 
+     * @param newRangeToEndPointMap all new ranges mapped to the replicas 
+     *        responsible for those ranges.
+     * @param expandedRangeToOldRangeMap mapping between the expanded ranges
+     *        and the ranges whose aggregate is the expanded range.
+     */
+    private void removeExpandedRangesFromNewConfiguration(Map<Range, List<EndPoint>> newRangeToEndPointMap, Map<Range, List<Range>> expandedRangeToOldRangeMap)
+    {
+        /* Get the replicas for the expanded ranges */
+        Map<Range, List<EndPoint>> replicasForExpandedRanges = new HashMap<Range, List<EndPoint>>();
+        Set<Range> expandedRanges = expandedRangeToOldRangeMap.keySet();
+        for ( Range expandedRange : expandedRanges )
+        {            
+            replicasForExpandedRanges.put( expandedRange, newRangeToEndPointMap.get(expandedRange) );
+            newRangeToEndPointMap.remove(expandedRange);            
+        }
+        /* replace the expanded ranges in the newRangeToEndPointMap with the subRanges */
+        for ( Range expandedRange : expandedRanges )
+        {
+            List<Range> subRanges = expandedRangeToOldRangeMap.get(expandedRange);
+            List<EndPoint> replicas = replicasForExpandedRanges.get(expandedRange);          
+            for ( Range subRange : subRanges )
+            {
+                newRangeToEndPointMap.put(subRange, new ArrayList<EndPoint>(replicas));
+            }
+        }        
+    }
+    
+    private Set<Token> getTokensForLeavingNodes()
+    {
+        Set<Token> tokens = new HashSet<Token>();
+        for ( EndPoint target : targets_ )
+        {
+            tokens.add(tokenMetadata_.getToken(target));
+        }        
+        return tokens;
+    }
+    
+    /**
+     * Here we are removing the nodes that need to leave the
+     * ring and trying to calculate what the ranges would look
+     * like w/o them. For eg if we remove two nodes A and D from
+     * the ring and the order of nodes on the ring is A, B, C
+     * and D. When B is removed the range of C is the old range 
+     * of C and the old range of B. We want a mapping from old
+     * range of B to new range of B. We have 
+     * A----B----C----D----E----F----G and we remove b and e
+     * then we want a mapping from (a, c] --> (a,b], (b, c] and 
+     * (d, f] --> (d, e], (d,f].
+     * @param oldRanges ranges with the previous configuration
+     * @param newRanges ranges with the target endpoints removed.
+     * @return map of expanded range to the list whose aggregate is
+     *             the expanded range.
+     */
+    protected static Map<Range, List<Range>> getExpandedRangeToOldRangeMapping(Range[] oldRanges, Range[] newRanges)
+    {
+        Map<Range, List<Range>> map = new HashMap<Range, List<Range>>();   
+        List<Range> oRanges = new ArrayList<Range>();
+        Collections.addAll(oRanges, oldRanges);
+        List<Range> nRanges = new ArrayList<Range>();
+        Collections.addAll(nRanges, newRanges);
+        
+        /*
+         * Remove the ranges that are the same. 
+         * Now we will be left with the expanded 
+         * ranges in the nRanges list and the 
+         * smaller ranges in the oRanges list. 
+        */
+        for( Range oRange : oldRanges )
+        {            
+            boolean bVal = nRanges.remove(oRange);
+            if ( bVal )
+                oRanges.remove(oRange);
+        }
+        
+        int nSize = nRanges.size();
+        int oSize = oRanges.size();
+        /*
+         * Establish the mapping between expanded ranges
+         * to the smaller ranges whose aggregate is the
+         * expanded range. 
+        */
+        for ( int i = 0; i < nSize; ++i )
+        {
+            Range nRange = nRanges.get(i);
+            for ( int j = 0; j < oSize; ++j )
+            {
+                Range oRange = oRanges.get(j);
+                if ( nRange.contains(oRange.right()) )
+                {
+                    List<Range> smallerRanges = map.get(nRange);
+                    if ( smallerRanges == null )
+                    {
+                        smallerRanges = new ArrayList<Range>();
+                        map.put(nRange, smallerRanges);
+                    }
+                    smallerRanges.add(oRange);
+                    continue;
+                }
+            }
+        }
+        
+        return map;
+    }
+
+    public static void main(String[] args) throws Throwable
+    {
+        StorageService ss = StorageService.instance();
+        ss.updateTokenMetadata(new BigIntegerToken("3"), new EndPoint("A", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("6"), new EndPoint("B", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("9"), new EndPoint("C", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("12"), new EndPoint("D", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("15"), new EndPoint("E", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("18"), new EndPoint("F", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("21"), new EndPoint("G", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("24"), new EndPoint("H", 7000));
+        
+        Runnable runnable = new LeaveJoinProtocolImpl( new EndPoint[]{new EndPoint("C", 7000), new EndPoint("D", 7000)}, new Token[]{new BigIntegerToken("22"), new BigIntegerToken("23")} );
+        runnable.run();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
index e69de29b..07c84563 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
@@ -0,0 +1,117 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.UnsupportedEncodingException;
+import java.text.Collator;
+import java.util.Comparator;
+import java.util.Locale;
+import java.util.Random;
+
+public class OrderPreservingPartitioner implements IPartitioner
+{
+    // TODO make locale configurable.  But don't just leave it up to the OS or you could really screw
+    // people over if they deploy on nodes with different OS locales.
+    static final Collator collator = Collator.getInstance(new Locale("en", "US")); 
+
+    private static final Comparator<String> comparator = new Comparator<String>() {
+        public int compare(String o1, String o2)
+        {
+            return collator.compare(o1, o2);
+        }
+    };
+    private static final Comparator<String> reverseComparator = new Comparator<String>() {
+        public int compare(String o1, String o2)
+        {
+            return -comparator.compare(o1, o2);
+        }
+    };
+
+    public String decorateKey(String key)
+    {
+        return key;
+    }
+
+    public String undecorateKey(String decoratedKey)
+    {
+        return decoratedKey;
+    }
+
+    public Comparator<String> getDecoratedKeyComparator()
+    {
+        return comparator;
+    }
+
+    public Comparator<String> getReverseDecoratedKeyComparator()
+    {
+        return reverseComparator;
+    }
+
+    public StringToken getDefaultToken()
+    {
+        String chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";
+        Random r = new Random();
+        StringBuffer buffer = new StringBuffer();
+        for (int j = 0; j < 16; j++) {
+            buffer.append(chars.charAt(r.nextInt(chars.length())));
+        }
+        return new StringToken(buffer.toString());
+    }
+
+    private final Token.TokenFactory<String> tokenFactory = new Token.TokenFactory<String>() {
+        public byte[] toByteArray(Token<String> stringToken)
+        {
+            try
+            {
+                return stringToken.token.getBytes("UTF-8");
+            }
+            catch (UnsupportedEncodingException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+
+        public Token<String> fromByteArray(byte[] bytes)
+        {
+            try
+            {
+                return new StringToken(new String(bytes, "UTF-8"));
+            }
+            catch (UnsupportedEncodingException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+
+        public Token<String> fromString(String string)
+        {
+            return new StringToken(string);
+        }
+    };
+
+    public Token.TokenFactory<String> getTokenFactory()
+    {
+        return tokenFactory;
+    }
+
+    public Token getInitialToken(String key)
+    {
+        return new StringToken(key);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/RandomPartitioner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/RandomPartitioner.java
index e69de29b..f5556db4 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/RandomPartitioner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/RandomPartitioner.java
@@ -0,0 +1,110 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.math.BigInteger;
+import java.util.Comparator;
+
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.GuidGenerator;
+
+/**
+ * This class generates a BigIntegerToken using MD5 hash.
+ */
+public class RandomPartitioner implements IPartitioner
+{
+    private static final Comparator<String> comparator = new Comparator<String>()
+    {
+        public int compare(String o1, String o2)
+        {
+            String[] split1 = o1.split(":", 2);
+            String[] split2 = o2.split(":", 2);
+            BigInteger i1 = new BigInteger(split1[0]);
+            BigInteger i2 = new BigInteger(split2[0]);
+            int v = i1.compareTo(i2);
+            if (v != 0) {
+                return v;
+            }
+            return split1[1].compareTo(split2[1]);
+        }
+    };
+    private static final Comparator<String> rcomparator = new Comparator<String>()
+    {
+        public int compare(String o1, String o2)
+        {
+            return -comparator.compare(o1, o2);
+        }
+    };
+
+    public String decorateKey(String key)
+    {
+        return FBUtilities.hash(key).toString() + ":" + key;
+    }
+
+    public String undecorateKey(String decoratedKey)
+    {
+        return decoratedKey.split(":", 2)[1];
+    }
+
+    public Comparator<String> getDecoratedKeyComparator()
+    {
+        return comparator;
+    }
+
+    public Comparator<String> getReverseDecoratedKeyComparator()
+    {
+        return rcomparator;
+    }
+
+    public BigIntegerToken getDefaultToken()
+    {
+        String guid = GuidGenerator.guid();
+        BigInteger token = FBUtilities.hash(guid);
+        if ( token.signum() == -1 )
+            token = token.multiply(BigInteger.valueOf(-1L));
+        return new BigIntegerToken(token);
+    }
+
+    private final Token.TokenFactory<BigInteger> tokenFactory = new Token.TokenFactory<BigInteger>() {
+        public byte[] toByteArray(Token<BigInteger> bigIntegerToken)
+        {
+            return bigIntegerToken.token.toByteArray();
+        }
+
+        public Token<BigInteger> fromByteArray(byte[] bytes)
+        {
+            return new BigIntegerToken(new BigInteger(bytes));
+        }
+
+        public Token<BigInteger> fromString(String string)
+        {
+            return new BigIntegerToken(new BigInteger(string));
+        }
+    };
+
+    public Token.TokenFactory<BigInteger> getTokenFactory()
+    {
+        return tokenFactory;
+    }
+
+    public Token getInitialToken(String key)
+    {
+        return new BigIntegerToken(FBUtilities.hash(key));
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/Range.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/Range.java
index e69de29b..7500eb3b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/Range.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/Range.java
@@ -0,0 +1,182 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.List;
+import java.math.BigInteger;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.service.StorageService;
+
+
+/**
+ * A representation of the range that a node is responsible for on the DHT ring.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Range implements Comparable<Range>
+{
+    private static ICompactSerializer<Range> serializer_;
+    static
+    {
+        serializer_ = new RangeSerializer();
+    }
+    
+    public static ICompactSerializer<Range> serializer()
+    {
+        return serializer_;
+    }
+    
+    private final Token left_;
+    private final Token right_;
+
+    public Range(Token left, Token right)
+    {
+        left_ = left;
+        right_ = right;
+    }
+
+    /**
+     * Returns the left endpoint of a range.
+     * @return left endpoint
+     */
+    public Token left()
+    {
+        return left_;
+    }
+    
+    /**
+     * Returns the right endpoint of a range.
+     * @return right endpoint
+     */
+    public Token right()
+    {
+        return right_;
+    }
+
+    /**
+     * Helps determine if a given point on the DHT ring is contained
+     * in the range in question.
+     * @param bi point in question
+     * @return true if the point contains within the range else false.
+     */
+    public boolean contains(Token bi)
+    {
+        if ( left_.compareTo(right_) > 0 )
+        {
+            /* 
+             * left is greater than right we are wrapping around.
+             * So if the interval is [a,b) where a > b then we have
+             * 3 cases one of which holds for any given token k.
+             * (1) k > a -- return true
+             * (2) k < b -- return true
+             * (3) b < k < a -- return false
+            */
+            if ( bi.compareTo(left_) >= 0 )
+                return true;
+            else return right_.compareTo(bi) > 0;
+        }
+        else if ( left_.compareTo(right_) < 0 )
+        {
+            /*
+             * This is the range [a, b) where a < b. 
+            */
+            return ( bi.compareTo(left_) >= 0 && right_.compareTo(bi) >=0 );
+        }        
+        else
+    	{
+    		return true;
+    	}    	
+    }
+
+    /**
+     * Tells if the given range is a wrap around.
+     * @param range
+     * @return
+     */
+    private static boolean isWrapAround(Range range)
+    {
+        return range.left_.compareTo(range.right_) > 0;
+    }
+    
+    public int compareTo(Range rhs)
+    {
+        /* 
+         * If the range represented by the "this" pointer
+         * is a wrap around then it is the smaller one.
+        */
+        if ( isWrapAround(this) )
+            return -1;
+        
+        if ( isWrapAround(rhs) )
+            return 1;
+        
+        return right_.compareTo(rhs.right_);
+    }
+    
+
+    public static boolean isTokenInRanges(Token token, List<Range> ranges)
+    {
+        assert ranges != null;
+
+        for (Range range : ranges)
+        {
+            if(range.contains(token))
+            {
+                return true;
+            }
+        }
+        return false;
+    }
+
+    public boolean equals(Object o)
+    {
+        if ( !(o instanceof Range) )
+            return false;
+        Range rhs = (Range)o;
+        return left_.equals(rhs.left_) && right_.equals(rhs.right_);
+    }
+    
+    public int hashCode()
+    {
+        return toString().hashCode();
+    }
+    
+    public String toString()
+    {
+        return "(" + left_ + "," + right_ + "]";
+    }
+}
+
+class RangeSerializer implements ICompactSerializer<Range>
+{
+    public void serialize(Range range, DataOutputStream dos) throws IOException
+    {
+        Token.serializer().serialize(range.left(), dos);
+        Token.serializer().serialize(range.right(), dos);
+    }
+
+    public Range deserialize(DataInputStream dis) throws IOException
+    {
+        return new Range(Token.serializer().deserialize(dis), Token.serializer().deserialize(dis));
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/StringToken.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/StringToken.java
index e69de29b..5b6ebe71 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/StringToken.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/StringToken.java
@@ -0,0 +1,14 @@
+package org.apache.cassandra.dht;
+
+public class StringToken extends Token<String>
+{
+    public StringToken(String token)
+    {
+        super(token);
+    }
+
+    public int compareTo(Token<String> o)
+    {
+        return OrderPreservingPartitioner.collator.compare(this.token, o.token);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/Token.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/Token.java
index e69de29b..03e3c334 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/Token.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/dht/Token.java
@@ -0,0 +1,77 @@
+package org.apache.cassandra.dht;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.service.StorageService;
+
+public abstract class Token<T extends Comparable> implements Comparable<Token<T>>
+{
+    private static final TokenSerializer serializer = new TokenSerializer();
+    public static TokenSerializer serializer()
+    {
+        return serializer;
+    }
+
+    T token;
+
+    protected Token(T token)
+    {
+        this.token = token;
+    }
+
+    /**
+     * This determines the comparison for node destination purposes.
+     */
+    public int compareTo(Token<T> o)
+    {
+        return token.compareTo(o.token);
+    }
+
+    public String toString()
+    {
+        return "Token(" + token + ")";
+    }
+
+    public boolean equals(Object obj)
+    {
+        if (!(obj instanceof Token)) {
+            return false;
+        }
+        return token.equals(((Token)obj).token);
+    }
+
+    public int hashCode()
+    {
+        return token.hashCode();
+    }
+
+    public static abstract class TokenFactory<T extends Comparable>
+    {
+        public abstract byte[] toByteArray(Token<T> token);
+        public abstract Token<T> fromByteArray(byte[] bytes);
+        public abstract Token<T> fromString(String string);
+    }
+
+    public static class TokenSerializer implements ICompactSerializer<Token>
+    {
+        public void serialize(Token token, DataOutputStream dos) throws IOException
+        {
+            IPartitioner p = StorageService.getPartitioner();
+            byte[] b = p.getTokenFactory().toByteArray(token);
+            dos.writeInt(b.length);
+            dos.write(b);
+        }
+
+        public Token deserialize(DataInputStream dis) throws IOException
+        {
+            IPartitioner p = StorageService.getPartitioner();
+            int size = dis.readInt();
+            byte[] bytes = new byte[size];
+            dis.readFully(bytes);
+            return p.getTokenFactory().fromByteArray(bytes);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/ApplicationState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/ApplicationState.java
index e69de29b..fc14a431 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/ApplicationState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/ApplicationState.java
@@ -0,0 +1,104 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.IFileWriter;
+
+
+/**
+ * This abstraction represents the state associated with a particular node which an
+ * application wants to make available to the rest of the nodes in the cluster. 
+ * Whenever a peice of state needs to be disseminated to the rest of cluster wrap
+ * the state in an instance of <i>ApplicationState</i> and add it to the Gossiper.
+ *  
+ * For eg. if we want to disseminate load information for node A do the following:
+ * 
+ *      ApplicationState loadState = new ApplicationState(<string reprensentation of load>);
+ *      Gossiper.instance().addApplicationState("LOAD STATE", loadState);
+ *  
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ApplicationState
+{
+    private static ICompactSerializer<ApplicationState> serializer_;
+    static
+    {
+        serializer_ = new ApplicationStateSerializer();
+    }
+    
+    int version_;
+    String state_;
+
+        
+    ApplicationState(String state, int version)
+    {
+        state_ = state;
+        version_ = version;
+    }
+
+    public static ICompactSerializer<ApplicationState> serializer()
+    {
+        return serializer_;
+    }
+    
+    /**
+     * Wraps the specified state into a ApplicationState instance.
+     * @param state string representation of arbitrary state.
+     */
+    public ApplicationState(String state)
+    {
+        state_ = state;
+        version_ = VersionGenerator.getNextVersion();
+    }
+        
+    public String getState()
+    {
+        return state_;
+    }
+    
+    int getStateVersion()
+    {
+        return version_;
+    }
+}
+
+class ApplicationStateSerializer implements ICompactSerializer<ApplicationState>
+{
+    public void serialize(ApplicationState appState, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(appState.state_);
+        dos.writeInt(appState.version_);
+    }
+
+    public ApplicationState deserialize(DataInputStream dis) throws IOException
+    {
+        String state = dis.readUTF();
+        int version = dis.readInt();
+        return new ApplicationState(state, version);
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/EndPointState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/EndPointState.java
index e69de29b..6df30386 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/EndPointState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/EndPointState.java
@@ -0,0 +1,186 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.IFileWriter;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
+
+/**
+ * This abstraction represents both the HeartBeatState and the ApplicationState in an EndPointState
+ * instance. Any state for a given endpoint can be retrieved from this instance.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class EndPointState
+{
+    private static ICompactSerializer<EndPointState> serializer_;
+    static
+    {
+        serializer_ = new EndPointStateSerializer();
+    }
+    
+    HeartBeatState hbState_;
+    Map<String, ApplicationState> applicationState_ = new Hashtable<String, ApplicationState>();
+    
+    /* fields below do not get serialized */
+    long updateTimestamp_;
+    boolean isAlive_;
+    boolean isAGossiper_;
+
+    public static ICompactSerializer<EndPointState> serializer()
+    {
+        return serializer_;
+    }
+    
+    EndPointState(HeartBeatState hbState) 
+    { 
+        hbState_ = hbState; 
+        updateTimestamp_ = System.currentTimeMillis(); 
+        isAlive_ = true; 
+        isAGossiper_ = false;
+    }
+        
+    HeartBeatState getHeartBeatState()
+    {
+        return hbState_;
+    }
+    
+    synchronized void setHeartBeatState(HeartBeatState hbState)
+    {
+        updateTimestamp();
+        hbState_ = hbState;
+    }
+    
+    public ApplicationState getApplicationState(String key)
+    {
+        return applicationState_.get(key);
+    }
+    
+    public Map<String, ApplicationState> getApplicationState()
+    {
+        return applicationState_;
+    }
+    
+    void addApplicationState(String key, ApplicationState appState)
+    {        
+        applicationState_.put(key, appState);        
+    }
+
+    /* getters and setters */
+    long getUpdateTimestamp()
+    {
+        return updateTimestamp_;
+    }
+    
+    synchronized void updateTimestamp()
+    {
+        updateTimestamp_ = System.currentTimeMillis();
+    }
+    
+    public boolean isAlive()
+    {        
+        return isAlive_;
+    }
+
+    synchronized void isAlive(boolean value)
+    {        
+        isAlive_ = value;        
+    }
+
+    
+    boolean isAGossiper()
+    {        
+        return isAGossiper_;
+    }
+
+    synchronized void isAGossiper(boolean value)
+    {                
+        //isAlive_ = false;
+        isAGossiper_ = value;        
+    }
+}
+
+class EndPointStateSerializer implements ICompactSerializer<EndPointState>
+{
+    private static Logger logger_ = Logger.getLogger(EndPointStateSerializer.class);
+    
+    public void serialize(EndPointState epState, DataOutputStream dos) throws IOException
+    {
+        /* These are for estimating whether we overshoot the MTU limit */
+        int estimate = 0;
+
+        /* serialize the HeartBeatState */
+        HeartBeatState hbState = epState.getHeartBeatState();
+        HeartBeatState.serializer().serialize(hbState, dos);
+
+        /* serialize the map of ApplicationState objects */
+        int size = epState.applicationState_.size();
+        dos.writeInt(size);
+        if ( size > 0 )
+        {   
+            Set<String> keys = epState.applicationState_.keySet();
+            for( String key : keys )
+            {
+                if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
+                {
+                    logger_.info("@@@@ Breaking out to respect the MTU size in EndPointState serializer. Estimate is " + estimate + " @@@@");
+                    break;
+                }
+            
+                ApplicationState appState = epState.applicationState_.get(key);
+                if ( appState != null )
+                {
+                    int pre = dos.size();
+                    dos.writeUTF(key);
+                    ApplicationState.serializer().serialize(appState, dos);                    
+                    int post = dos.size();
+                    estimate = post - pre;
+                }                
+            }
+        }
+    }
+
+    public EndPointState deserialize(DataInputStream dis) throws IOException
+    {
+        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis);
+        EndPointState epState = new EndPointState(hbState);               
+
+        int appStateSize = dis.readInt();
+        for ( int i = 0; i < appStateSize; ++i )
+        {
+            if ( dis.available() == 0 )
+            {
+                break;
+            }
+            
+            String key = dis.readUTF();    
+            ApplicationState appState = ApplicationState.serializer().deserialize(dis);            
+            epState.addApplicationState(key, appState);            
+        }
+        return epState;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/FailureDetector.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/FailureDetector.java
index e69de29b..4614cfe6 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/FailureDetector.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/FailureDetector.java
@@ -0,0 +1,340 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.FileOutputStream;
+import java.lang.management.ManagementFactory;
+import java.net.UnknownHostException;
+import java.util.*;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+
+import org.apache.commons.lang.StringUtils;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * This FailureDetector is an implementation of the paper titled
+ * "The Phi Accrual Failure Detector" by Hayashibara. 
+ * Check the paper and the <i>IFailureDetector</i> interface for details.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class FailureDetector implements IFailureDetector, FailureDetectorMBean
+{
+    private static Logger logger_ = Logger.getLogger(FailureDetector.class);
+    private static final int sampleSize_ = 1000;
+    private static final int phiSuspectThreshold_ = 5;
+    private static final int phiConvictThreshold_ = 8;
+    /* The Failure Detector has to have been up for atleast 1 min. */
+    private static final long uptimeThreshold_ = 60000;
+    private static IFailureDetector failureDetector_;
+    /* Used to lock the factory for creation of FailureDetector instance */
+    private static Lock createLock_ = new ReentrantLock();
+    /* The time when the module was instantiated. */
+    private static long creationTime_;
+    
+    public static IFailureDetector instance()
+    {        
+        if ( failureDetector_ == null )
+        {
+            FailureDetector.createLock_.lock();
+            try
+            {
+                if ( failureDetector_ == null )
+                {
+                    failureDetector_ = new FailureDetector();
+                }
+            }
+            finally
+            {
+                createLock_.unlock();
+            }
+        }        
+        return failureDetector_;
+    }
+    
+    private Map<EndPoint, ArrivalWindow> arrivalSamples_ = new Hashtable<EndPoint, ArrivalWindow>();
+    private List<IFailureDetectionEventListener> fdEvntListeners_ = new ArrayList<IFailureDetectionEventListener>();
+    
+    public FailureDetector()
+    {
+        creationTime_ = System.currentTimeMillis();
+        // Register this instance with JMX
+        try
+        {
+            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
+            mbs.registerMBean(this, new ObjectName("org.apache.cassandra.gms:type=FailureDetector"));
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+        }
+    }
+    
+    /**
+     * Dump the inter arrival times for examination if necessary.
+     */
+    public void dumpInterArrivalTimes()
+    {
+        try
+        {
+            FileOutputStream fos = new FileOutputStream("/var/tmp/output-" + System.currentTimeMillis() + ".dat", true);
+            fos.write(toString().getBytes());
+            fos.close();
+        }
+        catch(Throwable th)
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }
+    }
+    
+    /**
+     * We dump the arrival window for any endpoint only if the 
+     * local Failure Detector module has been up for more than a 
+     * minute.
+     * 
+     * @param ep for which the arrival window needs to be dumped.
+     */
+    private void dumpInterArrivalTimes(EndPoint ep)
+    {
+        long now = System.currentTimeMillis();
+        if ( (now - FailureDetector.creationTime_) <= FailureDetector.uptimeThreshold_ )
+            return;
+        try
+        {
+            FileOutputStream fos = new FileOutputStream("/var/tmp/output-" + System.currentTimeMillis() + "-" + ep + ".dat", true);
+            ArrivalWindow hWnd = arrivalSamples_.get(ep);
+            fos.write(hWnd.toString().getBytes());
+            fos.close();
+        }
+        catch(Throwable th)
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }
+    }
+    
+    public boolean isAlive(EndPoint ep)
+    {
+        try
+        {
+            /* If the endpoint in question is the local endpoint return true. */
+            String localHost = FBUtilities.getHostAddress();
+            if ( localHost.equals( ep.getHost() ) )
+                    return true;
+        }
+        catch( UnknownHostException ex )
+        {
+            logger_.info( LogUtil.throwableToString(ex) );
+        }
+    	/* Incoming port is assumed to be the Storage port. We need to change it to the control port */
+    	EndPoint ep2 = new EndPoint(ep.getHost(), DatabaseDescriptor.getControlPort());        
+        EndPointState epState = Gossiper.instance().getEndPointStateForEndPoint(ep2);
+        return epState.isAlive();
+    }
+    
+    public void report(EndPoint ep)
+    {
+        long now = System.currentTimeMillis();
+        ArrivalWindow hbWnd = arrivalSamples_.get(ep);
+        if ( hbWnd == null )
+        {
+            hbWnd = new ArrivalWindow(sampleSize_);
+            arrivalSamples_.put(ep, hbWnd);
+        }
+        hbWnd.add(now);  
+    }
+    
+    public void intepret(EndPoint ep)
+    {
+        ArrivalWindow hbWnd = arrivalSamples_.get(ep);
+        if ( hbWnd == null )
+        {            
+            return;
+        }
+        long now = System.currentTimeMillis();
+        /* We need this so that we do not suspect a convict. */
+        boolean isConvicted = false;
+        double phi = hbWnd.phi(now);
+        logger_.trace("PHI for " + ep + " : " + phi);
+        
+        /*
+        if ( phi > phiConvictThreshold_ )
+        {            
+            isConvicted = true;     
+            for ( IFailureDetectionEventListener listener : fdEvntListeners_ )
+            {
+                listener.convict(ep);                
+            }
+        }
+        */
+        if ( !isConvicted && phi > phiSuspectThreshold_ )
+        {     
+            for ( IFailureDetectionEventListener listener : fdEvntListeners_ )
+            {
+                listener.suspect(ep);
+            }
+        }        
+    }
+    
+    public void registerFailureDetectionEventListener(IFailureDetectionEventListener listener)
+    {
+        fdEvntListeners_.add(listener);
+    }
+    
+    public void unregisterFailureDetectionEventListener(IFailureDetectionEventListener listener)
+    {
+        fdEvntListeners_.remove(listener);
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder();
+        Set<EndPoint> eps = arrivalSamples_.keySet();
+        
+        sb.append("-----------------------------------------------------------------------");
+        for ( EndPoint ep : eps )
+        {
+            ArrivalWindow hWnd = arrivalSamples_.get(ep);
+            sb.append(ep + " : ");
+            sb.append(hWnd.toString());
+            sb.append( System.getProperty("line.separator") );
+        }
+        sb.append("-----------------------------------------------------------------------");
+        return sb.toString();
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {           
+    }
+}
+
+class ArrivalWindow
+{
+    private static Logger logger_ = Logger.getLogger(ArrivalWindow.class);
+    private double tLast_ = 0L;
+    private Deque<Double> arrivalIntervals_;
+    private int size_;
+    
+    ArrivalWindow(int size)
+    {
+        size_ = size;
+        arrivalIntervals_ = new ArrayDeque<Double>(size);
+    }
+    
+    synchronized void add(double value)
+    {
+        if ( arrivalIntervals_.size() == size_ )
+        {                          
+            arrivalIntervals_.remove();            
+        }
+        
+        double interArrivalTime;
+        if ( tLast_ > 0L )
+        {                        
+            interArrivalTime = (value - tLast_);            
+        }
+        else
+        {
+            interArrivalTime = Gossiper.intervalInMillis_ / 2;
+        }
+        tLast_ = value;            
+        arrivalIntervals_.add(interArrivalTime);        
+    }
+    
+    synchronized double sum()
+    {
+        double sum = 0d;
+        for (Double interval : arrivalIntervals_)
+        {
+            sum += interval;
+        }
+        return sum;
+    }
+    
+    synchronized double sumOfDeviations()
+    {
+        double sumOfDeviations = 0d;
+        double mean = mean();
+
+        for (Double interval : arrivalIntervals_)
+        {
+            double v = interval - mean;
+            sumOfDeviations += v * v;
+        }
+
+        return sumOfDeviations;
+    }
+    
+    synchronized double mean()
+    {
+        return sum()/arrivalIntervals_.size();
+    }
+    
+    synchronized double variance()
+    {                
+        return sumOfDeviations() / (arrivalIntervals_.size());        
+    }
+    
+    double deviation()
+    {        
+        return Math.sqrt(variance());
+    }
+    
+    void clear()
+    {
+        arrivalIntervals_.clear();
+    }
+    
+    double p(double t)
+    {
+        // Stat stat = new Stat();
+        double mean = mean();        
+        double deviation = deviation();   
+        /* Exponential CDF = 1 -e^-lambda*x */
+        double exponent = (-1)*(t)/mean;
+        return 1 - ( 1 - Math.pow(Math.E, exponent) );
+        // return stat.gaussianCDF(mean, deviation, t, Double.POSITIVE_INFINITY);             
+    }
+    
+    double phi(long tnow)
+    {            
+        int size = arrivalIntervals_.size();
+        double log = 0d;
+        if ( size > 0 )
+        {
+            double t = tnow - tLast_;                
+            double probability = p(t);       
+            log = (-1) * Math.log10( probability );                                 
+        }
+        return log;           
+    } 
+    
+    public String toString()
+    {
+        return StringUtils.join(arrivalIntervals_, " ");
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/FailureDetectorMBean.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/FailureDetectorMBean.java
index e69de29b..3c9f7e5a 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/FailureDetectorMBean.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/FailureDetectorMBean.java
@@ -0,0 +1,24 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+public interface FailureDetectorMBean
+{
+    public void dumpInterArrivalTimes();
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigest.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigest.java
index e69de29b..0f62a452 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigest.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigest.java
@@ -0,0 +1,110 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.CompactEndPointSerializationHelper;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.*;
+
+/**
+ * Contains information about a specified list of EndPoints and the largest version 
+ * of the state they have generated as known by the local endpoint.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class GossipDigest implements Comparable<GossipDigest>
+{
+    private static ICompactSerializer<GossipDigest> serializer_;
+    static
+    {
+        serializer_ = new GossipDigestSerializer();
+    }
+    
+    EndPoint endPoint_;
+    int generation_;
+    int maxVersion_;
+
+    public static ICompactSerializer<GossipDigest> serializer()
+    {
+        return serializer_;
+    }
+    
+    GossipDigest(EndPoint endPoint, int generation, int maxVersion)
+    {
+        endPoint_ = endPoint;
+        generation_ = generation; 
+        maxVersion_ = maxVersion;
+    }
+    
+    EndPoint getEndPoint()
+    {
+        return endPoint_;
+    }
+    
+    int getGeneration()
+    {
+        return generation_;
+    }
+    
+    int getMaxVersion()
+    {
+        return maxVersion_;
+    }
+    
+    public int compareTo(GossipDigest gDigest)
+    {
+        if ( generation_ != gDigest.generation_ )
+            return ( generation_ - gDigest.generation_ );
+        return (maxVersion_ - gDigest.maxVersion_);
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder();
+        sb.append(endPoint_);
+        sb.append(":");
+        sb.append(generation_);
+        sb.append(":");
+        sb.append(maxVersion_);
+        return sb.toString();
+    }
+}
+
+class GossipDigestSerializer implements ICompactSerializer<GossipDigest>
+{       
+    public void serialize(GossipDigest gDigest, DataOutputStream dos) throws IOException
+    {        
+        CompactEndPointSerializationHelper.serialize(gDigest.endPoint_, dos);
+        dos.writeInt(gDigest.generation_);
+        dos.writeInt(gDigest.maxVersion_);
+    }
+
+    public GossipDigest deserialize(DataInputStream dis) throws IOException
+    {
+        EndPoint endPoint = CompactEndPointSerializationHelper.deserialize(dis);
+        int generation = dis.readInt();
+        int version = dis.readInt();
+        return new GossipDigest(endPoint, generation, version);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestAck2Message.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestAck2Message.java
index e69de29b..4ba023ee 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestAck2Message.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestAck2Message.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.*;
+
+
+/**
+ * This message gets sent out as a result of the receipt of a GossipDigestAckMessage. This the 
+ * last stage of the 3 way messaging of the Gossip protocol.
+ *  
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class GossipDigestAck2Message
+{
+    private static  ICompactSerializer<GossipDigestAck2Message> serializer_;
+    static
+    {
+        serializer_ = new GossipDigestAck2MessageSerializer();
+    }
+    
+    Map<EndPoint, EndPointState> epStateMap_ = new HashMap<EndPoint, EndPointState>();
+
+    public static ICompactSerializer<GossipDigestAck2Message> serializer()
+    {
+        return serializer_;
+    }
+    
+    GossipDigestAck2Message(Map<EndPoint, EndPointState> epStateMap)
+    {
+        epStateMap_ = epStateMap;
+    }
+        
+    Map<EndPoint, EndPointState> getEndPointStateMap()
+    {
+         return epStateMap_;
+    }
+}
+
+class GossipDigestAck2MessageSerializer implements ICompactSerializer<GossipDigestAck2Message>
+{
+    public void serialize(GossipDigestAck2Message gDigestAck2Message, DataOutputStream dos) throws IOException
+    {
+        /* Use the EndPointState */
+        EndPointStatesSerializationHelper.serialize(gDigestAck2Message.epStateMap_, dos);
+    }
+
+    public GossipDigestAck2Message deserialize(DataInputStream dis) throws IOException
+    {
+        Map<EndPoint, EndPointState> epStateMap = EndPointStatesSerializationHelper.deserialize(dis);
+        return new GossipDigestAck2Message(epStateMap);        
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestAckMessage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestAckMessage.java
index e69de29b..53501c0b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestAckMessage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestAckMessage.java
@@ -0,0 +1,102 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+
+
+
+/**
+ * This message gets sent out as a result of the receipt of a GossipDigestSynMessage by an
+ * endpoint. This is the 2 stage of the 3 way messaging in the Gossip protocol.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class GossipDigestAckMessage
+{
+    private static ICompactSerializer<GossipDigestAckMessage> serializer_;
+    static
+    {
+        serializer_ = new GossipDigestAckMessageSerializer();
+    }
+    
+    List<GossipDigest> gDigestList_ = new ArrayList<GossipDigest>();
+    Map<EndPoint, EndPointState> epStateMap_ = new HashMap<EndPoint, EndPointState>();
+    
+    static ICompactSerializer<GossipDigestAckMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    GossipDigestAckMessage(List<GossipDigest> gDigestList, Map<EndPoint, EndPointState> epStateMap)
+    {
+        gDigestList_ = gDigestList;
+        epStateMap_ = epStateMap;
+    }
+    
+    void addGossipDigest(EndPoint ep, int generation, int version)
+    {
+        gDigestList_.add( new GossipDigest(ep, generation, version) );
+    }
+    
+    List<GossipDigest> getGossipDigestList()
+    {
+        return gDigestList_;
+    }
+    
+    Map<EndPoint, EndPointState> getEndPointStateMap()
+    {
+        return epStateMap_;
+    }
+}
+
+class GossipDigestAckMessageSerializer implements ICompactSerializer<GossipDigestAckMessage>
+{
+    public void serialize(GossipDigestAckMessage gDigestAckMessage, DataOutputStream dos) throws IOException
+    {
+        /* Use the helper to serialize the GossipDigestList */
+        boolean bContinue = GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList_, dos);
+        dos.writeBoolean(bContinue);
+        /* Use the EndPointState */
+        if ( bContinue )
+        {
+            EndPointStatesSerializationHelper.serialize(gDigestAckMessage.epStateMap_, dos);            
+        }
+    }
+
+    public GossipDigestAckMessage deserialize(DataInputStream dis) throws IOException
+    {
+        Map<EndPoint, EndPointState> epStateMap = new HashMap<EndPoint, EndPointState>();
+        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis);                
+        boolean bContinue = dis.readBoolean();
+
+        if ( bContinue )
+        {
+            epStateMap = EndPointStatesSerializationHelper.deserialize(dis);                                    
+        }
+        return new GossipDigestAckMessage(gDigestList, epStateMap);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestSynMessage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestSynMessage.java
index e69de29b..96ddb302 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestSynMessage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/GossipDigestSynMessage.java
@@ -0,0 +1,184 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.CompactEndPointSerializationHelper;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.utils.Log4jLogger;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
+
+
+/**
+ * This is the first message that gets sent out as a start of the Gossip protocol in a 
+ * round. 
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class GossipDigestSynMessage
+{
+    private static ICompactSerializer<GossipDigestSynMessage> serializer_;
+    static
+    {
+        serializer_ = new GossipDigestSynMessageSerializer();
+    }
+    
+    String clusterId_;
+    List<GossipDigest> gDigests_ = new ArrayList<GossipDigest>();
+
+    public static ICompactSerializer<GossipDigestSynMessage> serializer()
+    {
+        return serializer_;
+    }
+ 
+    public GossipDigestSynMessage(String clusterId, List<GossipDigest> gDigests)
+    {      
+        clusterId_ = clusterId;
+        gDigests_ = gDigests;
+    }
+    
+    List<GossipDigest> getGossipDigests()
+    {
+        return gDigests_;
+    }
+}
+
+class GossipDigestSerializationHelper
+{
+    private static Logger logger_ = Logger.getLogger(GossipDigestSerializationHelper.class);
+    
+    static boolean serialize(List<GossipDigest> gDigestList, DataOutputStream dos) throws IOException
+    {
+        boolean bVal = true;
+        int size = gDigestList.size();                        
+        dos.writeInt(size);
+        
+        int estimate = 0;            
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
+            {
+                logger_.info("@@@@ Breaking out to respect the MTU size in GD @@@@");
+                bVal = false;
+                break;
+            }
+            int pre = dos.size();               
+            GossipDigest.serializer().serialize( gDigest, dos );
+            int post = dos.size();
+            estimate = post - pre;
+        }
+        return bVal;
+    }
+
+    static List<GossipDigest> deserialize(DataInputStream dis) throws IOException
+    {
+        int size = dis.readInt();            
+        List<GossipDigest> gDigests = new ArrayList<GossipDigest>();
+        
+        for ( int i = 0; i < size; ++i )
+        {
+            if ( dis.available() == 0 )
+            {
+                logger_.info("Remaining bytes zero. Stopping deserialization of GossipDigests.");
+                break;
+            }
+                            
+            GossipDigest gDigest = GossipDigest.serializer().deserialize(dis);                
+            gDigests.add( gDigest );                
+        }        
+        return gDigests;
+    }
+}
+
+class EndPointStatesSerializationHelper
+{
+    private static Log4jLogger logger_ = new Log4jLogger(EndPointStatesSerializationHelper.class.getName());
+    
+    static boolean serialize(Map<EndPoint, EndPointState> epStateMap, DataOutputStream dos) throws IOException
+    {
+        boolean bVal = true;
+        int estimate = 0;                
+        int size = epStateMap.size();
+        dos.writeInt(size);
+    
+        Set<EndPoint> eps = epStateMap.keySet();
+        for( EndPoint ep : eps )
+        {
+            if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
+            {
+                logger_.info("@@@@ Breaking out to respect the MTU size in EPS. Estimate is " + estimate + " @@@@");
+                bVal = false;
+                break;
+            }
+    
+            int pre = dos.size();
+            CompactEndPointSerializationHelper.serialize(ep, dos);
+            EndPointState epState = epStateMap.get(ep);            
+            EndPointState.serializer().serialize(epState, dos);
+            int post = dos.size();
+            estimate = post - pre;
+        }
+        return bVal;
+    }
+
+    static Map<EndPoint, EndPointState> deserialize(DataInputStream dis) throws IOException
+    {
+        int size = dis.readInt();            
+        Map<EndPoint, EndPointState> epStateMap = new HashMap<EndPoint, EndPointState>();
+        
+        for ( int i = 0; i < size; ++i )
+        {
+            if ( dis.available() == 0 )
+            {
+                logger_.info("Remaining bytes zero. Stopping deserialization in EndPointState.");
+                break;
+            }
+            // int length = dis.readInt();            
+            EndPoint ep = CompactEndPointSerializationHelper.deserialize(dis);
+            EndPointState epState = EndPointState.serializer().deserialize(dis);            
+            epStateMap.put(ep, epState);
+        }        
+        return epStateMap;
+    }
+}
+
+class GossipDigestSynMessageSerializer implements ICompactSerializer<GossipDigestSynMessage>
+{   
+    public void serialize(GossipDigestSynMessage gDigestSynMessage, DataOutputStream dos) throws IOException
+    {    
+        dos.writeUTF(gDigestSynMessage.clusterId_);
+        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests_, dos);
+    }
+
+    public GossipDigestSynMessage deserialize(DataInputStream dis) throws IOException
+    {
+        String clusterId = dis.readUTF();
+        List<GossipDigest> gDigests = GossipDigestSerializationHelper.deserialize(dis);
+        return new GossipDigestSynMessage(clusterId, gDigests);
+    }
+
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/Gossiper.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/Gossiper.java
index e69de29b..c2ccbc5c 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/Gossiper.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/Gossiper.java
@@ -0,0 +1,1126 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.*;
+import java.util.*;
+import java.net.InetAddress;
+
+import org.apache.cassandra.concurrent.SingleThreadedStage;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.IComponentShutdown;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
+import org.apache.cassandra.net.*;
+
+/**
+ * This module is responsible for Gossiping information for the local endpoint. This abstraction
+ * maintains the list of live and dead endpoints. Periodically i.e. every 1 second this module
+ * chooses a random node and initiates a round of Gossip with it. A round of Gossip involves 3
+ * rounds of messaging. For instance if node A wants to initiate a round of Gossip with node B
+ * it starts off by sending node B a GossipDigestSynMessage. Node B on receipt of this message
+ * sends node A a GossipDigestAckMessage. On receipt of this message node A sends node B a
+ * GossipDigestAck2Message which completes a round of Gossip. This module as and when it hears one
+ * of the three above mentioned messages updates the Failure Detector with the liveness information.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Gossiper implements IFailureDetectionEventListener, IEndPointStateChangePublisher, IComponentShutdown
+{
+    private class GossipTimerTask extends TimerTask
+    {
+        public void run()
+        {
+            try
+            {
+                synchronized( Gossiper.instance() )
+                {
+                	/* Update the local heartbeat counter. */
+                    endPointStateMap_.get(localEndPoint_).getHeartBeatState().updateHeartBeat();
+                    List<GossipDigest> gDigests = new ArrayList<GossipDigest>();
+                    Gossiper.instance().makeRandomGossipDigest(gDigests);
+
+                    if ( gDigests.size() > 0 )
+                    {
+                        Message message = makeGossipDigestSynMessage(gDigests);
+                        /* Gossip to some random live member */
+                        boolean bVal = doGossipToLiveMember(message);
+
+                        /* Gossip to some unreachable member with some probability to check if he is back up */
+                        doGossipToUnreachableMember(message);
+
+                        /* Gossip to the seed. */
+                        if ( !bVal )
+                            doGossipToSeed(message);
+
+                        logger_.trace("Performing status check ...");
+                        doStatusCheck();
+                    }
+                }
+            }
+            catch ( Throwable th )
+            {
+                logger_.info( LogUtil.throwableToString(th) );
+            }
+        }
+    }
+
+    final static int MAX_GOSSIP_PACKET_SIZE = 1428;
+    /* GS - abbreviation for GOSSIPER_STAGE */
+    final static String GOSSIP_STAGE = "GS";
+    /* GSV - abbreviation for GOSSIP-DIGEST-SYN-VERB */
+    final static String JOIN_VERB_HANDLER = "JVH";
+    /* GSV - abbreviation for GOSSIP-DIGEST-SYN-VERB */
+    final static String GOSSIP_DIGEST_SYN_VERB = "GSV";
+    /* GAV - abbreviation for GOSSIP-DIGEST-ACK-VERB */
+    final static String GOSSIP_DIGEST_ACK_VERB = "GAV";
+    /* GA2V - abbreviation for GOSSIP-DIGEST-ACK2-VERB */
+    final static String GOSSIP_DIGEST_ACK2_VERB = "GA2V";
+    final static int intervalInMillis_ = 1000;
+    private static Logger logger_ = Logger.getLogger(Gossiper.class);
+    static Gossiper gossiper_;
+
+    public synchronized static Gossiper instance()
+    {
+        if ( gossiper_ == null )
+        {
+            gossiper_ = new Gossiper();
+        }
+        return gossiper_;
+    }
+
+    private Timer gossipTimer_ = new Timer(false);
+    private EndPoint localEndPoint_;
+    private long aVeryLongTime_;
+    private Random random_ = new Random();
+    /* round robin index through live endpoint set */
+    private int rrIndex_ = 0;
+
+    /* subscribers for interest in EndPointState change */
+    private List<IEndPointStateChangeSubscriber> subscribers_ = new ArrayList<IEndPointStateChangeSubscriber>();
+
+    /* live member set */
+    private Set<EndPoint> liveEndpoints_ = new HashSet<EndPoint>();
+
+    /* unreachable member set */
+    private Set<EndPoint> unreachableEndpoints_ = new HashSet<EndPoint>();
+
+    /* initial seeds for joining the cluster */
+    private Set<EndPoint> seeds_ = new HashSet<EndPoint>();
+
+    /* map where key is the endpoint and value is the state associated with the endpoint */
+    Map<EndPoint, EndPointState> endPointStateMap_ = new Hashtable<EndPoint, EndPointState>();
+
+    /* private CTOR */
+    Gossiper()
+    {
+        aVeryLongTime_ = 259200 * 1000;
+        /* register with the Failure Detector for receiving Failure detector events */
+        FailureDetector.instance().registerFailureDetectionEventListener(this);
+        /* register the verbs */
+        MessagingService.getMessagingInstance().registerVerbHandlers(JOIN_VERB_HANDLER, new JoinVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(GOSSIP_DIGEST_SYN_VERB, new GossipDigestSynVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(GOSSIP_DIGEST_ACK_VERB, new GossipDigestAckVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(GOSSIP_DIGEST_ACK2_VERB, new GossipDigestAck2VerbHandler());
+        /* register the Gossip stage */
+        StageManager.registerStage( Gossiper.GOSSIP_STAGE, new SingleThreadedStage("GMFD") );
+        /* register with Storage Service for shutdown */
+        StorageService.instance().registerComponentForShutdown(this);
+    }
+
+    public void register(IEndPointStateChangeSubscriber subscriber)
+    {
+        subscribers_.add(subscriber);
+    }
+
+    public void unregister(IEndPointStateChangeSubscriber subscriber)
+    {
+        subscribers_.remove(subscriber);
+    }
+
+    public Set<EndPoint> getAllMembers()
+    {
+        Set<EndPoint> allMbrs = new HashSet<EndPoint>();
+        allMbrs.addAll(getLiveMembers());
+        allMbrs.addAll(getUnreachableMembers());
+        return allMbrs;
+    }
+
+    public Set<EndPoint> getLiveMembers()
+    {
+        Set<EndPoint> liveMbrs = new HashSet<EndPoint>(liveEndpoints_);
+        liveMbrs.add( new EndPoint( localEndPoint_.getHost(), localEndPoint_.getPort() ) );
+        return liveMbrs;
+    }
+
+    public Set<EndPoint> getUnreachableMembers()
+    {
+        return new HashSet<EndPoint>(unreachableEndpoints_);
+    }
+
+    /**
+     * This method is used to forcibly remove a node from the membership
+     * set. He is forgotten locally immediately.
+     *
+     * param@ ep the endpoint to be removed from membership.
+     */
+    public synchronized void removeFromMembership(EndPoint ep)
+    {
+        endPointStateMap_.remove(ep);
+        liveEndpoints_.remove(ep);
+        unreachableEndpoints_ .remove(ep);
+    }
+
+    /**
+     * This method is part of IFailureDetectionEventListener interface. This is invoked
+     * by the Failure Detector when it convicts an end point.
+     *
+     * param @ endpoint end point that is convicted.
+    */
+
+    public void convict(EndPoint endpoint)
+    {
+        EndPointState epState = endPointStateMap_.get(endpoint);
+        if ( epState != null )
+        {
+            if ( !epState.isAlive() && epState.isAGossiper() )
+            {
+                /*
+                 * just to be sure - is invoked just to make sure that
+                 * it was called atleast once.
+                */
+                if ( liveEndpoints_.contains(endpoint) )
+                {
+                    logger_.info("EndPoint " + endpoint + " is now dead.");
+                    isAlive(endpoint, epState, false);
+
+                    /* Notify an endpoint is dead to interested parties. */
+                    EndPointState deltaState = new EndPointState(epState.getHeartBeatState());
+                    doNotifications(endpoint, deltaState);
+                }
+                epState.isAGossiper(false);
+            }
+        }
+    }
+
+    /**
+     * This method is part of IFailureDetectionEventListener interface. This is invoked
+     * by the Failure Detector when it suspects an end point.
+     *
+     * param @ endpoint end point that is suspected.
+    */
+    public void suspect(EndPoint endpoint)
+    {
+        EndPointState epState = endPointStateMap_.get(endpoint);
+        if ( epState.isAlive() )
+        {
+            logger_.info("EndPoint " + endpoint + " is now dead.");
+            isAlive(endpoint, epState, false);
+
+            /* Notify an endpoint is dead to interested parties. */
+            EndPointState deltaState = new EndPointState(epState.getHeartBeatState());
+            doNotifications(endpoint, deltaState);
+        }
+    }
+
+    int getMaxEndPointStateVersion(EndPointState epState)
+    {
+        List<Integer> versions = new ArrayList<Integer>();
+        versions.add( epState.getHeartBeatState().getHeartBeatVersion() );
+        Map<String, ApplicationState> appStateMap = epState.getApplicationState();
+
+        Set<String> keys = appStateMap.keySet();
+        for ( String key : keys )
+        {
+            int stateVersion = appStateMap.get(key).getStateVersion();
+            versions.add( stateVersion );
+        }
+
+        /* sort to get the max version to build GossipDigest for this endpoint */
+        Collections.sort(versions);
+        int maxVersion = versions.get(versions.size() - 1);
+        versions.clear();
+        return maxVersion;
+    }
+
+    /**
+     * Removes the endpoint from unreachable endpoint set
+     *
+     * @param endpoint endpoint to be removed from the current membership.
+    */
+    void evictFromMembership(EndPoint endpoint)
+    {
+        unreachableEndpoints_.remove(endpoint);
+    }
+
+    /* No locking required since it is called from a method that already has acquired a lock */
+    @Deprecated
+    void makeGossipDigest(List<GossipDigest> gDigests)
+    {
+        /* Add the local endpoint state */
+        EndPointState epState = endPointStateMap_.get(localEndPoint_);
+        int generation = epState.getHeartBeatState().getGeneration();
+        int maxVersion = getMaxEndPointStateVersion(epState);
+        gDigests.add( new GossipDigest(localEndPoint_, generation, maxVersion) );
+
+        for ( EndPoint liveEndPoint : liveEndpoints_ )
+        {
+            epState = endPointStateMap_.get(liveEndPoint);
+            if ( epState != null )
+            {
+                generation = epState.getHeartBeatState().getGeneration();
+                maxVersion = getMaxEndPointStateVersion(epState);
+                gDigests.add( new GossipDigest(liveEndPoint, generation, maxVersion) );
+            }
+            else
+            {
+            	gDigests.add( new GossipDigest(liveEndPoint, 0, 0) );
+            }
+        }
+    }
+
+    /**
+     * No locking required since it is called from a method that already
+     * has acquired a lock. The gossip digest is built based on randomization
+     * rather than just looping through the collection of live endpoints.
+     *
+     * @param gDigests list of Gossip Digests.
+    */
+    void makeRandomGossipDigest(List<GossipDigest> gDigests)
+    {
+        /* Add the local endpoint state */
+        EndPointState epState = endPointStateMap_.get(localEndPoint_);
+        int generation = epState.getHeartBeatState().getGeneration();
+        int maxVersion = getMaxEndPointStateVersion(epState);
+        gDigests.add( new GossipDigest(localEndPoint_, generation, maxVersion) );
+
+        List<EndPoint> endpoints = new ArrayList<EndPoint>( liveEndpoints_ );
+        Collections.shuffle(endpoints, random_);
+        for ( EndPoint liveEndPoint : endpoints )
+        {
+            epState = endPointStateMap_.get(liveEndPoint);
+            if ( epState != null )
+            {
+                generation = epState.getHeartBeatState().getGeneration();
+                maxVersion = getMaxEndPointStateVersion(epState);
+                gDigests.add( new GossipDigest(liveEndPoint, generation, maxVersion) );
+            }
+            else
+            {
+            	gDigests.add( new GossipDigest(liveEndPoint, 0, 0) );
+            }
+        }
+
+        /* FOR DEBUG ONLY - remove later */
+        StringBuilder sb = new StringBuilder();
+        for ( GossipDigest gDigest : gDigests )
+        {
+            sb.append(gDigest);
+            sb.append(" ");
+        }
+        logger_.trace("Gossip Digests are : " + sb.toString());
+    }
+
+    public int getCurrentGenerationNumber(EndPoint endpoint)
+    {
+    	return endPointStateMap_.get(endpoint).getHeartBeatState().getGeneration();
+    }
+
+    Message makeGossipDigestSynMessage(List<GossipDigest> gDigests) throws IOException
+    {
+        GossipDigestSynMessage gDigestMessage = new GossipDigestSynMessage(DatabaseDescriptor.getClusterName(), gDigests);
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
+        DataOutputStream dos = new DataOutputStream( bos );
+        GossipDigestSynMessage.serializer().serialize(gDigestMessage, dos);
+        Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_SYN_VERB, new Object[]{bos.toByteArray()});
+        return message;
+    }
+
+    Message makeGossipDigestAckMessage(GossipDigestAckMessage gDigestAckMessage) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
+        DataOutputStream dos = new DataOutputStream(bos);
+        GossipDigestAckMessage.serializer().serialize(gDigestAckMessage, dos);
+        logger_.trace("@@@@ Size of GossipDigestAckMessage is " + bos.toByteArray().length);
+        Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_ACK_VERB, new Object[]{bos.toByteArray()});
+        return message;
+    }
+
+    Message makeGossipDigestAck2Message(GossipDigestAck2Message gDigestAck2Message) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
+        DataOutputStream dos = new DataOutputStream(bos);
+        GossipDigestAck2Message.serializer().serialize(gDigestAck2Message, dos);
+        Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_ACK2_VERB, new Object[]{bos.toByteArray()});
+        return message;
+    }
+
+    boolean sendGossipToLiveNode(Message message)
+    {
+        int size = liveEndpoints_.size();
+        List<EndPoint> eps = new ArrayList<EndPoint>(liveEndpoints_);
+
+        if ( rrIndex_ >= size )
+        {
+            rrIndex_ = -1;
+        }
+
+        EndPoint to = eps.get(++rrIndex_);
+        logger_.trace("Sending a GossipDigestSynMessage to " + to + " ...");
+        MessagingService.getMessagingInstance().sendUdpOneWay(message, to);
+        return seeds_.contains(to);
+    }
+
+    /**
+     * Returns true if the chosen target was also a seed. False otherwise
+     *
+     *  @param message message to sent
+     *  @param epSet a set of endpoint from which a random endpoint is chosen.
+     *  @return true if the chosen endpoint is also a seed.
+     */
+    boolean sendGossip(Message message, Set<EndPoint> epSet)
+    {
+        int size = epSet.size();
+        /* Generate a random number from 0 -> size */
+        List<EndPoint> liveEndPoints = new ArrayList<EndPoint>(epSet);
+        int index = (size == 1) ? 0 : random_.nextInt(size);
+        EndPoint to = liveEndPoints.get(index);
+        logger_.trace("Sending a GossipDigestSynMessage to " + to + " ...");
+        MessagingService.getMessagingInstance().sendUdpOneWay(message, to);
+        return seeds_.contains(to);
+    }
+
+    /* Sends a Gossip message to a live member and returns a reference to the member */
+    boolean doGossipToLiveMember(Message message)
+    {
+        int size = liveEndpoints_.size();
+        if ( size == 0 )
+            return false;
+        // return sendGossipToLiveNode(message);
+        /* Use this for a cluster size >= 30 */
+        return sendGossip(message, liveEndpoints_);
+    }
+
+    /* Sends a Gossip message to an unreachable member */
+    void doGossipToUnreachableMember(Message message)
+    {
+        double liveEndPoints = liveEndpoints_.size();
+        double unreachableEndPoints = unreachableEndpoints_.size();
+        if ( unreachableEndPoints > 0 )
+        {
+            /* based on some probability */
+            double prob = unreachableEndPoints / (liveEndPoints + 1);
+            double randDbl = random_.nextDouble();
+            if ( randDbl < prob )
+                sendGossip(message, unreachableEndpoints_);
+        }
+    }
+
+    /* Gossip to a seed for facilitating partition healing */
+    void doGossipToSeed(Message message)
+    {
+        int size = seeds_.size();
+        if ( size > 0 )
+        {
+            if ( size == 1 && seeds_.contains(localEndPoint_) )
+            {
+                return;
+            }
+
+            if ( liveEndpoints_.size() == 0 )
+            {
+                sendGossip(message, seeds_);
+            }
+            else
+            {
+                /* Gossip with the seed with some probability. */
+                double probability = seeds_.size() / ( liveEndpoints_.size() + unreachableEndpoints_.size() );
+                double randDbl = random_.nextDouble();
+                if ( randDbl <= probability )
+                    sendGossip(message, seeds_);
+            }
+        }
+    }
+
+    void doStatusCheck()
+    {
+        Set<EndPoint> eps = endPointStateMap_.keySet();
+
+        for ( EndPoint endpoint : eps )
+        {
+            if ( endpoint.equals(localEndPoint_) )
+                continue;
+
+            FailureDetector.instance().intepret(endpoint);
+            EndPointState epState = endPointStateMap_.get(endpoint);
+            if ( epState != null )
+            {
+                long duration = System.currentTimeMillis() - epState.getUpdateTimestamp();
+                if ( !epState.isAlive() && (duration > aVeryLongTime_) )
+                {
+                    evictFromMembership(endpoint);
+                }
+            }
+        }
+    }
+
+    EndPointState getEndPointStateForEndPoint(EndPoint ep)
+    {
+        return endPointStateMap_.get(ep);
+    }
+
+    synchronized EndPointState getStateForVersionBiggerThan(EndPoint forEndpoint, int version)
+    {
+        EndPointState epState = endPointStateMap_.get(forEndpoint);
+        EndPointState reqdEndPointState = null;
+
+        if ( epState != null )
+        {
+            /*
+             * Here we try to include the Heart Beat state only if it is
+             * greater than the version passed in. It might happen that
+             * the heart beat version maybe lesser than the version passed
+             * in and some application state has a version that is greater
+             * than the version passed in. In this case we also send the old
+             * heart beat and throw it away on the receiver if it is redundant.
+            */
+            int localHbVersion = epState.getHeartBeatState().getHeartBeatVersion();
+            if ( localHbVersion > version )
+            {
+                reqdEndPointState = new EndPointState(epState.getHeartBeatState());
+            }
+            Map<String, ApplicationState> appStateMap = epState.getApplicationState();
+            /* Accumulate all application states whose versions are greater than "version" variable */
+            Set<String> keys = appStateMap.keySet();
+            for ( String key : keys )
+            {
+                ApplicationState appState = appStateMap.get(key);
+                if ( appState.getStateVersion() > version )
+                {
+                    if ( reqdEndPointState == null )
+                    {
+                        reqdEndPointState = new EndPointState(epState.getHeartBeatState());
+                    }
+                    reqdEndPointState.addApplicationState(key, appState);
+                }
+            }
+        }
+        return reqdEndPointState;
+    }
+
+    /*
+     * This method is called only from the JoinVerbHandler. This happens
+     * when a new node coming up multicasts the JoinMessage. Here we need
+     * to add the endPoint to the list of live endpoints.
+    */
+    synchronized void join(EndPoint from)
+    {
+        if ( !from.equals( localEndPoint_ ) )
+        {
+            /* Mark this endpoint as "live" */
+        	liveEndpoints_.add(from);
+            unreachableEndpoints_.remove(from);
+        }
+    }
+
+    void notifyFailureDetector(List<GossipDigest> gDigests)
+    {
+        IFailureDetector fd = FailureDetector.instance();
+        for ( GossipDigest gDigest : gDigests )
+        {
+            EndPointState localEndPointState = endPointStateMap_.get(gDigest.endPoint_);
+            /*
+             * If the local endpoint state exists then report to the FD only
+             * if the versions workout.
+            */
+            if ( localEndPointState != null )
+            {
+                int localGeneration = endPointStateMap_.get(gDigest.endPoint_).getHeartBeatState().generation_;
+                int remoteGeneration = gDigest.generation_;
+                if ( remoteGeneration > localGeneration )
+                {
+                    logger_.debug("Reporting " + gDigest.endPoint_ + " to the FD.");
+                    fd.report(gDigest.endPoint_);
+                    continue;
+                }
+
+                if ( remoteGeneration == localGeneration )
+                {
+                    int localVersion = getMaxEndPointStateVersion(localEndPointState);
+                    //int localVersion = endPointStateMap_.get(gDigest.endPoint_).getHeartBeatState().getHeartBeatVersion();
+                    int remoteVersion = gDigest.maxVersion_;
+                    if ( remoteVersion > localVersion )
+                    {
+                        logger_.debug("Reporting " + gDigest.endPoint_ + " to the FD.");
+                        fd.report(gDigest.endPoint_);
+                    }
+                }
+            }
+        }
+    }
+
+    void notifyFailureDetector(Map<EndPoint, EndPointState> remoteEpStateMap)
+    {
+        IFailureDetector fd = FailureDetector.instance();
+        Set<EndPoint> endpoints = remoteEpStateMap.keySet();
+        for ( EndPoint endpoint : endpoints )
+        {
+            EndPointState remoteEndPointState = remoteEpStateMap.get(endpoint);
+            EndPointState localEndPointState = endPointStateMap_.get(endpoint);
+            /*
+             * If the local endpoint state exists then report to the FD only
+             * if the versions workout.
+            */
+            if ( localEndPointState != null )
+            {
+                int localGeneration = localEndPointState.getHeartBeatState().generation_;
+                int remoteGeneration = remoteEndPointState.getHeartBeatState().generation_;
+                if ( remoteGeneration > localGeneration )
+                {
+                    logger_.debug("Reporting " + endpoint + " to the FD.");
+                    fd.report(endpoint);
+                    continue;
+                }
+
+                if ( remoteGeneration == localGeneration )
+                {
+                    int localVersion = getMaxEndPointStateVersion(localEndPointState);
+                    //int localVersion = localEndPointState.getHeartBeatState().getHeartBeatVersion();
+                    int remoteVersion = remoteEndPointState.getHeartBeatState().getHeartBeatVersion();
+                    if ( remoteVersion > localVersion )
+                    {
+                        logger_.debug("Reporting " + endpoint + " to the FD.");
+                        fd.report(endpoint);
+                    }
+                }
+            }
+        }
+    }
+
+    void resusitate(EndPoint addr, EndPointState localState)
+    {
+        logger_.debug("Attempting to resusitate " + addr);
+        if ( !localState.isAlive() )
+        {
+            isAlive(addr, localState, true);
+            logger_.info("EndPoint " + addr + " is now UP");
+        }
+    }
+
+    private void handleNewJoin(EndPoint ep, EndPointState epState)
+    {
+    	logger_.info("Node " + ep + " has now joined.");
+        /* Mark this endpoint as "live" */
+        endPointStateMap_.put(ep, epState);
+        isAlive(ep, epState, true);
+        /* Notify interested parties about endpoint state change */
+        doNotifications(ep, epState);
+    }
+
+    synchronized void applyStateLocally(Map<EndPoint, EndPointState> epStateMap)
+    {
+        Set<EndPoint> eps = epStateMap.keySet();
+        for( EndPoint ep : eps )
+        {
+            if ( ep.equals( localEndPoint_ ) )
+                continue;
+
+            EndPointState localEpStatePtr = endPointStateMap_.get(ep);
+            EndPointState remoteState = epStateMap.get(ep);
+            /*
+                If state does not exist just add it. If it does then add it only if the version
+                of the remote copy is greater than the local copy.
+            */
+            if ( localEpStatePtr != null )
+            {
+            	int localGeneration = localEpStatePtr.getHeartBeatState().getGeneration();
+            	int remoteGeneration = remoteState.getHeartBeatState().getGeneration();
+
+            	if (remoteGeneration > localGeneration)
+            	{
+            		handleNewJoin(ep, remoteState);
+            	}
+            	else if ( remoteGeneration == localGeneration )
+            	{
+	                /* manage the membership state */
+	                int localMaxVersion = getMaxEndPointStateVersion(localEpStatePtr);
+	                int remoteMaxVersion = getMaxEndPointStateVersion(remoteState);
+	                if ( remoteMaxVersion > localMaxVersion )
+	                {
+	                    resusitate(ep, localEpStatePtr);
+	                    applyHeartBeatStateLocally(ep, localEpStatePtr, remoteState);
+	                    /* apply ApplicationState */
+	                    applyApplicationStateLocally(ep, localEpStatePtr, remoteState);
+	                }
+            	}
+            }
+            else
+            {
+            	handleNewJoin(ep, remoteState);
+            }
+        }
+    }
+
+    void applyHeartBeatStateLocally(EndPoint addr, EndPointState localState, EndPointState remoteState)
+    {
+        HeartBeatState localHbState = localState.getHeartBeatState();
+        HeartBeatState remoteHbState = remoteState.getHeartBeatState();
+
+        if ( remoteHbState.getGeneration() > localHbState.getGeneration() )
+        {
+            resusitate(addr, localState);
+            localState.setHeartBeatState(remoteHbState);
+        }
+        if ( localHbState.getGeneration() == remoteHbState.getGeneration() )
+        {
+            if ( remoteHbState.getHeartBeatVersion() > localHbState.getHeartBeatVersion() )
+            {
+                int oldVersion = localHbState.getHeartBeatVersion();
+                localState.setHeartBeatState(remoteHbState);
+                logger_.debug("Updating heartbeat state version to " + localState.getHeartBeatState().getHeartBeatVersion() + " from " + oldVersion + " for " + addr + " ...");
+            }
+        }
+    }
+
+    void applyApplicationStateLocally(EndPoint addr, EndPointState localStatePtr, EndPointState remoteStatePtr)
+    {
+        Map<String, ApplicationState> localAppStateMap = localStatePtr.getApplicationState();
+        Map<String, ApplicationState> remoteAppStateMap = remoteStatePtr.getApplicationState();
+
+        Set<String> remoteKeys = remoteAppStateMap.keySet();
+        for ( String remoteKey : remoteKeys )
+        {
+            ApplicationState remoteAppState = remoteAppStateMap.get(remoteKey);
+            ApplicationState localAppState = localAppStateMap.get(remoteKey);
+
+            /* If state doesn't exist locally for this key then just apply it */
+            if ( localAppState == null )
+            {
+                localStatePtr.addApplicationState(remoteKey, remoteAppState);
+                /* notify interested parties of endpoint state change */
+                EndPointState deltaState = new EndPointState(localStatePtr.getHeartBeatState());
+                deltaState.addApplicationState(remoteKey, remoteAppState);
+                doNotifications(addr, deltaState);
+                continue;
+            }
+
+            int remoteGeneration = remoteStatePtr.getHeartBeatState().getGeneration();
+            int localGeneration = localStatePtr.getHeartBeatState().getGeneration();
+
+            /* If the remoteGeneration is greater than localGeneration then apply state blindly */
+            if ( remoteGeneration > localGeneration )
+            {
+                localStatePtr.addApplicationState(remoteKey, remoteAppState);
+                /* notify interested parties of endpoint state change */
+                EndPointState deltaState = new EndPointState(localStatePtr.getHeartBeatState());
+                deltaState.addApplicationState(remoteKey, remoteAppState);
+                doNotifications(addr, deltaState);
+                continue;
+            }
+
+            /* If the generations are the same then apply state if the remote version is greater than local version. */
+            if ( remoteGeneration == localGeneration )
+            {
+                int remoteVersion = remoteAppState.getStateVersion();
+                int localVersion = localAppState.getStateVersion();
+
+                if ( remoteVersion > localVersion )
+                {
+                    localStatePtr.addApplicationState(remoteKey, remoteAppState);
+                    /* notify interested parties of endpoint state change */
+                    EndPointState deltaState = new EndPointState(localStatePtr.getHeartBeatState());
+                    deltaState.addApplicationState(remoteKey, remoteAppState);
+                    doNotifications(addr, deltaState);
+                }
+            }
+        }
+    }
+
+    void doNotifications(EndPoint addr, EndPointState epState)
+    {
+        for ( IEndPointStateChangeSubscriber subscriber : subscribers_ )
+        {
+            subscriber.onChange(addr, epState);
+        }
+    }
+
+    synchronized void isAlive(EndPoint addr, EndPointState epState, boolean value)
+    {
+        epState.isAlive(value);
+        if ( value )
+        {
+            liveEndpoints_.add(addr);
+            unreachableEndpoints_.remove(addr);
+        }
+        else
+        {
+            liveEndpoints_.remove(addr);
+            unreachableEndpoints_.add(addr);
+        }
+        if ( epState.isAGossiper() )
+            return;
+        epState.isAGossiper(true);
+    }
+
+    /* These are helper methods used from GossipDigestSynVerbHandler */
+    Map<EndPoint, GossipDigest> getEndPointGossipDigestMap(List<GossipDigest> gDigestList)
+    {
+        Map<EndPoint, GossipDigest> epMap = new HashMap<EndPoint, GossipDigest>();
+        for( GossipDigest gDigest : gDigestList )
+        {
+            epMap.put( gDigest.getEndPoint(), gDigest );
+        }
+        return epMap;
+    }
+
+    /* This is a helper method to get all EndPoints from a list of GossipDigests */
+    EndPoint[] getEndPointsFromGossipDigest(List<GossipDigest> gDigestList)
+    {
+        Set<EndPoint> set = new HashSet<EndPoint>();
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            set.add( gDigest.getEndPoint() );
+        }
+        return set.toArray( new EndPoint[0] );
+    }
+
+    /* Request all the state for the endpoint in the gDigest */
+    void requestAll(GossipDigest gDigest, List<GossipDigest> deltaGossipDigestList, int remoteGeneration)
+    {
+        /* We are here since we have no data for this endpoint locally so request everthing. */
+        deltaGossipDigestList.add( new GossipDigest(gDigest.getEndPoint(), remoteGeneration, 0) );
+    }
+
+    /* Send all the data with version greater than maxRemoteVersion */
+    void sendAll(GossipDigest gDigest, Map<EndPoint, EndPointState> deltaEpStateMap, int maxRemoteVersion)
+    {
+        EndPointState localEpStatePtr = getStateForVersionBiggerThan(gDigest.getEndPoint(), maxRemoteVersion) ;
+        if ( localEpStatePtr != null )
+            deltaEpStateMap.put(gDigest.getEndPoint(), localEpStatePtr);
+    }
+
+    /*
+        This method is used to figure the state that the Gossiper has but Gossipee doesn't. The delta digests
+        and the delta state are built up.
+    */
+    synchronized void examineGossiper(List<GossipDigest> gDigestList, List<GossipDigest> deltaGossipDigestList, Map<EndPoint, EndPointState> deltaEpStateMap)
+    {
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            int remoteGeneration = gDigest.getGeneration();
+            int maxRemoteVersion = gDigest.getMaxVersion();
+            /* Get state associated with the end point in digest */
+            EndPointState epStatePtr = endPointStateMap_.get(gDigest.getEndPoint());
+            /*
+                Here we need to fire a GossipDigestAckMessage. If we have some data associated with this endpoint locally
+                then we follow the "if" path of the logic. If we have absolutely nothing for this endpoint we need to
+                request all the data for this endpoint.
+            */
+            if ( epStatePtr != null )
+            {
+                int localGeneration = epStatePtr.getHeartBeatState().getGeneration();
+                /* get the max version of all keys in the state associated with this endpoint */
+                int maxLocalVersion = getMaxEndPointStateVersion(epStatePtr);
+                if ( remoteGeneration == localGeneration && maxRemoteVersion == maxLocalVersion )
+                    continue;
+
+                if ( remoteGeneration > localGeneration )
+                {
+                    /* we request everything from the gossiper */
+                    requestAll(gDigest, deltaGossipDigestList, remoteGeneration);
+                }
+                if ( remoteGeneration < localGeneration )
+                {
+                    /* send all data with generation = localgeneration and version > 0 */
+                    sendAll(gDigest, deltaEpStateMap, 0);
+                }
+                if ( remoteGeneration == localGeneration )
+                {
+                    /*
+                        If the max remote version is greater then we request the remote endpoint send us all the data
+                        for this endpoint with version greater than the max version number we have locally for this
+                        endpoint.
+                        If the max remote version is lesser, then we send all the data we have locally for this endpoint
+                        with version greater than the max remote version.
+                    */
+                    if ( maxRemoteVersion > maxLocalVersion )
+                    {
+                        deltaGossipDigestList.add( new GossipDigest(gDigest.getEndPoint(), remoteGeneration, maxLocalVersion) );
+                    }
+                    if ( maxRemoteVersion < maxLocalVersion )
+                    {
+                        /* send all data with generation = localgeneration and version > maxRemoteVersion */
+                        sendAll(gDigest, deltaEpStateMap, maxRemoteVersion);
+                    }
+                }
+            }
+            else
+            {
+                /* We are here since we have no data for this endpoint locally so request everthing. */
+                requestAll(gDigest, deltaGossipDigestList, remoteGeneration);
+            }
+        }
+    }
+
+    public void start(EndPoint localEndPoint, int generationNbr) throws IOException
+    {
+        localEndPoint_ = localEndPoint;
+        /* Get the seeds from the config and initialize them. */
+        Set<String> seedHosts = DatabaseDescriptor.getSeeds();
+        for( String seedHost : seedHosts )
+        {
+            EndPoint seed = new EndPoint(InetAddress.getByName(seedHost).getHostAddress(),
+                                         DatabaseDescriptor.getControlPort());
+            if ( seed.equals(localEndPoint) )
+                continue;
+            seeds_.add(seed);
+        }
+
+        /* initialize the heartbeat state for this localEndPoint */
+        EndPointState localState = endPointStateMap_.get(localEndPoint_);
+        if ( localState == null )
+        {
+            HeartBeatState hbState = new HeartBeatState(generationNbr, 0);
+            localState = new EndPointState(hbState);
+            localState.isAlive(true);
+            localState.isAGossiper(true);
+            endPointStateMap_.put(localEndPoint_, localState);
+        }
+
+        /* starts a timer thread */
+        gossipTimer_.schedule( new GossipTimerTask(), Gossiper.intervalInMillis_, Gossiper.intervalInMillis_);
+    }
+
+    public void shutdown()
+    {
+    	/* This prevents this guy from responding to Gossip messages */
+    	MessagingService.getMessagingInstance().deregisterVerbHandlers(GOSSIP_DIGEST_SYN_VERB);
+        MessagingService.getMessagingInstance().deregisterVerbHandlers(GOSSIP_DIGEST_ACK_VERB);
+        MessagingService.getMessagingInstance().deregisterVerbHandlers(GOSSIP_DIGEST_ACK2_VERB);
+    	/* This prevents this guy from Gossiping */
+        gossipTimer_.cancel();
+    }
+
+    public synchronized void addApplicationState(String key, ApplicationState appState)
+    {
+        EndPointState epState = endPointStateMap_.get(localEndPoint_);
+        if ( epState != null )
+        {
+            epState.addApplicationState(key, appState);
+        }
+    }
+
+    public void stop()
+    {
+        gossipTimer_.cancel();
+    }
+}
+
+class JoinVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger( JoinVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        EndPoint from = message.getFrom();
+        logger_.debug("Received a JoinMessage from " + from);
+
+        byte[] bytes = (byte[])message.getMessageBody()[0];
+        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+
+        try
+        {
+            JoinMessage joinMessage = JoinMessage.serializer().deserialize(dis);
+            if ( joinMessage.clusterId_.equals( DatabaseDescriptor.getClusterName() ) )
+            {
+                Gossiper.instance().join(from);
+            }
+        }
+        catch ( IOException ex )
+        {
+            logger_.info( LogUtil.throwableToString(ex) );
+        }
+    }
+}
+
+class GossipDigestSynVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger( GossipDigestSynVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        EndPoint from = message.getFrom();
+        logger_.trace("Received a GossipDigestSynMessage from " + from);
+
+        byte[] bytes = (byte[])message.getMessageBody()[0];
+        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+
+        try
+        {
+            GossipDigestSynMessage gDigestMessage = GossipDigestSynMessage.serializer().deserialize(dis);
+            /* If the message is from a different cluster throw it away. */
+            if ( !gDigestMessage.clusterId_.equals(DatabaseDescriptor.getClusterName()) )
+                return;
+
+            List<GossipDigest> gDigestList = gDigestMessage.getGossipDigests();
+            /* Notify the Failure Detector */
+            Gossiper.instance().notifyFailureDetector(gDigestList);
+
+            doSort(gDigestList);
+
+            List<GossipDigest> deltaGossipDigestList = new ArrayList<GossipDigest>();
+            Map<EndPoint, EndPointState> deltaEpStateMap = new HashMap<EndPoint, EndPointState>();
+            Gossiper.instance().examineGossiper(gDigestList, deltaGossipDigestList, deltaEpStateMap);
+
+            GossipDigestAckMessage gDigestAck = new GossipDigestAckMessage(deltaGossipDigestList, deltaEpStateMap);
+            Message gDigestAckMessage = Gossiper.instance().makeGossipDigestAckMessage(gDigestAck);
+            logger_.trace("Sending a GossipDigestAckMessage to " + from);
+            MessagingService.getMessagingInstance().sendUdpOneWay(gDigestAckMessage, from);
+        }
+        catch (IOException e)
+        {
+            logger_.info( LogUtil.throwableToString(e) );
+        }
+    }
+
+    /*
+     * First construct a map whose key is the endpoint in the GossipDigest and the value is the
+     * GossipDigest itself. Then build a list of version differences i.e difference between the
+     * version in the GossipDigest and the version in the local state for a given EndPoint.
+     * Sort this list. Now loop through the sorted list and retrieve the GossipDigest corresponding
+     * to the endpoint from the map that was initially constructed.
+    */
+    private void doSort(List<GossipDigest> gDigestList)
+    {
+        /* Construct a map of endpoint to GossipDigest. */
+        Map<EndPoint, GossipDigest> epToDigestMap = new HashMap<EndPoint, GossipDigest>();
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            epToDigestMap.put(gDigest.getEndPoint(), gDigest);
+        }
+
+        /*
+         * These digests have their maxVersion set to the difference of the version
+         * of the local EndPointState and the version found in the GossipDigest.
+        */
+        List<GossipDigest> diffDigests = new ArrayList<GossipDigest>();
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            EndPoint ep = gDigest.getEndPoint();
+            EndPointState epState = Gossiper.instance().getEndPointStateForEndPoint(ep);
+            int version = (epState != null) ? Gossiper.instance().getMaxEndPointStateVersion( epState ) : 0;
+            int diffVersion = Math.abs(version - gDigest.getMaxVersion() );
+            diffDigests.add( new GossipDigest(ep, gDigest.getGeneration(), diffVersion) );
+        }
+
+        gDigestList.clear();
+        Collections.sort(diffDigests);
+        int size = diffDigests.size();
+        /*
+         * Report the digests in descending order. This takes care of the endpoints
+         * that are far behind w.r.t this local endpoint
+        */
+        for ( int i = size - 1; i >= 0; --i )
+        {
+            gDigestList.add( epToDigestMap.get(diffDigests.get(i).getEndPoint()) );
+        }
+    }
+}
+
+class GossipDigestAckVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(GossipDigestAckVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        EndPoint from = message.getFrom();
+        logger_.trace("Received a GossipDigestAckMessage from " + from);
+
+        byte[] bytes = (byte[])message.getMessageBody()[0];
+        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+
+        try
+        {
+            GossipDigestAckMessage gDigestAckMessage = GossipDigestAckMessage.serializer().deserialize(dis);
+            List<GossipDigest> gDigestList = gDigestAckMessage.getGossipDigestList();
+            Map<EndPoint, EndPointState> epStateMap = gDigestAckMessage.getEndPointStateMap();
+
+            if ( epStateMap.size() > 0 )
+            {
+                /* Notify the Failure Detector */
+                Gossiper.instance().notifyFailureDetector(epStateMap);
+                Gossiper.instance().applyStateLocally(epStateMap);
+            }
+
+            /* Get the state required to send to this gossipee - construct GossipDigestAck2Message */
+            Map<EndPoint, EndPointState> deltaEpStateMap = new HashMap<EndPoint, EndPointState>();
+            for( GossipDigest gDigest : gDigestList )
+            {
+                EndPoint addr = gDigest.getEndPoint();
+                EndPointState localEpStatePtr = Gossiper.instance().getStateForVersionBiggerThan(addr, gDigest.getMaxVersion());
+                if ( localEpStatePtr != null )
+                    deltaEpStateMap.put(addr, localEpStatePtr);
+            }
+
+            GossipDigestAck2Message gDigestAck2 = new GossipDigestAck2Message(deltaEpStateMap);
+            Message gDigestAck2Message = Gossiper.instance().makeGossipDigestAck2Message(gDigestAck2);
+            logger_.trace("Sending a GossipDigestAck2Message to " + from);
+            MessagingService.getMessagingInstance().sendUdpOneWay(gDigestAck2Message, from);
+        }
+        catch ( IOException e )
+        {
+            logger_.info( LogUtil.throwableToString(e) );
+        }
+    }
+}
+
+class GossipDigestAck2VerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(GossipDigestAck2VerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        EndPoint from = message.getFrom();
+        logger_.trace("Received a GossipDigestAck2Message from " + from);
+
+        byte[] bytes = (byte[])message.getMessageBody()[0];
+        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+        try
+        {
+            GossipDigestAck2Message gDigestAck2Message = GossipDigestAck2Message.serializer().deserialize(dis);
+            Map<EndPoint, EndPointState> remoteEpStateMap = gDigestAck2Message.getEndPointStateMap();
+            /* Notify the Failure Detector */
+            Gossiper.instance().notifyFailureDetector(remoteEpStateMap);
+            Gossiper.instance().applyStateLocally(remoteEpStateMap);
+        }
+        catch ( IOException e )
+        {
+            logger_.info( LogUtil.throwableToString(e) );
+        }
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/HeartBeatState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/HeartBeatState.java
index e69de29b..b99720d3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/HeartBeatState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/HeartBeatState.java
@@ -0,0 +1,109 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.util.concurrent.atomic.AtomicInteger;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+
+/**
+ * HeartBeat State associated with any given endpoint. 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class HeartBeatState
+{
+    private static ICompactSerializer<HeartBeatState> serializer_;
+    
+    static
+    {
+        serializer_ = new HeartBeatStateSerializer();
+    }
+    
+    int generation_;
+    AtomicInteger heartbeat_;
+    int version_;
+
+    HeartBeatState()
+    {
+    }
+    
+    HeartBeatState(int generation, int heartbeat)
+    {
+        this(generation, heartbeat, 0);
+    }
+    
+    HeartBeatState(int generation, int heartbeat, int version)
+    {
+        generation_ = generation;
+        heartbeat_ = new AtomicInteger(heartbeat);
+        version_ = version;
+    }
+
+    public static ICompactSerializer<HeartBeatState> serializer()
+    {
+        return serializer_;
+    }
+    
+    int getGeneration()
+    {
+        return generation_;
+    }
+    
+    void updateGeneration()
+    {
+        ++generation_;
+        version_ = VersionGenerator.getNextVersion();
+    }
+    
+    int getHeartBeat()
+    {
+        return heartbeat_.get();
+    }
+    
+    void updateHeartBeat()
+    {
+        heartbeat_.incrementAndGet();      
+        version_ = VersionGenerator.getNextVersion();
+    }
+    
+    int getHeartBeatVersion()
+    {
+        return version_;
+    }
+};
+
+class HeartBeatStateSerializer implements ICompactSerializer<HeartBeatState>
+{
+    public void serialize(HeartBeatState hbState, DataOutputStream dos) throws IOException
+    {
+        dos.writeInt(hbState.generation_);
+        dos.writeInt(hbState.heartbeat_.get());
+        dos.writeInt(hbState.version_);
+    }
+    
+    public HeartBeatState deserialize(DataInputStream dis) throws IOException
+    {
+        return new HeartBeatState(dis.readInt(), dis.readInt(), dis.readInt());
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IEndPointStateChangePublisher.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IEndPointStateChangePublisher.java
index e69de29b..d456300e 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IEndPointStateChangePublisher.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IEndPointStateChangePublisher.java
@@ -0,0 +1,41 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+/**
+ * This is implemented by the Gossiper module to publish change events to interested parties.
+ * Interested parties register/unregister interest by invoking the methods of this interface.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IEndPointStateChangePublisher
+{
+    /**
+     * Register for interesting state changes.
+     * @param subcriber module which implements the IEndPointStateChangeSubscriber
+     */
+    public void register(IEndPointStateChangeSubscriber subcriber);
+    
+    /**
+     * Unregister interest for state changes.
+     * @param subcriber module which implements the IEndPointStateChangeSubscriber
+     */
+    public void unregister(IEndPointStateChangeSubscriber subcriber);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IEndPointStateChangeSubscriber.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IEndPointStateChangeSubscriber.java
index e69de29b..8b8709dd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IEndPointStateChangeSubscriber.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IEndPointStateChangeSubscriber.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * This is called by an instance of the IEndPointStateChangePublisher to notify
+ * interested parties about changes in the the state associated with any endpoint.
+ * For instance if node A figures there is a changes in state for an endpoint B
+ * it notifies all interested parties of this change. It is upto to the registered
+ * instance to decide what he does with this change. Not all modules maybe interested 
+ * in all state changes.
+ *  
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IEndPointStateChangeSubscriber
+{
+    /**
+     * Use to inform interested parties about the change in the state
+     * for specified endpoint
+     * 
+     * @param endpoint endpoint for which the state change occured.
+     * @param epState state that actually changed for the above endpoint.
+     */
+    public void onChange(EndPoint endpoint, EndPointState epState);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java
index e69de29b..1cfed5ad 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * Implemented by the Gossiper to either convict/suspect an endpoint
+ * based on the PHI calculated by the Failure Detector on the inter-arrival
+ * times of the heart beats.
+ *  
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IFailureDetectionEventListener
+{  
+    /**
+     * Convict the specified endpoint.
+     * @param ep endpoint to be convicted
+     */
+    public void convict(EndPoint ep);
+    
+    /**
+     * Suspect the specified endpoint.
+     * @param ep endpoint to be suspected.
+     */
+    public void suspect(EndPoint ep);    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureDetector.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureDetector.java
index e69de29b..95f49c07 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureDetector.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureDetector.java
@@ -0,0 +1,72 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * An interface that provides an application with the ability
+ * to query liveness information of a node in the cluster. It 
+ * also exposes methods which help an application register callbacks
+ * for notifications of liveness information of nodes.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IFailureDetector
+{
+    /**
+     * Failure Detector's knowledge of whether a node is up or
+     * down.
+     * 
+     * @param ep endpoint in question.
+     * @return true if UP and false if DOWN.
+     */
+    public boolean isAlive(EndPoint ep);
+    
+    /**
+     * This method is invoked by any entity wanting to interrogate the status of an endpoint. 
+     * In our case it would be the Gossiper. The Failure Detector will then calculate Phi and
+     * deem an endpoint as suspicious or alive as explained in the Hayashibara paper. 
+     * 
+     * param ep endpoint for which we interpret the inter arrival times.
+    */
+    public void intepret(EndPoint ep);
+    
+    /**
+     * This method is invoked by the receiver of the heartbeat. In our case it would be
+     * the Gossiper. Gossiper inform the Failure Detector on receipt of a heartbeat. The
+     * FailureDetector will then sample the arrival time as explained in the paper.
+     * 
+     * param ep endpoint being reported.
+    */
+    public void report(EndPoint ep);
+    
+    /**
+     * Register interest for Failure Detector events. 
+     * @param listener implementation of an application provided IFailureDetectionEventListener 
+     */
+    public void registerFailureDetectionEventListener(IFailureDetectionEventListener listener);
+    
+    /**
+     * Un-register interest for Failure Detector events. 
+     * @param listener implementation of an application provided IFailureDetectionEventListener 
+     */
+    public void unregisterFailureDetectionEventListener(IFailureDetectionEventListener listener);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureNotification.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureNotification.java
index e69de29b..a27edd36 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureNotification.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/IFailureNotification.java
@@ -0,0 +1,31 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IFailureNotification
+{   
+    public void suspect(EndPoint ep);
+    public void revive(EndPoint ep);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/JoinMessage.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/JoinMessage.java
index e69de29b..c8f12048 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/JoinMessage.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/JoinMessage.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class JoinMessage
+{
+    private static ICompactSerializer<JoinMessage> serializer_;
+    static
+    {
+        serializer_ = new JoinMessageSerializer();
+    }
+    
+    static ICompactSerializer<JoinMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    String clusterId_;
+    
+    JoinMessage(String clusterId)
+    {
+        clusterId_ = clusterId;
+    }
+}
+
+class JoinMessageSerializer implements ICompactSerializer<JoinMessage>
+{
+    public void serialize(JoinMessage joinMessage, DataOutputStream dos) throws IOException
+    {    
+        dos.writeUTF(joinMessage.clusterId_);         
+    }
+
+    public JoinMessage deserialize(DataInputStream dis) throws IOException
+    {
+        String clusterId = dis.readUTF();
+        return new JoinMessage(clusterId);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/PureRandom.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/PureRandom.java
index e69de29b..a8fcf5b3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/PureRandom.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/PureRandom.java
@@ -0,0 +1,83 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.util.Random;
+import java.util.BitSet;
+
+
+/**
+ * Implementation of a PureRandomNumber generator. Use this class cautiously. Not
+ * for general purpose use. Currently this is used by the Gossiper to choose a random
+ * endpoint to Gossip to.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class PureRandom extends Random
+{
+    private BitSet bs_ = new BitSet();
+    private int lastUb_;
+
+    PureRandom()
+    {
+        super();
+    }
+
+    public int nextInt(int ub)
+    {
+    	if (ub <= 0)
+    		throw new IllegalArgumentException("ub must be positive");
+
+        if ( lastUb_ !=  ub )
+        {
+            bs_.clear();
+            lastUb_ = ub;
+        }
+        else if(bs_.cardinality() == ub)
+        {
+        	bs_.clear();
+        }
+
+        int value = super.nextInt(ub);
+        while ( bs_.get(value) )
+        {
+            value = super.nextInt(ub);
+        }
+        bs_.set(value);
+        return value;
+    }
+
+    public static void main(String[] args) throws Throwable
+    {
+    	Random pr = new PureRandom();
+        int ubs[] = new int[] { 2, 3, 1, 10, 5, 0};
+
+        for (int ub : ubs)
+        {
+            System.out.println("UB: " + String.valueOf(ub));
+            for (int j = 0; j < 10; j++)
+            {
+                int junk = pr.nextInt(ub);
+                // Do something with junk so JVM doesn't optimize away
+                System.out.println(junk);
+            }
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/VersionGenerator.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/VersionGenerator.java
index e69de29b..d772da81 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/VersionGenerator.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/gms/VersionGenerator.java
@@ -0,0 +1,37 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * A unique version number generator for any state that is generated by the 
+ * local node.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class VersionGenerator
+{
+    private static AtomicInteger version_ = new AtomicInteger(0);
+    
+    public static int getNextVersion()
+    {
+        return version_.incrementAndGet();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/BufferedRandomAccessFile.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/BufferedRandomAccessFile.java
index e69de29b..f8933ea2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/BufferedRandomAccessFile.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/BufferedRandomAccessFile.java
@@ -0,0 +1,373 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.RandomAccessFile;
+import java.util.Arrays;
+
+/**
+ * A <code>BufferedRandomAccessFile</code> is like a
+ * <code>RandomAccessFile</code>, but it uses a private buffer so that most
+ * operations do not require a disk access.
+ * <P>
+ * 
+ * Note: The operations on this class are unmonitored. Also, the correct
+ * functioning of the <code>RandomAccessFile</code> methods that are not
+ * overridden here relies on the implementation of those methods in the
+ * superclass.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class BufferedRandomAccessFile extends RandomAccessFile
+{
+    static final int LogBuffSz_ = 16; // 64K buffer
+    public static final int BuffSz_ = (1 << LogBuffSz_);
+    static final long BuffMask_ = ~(((long) BuffSz_) - 1L);
+    
+    /*
+     * This implementation is based on the buffer implementation in Modula-3's
+     * "Rd", "Wr", "RdClass", and "WrClass" interfaces.
+     */
+    private boolean dirty_; // true iff unflushed bytes exist
+    private long curr_; // current position in file
+    private long lo_, hi_; // bounds on characters in "buff"
+    private byte[] buff_; // local buffer
+    private long maxHi_; // this.lo + this.buff.length
+    private boolean hitEOF_; // buffer contains last file block?
+    private long diskPos_; // disk position
+    
+    /*
+     * To describe the above fields, we introduce the following abstractions for
+     * the file "f":
+     * 
+     * len(f) the length of the file curr(f) the current position in the file
+     * c(f) the abstract contents of the file disk(f) the contents of f's
+     * backing disk file closed(f) true iff the file is closed
+     * 
+     * "curr(f)" is an index in the closed interval [0, len(f)]. "c(f)" is a
+     * character sequence of length "len(f)". "c(f)" and "disk(f)" may differ if
+     * "c(f)" contains unflushed writes not reflected in "disk(f)". The flush
+     * operation has the effect of making "disk(f)" identical to "c(f)".
+     * 
+     * A file is said to be *valid* if the following conditions hold:
+     * 
+     * V1. The "closed" and "curr" fields are correct:
+     * 
+     * f.closed == closed(f) f.curr == curr(f)
+     * 
+     * V2. The current position is either contained in the buffer, or just past
+     * the buffer:
+     * 
+     * f.lo <= f.curr <= f.hi
+     * 
+     * V3. Any (possibly) unflushed characters are stored in "f.buff":
+     * 
+     * (forall i in [f.lo, f.curr): c(f)[i] == f.buff[i - f.lo])
+     * 
+     * V4. For all characters not covered by V3, c(f) and disk(f) agree:
+     * 
+     * (forall i in [f.lo, len(f)): i not in [f.lo, f.curr) => c(f)[i] ==
+     * disk(f)[i])
+     * 
+     * V5. "f.dirty" is true iff the buffer contains bytes that should be
+     * flushed to the file; by V3 and V4, only part of the buffer can be dirty.
+     * 
+     * f.dirty == (exists i in [f.lo, f.curr): c(f)[i] != f.buff[i - f.lo])
+     * 
+     * V6. this.maxHi == this.lo + this.buff.length
+     * 
+     * Note that "f.buff" can be "null" in a valid file, since the range of
+     * characters in V3 is empty when "f.lo == f.curr".
+     * 
+     * A file is said to be *ready* if the buffer contains the current position,
+     * i.e., when:
+     * 
+     * R1. !f.closed && f.buff != null && f.lo <= f.curr && f.curr < f.hi
+     * 
+     * When a file is ready, reading or writing a single byte can be performed
+     * by reading or writing the in-memory buffer without performing a disk
+     * operation.
+     */
+    
+    /**
+     * Open a new <code>BufferedRandomAccessFile</code> on <code>file</code>
+     * in mode <code>mode</code>, which should be "r" for reading only, or
+     * "rw" for reading and writing.
+     */
+    public BufferedRandomAccessFile(File file, String mode) throws IOException
+    {
+        super(file, mode);
+        this.init(0);
+    }
+    
+    public BufferedRandomAccessFile(File file, String mode, int size) throws IOException
+    {
+        super(file, mode);
+        this.init(size);
+    }
+    
+    /**
+     * Open a new <code>BufferedRandomAccessFile</code> on the file named
+     * <code>name</code> in mode <code>mode</code>, which should be "r" for
+     * reading only, or "rw" for reading and writing.
+     */
+    public BufferedRandomAccessFile(String name, String mode) throws IOException
+    {
+        super(name, mode);
+        this.init(0);
+    }
+    
+    public BufferedRandomAccessFile(String name, String mode, int size) throws FileNotFoundException
+    {
+        super(name, mode);
+        this.init(size);
+    }
+    
+    private void init(int size)
+    {
+        this.dirty_ = false;
+        this.lo_ = this.curr_ = this.hi_ = 0;
+        this.buff_ = (size > BuffSz_) ? new byte[size] : new byte[BuffSz_];
+        this.maxHi_ = (long) BuffSz_;
+        this.hitEOF_ = false;
+        this.diskPos_ = 0L;
+    }
+    
+    public void close() throws IOException
+    {
+        this.flush();
+        super.close();
+    }
+    
+    /**
+     * Flush any bytes in the file's buffer that have not yet been written to
+     * disk. If the file was created read-only, this method is a no-op.
+     */
+    public void flush() throws IOException
+    {        
+        this.flushBuffer();
+    }
+    
+    /* Flush any dirty bytes in the buffer to disk. */
+    private void flushBuffer() throws IOException
+    {   
+        if (this.dirty_)
+        {
+            if (this.diskPos_ != this.lo_)
+                super.seek(this.lo_);
+            int len = (int) (this.curr_ - this.lo_);
+            super.write(this.buff_, 0, len);
+            this.diskPos_ = this.curr_;             
+            this.dirty_ = false;
+        }
+    }
+    
+    /*
+     * Read at most "this.buff.length" bytes into "this.buff", returning the
+     * number of bytes read. If the return result is less than
+     * "this.buff.length", then EOF was read.
+     */
+    private int fillBuffer() throws IOException
+    {
+        int cnt = 0;
+        int rem = this.buff_.length;
+        while (rem > 0)
+        {
+            int n = super.read(this.buff_, cnt, rem);
+            if (n < 0)
+                break;
+            cnt += n;
+            rem -= n;
+        }
+        if ( (cnt < 0) && (this.hitEOF_ = (cnt < this.buff_.length)) )
+        {
+            // make sure buffer that wasn't read is initialized with -1
+            Arrays.fill(this.buff_, cnt, this.buff_.length, (byte) 0xff);
+        }
+        this.diskPos_ += cnt;
+        return cnt;
+    }
+    
+    /*
+     * This method positions <code>this.curr</code> at position <code>pos</code>.
+     * If <code>pos</code> does not fall in the current buffer, it flushes the
+     * current buffer and loads the correct one.<p>
+     * 
+     * On exit from this routine <code>this.curr == this.hi</code> iff <code>pos</code>
+     * is at or past the end-of-file, which can only happen if the file was
+     * opened in read-only mode.
+     */
+    public void seek(long pos) throws IOException
+    {
+        if (pos >= this.hi_ || pos < this.lo_)
+        {
+            // seeking outside of current buffer -- flush and read             
+            this.flushBuffer();
+            this.lo_ = pos & BuffMask_; // start at BuffSz boundary
+            this.maxHi_ = this.lo_ + (long) this.buff_.length;
+            if (this.diskPos_ != this.lo_)
+            {
+                super.seek(this.lo_);
+                this.diskPos_ = this.lo_;
+            }
+            int n = this.fillBuffer();
+            this.hi_ = this.lo_ + (long) n;
+        }
+        else
+        {
+            // seeking inside current buffer -- no read required
+            if (pos < this.curr_)
+            {
+                // if seeking backwards, we must flush to maintain V4
+                this.flushBuffer();
+            }
+        }
+        this.curr_ = pos;
+    }
+    
+    public long getFilePointer()
+    {
+        return this.curr_;
+    }
+    
+    public long length() throws IOException
+    {
+        return Math.max(this.curr_, super.length());
+    }
+    
+    public int read() throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            // test for EOF
+            // if (this.hi < this.maxHi) return -1;
+            if (this.hitEOF_)
+                return -1;
+            
+            // slow path -- read another buffer
+            this.seek(this.curr_);
+            if (this.curr_ == this.hi_)
+                return -1;
+        }
+        byte res = this.buff_[(int) (this.curr_ - this.lo_)];
+        this.curr_++;
+        return ((int) res) & 0xFF; // convert byte -> int
+    }
+    
+    public int read(byte[] b) throws IOException
+    {
+        return this.read(b, 0, b.length);
+    }
+    
+    public int read(byte[] b, int off, int len) throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            // test for EOF
+            // if (this.hi < this.maxHi) return -1;
+            if (this.hitEOF_)
+                return -1;
+            
+            // slow path -- read another buffer
+            this.seek(this.curr_);
+            if (this.curr_ == this.hi_)
+                return -1;
+        }
+        len = Math.min(len, (int) (this.hi_ - this.curr_));
+        int buffOff = (int) (this.curr_ - this.lo_);
+        System.arraycopy(this.buff_, buffOff, b, off, len);
+        this.curr_ += len;
+        return len;
+    }
+    
+    public void write(int b) throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            if (this.hitEOF_ && this.hi_ < this.maxHi_)
+            {
+                // at EOF -- bump "hi"
+                this.hi_++;
+            }
+            else
+            {
+                // slow path -- write current buffer; read next one
+                this.seek(this.curr_);
+                if (this.curr_ == this.hi_)
+                {
+                    // appending to EOF -- bump "hi"
+                    this.hi_++;
+                }
+            }
+        }
+        this.buff_[(int) (this.curr_ - this.lo_)] = (byte) b;
+        this.curr_++;
+        this.dirty_ = true;
+    }
+    
+    public void write(byte[] b) throws IOException
+    {
+        this.write(b, 0, b.length);
+    }
+    
+    public void write(byte[] b, int off, int len) throws IOException
+    {        
+        while (len > 0)
+        {              
+            int n = this.writeAtMost(b, off, len);
+            off += n;
+            len -= n;
+            this.dirty_ = true;
+        }        
+    }
+    
+    /*
+     * Write at most "len" bytes to "b" starting at position "off", and return
+     * the number of bytes written.
+     */
+    private int writeAtMost(byte[] b, int off, int len) throws IOException
+    {        
+        if (this.curr_ >= this.hi_)
+        {
+            if (this.hitEOF_ && this.hi_ < this.maxHi_)
+            {
+                // at EOF -- bump "hi"
+                this.hi_ = this.maxHi_;
+            }
+            else
+            {                                
+                // slow path -- write current buffer; read next one                
+                this.seek(this.curr_);
+                if (this.curr_ == this.hi_)
+                {
+                    // appending to EOF -- bump "hi"
+                    this.hi_ = this.maxHi_;
+                }
+            }
+        }
+        len = Math.min(len, (int) (this.hi_ - this.curr_));
+        int buffOff = (int) (this.curr_ - this.lo_);
+        System.arraycopy(b, off, this.buff_, buffOff, len);
+        this.curr_ += len;
+        return len;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ChecksumManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ChecksumManager.java
index e69de29b..6867dc54 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ChecksumManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ChecksumManager.java
@@ -0,0 +1,481 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io;
+
+import java.io.BufferedInputStream;
+import java.io.DataInputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.RandomAccessFile;
+import java.lang.reflect.Method;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.zip.Adler32;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import bak.pcj.map.AbstractLongKeyLongMap;
+import bak.pcj.map.LongKeyLongChainedHashMap;
+
+/**
+ * This class manages the persistence of checksums and keeps
+ * them in memory. It maintains a mapping of data files on
+ * disk to their corresponding checksum files. It is also
+ * loads the checksums in memory on start up.
+ * 
+ * @author alakshman
+ *
+ */
+public class ChecksumManager
+{    
+    private static Logger logger_ = Logger.getLogger(ChecksumManager.class);
+    /* Keeps a mapping of checksum manager instances to data file */
+    private static Map<String, ChecksumManager> chksumMgrs_ = new HashMap<String, ChecksumManager>();
+    private static Lock lock_ = new ReentrantLock();
+    private static final String checksumPrefix_ = "Checksum-";
+    private static final int bufferSize_ = 8*1024*1024;
+    private static final long chunkMask_ = 0x00000000FFFFFFFFL;
+    private static final long fileIdMask_ = 0x7FFFFFFF00000000L;
+    /* Map where checksums are cached. */
+    private static AbstractLongKeyLongMap chksums_ = new LongKeyLongChainedHashMap();
+
+    public static ChecksumManager instance(String dataFile) throws IOException
+    {
+        ChecksumManager chksumMgr = chksumMgrs_.get(dataFile);
+        if ( chksumMgr == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( chksumMgr == null )
+                {
+                    chksumMgr = new ChecksumManager(dataFile);
+                    chksumMgrs_.put(dataFile, chksumMgr);
+                }
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return chksumMgr;
+    }
+    
+    /* TODO: Debug only */
+    public static ChecksumManager instance(String dataFile, String chkSumFile) throws IOException
+    {
+        ChecksumManager chksumMgr = chksumMgrs_.get(dataFile);
+        if ( chksumMgr == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( chksumMgr == null )
+                {
+                    chksumMgr = new ChecksumManager(dataFile, chkSumFile);
+                    chksumMgrs_.put(dataFile, chksumMgr);
+                }
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return chksumMgr;
+    }
+    
+    /**
+     * This method returns true if the file specified is a 
+     * checksum file and false otherwise.
+     * 
+     * @param file we are interested in.
+     * @return true if checksum file false otherwise.
+     */
+    public static boolean isChecksumFile(String file)
+    {
+        return file.contains(ChecksumManager.checksumPrefix_);
+    }
+    
+    /**
+     * On start read all the check sum files on disk and
+     * pull them into memory.
+     * @throws IOException
+     */
+    public static void onStart() throws IOException
+    {
+        String[] directories = DatabaseDescriptor.getAllDataFileLocations(); 
+        List<File> allFiles = new ArrayList<File>();
+        for ( String directory : directories )
+        {
+            File file = new File(directory);
+            File[] files = file.listFiles();
+            for ( File f : files )
+            {
+                if ( f.getName().contains(ChecksumManager.checksumPrefix_) )
+                {
+                    allFiles.add(f);
+                }
+            }
+        }
+        
+        for ( File file : allFiles )
+        {                           
+            int fId = ChecksumManager.getChecksumFileId(file.getName());
+            RandomAccessFile chksumRdr = new RandomAccessFile(file, "r");            
+            long size = chksumRdr.length();
+            int chunk = 0;
+            
+            while ( chksumRdr.getFilePointer() != size )
+            {
+                long value = chksumRdr.readLong();
+                long key = ChecksumManager.key(fId, ++chunk);
+                chksums_.put(key, value);
+            }
+        }
+    }
+    
+    /**
+     * On delete of this dataFile remove the checksums associated with
+     * this file from memory, remove the check sum manager instance.
+     * 
+     * @param dataFile data file that is being deleted.
+     * @throws IOException
+     */
+    public static void onFileDelete(String dataFile) throws IOException
+    {
+        File f = new File(dataFile);
+        long size = f.length();
+        int fileId = ChecksumManager.getFileId(f.getName());
+        int chunks = (int)(size >> 16L);
+        
+        for ( int i = 0; i < chunks; ++i )
+        {
+            long key = ChecksumManager.key(fileId, i);
+            chksums_.remove(key);
+        }
+        
+        /* remove the check sum manager instance */
+        chksumMgrs_.remove(dataFile);
+        String chksumFile = ChecksumManager.constructChksumFileNameFromDataFileName(f);
+        FileUtils.delete(chksumFile);
+    }
+    
+    private static long key(int fileId, int chunkId)
+    {
+        long key = 0;
+        key |= fileId;
+        key <<= 32;
+        key |= chunkId;
+        return key;
+    }
+    
+    public static int getFileId(String file)
+    {
+        String filename = new File(file).getName();
+        /*
+         * File name is of the form <table>-<column family>-<index>-Data.db.
+         * Always split and then use the value which is at index length - 2.
+         */
+        String[] peices = filename.split("-");
+        return Integer.parseInt( peices[peices.length - 2] );
+    }
+    
+    static void close(String dataFile) throws IOException
+    {
+        ChecksumManager.chksumMgrs_.get(dataFile).close();
+    }
+    
+    private static int getChecksumFileId(String file)
+    {
+        String filename = new File(file).getName();
+        /*
+         * File name is of the form <table>-<column family>-Checksum-<index>.db.
+         * This tokenizer will strip the .db portion.
+         */
+        String[] peices = filename.split("-");
+        return Integer.parseInt( peices[3] );
+    }
+    
+    private static String constructChksumFileNameFromDataFileName(File file)
+    {
+        String directory = file.getParent();
+        String f = file.getName();
+        /* we need the table and the column family name. */
+        String[] peices = f.split("-");
+        /* we need the index part of the file name */
+        int fId = ChecksumManager.getFileId(f);
+        String chkSumFile = directory + System.getProperty("file.separator") + peices[0] + "-" + peices[1] + "-" + checksumPrefix_ + fId + "-" + "Data" + ".db";
+        return chkSumFile;
+    }
+    
+    private RandomAccessFile raf_;
+    private Adler32 adler_ = new Adler32();
+    
+    ChecksumManager(String dataFile) throws IOException
+    {
+        File file = new File(dataFile);
+        String chkSumFile = ChecksumManager.constructChksumFileNameFromDataFileName(file);
+        raf_ = new BufferedRandomAccessFile(chkSumFile, "rw");
+    }
+    
+    /* TODO: Remove later. */
+    ChecksumManager(String dataFile, String chkSumFile) throws IOException
+    {
+        File file = new File(dataFile);
+        String directory = file.getParent();
+        String f = file.getName();
+        int fId = ChecksumManager.getFileId(f);        
+        raf_ = new BufferedRandomAccessFile(chkSumFile, "rw");
+        
+        file = new File(chkSumFile);        
+        ChecksumReader chksumRdr = new ChecksumReader(file.getAbsolutePath(), 0L, file.length());
+                    
+        int chunk = 0;
+        while ( !chksumRdr.isEOF() )
+        {
+            long value = chksumRdr.readLong();
+            long key = ChecksumManager.key(fId, ++chunk);
+            chksums_.put(key, value);
+        }
+    }
+    
+    /**
+     * Log the checksum for the the specified file and chunk
+     * within the file.
+     * @param fileId id associated with the file
+     * @param chunkId chunk within the file.
+     * @param buffer for which the checksum needs to be calculated.
+     * @throws IOException
+     */
+    void logChecksum(int fileId, int chunkId, byte[] buffer)
+    {
+        logChecksum(fileId, chunkId, buffer, 0, buffer.length);
+    }
+    
+    /**
+     * Log the checksum for the the specified file and chunk
+     * within the file.
+     * @param fileId id associated with the file
+     * @param chunkId chunk within the file.
+     * @param buffer for which the checksum needs to be calculated.
+     * @param startoffset offset to start within the buffer
+     * @param length size of the checksum buffer.
+     * @throws IOException
+     */
+    void logChecksum(int fileId, int chunkId, byte[] buffer, int startOffset, int length)
+    {
+        try
+        {            
+            adler_.update(buffer, startOffset, length);
+            long chksum = adler_.getValue();
+            adler_.reset();
+            /* log checksums to disk */
+            raf_.writeLong(chksum);
+            /* add the chksum to memory */
+            long key = ChecksumManager.key(fileId, chunkId);
+            chksums_.put(key, chksum);
+        }
+        catch ( IOException ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+    }
+    
+    /**
+     * Validate checksums for the data in the buffer.
+     * @file name of the file from which data is being
+     *       read.
+     * @chunkId chunkId
+     * @param buffer with data for which checksum needs to be 
+     *        verified.
+     * @throws IOException
+     */
+    void validateChecksum(String file, int chunkId, byte[] buffer) throws IOException
+    {                
+        validateChecksum(file, chunkId, buffer, 0, buffer.length);
+    }
+    
+    /**
+     * Validate checksums for the data in the buffer for the region
+     * that is encapsulated in the section object
+     * @file name of the file from which data is being
+     *       read.
+     * @chunkId chunkId     
+     * @param buffer with data for which checksum needs to be 
+     *        verified.
+     * @param startOffset within the buffer
+     * @param length of the data whose checksum needs to be verified.
+     * @throws IOException
+     */
+    void validateChecksum(String file, int chunkId, byte[] buffer, int startOffset, int length) throws IOException
+    {            
+        int fId = ChecksumManager.getFileId(file);
+        long key = ChecksumManager.key(fId, chunkId);
+        adler_.update(buffer, startOffset, length);
+        long currentChksum = adler_.getValue();
+        adler_.reset();
+        long oldChksum = chksums_.get(key);
+        if ( currentChksum != oldChksum )
+        {                                   
+            throw new IOException("Checksums do not match in file " + file + " for chunk " + chunkId + ".");
+        }        
+    }
+    
+    
+    /**
+     * Get the checksum for the specified file's chunk
+     * @param fileId id associated with the file.
+     * @param chunkId chunk within the file.
+     * @return associated checksum for the chunk
+     */
+    long getChecksum(int fileId, int chunkId)
+    {        
+        long key = ChecksumManager.key(fileId, chunkId);
+        return chksums_.get(key);
+    }
+    
+    /**
+     * Close the file handler.
+     * 
+     * @throws IOException
+     */
+    void close() throws IOException
+    {
+        raf_.close();
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {
+        ChecksumReader rdr = new ChecksumReader("C:\\Engagements\\Cassandra\\Checksum-1.db");
+        while ( !rdr.isEOF() )
+        {
+            System.out.println(rdr.readLong());
+        }
+        rdr.close();
+    }
+}
+
+/**
+ * ChecksumReader is used to memory map the checksum files and
+ * load the data into memory.
+ * 
+ * @author alakshman
+ *
+ */
+class ChecksumReader 
+{
+    private static Logger logger_ = Logger.getLogger(ChecksumReader.class);
+    private String filename_;
+    private MappedByteBuffer buffer_;
+
+    ChecksumReader(String filename) throws IOException
+    {
+        filename_ = filename;
+        File f = new File(filename);
+        map(0, f.length());
+    }
+    
+    ChecksumReader(String filename, long start, long end) throws IOException
+    {        
+        filename_ = filename;
+        map(start, end);
+    }
+
+    public void map() throws IOException
+    {
+        RandomAccessFile file = new RandomAccessFile(filename_, "rw");
+        try
+        {
+            buffer_ = file.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, file.length() );
+            buffer_.load();
+        }
+        finally
+        {
+            file.close();
+        }
+    }
+
+    public void map(long start, long end) throws IOException
+    {
+        if ( start < 0 || end < 0 || end < start )
+            throw new IllegalArgumentException("Invalid values for start and end.");
+
+        RandomAccessFile file = new RandomAccessFile(filename_, "rw");
+        try
+        {
+            if ( end == 0 )
+                end = file.length();
+            buffer_ = file.getChannel().map(FileChannel.MapMode.READ_ONLY, start, end);
+            buffer_.load();
+        }
+        finally
+        {
+            file.close();
+        }
+    }
+
+    void unmap(final Object buffer)
+    {
+        AccessController.doPrivileged( new PrivilegedAction<MappedByteBuffer>()
+                {
+            public MappedByteBuffer run()
+            {
+                try
+                {
+                    Method getCleanerMethod = buffer.getClass().getMethod("cleaner", new Class[0]);
+                    getCleanerMethod.setAccessible(true);
+                    sun.misc.Cleaner cleaner = (sun.misc.Cleaner)getCleanerMethod.invoke(buffer,new Object[0]);
+                    cleaner.clean();
+                }
+                catch(Throwable e)
+                {
+                    logger_.debug( LogUtil.throwableToString(e) );
+                }
+                return null;
+            }
+                });
+    }
+    
+    public long readLong() throws IOException
+    {
+        return buffer_.getLong();
+    }
+    
+    public boolean isEOF()
+    {
+        return ( buffer_.remaining() == 0 );
+    }
+
+    
+    public void close() throws IOException
+    {
+        unmap(buffer_);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ChecksumRandomAccessFile.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ChecksumRandomAccessFile.java
index e69de29b..347b2467 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ChecksumRandomAccessFile.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ChecksumRandomAccessFile.java
@@ -0,0 +1,456 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+import java.util.Arrays;
+import java.util.Random;
+
+import org.apache.cassandra.utils.FBUtilities;
+
+
+/**
+ * A <code>ChecksumRandomAccessFile</code> is like a
+ * <code>RandomAccessFile</code>, but it uses a private buffer so that most
+ * operations do not require a disk access.
+ * <P>
+ * 
+ * Note: The operations on this class are unmonitored. Also, the correct
+ * functioning of the <code>RandomAccessFile</code> methods that are not
+ * overridden here relies on the implementation of those methods in the
+ * superclass.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class ChecksumRandomAccessFile extends RandomAccessFile
+{ 
+    private static enum ChecksumOperations
+    {
+        LOG,
+        VERIFY
+    }
+    
+    static final int LogBuffSz_ = 16; // 64K buffer
+    private static final int checksumSz_ = (1 << LogBuffSz_); // 64K
+    public static final int BuffSz_ = (1 << LogBuffSz_);
+    static final long BuffMask_ = ~(((long) BuffSz_) - 1L);
+    
+    /*
+     * This implementation is based on the buffer implementation in Modula-3's
+     * "Rd", "Wr", "RdClass", and "WrClass" interfaces.
+     */
+    private boolean dirty_; // true iff unflushed bytes exist
+    private boolean closed_; // true iff the file is closed
+    private long curr_; // current position in file
+    private long lo_, hi_; // bounds on characters in "buff"
+    private byte[] buff_; // local buffer
+    private long maxHi_; // this.lo + this.buff.length
+    private boolean hitEOF_; // buffer contains last file block?
+    private long diskPos_; // disk position
+    private String filename_; // file name
+    
+    /*
+     * To describe the above fields, we introduce the following abstractions for
+     * the file "f":
+     * 
+     * len(f) the length of the file curr(f) the current position in the file
+     * c(f) the abstract contents of the file disk(f) the contents of f's
+     * backing disk file closed(f) true iff the file is closed
+     * 
+     * "curr(f)" is an index in the closed interval [0, len(f)]. "c(f)" is a
+     * character sequence of length "len(f)". "c(f)" and "disk(f)" may differ if
+     * "c(f)" contains unflushed writes not reflected in "disk(f)". The flush
+     * operation has the effect of making "disk(f)" identical to "c(f)".
+     * 
+     * A file is said to be *valid* if the following conditions hold:
+     * 
+     * V1. The "closed" and "curr" fields are correct:
+     * 
+     * f.closed == closed(f) f.curr == curr(f)
+     * 
+     * V2. The current position is either contained in the buffer, or just past
+     * the buffer:
+     * 
+     * f.lo <= f.curr <= f.hi
+     * 
+     * V3. Any (possibly) unflushed characters are stored in "f.buff":
+     * 
+     * (forall i in [f.lo, f.curr): c(f)[i] == f.buff[i - f.lo])
+     * 
+     * V4. For all characters not covered by V3, c(f) and disk(f) agree:
+     * 
+     * (forall i in [f.lo, len(f)): i not in [f.lo, f.curr) => c(f)[i] ==
+     * disk(f)[i])
+     * 
+     * V5. "f.dirty" is true iff the buffer contains bytes that should be
+     * flushed to the file; by V3 and V4, only part of the buffer can be dirty.
+     * 
+     * f.dirty == (exists i in [f.lo, f.curr): c(f)[i] != f.buff[i - f.lo])
+     * 
+     * V6. this.maxHi == this.lo + this.buff.length
+     * 
+     * Note that "f.buff" can be "null" in a valid file, since the range of
+     * characters in V3 is empty when "f.lo == f.curr".
+     * 
+     * A file is said to be *ready* if the buffer contains the current position,
+     * i.e., when:
+     * 
+     * R1. !f.closed && f.buff != null && f.lo <= f.curr && f.curr < f.hi
+     * 
+     * When a file is ready, reading or writing a single byte can be performed
+     * by reading or writing the in-memory buffer without performing a disk
+     * operation.
+     */
+    
+    /**
+     * Open a new <code>BufferedRandomAccessFile</code> on <code>file</code>
+     * in mode <code>mode</code>, which should be "r" for reading only, or
+     * "rw" for reading and writing.
+     */
+    public ChecksumRandomAccessFile(File file, String mode) throws IOException
+    {
+        super(file, mode);
+        this.init(file.getAbsolutePath(), 0);
+    }
+    
+    public ChecksumRandomAccessFile(File file, String mode, int size) throws IOException
+    {
+        super(file, mode);
+        this.init(file.getAbsolutePath(), size);
+    }
+    
+    /**
+     * Open a new <code>BufferedRandomAccessFile</code> on the file named
+     * <code>name</code> in mode <code>mode</code>, which should be "r" for
+     * reading only, or "rw" for reading and writing.
+     */
+    public ChecksumRandomAccessFile(String name, String mode) throws IOException
+    {
+        super(name, mode);
+        this.init(name, 0);
+    }
+    
+    public ChecksumRandomAccessFile(String name, String mode, int size) throws FileNotFoundException
+    {
+        super(name, mode);
+        this.init(name, size);
+    }
+    
+    private void init(String name, int size)
+    {
+        this.dirty_ = this.closed_ = false;
+        this.lo_ = this.curr_ = this.hi_ = 0;
+        this.buff_ = (size > BuffSz_) ? new byte[size] : new byte[BuffSz_];
+        this.maxHi_ = (long) BuffSz_;
+        this.hitEOF_ = false;
+        this.diskPos_ = 0L;
+        this.filename_ = name;
+    }
+    
+    public void close() throws IOException
+    {
+        this.flush();
+        this.closed_ = true;
+        super.close();
+    }
+    
+    /**
+     * Flush any bytes in the file's buffer that have not yet been written to
+     * disk. If the file was created read-only, this method is a no-op.
+     */
+    public void flush() throws IOException
+    {
+        this.flushBuffer();
+    }
+    
+    private void doChecksumOperation(ChecksumOperations chksumOps) throws IOException
+    {        
+        int buffSz = buff_.length;
+        /*
+         * If the diskPos_ is at the buffer boundary then return 
+         * diskPos_ - 1 else return the actual diskPos_.
+        */        
+        long currentPosition = ( (diskPos_ % buffSz) == 0 ) ? diskPos_ - 1 : diskPos_;
+        /* Tells me which buffered chunk I am in. */
+        long chunk = (currentPosition / buffSz) + 1; 
+        /* Number of checksum chunks within a buffer */
+        int chksumChunks = buff_.length / checksumSz_;  
+        /* Position of the start of the previous buffer boundary */
+        long pos = (chunk == 0) ? 0 : (chunk - 1)*buffSz;
+        int startOffset = 0;
+        int chksumChunkId = (int)(chksumChunks*(chunk - 1) + 1);
+        do
+        {            
+            int fId = ChecksumManager.getFileId(filename_);               
+            switch( chksumOps )
+            {
+                case LOG:                    
+                    ChecksumManager.instance(filename_).logChecksum(fId, chksumChunkId++, buff_, startOffset, checksumSz_);
+                    break;
+                case VERIFY:
+                    ChecksumManager.instance(filename_).validateChecksum(filename_, chksumChunkId++, buff_, startOffset, checksumSz_);
+                    break;
+            }
+            pos += checksumSz_;
+            startOffset += checksumSz_;
+        }
+        while ( pos < currentPosition );
+    }
+    
+    /**
+     * Flush any dirty bytes in the buffer to disk. 
+    */
+    private void flushBuffer() throws IOException
+    {
+        if (this.dirty_)
+        {
+            if (this.diskPos_ != this.lo_)
+                super.seek(this.lo_);
+            int len = (int) (this.curr_ - this.lo_);            
+            super.write(this.buff_, 0, len);
+            this.diskPos_ = this.curr_;
+            this.dirty_ = false;
+            /* checksum the data before we write to disk */
+            doChecksumOperation(ChecksumOperations.LOG);
+        }
+    }
+    
+    /**
+     * Read at most "this.buff.length" bytes into "this.buff", returning the
+     * number of bytes read. If the return result is less than
+     * "this.buff.length", then EOF was read.
+     */
+    private int fillBuffer() throws IOException
+    {
+        int cnt = 0;
+        int rem = this.buff_.length;
+        int n = -1;
+        while (rem > 0)
+        {
+            n = super.read(this.buff_, cnt, rem);
+            if (n < 0)
+                break;
+            cnt += n;
+            rem -= n;
+        }
+        if (this.hitEOF_ = (cnt < this.buff_.length))
+        {
+            // make sure buffer that wasn't read is initialized with -1
+            Arrays.fill(this.buff_, cnt, this.buff_.length, (byte) 0xff);             
+        }                
+        this.diskPos_ += cnt;
+        return cnt;
+    }
+    
+    /**
+     * This method positions <code>this.curr</code> at position <code>pos</code>.
+     * If <code>pos</code> does not fall in the current buffer, it flushes the
+     * current buffer and loads the correct one.<p>
+     * 
+     * On exit from this routine <code>this.curr == this.hi</code> iff <code>pos</code>
+     * is at or past the end-of-file, which can only happen if the file was
+     * opened in read-only mode.
+     */
+    public void seek(long pos) throws IOException
+    {
+        if (pos >= this.hi_ || pos < this.lo_)
+        {
+            // seeking outside of current buffer -- flush and read
+            this.flushBuffer();
+            this.lo_ = pos & BuffMask_; // start at BuffSz boundary
+            this.maxHi_ = this.lo_ + (long) this.buff_.length;
+            if (this.diskPos_ != this.lo_)
+            {
+                super.seek(this.lo_);
+                this.diskPos_ = this.lo_;
+            }
+            int n = this.fillBuffer();
+            if ( n > 0 )
+            {
+                doChecksumOperation(ChecksumOperations.VERIFY);
+            }
+            this.hi_ = this.lo_ + (long) n;
+        }
+        else
+        {
+            // seeking inside current buffer -- no read required
+            if (pos < this.curr_)
+            {
+                // if seeking backwards, we must flush to maintain V4
+                this.flushBuffer();
+            }
+        }
+        this.curr_ = pos;
+    }
+    
+    public long getFilePointer()
+    {
+        return this.curr_;
+    }
+    
+    public long length() throws IOException
+    {
+        return Math.max(this.curr_, super.length());
+    }
+    
+    public int read() throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            // test for EOF
+            // if (this.hi < this.maxHi) return -1;
+            if (this.hitEOF_)
+                return -1;
+            
+            // slow path -- read another buffer
+            this.seek(this.curr_);
+            if (this.curr_ == this.hi_)
+                return -1;
+        }
+        byte res = this.buff_[(int) (this.curr_ - this.lo_)];
+        this.curr_++;
+        return ((int) res) & 0xFF; // convert byte -> int
+    }
+    
+    public int read(byte[] b) throws IOException
+    {
+        return this.read(b, 0, b.length);
+    }
+    
+    public int read(byte[] b, int off, int len) throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            // test for EOF
+            // if (this.hi < this.maxHi) return -1;
+            if (this.hitEOF_)
+                return -1;
+            
+            // slow path -- read another buffer
+            this.seek(this.curr_);
+            if (this.curr_ == this.hi_)
+                return -1;
+        }
+        len = Math.min(len, (int) (this.hi_ - this.curr_));
+        int buffOff = (int) (this.curr_ - this.lo_);
+        System.arraycopy(this.buff_, buffOff, b, off, len);
+        this.curr_ += len;
+        return len;
+    }
+    
+    public void write(int b) throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            if (this.hitEOF_ && this.hi_ < this.maxHi_)
+            {
+                // at EOF -- bump "hi"
+                this.hi_++;
+            }
+            else
+            {
+                // slow path -- write current buffer; read next one
+                this.seek(this.curr_);
+                if (this.curr_ == this.hi_)
+                {
+                    // appending to EOF -- bump "hi"
+                    this.hi_++;
+                }
+            }
+        }
+        this.buff_[(int) (this.curr_ - this.lo_)] = (byte) b;
+        this.curr_++;
+        this.dirty_ = true;
+    }
+    
+    public void write(byte[] b) throws IOException
+    {
+        this.write(b, 0, b.length);
+    }
+    
+    public void write(byte[] b, int off, int len) throws IOException
+    {
+        while (len > 0)
+        {
+            int n = this.writeAtMost(b, off, len);
+            off += n;
+            len -= n;
+            this.dirty_ = true;
+        }
+    }
+    
+    /*
+     * Write at most "len" bytes to "b" starting at position "off", and return
+     * the number of bytes written.
+     */
+    private int writeAtMost(byte[] b, int off, int len) throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            if (this.hitEOF_ && this.hi_ < this.maxHi_)
+            {
+                // at EOF -- bump "hi"
+                this.hi_ = this.maxHi_;
+            }
+            else
+            {
+                // slow path -- write current buffer; read next one
+                this.seek(this.curr_);
+                if (this.curr_ == this.hi_)
+                {
+                    // appending to EOF -- bump "hi"
+                    this.hi_ = this.maxHi_;
+                }
+            }
+        }
+        len = Math.min(len, (int) (this.hi_ - this.curr_));
+        int buffOff = (int) (this.curr_ - this.lo_);
+        System.arraycopy(b, off, this.buff_, buffOff, len);
+        this.curr_ += len;
+        return len;
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {
+        
+        RandomAccessFile raf = new ChecksumRandomAccessFile("C:\\Engagements\\Table-ColumnFamily-1-Data.dat", "rw");
+        byte[] bytes = new byte[32*1024];
+        Random random = new Random();
+        
+        for ( int i = 0; i < 16; ++i )
+        {
+            random.nextBytes(bytes);
+            raf.write(bytes);
+        }
+        raf.close();
+        
+        String file = "C:\\Engagements\\Checksum-1.db";
+        ChecksumManager.instance("C:\\Engagements\\Table-ColumnFamily-1-Data.dat", file);
+        
+        raf = new ChecksumRandomAccessFile("C:\\Engagements\\Table-ColumnFamily-1-Data.dat", "rw", 4*1024*1024);
+        bytes = new byte[32*1024];
+        
+        for ( int i = 0; i < 16; ++i )
+        {           
+            raf.readFully(bytes);
+        }
+        raf.close();
+       
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/Coordinate.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/Coordinate.java
index e69de29b..e5eaf6a8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/Coordinate.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/Coordinate.java
@@ -0,0 +1,34 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io;
+
+/**
+ * Section of a file that needs to be scanned
+ * is represented by this class.
+*/
+public class Coordinate
+{
+    public final long start_;
+    public final long end_;
+    
+    Coordinate(long start, long end)
+    {
+        start_ = start;
+        end_ = end;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/DataInputBuffer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/DataInputBuffer.java
index e69de29b..1557fd7b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/DataInputBuffer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/DataInputBuffer.java
@@ -0,0 +1,104 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+import java.nio.ByteBuffer;
+import java.util.Random;
+
+import org.apache.cassandra.continuations.Suspendable;
+
+
+/**
+ * An implementation of the DataInputStream interface. This instance is completely thread 
+ * unsafe.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class DataInputBuffer extends DataInputStream
+{
+    private static class Buffer extends ByteArrayInputStream
+    {        
+        public Buffer()
+        {
+            super(new byte[] {});
+        }
+
+        public void reset(byte[] input, int start, int length)
+        {
+            this.buf = input;
+            this.count = start + length;
+            this.mark = start;
+            this.pos = start;
+        }
+        
+        public int getPosition()
+        {
+            return pos;
+        }
+        
+        public void setPosition(int position)
+        {
+            pos = position;
+        }        
+
+        public int getLength()
+        {
+            return count;
+        }
+    }
+
+    private Buffer buffer_;
+
+    /** Constructs a new empty buffer. */
+    public DataInputBuffer()
+    {
+        this(new Buffer());
+    }
+
+    private DataInputBuffer(Buffer buffer)
+    {
+        super(buffer);
+        this.buffer_ = buffer;
+    }
+   
+    /** Resets the data that the buffer reads. */
+    public void reset(byte[] input, int length)
+    {
+        buffer_.reset(input, 0, length);
+    }
+
+    /** Resets the data that the buffer reads. */
+    public void reset(byte[] input, int start, int length)
+    {
+        buffer_.reset(input, start, length);
+    }
+
+    /** Returns the length of the input. */
+    public int getLength()
+    {
+        return buffer_.getLength();
+    }
+
+    public int getPosition()
+    {
+        return buffer_.getPosition();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/DataOutputBuffer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/DataOutputBuffer.java
index e69de29b..ea68a2f8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/DataOutputBuffer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/DataOutputBuffer.java
@@ -0,0 +1,111 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+import java.nio.ByteBuffer;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.util.Arrays;
+
+import org.apache.cassandra.continuations.Suspendable;
+
+
+/**
+ * An implementation of the DataOutputStream interface. This class is completely thread
+ * unsafe.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class DataOutputBuffer extends DataOutputStream
+{
+    private static class Buffer extends ByteArrayOutputStream
+    {
+        public byte[] getData()
+        {
+            return buf;
+        }
+        
+        public int getLength()
+        {
+            return count;
+        }
+        
+        public void reset()
+        {
+            count = 0;
+        }
+        
+        public void write(DataInput in, int len) throws IOException
+        {
+            int newcount = count + len;
+            if (newcount > buf.length)
+            {
+                byte newbuf[] = new byte[Math.max(buf.length << 1, newcount)];
+                System.arraycopy(buf, 0, newbuf, 0, count);
+                buf = newbuf;
+            }
+            in.readFully(buf, count, len);
+            count = newcount;
+        }
+    }
+    
+    private Buffer buffer;
+    
+    /** Constructs a new empty buffer. */
+    public DataOutputBuffer()
+    {
+        this(new Buffer());
+    }
+    
+    private DataOutputBuffer(Buffer buffer)
+    {
+        super(buffer);
+        this.buffer = buffer;
+    }
+    
+    /**
+     * Returns the current contents of the buffer. Data is only valid to
+     * {@link #getLength()}.
+     */
+    public byte[] getData()
+    {
+        return buffer.getData();
+    }
+    
+    /** Returns the length of the valid data currently in the buffer. */
+    public int getLength()
+    {
+        return buffer.getLength();
+    }
+    
+    /** Resets the buffer to empty. */
+    public DataOutputBuffer reset()
+    {
+        this.written = 0;
+        buffer.reset();
+        return this;
+    }
+    
+    /** Writes bytes from a DataInput directly into the buffer. */
+    public void write(DataInput in, int length) throws IOException
+    {
+        buffer.write(in, length);
+    }   
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/FastBufferedInputStream.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/FastBufferedInputStream.java
index e69de29b..02b2bc4b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/FastBufferedInputStream.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/FastBufferedInputStream.java
@@ -0,0 +1,486 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;
+
+/**
+ * A <code>BufferedInputStream</code> adds functionality to another input
+ * stream-namely, the ability to buffer the input and to support the
+ * <code>mark</code> and <code>reset</code> methods. When the
+ * <code>BufferedInputStream</code> is created, an internal buffer array is
+ * created. As bytes from the stream are read or skipped, the internal buffer is
+ * refilled as necessary from the contained input stream, many bytes at a time.
+ * The <code>mark</code> operation remembers a point in the input stream and
+ * the <code>reset</code> operation causes all the bytes read since the most
+ * recent <code>mark</code> operation to be reread before new bytes are taken
+ * from the contained input stream.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class FastBufferedInputStream extends FilterInputStream
+{
+    
+    private static int defaultBufferSize = 8192;
+    
+    /**
+     * The internal buffer array where the data is stored. When necessary, it
+     * may be replaced by another array of a different size.
+     */
+    protected volatile byte buf[];
+    
+    /**
+     * Atomic updater to provide compareAndSet for buf. This is necessary
+     * because closes can be asynchronous. We use nullness of buf[] as primary
+     * indicator that this stream is closed. (The "in" field is also nulled out
+     * on close.)
+     */
+    private static final AtomicReferenceFieldUpdater<FastBufferedInputStream, byte[]> bufUpdater = AtomicReferenceFieldUpdater
+    .newUpdater(FastBufferedInputStream.class, byte[].class, "buf");
+    
+    /**
+     * The index one greater than the index of the last valid byte in the
+     * buffer. This value is always in the range <code>0</code> through
+     * <code>buf.length</code>; elements <code>buf[0]</code> through
+     * <code>buf[count-1]
+     * </code>contain buffered input data obtained from the
+     * underlying input stream.
+     */
+    protected int count;
+    
+    /**
+     * The current position in the buffer. This is the index of the next
+     * character to be read from the <code>buf</code> array.
+     * <p>
+     * This value is always in the range <code>0</code> through
+     * <code>count</code>. If it is less than <code>count</code>, then
+     * <code>buf[pos]</code> is the next byte to be supplied as input; if it
+     * is equal to <code>count</code>, then the next <code>read</code> or
+     * <code>skip</code> operation will require more bytes to be read from the
+     * contained input stream.
+     * 
+     * @see java.io.BufferedInputStream#buf
+     */
+    protected int pos;
+    
+    /**
+     * The value of the <code>pos</code> field at the time the last
+     * <code>mark</code> method was called.
+     * <p>
+     * This value is always in the range <code>-1</code> through
+     * <code>pos</code>. If there is no marked position in the input stream,
+     * this field is <code>-1</code>. If there is a marked position in the
+     * input stream, then <code>buf[markpos]</code> is the first byte to be
+     * supplied as input after a <code>reset</code> operation. If
+     * <code>markpos</code> is not <code>-1</code>, then all bytes from
+     * positions <code>buf[markpos]</code> through <code>buf[pos-1]</code>
+     * must remain in the buffer array (though they may be moved to another
+     * place in the buffer array, with suitable adjustments to the values of
+     * <code>count</code>, <code>pos</code>, and <code>markpos</code>);
+     * they may not be discarded unless and until the difference between
+     * <code>pos</code> and <code>markpos</code> exceeds
+     * <code>marklimit</code>.
+     * 
+     * @see java.io.BufferedInputStream#mark(int)
+     * @see java.io.BufferedInputStream#pos
+     */
+    protected int markpos = -1;
+    
+    /**
+     * The maximum read ahead allowed after a call to the <code>mark</code>
+     * method before subsequent calls to the <code>reset</code> method fail.
+     * Whenever the difference between <code>pos</code> and
+     * <code>markpos</code> exceeds <code>marklimit</code>, then the mark
+     * may be dropped by setting <code>markpos</code> to <code>-1</code>.
+     * 
+     * @see java.io.BufferedInputStream#mark(int)
+     * @see java.io.BufferedInputStream#reset()
+     */
+    protected int marklimit;
+    
+    /**
+     * Check to make sure that underlying input stream has not been nulled out
+     * due to close; if not return it;
+     */
+    private InputStream getInIfOpen() throws IOException
+    {
+        InputStream input = in;
+        if (input == null)
+            throw new IOException("Stream closed");
+        return input;
+    }
+    
+    /**
+     * Check to make sure that buffer has not been nulled out due to close; if
+     * not return it;
+     */
+    private byte[] getBufIfOpen() throws IOException
+    {
+        byte[] buffer = buf;
+        if (buffer == null)
+            throw new IOException("Stream closed");
+        return buffer;
+    }
+    
+    /**
+     * Creates a <code>BufferedInputStream</code> and saves its argument, the
+     * input stream <code>in</code>, for later use. An internal buffer array
+     * is created and stored in <code>buf</code>.
+     * 
+     * @param in
+     *            the underlying input stream.
+     */
+    public FastBufferedInputStream(InputStream in)
+    {
+        this(in, defaultBufferSize);
+    }
+    
+    /**
+     * Creates a <code>BufferedInputStream</code> with the specified buffer
+     * size, and saves its argument, the input stream <code>in</code>, for
+     * later use. An internal buffer array of length <code>size</code> is
+     * created and stored in <code>buf</code>.
+     * 
+     * @param in
+     *            the underlying input stream.
+     * @param size
+     *            the buffer size.
+     * @exception IllegalArgumentException
+     *                if size <= 0.
+     */
+    public FastBufferedInputStream(InputStream in, int size)
+    {
+        super(in);
+        if (size <= 0)
+        {
+            throw new IllegalArgumentException("Buffer size <= 0");
+        }
+        buf = new byte[size];
+    }
+    
+    /**
+     * Fills the buffer with more data, taking into account shuffling and other
+     * tricks for dealing with marks. Assumes that it is being called by a
+     * synchronized method. This method also assumes that all data has already
+     * been read in, hence pos > count.
+     */
+    private void fill() throws IOException
+    {
+        byte[] buffer = getBufIfOpen();
+        if (markpos < 0)
+            pos = 0; /* no mark: throw away the buffer */
+        else if (pos >= buffer.length) /* no room left in buffer */
+            if (markpos > 0)
+            { /* can throw away early part of the buffer */
+                int sz = pos - markpos;
+                System.arraycopy(buffer, markpos, buffer, 0, sz);
+                pos = sz;
+                markpos = 0;
+            }
+            else if (buffer.length >= marklimit)
+            {
+                markpos = -1; /* buffer got too big, invalidate mark */
+                pos = 0; /* drop buffer contents */
+            }
+            else
+            { /* grow buffer */
+                int nsz = pos * 2;
+                if (nsz > marklimit)
+                    nsz = marklimit;
+                byte nbuf[] = new byte[nsz];
+                System.arraycopy(buffer, 0, nbuf, 0, pos);
+                if (!bufUpdater.compareAndSet(this, buffer, nbuf))
+                {
+                    // Can't replace buf if there was an async close.
+                    // Note: This would need to be changed if fill()
+                    // is ever made accessible to multiple threads.
+                    // But for now, the only way CAS can fail is via close.
+                    // assert buf == null;
+                    throw new IOException("Stream closed");
+                }
+                buffer = nbuf;
+            }
+        count = pos;
+        int n = getInIfOpen().read(buffer, pos, buffer.length - pos);
+        if (n > 0)
+            count = n + pos;
+    }
+    
+    /**
+     * See the general contract of the <code>read</code> method of
+     * <code>InputStream</code>.
+     * 
+     * @return the next byte of data, or <code>-1</code> if the end of the
+     *         stream is reached.
+     * @exception IOException
+     *                if this input stream has been closed by invoking its
+     *                {@link #close()} method, or an I/O error occurs.
+     * @see java.io.FilterInputStream#in
+     */
+    public  int read() throws IOException
+    {
+        if (pos >= count)
+        {
+            fill();
+            if (pos >= count)
+                return -1;
+        }
+        return getBufIfOpen()[pos++] & 0xff;
+    }
+    
+    /**
+     * Read characters into a portion of an array, reading from the underlying
+     * stream at most once if necessary.
+     */
+    private int read1(byte[] b, int off, int len) throws IOException
+    {
+        int avail = count - pos;
+        if (avail <= 0)
+        {
+            /*
+             * If the requested length is at least as large as the buffer, and
+             * if there is no mark/reset activity, do not bother to copy the
+             * bytes into the local buffer. In this way buffered streams will
+             * cascade harmlessly.
+             */
+            if (len >= getBufIfOpen().length && markpos < 0)
+            {
+                return getInIfOpen().read(b, off, len);
+            }
+            fill();
+            avail = count - pos;
+            if (avail <= 0)
+                return -1;
+        }
+        int cnt = (avail < len) ? avail : len;
+        System.arraycopy(getBufIfOpen(), pos, b, off, cnt);
+        pos += cnt;
+        return cnt;
+    }
+    
+    /**
+     * Reads bytes from this byte-input stream into the specified byte array,
+     * starting at the given offset.
+     * 
+     * <p>
+     * This method implements the general contract of the corresponding
+     * <code>{@link InputStream#read(byte[], int, int) read}</code> method of
+     * the <code>{@link InputStream}</code> class. As an additional
+     * convenience, it attempts to read as many bytes as possible by repeatedly
+     * invoking the <code>read</code> method of the underlying stream. This
+     * iterated <code>read</code> continues until one of the following
+     * conditions becomes true:
+     * <ul>
+     * 
+     * <li> The specified number of bytes have been read,
+     * 
+     * <li> The <code>read</code> method of the underlying stream returns
+     * <code>-1</code>, indicating end-of-file, or
+     * 
+     * <li> The <code>available</code> method of the underlying stream returns
+     * zero, indicating that further input requests would block.
+     * 
+     * </ul>
+     * If the first <code>read</code> on the underlying stream returns
+     * <code>-1</code> to indicate end-of-file then this method returns
+     * <code>-1</code>. Otherwise this method returns the number of bytes
+     * actually read.
+     * 
+     * <p>
+     * Subclasses of this class are encouraged, but not required, to attempt to
+     * read as many bytes as possible in the same fashion.
+     * 
+     * @param b
+     *            destination buffer.
+     * @param off
+     *            offset at which to start storing bytes.
+     * @param len
+     *            maximum number of bytes to read.
+     * @return the number of bytes read, or <code>-1</code> if the end of the
+     *         stream has been reached.
+     * @exception IOException
+     *                if this input stream has been closed by invoking its
+     *                {@link #close()} method, or an I/O error occurs.
+     */
+    public  int read(byte b[], int off, int len) throws IOException
+    {
+        getBufIfOpen(); // Check for closed stream
+        if ((off | len | (off + len) | (b.length - (off + len))) < 0)
+        {
+            throw new IndexOutOfBoundsException();
+        }
+        else if (len == 0)
+        {
+            return 0;
+        }
+        
+        int n = 0;
+        for (;;)
+        {
+            int nread = read1(b, off + n, len - n);
+            if (nread <= 0)
+                return (n == 0) ? nread : n;
+            n += nread;
+            if (n >= len)
+                return n;
+            // if not closed but no bytes available, return
+            InputStream input = in;
+            if (input != null && input.available() <= 0)
+                return n;
+        }
+    }
+    
+    /**
+     * See the general contract of the <code>skip</code> method of
+     * <code>InputStream</code>.
+     * 
+     * @exception IOException
+     *                if the stream does not support seek, or if this input
+     *                stream has been closed by invoking its {@link #close()}
+     *                method, or an I/O error occurs.
+     */
+    public  long skip(long n) throws IOException
+    {
+        getBufIfOpen(); // Check for closed stream
+        if (n <= 0)
+        {
+            return 0;
+        }
+        long avail = count - pos;
+        
+        if (avail <= 0)
+        {
+            // If no mark position set then don't keep in buffer
+            if (markpos < 0)
+                return getInIfOpen().skip(n);
+            
+            // Fill in buffer to save bytes for reset
+            fill();
+            avail = count - pos;
+            if (avail <= 0)
+                return 0;
+        }
+        
+        long skipped = (avail < n) ? avail : n;
+        pos += skipped;
+        return skipped;
+    }
+    
+    /**
+     * Returns an estimate of the number of bytes that can be read (or skipped
+     * over) from this input stream without blocking by the next invocation of a
+     * method for this input stream. The next invocation might be the same
+     * thread or another thread. A single read or skip of this many bytes will
+     * not block, but may read or skip fewer bytes.
+     * <p>
+     * This method returns the sum of the number of bytes remaining to be read
+     * in the buffer (<code>count&nbsp;- pos</code>) and the result of
+     * calling the {@link java.io.FilterInputStream#in in}.available().
+     * 
+     * @return an estimate of the number of bytes that can be read (or skipped
+     *         over) from this input stream without blocking.
+     * @exception IOException
+     *                if this input stream has been closed by invoking its
+     *                {@link #close()} method, or an I/O error occurs.
+     */
+    public  int available() throws IOException
+    {
+        return getInIfOpen().available() + (count - pos);
+    }
+    
+    /**
+     * See the general contract of the <code>mark</code> method of
+     * <code>InputStream</code>.
+     * 
+     * @param readlimit
+     *            the maximum limit of bytes that can be read before the mark
+     *            position becomes invalid.
+     * @see java.io.BufferedInputStream#reset()
+     */
+    public  void mark(int readlimit)
+    {
+        marklimit = readlimit;
+        markpos = pos;
+    }
+    
+    /**
+     * See the general contract of the <code>reset</code> method of
+     * <code>InputStream</code>.
+     * <p>
+     * If <code>markpos</code> is <code>-1</code> (no mark has been set or
+     * the mark has been invalidated), an <code>IOException</code> is thrown.
+     * Otherwise, <code>pos</code> is set equal to <code>markpos</code>.
+     * 
+     * @exception IOException
+     *                if this stream has not been marked or, if the mark has
+     *                been invalidated, or the stream has been closed by
+     *                invoking its {@link #close()} method, or an I/O error
+     *                occurs.
+     * @see java.io.BufferedInputStream#mark(int)
+     */
+    public  void reset() throws IOException
+    {
+        getBufIfOpen(); // Cause exception if closed
+        if (markpos < 0)
+            throw new IOException("Resetting to invalid mark");
+        pos = markpos;
+    }
+    
+    /**
+     * Tests if this input stream supports the <code>mark</code> and
+     * <code>reset</code> methods. The <code>markSupported</code> method of
+     * <code>BufferedInputStream</code> returns <code>true</code>.
+     * 
+     * @return a <code>boolean</code> indicating if this stream type supports
+     *         the <code>mark</code> and <code>reset</code> methods.
+     * @see java.io.InputStream#mark(int)
+     * @see java.io.InputStream#reset()
+     */
+    public boolean markSupported()
+    {
+        return true;
+    }
+    
+    /**
+     * Closes this input stream and releases any system resources associated
+     * with the stream. Once the stream has been closed, further read(),
+     * available(), reset(), or skip() invocations will throw an IOException.
+     * Closing a previously closed stream has no effect.
+     * 
+     * @exception IOException
+     *                if an I/O error occurs.
+     */
+    public void close() throws IOException
+    {
+        byte[] buffer;
+        while ((buffer = buf) != null)
+        {
+            if (bufUpdater.compareAndSet(this, buffer, null))
+            {
+                InputStream input = in;
+                in = null;
+                if (input != null)
+                    input.close();
+                return;
+            }
+            // Else retry in case a new buf was CASed in fill()
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/FastBufferedOutputStream.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/FastBufferedOutputStream.java
index e69de29b..9e9c7617 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/FastBufferedOutputStream.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/FastBufferedOutputStream.java
@@ -0,0 +1,162 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+
+/**
+ * The class implements a buffered output stream. By setting up such an output
+ * stream, an application can write bytes to the underlying output stream
+ * without necessarily causing a call to the underlying system for each byte
+ * written.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class FastBufferedOutputStream extends FilterOutputStream
+{
+    /**
+     * The internal buffer where data is stored.
+     */
+    protected byte buf[];
+    
+    /**
+     * The number of valid bytes in the buffer. This value is always in the
+     * range <tt>0</tt> through <tt>buf.length</tt>; elements
+     * <tt>buf[0]</tt> through <tt>buf[count-1]</tt> contain valid byte
+     * data.
+     */
+    protected int count;
+    
+    /**
+     * Creates a new buffered output stream to write data to the specified
+     * underlying output stream.
+     * 
+     * @param out
+     *            the underlying output stream.
+     */
+    public FastBufferedOutputStream(OutputStream out)
+    {
+        this(out, 8192);
+    }
+    
+    /**
+     * Creates a new buffered output stream to write data to the specified
+     * underlying output stream with the specified buffer size.
+     * 
+     * @param out
+     *            the underlying output stream.
+     * @param size
+     *            the buffer size.
+     * @exception IllegalArgumentException
+     *                if size &lt;= 0.
+     */
+    public FastBufferedOutputStream(OutputStream out, int size)
+    {
+        super(out);
+        if (size <= 0)
+        {
+            throw new IllegalArgumentException("Buffer size <= 0");
+        }
+        buf = new byte[size];
+    }
+    
+    /** Flush the internal buffer */
+    private void flushBuffer() throws IOException
+    {
+        if (count > 0)
+        {
+            out.write(buf, 0, count);
+            count = 0;
+        }
+    }
+    
+    /**
+     * Writes the specified byte to this buffered output stream.
+     * 
+     * @param b
+     *            the byte to be written.
+     * @exception IOException
+     *                if an I/O error occurs.
+     */
+    public void write(int b) throws IOException
+    {
+        if (count >= buf.length)
+        {
+            flushBuffer();
+        }
+        buf[count++] = (byte) b;
+    }
+    
+    /**
+     * Writes <code>len</code> bytes from the specified byte array starting at
+     * offset <code>off</code> to this buffered output stream.
+     * 
+     * <p>
+     * Ordinarily this method stores bytes from the given array into this
+     * stream's buffer, flushing the buffer to the underlying output stream as
+     * needed. If the requested length is at least as large as this stream's
+     * buffer, however, then this method will flush the buffer and write the
+     * bytes directly to the underlying output stream. Thus redundant
+     * <code>BufferedOutputStream</code>s will not copy data unnecessarily.
+     * 
+     * @param b
+     *            the data.
+     * @param off
+     *            the start offset in the data.
+     * @param len
+     *            the number of bytes to write.
+     * @exception IOException
+     *                if an I/O error occurs.
+     */
+    public void write(byte b[], int off, int len)
+    throws IOException
+    {
+        if (len >= buf.length)
+        {
+            /*
+             * If the request length exceeds the size of the output buffer,
+             * flush the output buffer and then write the data directly. In this
+             * way buffered streams will cascade harmlessly.
+             */
+            flushBuffer();
+            out.write(b, off, len);
+            return;
+        }
+        if (len > buf.length - count)
+        {
+            flushBuffer();
+        }
+        System.arraycopy(b, off, buf, count, len);
+        count += len;
+    }
+    
+    /**
+     * Flushes this buffered output stream. This forces any buffered output
+     * bytes to be written out to the underlying output stream.
+     * 
+     * @exception IOException
+     *                if an I/O error occurs.
+     * @see java.io.FilterOutputStream#out
+     */
+    public void flush() throws IOException
+    {
+        flushBuffer();
+        out.flush();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ICompactSerializer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ICompactSerializer.java
index e69de29b..19e44228 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ICompactSerializer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/ICompactSerializer.java
@@ -0,0 +1,48 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.DataOutput;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.DataInputStream;
+
+/**
+ * Allows for the controlled serialization/deserialization of a given type.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface ICompactSerializer<T>
+{
+	/**
+     * Serialize the specified type into the specified DataOutputStream instance.
+     * @param t type that needs to be serialized
+     * @param dos DataOutput into which serialization needs to happen.
+     * @throws IOException
+     */
+    public void serialize(T t, DataOutputStream dos) throws IOException;
+
+    /**
+     * Deserialize into the specified DataInputStream instance.
+     * @param dis DataInput from which deserialization needs to happen.
+     * @throws IOException
+     * @return the type that was deserialized
+     */
+    public T deserialize(DataInputStream dis) throws IOException;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IFileReader.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IFileReader.java
index e69de29b..208bf534 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IFileReader.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IFileReader.java
@@ -0,0 +1,129 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.security.MessageDigest;
+import java.util.List;
+
+/**
+ * Interface to read from the SequenceFile abstraction.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IFileReader
+{
+    public String getFileName();
+    public long getEOF() throws IOException;
+    public long getCurrentPosition() throws IOException;
+    public boolean isHealthyFileDescriptor() throws IOException;
+    public void seek(long position) throws IOException;
+    public boolean isEOF() throws IOException;
+
+    /**
+     * Be extremely careful while using this API. This currently
+     * used to read the commit log header from the commit logs.
+     * Treat this as an internal API.
+     * 
+     * @param bytes read into this byte array.
+    */
+    public void readDirect(byte[] bytes) throws IOException;
+    
+    /**
+     * Read a long value from the underlying sub system.
+     * @return value read
+     * @throws IOException
+     */
+    public long readLong() throws IOException;
+    
+    /**
+     * This functions is used to help out subsequent reads
+     * on the specified key. It reads the keys prior to this
+     * one on disk so that the buffer cache is hot.
+     * 
+     *  @param key key for which we are performing the touch.
+     *  @param fData true implies we fetch the data into buffer cache.
+    */
+    public long touch(String key , boolean fData) throws IOException;
+    
+    /**
+     * This method helps is retrieving the offset of the specified
+     * key in the file using the block index.
+     * 
+     * @param key key whose position we need in the block index.
+    */
+    public long getPositionFromBlockIndex(String key) throws IOException;
+    
+    /**
+     * This method returns the position of the specified key and the 
+     * size of its data segment from the block index.
+     * 
+     * @param key key whose block metadata we are interested in.
+     * @return an instance of the block metadata for this key.
+    */
+    public SSTable.BlockMetadata getBlockMetadata(String key) throws IOException;
+
+    /**
+     * This method dumps the next key/value into the DataOuputStream
+     * passed in.
+     *
+     * @param bufOut DataOutputStream that needs to be filled.
+     * @return number of bytes read.
+     * @throws IOException 
+    */
+    public long next(DataOutputBuffer bufOut) throws IOException;
+
+    /**
+     * This method dumps the next key/value into the DataOuputStream
+     * passed in.
+     *
+     * @param key key we are interested in.
+     * @param bufOut DataOutputStream that needs to be filled.
+     * @param section region of the file that needs to be read
+     * @throws IOException
+     * @return the number of bytes read.
+    */
+    public long next(String key, DataOutputBuffer bufOut, Coordinate section) throws IOException;
+
+    /**
+     * This method dumps the next key/value into the DataOuputStream
+     * passed in. Always use this method to query for application
+     * specific data as it will have indexes.
+     *
+     * @param key - key we are interested in.
+     * @param bufOut - DataOutputStream that needs to be filled.
+     * @param columnFamilyName The name of the column family only without the ":"
+     * @param columnNames - The list of columns in the cfName column family
+     * 					     that we want to return
+     * OR
+     * @param timeRange - time range we are interested in
+     * @param section region of the file that needs to be read
+     * @throws IOException
+     * @return number of bytes read.
+     *
+    */
+    public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, List<String> columnNames, IndexHelper.TimeRange timeRange, Coordinate section) throws IOException;
+
+    /**
+     * Close the file after reading.
+     * @throws IOException
+     */
+    public void close() throws IOException;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IFileWriter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IFileWriter.java
index e69de29b..a5c3be48 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IFileWriter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IFileWriter.java
@@ -0,0 +1,133 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.IOException;
+
+
+/**
+ * An interface for writing into the SequenceFile abstraction.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IFileWriter
+{
+    /**
+     * Get the current position of the file pointer.
+     * @return current file pointer position
+     * @throws IOException
+     */
+    public long getCurrentPosition() throws IOException;
+    
+    /**
+     * @return the last file modification time.
+     */
+    public long lastModified();
+    
+    /**
+     * Seeks the file pointer to the specified position.
+     * @param position position within the file to seek to.
+     * @throws IOException
+     */
+    public void seek(long position) throws IOException;
+    
+    /**
+     * Appends the buffer to the the underlying SequenceFile.
+     * @param buffer buffer which contains the serialized data.
+     * @throws IOException
+     */
+    public void append(DataOutputBuffer buffer) throws IOException;
+    
+    /**
+     * Appends the key and the value to the the underlying SequenceFile.
+     * @param keyBuffer buffer which contains the serialized key.
+     * @param buffer buffer which contains the serialized data.
+     * @throws IOException
+     */
+    public void append(DataOutputBuffer keyBuffer, DataOutputBuffer buffer) throws IOException;
+    
+    /**
+     * Appends the key and the value to the the underlying SequenceFile.
+     * @param key key associated with this peice of data.
+     * @param buffer buffer containing the serialized data.
+     * @throws IOException
+     */
+    public void append(String key, DataOutputBuffer buffer) throws IOException;
+    
+    /**
+     * Appends the key and the value to the the underlying SequenceFile.
+     * @param key key associated with this peice of data.
+     * @param value byte array containing the serialized data.
+     * @throws IOException
+     */
+    public void append(String key, byte[] value) throws IOException;
+    
+    /**
+     * Appends the key and the long value to the the underlying SequenceFile.
+     * This is used in the contruction of the index file associated with a 
+     * SSTable.
+     * @param key key associated with this peice of data.
+     * @param value value associated with this key.
+     * @throws IOException
+     */
+    public void append(String key, long value) throws IOException;
+    
+    /**
+     * Be extremely careful while using this API. This currently
+     * used to write the commit log header in the commit logs.
+     * If not used carefully it could completely screw up reads
+     * of other key/value pairs that are written. 
+     * 
+     * @param bytes serialized version of the commit log header.
+     * @throws IOException
+    */
+    public long writeDirect(byte[] bytes) throws IOException;
+    
+    /**
+     * Write a long into the underlying sub system.
+     * @param value long to be written
+     * @throws IOException
+     */
+    public void writeLong(long value) throws IOException;
+      
+    /**
+     * Close the file which is being used for the write.
+     * @throws IOException
+     */
+    public void close() throws IOException;  
+    
+    /**
+     * Close the file after appending the passed in footer information.
+     * @param footer footer information.
+     * @param size size of the footer.
+     * @throws IOException
+     */
+    public void close(byte[] footer, int size) throws IOException;
+    
+    /**
+     * @return the name of the file.
+     */
+    public String getFileName();    
+    
+    /**
+     * @return the size of the file.
+     * @throws IOException
+     */
+    public long getFileSize() throws IOException;    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IndexHelper.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IndexHelper.java
index e69de29b..1c7efaff 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IndexHelper.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IndexHelper.java
@@ -0,0 +1,592 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.DataInput;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.TypeInfo;
+import org.apache.cassandra.io.SSTable.KeyPositionInfo;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.FBUtilities;
+
+
+/**
+ * Provides helper to serialize, deserialize and use column indexes.
+ * Author : Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+
+public class IndexHelper
+{
+	/**
+	 * Serializes a column index to a data output stream
+	 * @param indexSizeInBytes Size of index to be written
+	 * @param columnIndexList List of column index entries as objects
+	 * @param dos the output stream into which the column index is to be written
+	 * @throws IOException
+	 */
+	public static void serialize(int indexSizeInBytes, List<ColumnIndexInfo> columnIndexList, DataOutputStream dos) throws IOException
+	{
+		/* if we have no data to index, the write that there is no index present */
+		if(indexSizeInBytes == 0 || columnIndexList == null || columnIndexList.size() == 0)
+		{
+			dos.writeBoolean(false);
+		}
+		else
+		{
+	        /* write if we are storing a column index */
+	    	dos.writeBoolean(true);
+	    	/* write the size of the index */
+	    	dos.writeInt(indexSizeInBytes);
+	        for( ColumnIndexInfo cIndexInfo : columnIndexList )
+	        {
+	        	cIndexInfo.serialize(dos);
+	        }
+		}
+	}
+    
+    /**
+     * Skip the bloom filter and the index and return the bytes read.
+     * @param in the data input from which the bloom filter and index 
+     *           should be skipped
+     * @return number of bytes read.
+     * @throws IOException
+     */
+    public static int skipBloomFilterAndIndex(DataInput in) throws IOException
+    {
+        int totalBytesRead = 0;
+        /* size of the bloom filter */
+        int size = in.readInt();
+        totalBytesRead += 4;
+        /* skip the serialized bloom filter */
+        in.skipBytes(size);
+        totalBytesRead += size;
+        /* skip the index on disk */
+        /* read if the file has column indexes */
+        boolean hasColumnIndexes = in.readBoolean();
+        totalBytesRead += 1;
+        if ( hasColumnIndexes )
+        {
+            totalBytesRead += skipIndex(in);
+        }
+        return totalBytesRead;
+    }
+    
+    /**
+     * Skip the bloom filter and return the bytes read.
+     * @param in the data input from which the bloom filter 
+     *           should be skipped
+     * @return number of bytes read.
+     * @throws IOException
+     */
+    public static int skipBloomFilter(DataInput in) throws IOException
+    {
+        int totalBytesRead = 0;
+        /* size of the bloom filter */
+        int size = in.readInt();
+        totalBytesRead += 4;
+        /* skip the serialized bloom filter */
+        in.skipBytes(size);
+        totalBytesRead += size;
+        return totalBytesRead;
+    }
+
+	/**
+	 * Skip the index and return the number of bytes read.
+	 * @param file the data input from which the index should be skipped
+	 * @return number of bytes read from the data input
+	 * @throws IOException
+	 */
+	public static int skipIndex(DataInput file) throws IOException
+	{
+        /* read only the column index list */
+        int columnIndexSize = file.readInt();
+        int totalBytesRead = 4;
+
+        /* skip the column index data */
+        file.skipBytes(columnIndexSize);
+        totalBytesRead += columnIndexSize;
+
+        return totalBytesRead;
+	}
+    
+    /**
+     * Deserialize the index into a structure and return the number of bytes read.
+     * @param in Input from which the serialized form of the index is read
+     * @param columnIndexList the structure which is filled in with the deserialized index
+     * @return number of bytes read from the input
+     * @throws IOException
+     */
+	static int deserializeIndex(String cfName, DataInput in, List<ColumnIndexInfo> columnIndexList) throws IOException
+	{
+		/* read only the column index list */
+		int columnIndexSize = in.readInt();
+		int totalBytesRead = 4;
+
+		/* read the indexes into a separate buffer */
+		DataOutputBuffer indexOut = new DataOutputBuffer();
+        /* write the data into buffer */
+		indexOut.write(in, columnIndexSize);
+		totalBytesRead += columnIndexSize;
+
+		/* now deserialize the index list */
+        DataInputBuffer indexIn = new DataInputBuffer();
+        indexIn.reset(indexOut.getData(), indexOut.getLength());
+        
+        TypeInfo typeInfo = DatabaseDescriptor.getTypeInfo(cfName);
+        if ( DatabaseDescriptor.getColumnFamilyType(cfName).equals("Super") || DatabaseDescriptor.isNameSortingEnabled(cfName) )
+        {
+            typeInfo = TypeInfo.STRING;
+        }
+        
+        while(indexIn.available() > 0)
+        {            
+            ColumnIndexInfo cIndexInfo = ColumnIndexFactory.instance(typeInfo);
+        	cIndexInfo = cIndexInfo.deserialize(indexIn);
+        	columnIndexList.add(cIndexInfo);
+        }
+
+		return totalBytesRead;
+	}
+
+    /**
+     * Returns the range in which a given column falls in the index
+     * @param column The column whose range needs to be found
+     * @param columnIndexList the in-memory representation of the column index
+     * @param dataSize the total size of the data
+     * @param totalNumCols total number of columns
+     * @return an object describing a subrange in which the column is serialized
+     */
+	static ColumnRange getColumnRangeFromNameIndex(IndexHelper.ColumnIndexInfo cIndexInfo, List<IndexHelper.ColumnIndexInfo> columnIndexList, int dataSize, int totalNumCols)
+	{
+		/* find the offset for the column */
+        int size = columnIndexList.size();
+        long start = 0;
+        long end = dataSize;
+        int numColumns = 0;      
+       
+        int index = Collections.binarySearch(columnIndexList, cIndexInfo);
+        if ( index < 0 )
+        {
+            /* We are here which means that the requested column is not an index. */
+            index = (++index)*(-1);
+        }
+        else
+        {
+        	++index;
+        }
+
+        /* calculate the starting offset from which we have to read */
+        start = (index == 0) ? 0 : columnIndexList.get(index - 1).position();
+
+        if( index < size )
+        {
+        	end = columnIndexList.get(index).position();
+            numColumns = columnIndexList.get(index).count();            
+        }
+        else
+        {
+        	end = dataSize;  
+            int totalColsIndexed = 0;
+            for( IndexHelper.ColumnIndexInfo colPosInfo : columnIndexList )
+            {
+                totalColsIndexed += colPosInfo.count();
+            }
+            numColumns = totalNumCols - totalColsIndexed;
+        }
+
+        return new ColumnRange(start, end, numColumns);
+	}
+
+	/**
+	 * Returns the sub-ranges that contain the list of columns in columnNames.
+	 * @param columnNames The list of columns whose subranges need to be found
+	 * @param columnIndexList the deserialized column indexes
+	 * @param dataSize the total size of data
+	 * @param totalNumCols the total number of columns
+	 * @return a list of subranges which contain all the columns in columnNames
+	 */
+	static List<ColumnRange> getMultiColumnRangesFromNameIndex(List<String> columnNames, List<IndexHelper.ColumnIndexInfo> columnIndexList, int dataSize, int totalNumCols)
+	{
+		List<ColumnRange> columnRanges = new ArrayList<ColumnRange>();				
+
+        if ( columnIndexList.size() == 0 )
+        {
+            columnRanges.add( new ColumnRange(0, dataSize, totalNumCols) );
+        }
+        else
+        {
+            Map<Long, Boolean> offset = new HashMap<Long, Boolean>();
+    		for(String column : columnNames)
+    		{
+                IndexHelper.ColumnIndexInfo cIndexInfo = new IndexHelper.ColumnNameIndexInfo(column);
+    			ColumnRange columnRange = getColumnRangeFromNameIndex(cIndexInfo, columnIndexList, dataSize, totalNumCols);   
+                if ( offset.get( columnRange.coordinate().start_ ) == null ) 
+                {
+                    columnRanges.add(columnRange);
+                    offset.put(columnRange.coordinate().start_, true);
+                }
+    		}
+        }
+
+		return columnRanges;
+	}
+    
+    /**
+     * Returns the range in which a given column falls in the index. This
+     * is used when time range queries are in play. For instance if we are
+     * looking for columns in the range [t, t2]
+     * @param cIndexInfo the time we are interested in.
+     * @param columnIndexList the in-memory representation of the column index
+     * @param dataSize the total size of the data
+     * @param totalNumCols total number of columns
+     * @return an object describing a subrange in which the column is serialized
+     */
+    static ColumnRange getColumnRangeFromTimeIndex(IndexHelper.TimeRange timeRange, List<IndexHelper.ColumnIndexInfo> columnIndexList, int dataSize, int totalNumCols)
+    {
+        /* if column indexes were not present for this column family, the handle accordingly */
+        if(columnIndexList.size() == 0)
+        {
+            return new ColumnRange(0, dataSize, totalNumCols);
+        }
+
+        /* find the offset for the column */
+        int size = columnIndexList.size();
+        long start = 0;
+        long end = dataSize;
+        int numColumns = 0;      
+       
+        /*
+         *  Time indicies are sorted in descending order. So
+         *  we need to apply a reverse compartor for the 
+         *  binary search.        
+        */        
+        Comparator<IndexHelper.ColumnIndexInfo> comparator = Collections.reverseOrder(); 
+        IndexHelper.ColumnIndexInfo rhs = IndexHelper.ColumnIndexFactory.instance(TypeInfo.LONG);
+        rhs.set(timeRange.rhs());
+        int index = Collections.binarySearch(columnIndexList, rhs, comparator);
+        if ( index < 0 )
+        {
+            /* We are here which means that the requested column is not an index. */
+            index = (++index)*(-1);
+        }
+        else
+        {
+            ++index;
+        }
+
+        /* 
+         * Calculate the starting offset from which we have to read. So
+         * we achieve this by performing the probe using the bigger timestamp
+         * and then scanning the column position chunks till we reach the
+         * lower timestamp in the time range.      
+        */
+        start = (index == 0) ? 0 : columnIndexList.get(index - 1).position();
+        /* add the number of colunms in the first chunk. */
+        numColumns += (index ==0) ? columnIndexList.get(0).count() : columnIndexList.get(index - 1).count(); 
+        if( index < size )
+        {            
+            int chunks = columnIndexList.size();
+            /* Index info for the lower bound of the time range */
+            IndexHelper.ColumnIndexInfo lhs = IndexHelper.ColumnIndexFactory.instance(TypeInfo.LONG);
+            lhs.set(timeRange.lhs());
+            int i = index + 1;
+            for ( ; i < chunks; ++i )
+            {
+                IndexHelper.ColumnIndexInfo cIndexInfo2 = columnIndexList.get(i);
+                if ( cIndexInfo2.compareTo(lhs) < 0 )
+                {
+                    numColumns += cIndexInfo2.count();
+                    break;
+                } 
+                numColumns += cIndexInfo2.count();
+            }
+            
+            end = columnIndexList.get(i).position();                       
+        }
+        else
+        {
+            end = dataSize;  
+            int totalColsIndexed = 0;
+            for( IndexHelper.ColumnIndexInfo colPosInfo : columnIndexList )
+            {
+                totalColsIndexed += colPosInfo.count();
+            }
+            numColumns = totalNumCols - totalColsIndexed;
+        }
+       
+        return new ColumnRange(start, end, numColumns);
+    }    
+    
+    public static class ColumnIndexFactory
+    {
+        public static ColumnIndexInfo instance(TypeInfo typeInfo)
+        {
+            ColumnIndexInfo cIndexInfo = null;
+            switch(typeInfo)
+            {
+                case STRING:
+                    cIndexInfo = new ColumnNameIndexInfo();
+                    break;
+                    
+                case LONG:
+                    cIndexInfo = new ColumnTimestampIndexInfo();
+                    break;
+            }
+            return cIndexInfo;
+        }    
+    }
+    
+    /**
+     * Encapsulates a time range. Queries use 
+     * this abstraction for indicating start 
+     * and end regions of a time filter.
+     * 
+     * @author alakshman
+     *
+     */
+    public static class TimeRange
+    {
+        private long lhs_;
+        private long rhs_;
+        
+        public TimeRange(long lhs, long rhs)
+        {
+            lhs_ = lhs;
+            rhs_ = rhs;
+        }
+        
+        public long lhs()
+        {
+            return lhs_;
+        }
+        
+        public long rhs()
+        {
+            return rhs_;
+        }
+    }
+    
+    /**
+     * A column range containing the start and end
+     * offset of the appropriate column index chunk
+     * and the number of columns in that chunk.
+     * @author alakshman
+     *
+     */
+    public static class ColumnRange
+    {
+        private Coordinate coordinate_;
+        private int columnCount_;
+        
+        ColumnRange(long start, long end, int columnCount)
+        {
+            coordinate_ = new Coordinate(start, end);
+            columnCount_ = columnCount;
+        }
+        
+        Coordinate coordinate()
+        {
+            return coordinate_;
+        }
+        
+        int count()
+        {
+            return columnCount_;
+        }                
+    }
+
+	/**
+	 * A helper class to generate indexes while
+     * the columns are sorted by name on disk.
+	*/
+    public static abstract class ColumnIndexInfo implements Comparable<ColumnIndexInfo>
+    {
+        private long position_;
+        private int columnCount_;        
+        
+        ColumnIndexInfo(long position, int columnCount)
+        {
+            position_ = position;
+            columnCount_ = columnCount;
+        }
+                
+        public long position()
+        {
+            return position_;
+        }
+        
+        public void position(long position)
+        {
+            position_ = position;
+        }
+        
+        int count()
+        {
+            return columnCount_;
+        }
+        
+        public void count(int count)
+        {
+            columnCount_ = count;
+        }
+                
+        public abstract void set(Object o);
+        public abstract void serialize(DataOutputStream dos) throws IOException;
+        public abstract ColumnIndexInfo deserialize(DataInputStream dis) throws IOException;
+        
+        public int size()
+        {
+            /* size of long for "position_"  + size of columnCount_ */
+            return (8 + 4);
+        }
+    }
+
+    static class ColumnNameIndexInfo extends ColumnIndexInfo
+    {
+        private String name_;       
+        
+        ColumnNameIndexInfo()
+        {
+            super(0L, 0);
+        }
+        
+        ColumnNameIndexInfo(String name)
+        {
+            this(name, 0L, 0);
+        }
+                
+        ColumnNameIndexInfo(String name, long position, int columnCount)
+        {
+            super(position, columnCount);
+            name_ = name;
+        }
+        
+        String name()
+        {
+            return name_;
+        }                
+        
+        public void set(Object o)
+        {
+            name_ = (String)o;
+        }
+        
+        public int compareTo(ColumnIndexInfo rhs)
+        {
+            IndexHelper.ColumnNameIndexInfo cIndexInfo = (IndexHelper.ColumnNameIndexInfo)rhs;
+            return name_.compareTo(cIndexInfo.name_);
+        }
+        
+        public void serialize(DataOutputStream dos) throws IOException
+        {
+            dos.writeLong(position()); 
+            dos.writeInt(count());
+            dos.writeUTF(name_);        
+        }
+        
+        public ColumnNameIndexInfo deserialize(DataInputStream dis) throws IOException
+        {
+            long position = dis.readLong();
+            int columnCount = dis.readInt();            
+            String name = dis.readUTF();       
+            return new ColumnNameIndexInfo(name, position, columnCount);
+        }
+        
+        public int size()
+        {
+            int size = super.size();
+            /* Size of the name_ as an UTF8 and the actual length as a short for the readUTF. */
+            size += FBUtilities.getUTF8Length(name_) + IColumn.UtfPrefix_;
+            return size;
+        }
+    }
+
+    static class ColumnTimestampIndexInfo extends ColumnIndexInfo
+    {
+        private long timestamp_;
+        
+        ColumnTimestampIndexInfo()
+        {
+            super(0L, 0);
+        }
+        
+        ColumnTimestampIndexInfo(long timestamp)
+        {
+            this(timestamp, 0L, 0);  
+        }
+        
+        ColumnTimestampIndexInfo(long timestamp, long position, int columnCount)
+        {
+            super(position, columnCount);
+            timestamp_ = timestamp;
+        }
+        
+        public long timestamp()
+        {
+            return timestamp_;
+        }
+        
+        public void set(Object o)
+        {
+            timestamp_ = (Long)o;
+        }
+        
+        public int compareTo(ColumnIndexInfo rhs)
+        {
+            ColumnTimestampIndexInfo cIndexInfo = (ColumnTimestampIndexInfo)rhs;
+            return Long.valueOf(timestamp_).compareTo(Long.valueOf(cIndexInfo.timestamp_));
+        }
+        
+        public void serialize(DataOutputStream dos) throws IOException
+        {
+            dos.writeLong(position()); 
+            dos.writeInt(count());
+            dos.writeLong(timestamp_);        
+        }
+        
+        public ColumnTimestampIndexInfo deserialize(DataInputStream dis) throws IOException
+        {
+            long position = dis.readLong();
+            int columnCount = dis.readInt();
+            long timestamp = dis.readLong();        
+            return new ColumnTimestampIndexInfo(timestamp, position, columnCount);
+        }
+        
+        public int size()
+        {
+            int size = super.size();
+            /* add the size of the timestamp which is a long */ 
+            size += 8;
+            return size;
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTable.java
index e69de29b..34d6e6a5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTable.java
@@ -0,0 +1,853 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Hashtable;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.utils.BasicUtilities;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.cassandra.dht.IPartitioner;
+
+/**
+ * This class is built on top of the SequenceFile. It stores
+ * data on disk in sorted fashion. However the sorting is upto
+ * the application. This class expects keys to be handed to it
+ * in sorted order. SSTable is broken up into blocks where each
+ * block contains 128 keys. At the end of the file  the block 
+ * index is written which contains the offsets to the keys in the
+ * block. SSTable also maintains an index file to which every 128th 
+ * key is written with a pointer to the block index which is the block 
+ * that actually contains the key. This index file is then read and 
+ * maintained in memory. SSTable is append only and immutable. SSTable
+ * on disk looks as follows:
+ * 
+ *                 -------------------------
+ *                 |------------------------|<-------|
+ *                 |                        |        |  BLOCK-INDEX PTR
+ *                 |                        |        |
+ *                 |------------------------|--------
+ *                 |------------------------|<-------|
+ *                 |                        |        |
+ *                 |                        |        |  BLOCK-INDEX PTR 
+ *                 |                        |        |
+ *                 |------------------------|---------
+ *                 |------------------------|<--------|
+ *                 |                        |         |
+ *                 |                        |         |
+ *                 |                        |         | BLOCK-INDEX PTR
+ *                 |                        |         |
+ *                 |------------------------|         |
+ *                 |------------------------|----------
+ *                 |------------------------|-----------------> BLOOM-FILTER
+ * version-info <--|----------|-------------|-------> relative offset to last block index.
+ *                 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SSTable
+{
+    private static Logger logger_ = Logger.getLogger(SSTable.class);
+    /* use this as a monitor to lock when loading index. */
+    private static Object indexLoadLock_ = new Object();
+    /* Every 128th key is an index. */
+    private static final int indexInterval_ = 128;
+    /* Key associated with block index written to disk */
+    public static final String blockIndexKey_ = "BLOCK-INDEX";
+    /* Position in SSTable after the first Block Index */
+    private static long positionAfterFirstBlockIndex_ = 0L;
+    /* Required extension for temporary files created during compactions. */
+    public static final String temporaryFile_ = "tmp";
+    /* Use this long as a 64 bit entity to turn on some bits for various settings */
+    private static final long version_ = 0L;
+    /*
+     * This map has the SSTable as key and a BloomFilter as value. This
+     * BloomFilter will tell us if a key/column pair is in the SSTable.
+     * If not we can avoid scanning it.
+     */
+    private static Map<String, BloomFilter> bfs_ = new Hashtable<String, BloomFilter>();
+    /* Maintains a touched set of keys */
+    private static LinkedHashMap<String, Long> touchCache_ = new TouchedKeyCache(DatabaseDescriptor.getTouchKeyCacheSize());        
+    
+    /**
+     * This class holds the position of a key in a block
+     * and the size of the data associated with this key. 
+    */
+    protected static class BlockMetadata
+    {
+        protected static final BlockMetadata NULL = new BlockMetadata(-1L, -1L);
+        
+        long position_;
+        long size_;
+        
+        BlockMetadata(long position, long size)
+        {
+            position_ = position;
+            size_ = size;
+        }
+    }
+    
+    /*
+     * This abstraction provides LRU symantics for the keys that are 
+     * "touched". Currently it holds the offset of the key in a data
+     * file. May change to hold a reference to a IFileReader which
+     * memory maps the key and its associated data on a touch.
+    */
+    private static class TouchedKeyCache extends LinkedHashMap<String, Long>
+    {
+        private final int capacity_;
+        
+        TouchedKeyCache(int capacity)
+        {
+            super(capacity + 1, 1.1f, true);
+            capacity_ = capacity;
+        }
+        
+        protected boolean removeEldestEntry(Map.Entry<String, Long> entry)
+        {
+            return ( size() > capacity_ );
+        }
+    }
+    
+    /**
+     * This is a simple container for the index Key and its corresponding position
+     * in the data file. Binary search is performed on a list of these objects
+     * to lookup keys within the SSTable data file.
+    */
+    public static class KeyPositionInfo implements Comparable<KeyPositionInfo>
+    {
+        private final String decoratedKey;
+        private long position_;
+        private IPartitioner partitioner;
+
+        public KeyPositionInfo(String decoratedKey, IPartitioner partitioner)
+        {
+            this.decoratedKey = decoratedKey;
+            this.partitioner = partitioner;
+        }
+
+        public KeyPositionInfo(String decoratedKey, IPartitioner partitioner, long position)
+        {
+            this(decoratedKey, partitioner);
+            position_ = position;
+        }
+
+        public String key()
+        {
+            return decoratedKey;
+        }
+
+        public long position()
+        {
+            return position_;
+        }
+
+        public int compareTo(KeyPositionInfo kPosInfo)
+        {
+            return partitioner.getDecoratedKeyComparator().compare(decoratedKey, kPosInfo.decoratedKey);
+        }
+
+        public String toString()
+        {
+        	return decoratedKey + ":" + position_;
+        }
+    }
+    
+    public static int indexInterval()
+    {
+    	return indexInterval_;
+    }
+    
+    /*
+     * Maintains a list of KeyPositionInfo objects per SSTable file loaded.
+     * We do this so that we don't read the index file into memory multiple
+     * times.
+    */
+    static Map<String, List<KeyPositionInfo>> indexMetadataMap_ = new Hashtable<String, List<KeyPositionInfo>>();
+    
+    /** 
+     * This method deletes both the specified data file
+     * and the associated index file
+     *
+     * @param dataFile - data file associated with the SSTable
+    */
+    public static void delete(String dataFile)
+    {        
+        /* remove the cached index table from memory */
+        indexMetadataMap_.remove(dataFile);
+        /* Delete the checksum file associated with this data file */
+        try
+        {
+            ChecksumManager.onFileDelete(dataFile);
+        }
+        catch ( IOException ex )
+        {
+            logger_.info( LogUtil.throwableToString(ex) );
+        }
+        
+        File file = new File(dataFile);
+        if ( file.exists() )
+        {
+            /* delete the data file */
+			if (file.delete())
+			{			    
+			    logger_.info("** Deleted " + file.getName() + " **");                
+			}
+			else
+			{			  
+			    logger_.error("Failed to delete " + file.getName());
+			}
+        }
+    }
+
+    public static int getApproximateKeyCount( List<String> dataFiles)
+    {
+    	int count = 0 ;
+
+    	for(String dataFile : dataFiles )
+    	{    		
+    		List<KeyPositionInfo> index = indexMetadataMap_.get(dataFile);
+    		if (index != null )
+    		{
+    			int indexKeyCount = index.size();
+    			count = count + (indexKeyCount+1) * indexInterval_ ;
+    	        logger_.debug("index size for bloom filter calc for file  : " + dataFile + "   : " + count);
+    		}
+    	}
+
+    	return count;
+    }
+
+    /**
+     * Get all indexed keys in the SSTable.
+    */
+    public static List<String> getIndexedKeys()
+    {
+        Set<String> indexFiles = indexMetadataMap_.keySet();
+        List<KeyPositionInfo> keyPositionInfos = new ArrayList<KeyPositionInfo>();
+
+        for ( String indexFile : indexFiles )
+        {
+            keyPositionInfos.addAll( indexMetadataMap_.get(indexFile) );
+        }
+
+        List<String> indexedKeys = new ArrayList<String>();
+        for ( KeyPositionInfo keyPositionInfo : keyPositionInfos )
+        {
+            indexedKeys.add(keyPositionInfo.decoratedKey);
+        }
+
+        Collections.sort(indexedKeys);
+        return indexedKeys;
+    }
+    
+    /*
+     * Intialize the index files and also cache the Bloom Filters
+     * associated with these files. Also caches the file handles 
+     * associated with these files.
+    */
+    public static void onStart(List<String> filenames) throws IOException
+    {
+        for ( String filename : filenames )
+        {
+            SSTable ssTable = null;
+            try
+            {
+                ssTable = new SSTable(filename, StorageService.getPartitioner());
+            }
+            catch ( IOException ex )
+            {
+                logger_.info("Deleting corrupted file " + filename);
+                FileUtils.delete(filename);
+                logger_.warn(LogUtil.throwableToString(ex));
+            }
+            finally
+            {
+                if ( ssTable != null )
+                {
+                    ssTable.close();
+                }
+            }
+        }
+    }
+
+    /*
+     * Stores the Bloom Filter associated with the given file.
+    */
+    public static void storeBloomFilter(String filename, BloomFilter bf)
+    {
+        bfs_.put(filename, bf);
+    }
+
+    /*
+     * Removes the bloom filter associated with the specified file.
+    */
+    public static void removeAssociatedBloomFilter(String filename)
+    {
+        bfs_.remove(filename);
+    }
+
+    /*
+     * Determines if the given key is in the specified file. If the
+     * key is not present then we skip processing this file.
+    */
+    public static boolean isKeyInFile(String clientKey, String filename)
+    {
+        boolean bVal = false;
+        BloomFilter bf = bfs_.get(filename);
+        if ( bf != null )
+        {
+            bVal = bf.isPresent(clientKey);
+        }
+        return bVal;
+    }
+
+    String dataFile_;
+    private IFileWriter dataWriter_;
+    private String lastWrittenKey_;    
+    private long firstBlockPosition_ = 0L;    
+    private int indexKeysWritten_ = 0;
+    /* Holds the keys and their respective positions of the current block index */
+    private SortedMap<String, BlockMetadata> blockIndex_;    
+    /* Holds all the block indicies for this SSTable */
+    private List<SortedMap<String, BlockMetadata>> blockIndexes_;
+    private IPartitioner partitioner_;
+    
+    /**
+     * This ctor basically gets passed in the full path name
+     * of the data file associated with this SSTable. Use this
+     * ctor to read the data in this file.
+    */
+    public SSTable(String dataFileName, IPartitioner partitioner) throws IOException
+    {        
+        dataFile_ = dataFileName;
+        partitioner_ = partitioner;
+        init();
+    }
+
+    /**
+     * This ctor is used for writing data into the SSTable. Use this
+     * version for non DB writes to the SSTable.
+    */
+    public SSTable(String directory, String filename, IPartitioner partitioner) throws IOException
+    {  
+        dataFile_ = directory + System.getProperty("file.separator") + filename + "-Data.db";
+        partitioner_ = partitioner;
+        blockIndex_ = new TreeMap<String, BlockMetadata>(partitioner_.getReverseDecoratedKeyComparator());
+        blockIndexes_ = new ArrayList<SortedMap<String, BlockMetadata>>();
+        dataWriter_ = SequenceFile.bufferedWriter(dataFile_, 4*1024*1024);
+        SSTable.positionAfterFirstBlockIndex_ = dataWriter_.getCurrentPosition();
+    } 
+
+    private void loadBloomFilter(IFileReader indexReader, long size) throws IOException
+    {        
+        /* read the position of the bloom filter */
+        indexReader.seek(size - 8);
+        byte[] bytes = new byte[8];
+        long currentPosition = indexReader.getCurrentPosition();
+        indexReader.readDirect(bytes);
+        long position = BasicUtilities.byteArrayToLong(bytes);
+        /* seek to the position of the bloom filter */
+        indexReader.seek(currentPosition - position);
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        DataInputBuffer bufIn = new DataInputBuffer();
+        /* read the bloom filter from disk */
+        indexReader.next(bufOut);   
+        bufOut.close();
+        bufIn.reset(bufOut.getData(), bufOut.getLength());
+        String clientKey = bufIn.readUTF();
+        if ( clientKey.equals(SequenceFile.marker_) )
+        {
+            /*
+             * We are now reading the serialized Bloom Filter. We read
+             * the length and then pass the bufIn to the serializer of
+             * the BloomFilter. We then store the Bloom filter in the
+             * map. However if the Bloom Filter already exists then we
+             * need not read the rest of the file.
+            */
+            bufIn.readInt();
+            if ( bfs_.get(dataFile_) == null )
+                bfs_.put(dataFile_, BloomFilter.serializer().deserialize(bufIn));
+        }
+    }
+    
+    private void loadIndexFile() throws IOException
+    {    
+        IFileReader indexReader = null;
+        /* Read all block indexes to maintain an index in memory */
+        try
+        {            
+            indexReader = SequenceFile.bufferedReader(dataFile_, 4*1024*1024);
+            long size = indexReader.getEOF();
+            
+            /* load the bloom filter into memory */
+            loadBloomFilter(indexReader, size);
+            /* read the position of the last block index */
+            byte[] bytes = new byte[8];
+            /* seek to the position to read the relative position of the first block index */
+            indexReader.seek(size - 16L);
+            /* the beginning of the first block index */
+            long currentPosition = indexReader.getCurrentPosition();
+            indexReader.readDirect(bytes);
+            long firstBlockIndexPosition = BasicUtilities.byteArrayToLong(bytes);  
+            List<KeyPositionInfo> keyPositionInfos = new ArrayList<KeyPositionInfo>();
+            indexMetadataMap_.put(dataFile_, keyPositionInfos);
+            DataOutputBuffer bufOut = new DataOutputBuffer();
+            DataInputBuffer bufIn = new DataInputBuffer();        
+            
+            long nextPosition = currentPosition - firstBlockIndexPosition;
+            indexReader.seek(nextPosition);
+            /* read the block indexes from the end of the file till we hit the first one. */
+            while ( nextPosition > 0 )
+            {
+                bufOut.reset();
+                /* position @ the current block index being processed */
+                currentPosition = indexReader.getCurrentPosition();
+                long bytesRead = indexReader.next(bufOut);
+                if ( bytesRead != -1 )
+                {
+                    bufIn.reset(bufOut.getData(), bufOut.getLength());
+                    /* read the block key. */
+                    String blockIndexKey = bufIn.readUTF();
+                    if ( !blockIndexKey.equals(SSTable.blockIndexKey_) )
+                    {
+                    	logger_.debug(" Done reading the block indexes, Index has been created");
+                    	break;
+                    }
+                    /* read the size of the block index */
+                    bufIn.readInt();                    
+                    /* Number of keys in the block. */
+                    int keys = bufIn.readInt();
+                    String largestKeyInBlock;
+                    for ( int i = 0; i < keys; ++i )
+                    {
+                        String keyInBlock = bufIn.readUTF();
+                        if ( i == 0 )
+                        {
+                            largestKeyInBlock = keyInBlock;
+                            /* relative offset in the block for the key*/
+                            bufIn.readLong();
+                            /* size of data associated with the key */
+                            bufIn.readLong();
+                            /* load the actual position of the block index into the index map */
+                            keyPositionInfos.add( new KeyPositionInfo(largestKeyInBlock, partitioner_, currentPosition) );
+                        }
+                        else
+                        {
+                            /*
+                             * This is not the key we are looking for. So read its position
+                             * and the size of the data associated with it. This was stored
+                             * as the BlockMetadata.
+                            */
+                            bufIn.readLong();
+                            bufIn.readLong();
+                        }
+                    }
+                }
+            }
+            bufIn.close();
+            bufOut.close();
+            Collections.sort(keyPositionInfos);
+        }        
+        finally
+        {
+            if ( indexReader != null )
+            {
+                indexReader.close();
+            }
+        }        
+    }
+
+    private void init() throws IOException
+    {        
+        /*
+         * this is to prevent multiple threads from
+         * loading the same index files multiple times
+         * into memory.
+        */
+        synchronized( indexLoadLock_ )
+        {
+            if ( indexMetadataMap_.get(dataFile_) == null )
+            {
+                long start = System.currentTimeMillis();
+                loadIndexFile();
+                logger_.debug("INDEX LOAD TIME: " + (System.currentTimeMillis() - start) + " ms.");                
+            }
+        }
+    }
+
+    private String getFile(String name) throws IOException
+    {
+        File file = new File(name);
+        if ( file.exists() )
+            return file.getAbsolutePath();
+        throw new IOException("File " + name + " was not found on disk.");
+    }
+
+    public String getDataFileLocation() throws IOException
+    {
+        return getFile(dataFile_);
+    }
+
+    /*
+     * Seeks to the specified key on disk.
+    */
+    public void touch(final String clientKey, boolean fData) throws IOException
+    {
+        if (touchCache_.containsKey(dataFile_ + ":" + clientKey))
+            return;
+        
+        IFileReader dataReader = SequenceFile.reader(dataFile_); 
+        try
+        {
+        	/* Morph the key */
+            String decoratedKey = partitioner_.decorateKey(clientKey);
+            Coordinate fileCoordinate = getCoordinates(decoratedKey, dataReader, partitioner_);
+            /* Get offset of key from block Index */
+            dataReader.seek(fileCoordinate.end_);
+            BlockMetadata blockMetadata = dataReader.getBlockMetadata(decoratedKey);
+            if ( blockMetadata.position_ != -1L )
+            {
+                touchCache_.put(dataFile_ + ":" + clientKey, blockMetadata.position_);
+            } 
+            
+            if ( fData )
+            {
+                /* Read the data associated with this key and pull it into the Buffer Cache */
+                if ( blockMetadata.position_ != -1L )
+                {
+                    dataReader.seek(blockMetadata.position_);
+                    DataOutputBuffer bufOut = new DataOutputBuffer();
+                    dataReader.next(bufOut);
+                    bufOut.reset();
+                    logger_.debug("Finished the touch of the key to pull it into buffer cache.");
+                }
+            }
+        }
+        finally
+        {
+            if ( dataReader != null )
+                dataReader.close();
+        }
+    }
+
+    private long beforeAppend(String decoratedKey) throws IOException
+    {
+    	if (decoratedKey == null )
+            throw new IOException("Keys must not be null.");
+        Comparator<String> c = partitioner_.getDecoratedKeyComparator();
+        if ( lastWrittenKey_ != null && c.compare(lastWrittenKey_, decoratedKey) > 0 )
+        {
+            logger_.info("Last written key : " + lastWrittenKey_);
+            logger_.info("Current key : " + decoratedKey);
+            logger_.info("Writing into file " + dataFile_);
+            throw new IOException("Keys must be written in ascending order.");
+        }
+        return (lastWrittenKey_ == null) ? SSTable.positionAfterFirstBlockIndex_ : dataWriter_.getCurrentPosition();
+    }
+
+    private void afterAppend(String decoratedKey, long position, long size) throws IOException
+    {
+        ++indexKeysWritten_;
+        lastWrittenKey_ = decoratedKey;
+        blockIndex_.put(decoratedKey, new BlockMetadata(position, size));
+        if ( indexKeysWritten_ == indexInterval_ )
+        {
+        	blockIndexes_.add(blockIndex_);
+        	blockIndex_ = new TreeMap<String, BlockMetadata>(partitioner_.getReverseDecoratedKeyComparator());
+            indexKeysWritten_ = 0;
+        }                
+    }
+
+    /**
+     * Dumps all the block indicies for this SSTable
+     * at the end of the file.
+     * @throws IOException
+     */
+    private void dumpBlockIndexes() throws IOException
+    {    	
+        long position = dataWriter_.getCurrentPosition();
+        firstBlockPosition_ = position;
+    	for( SortedMap<String, BlockMetadata> block : blockIndexes_ )
+    	{
+    		dumpBlockIndex( block );
+    	}  	
+    }    
+    
+    private void dumpBlockIndex( SortedMap<String, BlockMetadata> blockIndex) throws IOException
+    {
+        /* Block Index is empty so bail. */
+        if ( blockIndex.size() == 0 )
+            return;
+        
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        /* 
+         * Record the position where we start writing the block index. This is will be
+         * used as the position of the lastWrittenKey in the block in the index file
+        */
+        long position = dataWriter_.getCurrentPosition();
+        Set<String> keys = blockIndex.keySet();                
+        /* Number of keys in this block */
+        bufOut.writeInt(keys.size());
+        for ( String decoratedKey : keys )
+        {            
+            bufOut.writeUTF(decoratedKey);
+            BlockMetadata blockMetadata = blockIndex.get(decoratedKey);
+            /* position of the key as a relative offset */
+            bufOut.writeLong(position - blockMetadata.position_);
+            bufOut.writeLong(blockMetadata.size_);
+        }
+        /* Write out the block index. */
+        dataWriter_.append(SSTable.blockIndexKey_, bufOut);
+        /* Load this index into the in memory index map */
+        List<KeyPositionInfo> keyPositionInfos = SSTable.indexMetadataMap_.get(dataFile_);
+        if ( keyPositionInfos == null )
+        {
+        	keyPositionInfos = new ArrayList<KeyPositionInfo>();
+        	SSTable.indexMetadataMap_.put(dataFile_, keyPositionInfos);
+        }
+        
+        keyPositionInfos.add(new KeyPositionInfo(blockIndex.firstKey(), partitioner_, position));
+        blockIndex.clear();        
+    }
+
+    public void append(String decoratedKey, DataOutputBuffer buffer) throws IOException
+    {
+        long currentPosition = beforeAppend(decoratedKey);
+        dataWriter_.append(decoratedKey, buffer);
+        afterAppend(decoratedKey, currentPosition, buffer.getLength());
+    }
+
+    public void append(String decoratedKey, byte[] value) throws IOException
+    {
+        long currentPosition = beforeAppend(decoratedKey);
+        dataWriter_.append(decoratedKey, value);
+        afterAppend(decoratedKey, currentPosition, value.length );
+    }
+
+    public static Coordinate getCoordinates(String decoratedKey, IFileReader dataReader, IPartitioner partitioner) throws IOException
+    {
+    	List<KeyPositionInfo> indexInfo = indexMetadataMap_.get(dataReader.getFileName());
+    	int size = (indexInfo == null) ? 0 : indexInfo.size();
+    	long start = 0L;
+    	long end = dataReader.getEOF();
+        if ( size > 0 )
+        {
+            int index = Collections.binarySearch(indexInfo, new KeyPositionInfo(decoratedKey, partitioner));
+            if ( index < 0 )
+            {
+                /*
+                 * We are here which means that the requested
+                 * key is not an index.
+                */
+                index = (++index)*(-1);
+                /*
+                 * This means key is not present at all. Hence
+                 * a scan is in order.
+                */
+                start = (index == 0) ? 0 : indexInfo.get(index - 1).position();
+                if ( index < size )
+                {
+                    end = indexInfo.get(index).position();
+                }
+                else
+                {
+                    /* This is the Block Index in the file. */
+                    end = start;
+                }
+            }
+            else
+            {
+                /*
+                 * If we are here that means the key is in the index file
+                 * and we can retrieve it w/o a scan. In reality we would
+                 * like to have a retreive(key, fromPosition) but for now
+                 * we use scan(start, start + 1) - a hack.
+                */
+                start = indexInfo.get(index).position();                
+                end = start;
+            }
+        }
+        else
+        {
+            /*
+             * We are here which means there are less than
+             * 128 keys in the system and hence our only recourse
+             * is a linear scan from start to finish. Automatically
+             * use memory mapping since we have a huge file and very
+             * few keys.
+            */
+            end = dataReader.getEOF();
+        }  
+        
+        return new Coordinate(start, end);
+    }
+    
+    public DataInputBuffer next(final String clientKey, String cfName, List<String> columnNames) throws IOException
+    {
+        return next(clientKey, cfName, columnNames, null);
+    }
+
+    public DataInputBuffer next(final String clientKey, String cfName, List<String> columnNames, IndexHelper.TimeRange timeRange) throws IOException
+    {
+        IFileReader dataReader = null;
+        try
+        {
+            dataReader = SequenceFile.reader(dataFile_);
+            // dataReader = SequenceFile.chksumReader(dataFile_, 4*1024*1024);
+
+            /* Morph key into actual key based on the partition type. */
+            String decoratedKey = partitioner_.decorateKey(clientKey);
+            Coordinate fileCoordinate = getCoordinates(decoratedKey, dataReader, partitioner_);
+            /*
+             * we have the position we have to read from in order to get the
+             * column family, get the column family and column(s) needed.
+            */
+            DataOutputBuffer bufOut = new DataOutputBuffer();
+            DataInputBuffer bufIn = new DataInputBuffer();
+
+            long bytesRead = dataReader.next(decoratedKey, bufOut, cfName, columnNames, timeRange, fileCoordinate);
+            if ( bytesRead != -1L )
+            {
+                if ( bufOut.getLength() > 0 )
+                {
+                    bufIn.reset(bufOut.getData(), bufOut.getLength());
+                    /* read the key even though we do not use it */
+                    bufIn.readUTF();
+                    bufIn.readInt();
+                }
+            }
+            return bufIn;
+        }
+        finally
+        {
+            if (dataReader != null)
+            {
+                dataReader.close();
+            }
+        }
+    }
+
+    public DataInputBuffer next(String clientKey, String columnFamilyColumn) throws IOException
+    {
+        String[] values = RowMutation.getColumnAndColumnFamily(columnFamilyColumn);
+        String columnFamilyName = values[0];
+        List<String> cnNames = (values.length == 1) ? null : Arrays.asList(values[1]);
+        return next(clientKey, columnFamilyName, cnNames);
+    }
+
+    public void close() throws IOException
+    {
+        close( new byte[0], 0 );
+    }
+
+    public void close(BloomFilter bf) throws IOException
+    {
+        /* Any remnants in the blockIndex should be added to the dump */
+    	blockIndexes_.add(blockIndex_);
+    	dumpBlockIndexes();
+        
+    	/* reset the buffer and serialize the Bloom Filter. */
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        BloomFilter.serializer().serialize(bf, bufOut);
+        close(bufOut.getData(), bufOut.getLength());
+        bufOut.close();
+        // byte[] bytes = new byte[bufOut.getLength()];        
+        // System.arraycopy(bufOut.getData(), 0, bytes, 0, bufOut.getLength());
+        // close(bytes, bytes.length);             
+    }
+
+    /**
+     * Renames a temporary SSTable file to a valid data and index file
+     */
+    public void closeRename(BloomFilter bf) throws IOException
+    {
+    	close(bf);
+        String tmpDataFile = dataFile_;
+    	String dataFileName = dataFile_.replace("-" + temporaryFile_,"");    	
+    	File dataFile = new File(dataFile_);
+    	dataFile.renameTo(new File(dataFileName));    	    	
+    	dataFile_ = dataFileName;        
+    	/* Now repair the in memory index associated with the old name */
+    	List<KeyPositionInfo> keyPositionInfos = SSTable.indexMetadataMap_.remove(tmpDataFile);    	    	  	    	
+    	SSTable.indexMetadataMap_.put(dataFile_, keyPositionInfos);
+    }
+    
+    public void closeRename(BloomFilter bf, List<String> files) throws IOException
+    {
+        close( bf);
+        String tmpDataFile = dataFile_;
+        String dataFileName = dataFile_.replace("-" + temporaryFile_,"");
+        File dataFile = new File(dataFile_);
+        dataFile.renameTo(new File(dataFileName));
+        dataFile_ = dataFileName;
+        /* Now repair the in memory index associated with the old name */
+        List<KeyPositionInfo> keyPositionInfos = SSTable.indexMetadataMap_.remove(tmpDataFile);                         
+        SSTable.indexMetadataMap_.put(dataFile_, keyPositionInfos);
+        if ( files != null )
+        {            
+            files.add(dataFile_);
+        }
+    }
+    
+    private void close(byte[] footer, int size) throws IOException
+    {
+        /*
+         * Write the bloom filter for this SSTable.
+         * Then write three longs one which is a version
+         * and one which is a pointer to the last written
+         * block index and the last one is the position of
+         * the Bloom Filter.
+         */
+        if ( dataWriter_ != null )
+        {            
+            long bloomFilterPosition = dataWriter_.getCurrentPosition();
+            dataWriter_.close(footer, size);
+            /* write the version field into the SSTable */           
+            dataWriter_.writeDirect(BasicUtilities.longToByteArray(version_));
+            /* write the relative position of the first block index from current position */
+            long blockPosition = dataWriter_.getCurrentPosition() - firstBlockPosition_;
+            dataWriter_.writeDirect(BasicUtilities.longToByteArray(blockPosition));
+            
+            /* write the position of the bloom filter */
+            long bloomFilterRelativePosition = dataWriter_.getCurrentPosition() - bloomFilterPosition;
+            dataWriter_.writeDirect(BasicUtilities.longToByteArray(bloomFilterRelativePosition));            
+            dataWriter_.close();
+        }
+    } 
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SequenceFile.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SequenceFile.java
index e69de29b..92c35a72 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SequenceFile.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SequenceFile.java
@@ -0,0 +1,1305 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+import java.lang.reflect.Method;
+import java.nio.ByteBuffer;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.LogUtil;
+
+import org.apache.log4j.Logger;
+
+/**
+ * This class writes key/value pairs seqeuntially to disk. It is
+ * also used to read sequentially from disk. However one could
+ * jump to random positions to read data from the file. This class
+ * also has many implementations of the IFileWriter and IFileReader
+ * interfaces which are exposed through factory methods.
+ * <p/>
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com ) & Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+
+public class SequenceFile
+{
+    public static abstract class AbstractWriter implements IFileWriter
+    {
+        protected String filename_;
+
+        AbstractWriter(String filename)
+        {
+            filename_ = filename;
+        }
+
+        public String getFileName()
+        {
+            return filename_;
+        }
+
+        public long lastModified()
+        {
+            File file = new File(filename_);
+            return file.lastModified();
+        }
+    }
+
+    public static class Writer extends AbstractWriter
+    {
+        protected RandomAccessFile file_;
+
+        Writer(String filename) throws IOException
+        {
+            super(filename);
+            init(filename);
+        }
+
+        Writer(String filename, int size) throws IOException
+        {
+            super(filename);
+            init(filename, size);
+        }
+
+        protected void init(String filename) throws IOException
+        {
+            File file = new File(filename);
+            if (!file.exists())
+            {
+                file.createNewFile();
+            }
+            file_ = new RandomAccessFile(file, "rw");
+        }
+
+        protected void init(String filename, int size) throws IOException
+        {
+            init(filename);
+        }
+
+        public long getCurrentPosition() throws IOException
+        {
+            return file_.getFilePointer();
+        }
+
+        public void seek(long position) throws IOException
+        {
+            file_.seek(position);
+        }
+
+        public void append(DataOutputBuffer buffer) throws IOException
+        {
+            file_.write(buffer.getData(), 0, buffer.getLength());
+        }
+
+        public void append(DataOutputBuffer keyBuffer, DataOutputBuffer buffer) throws IOException
+        {
+            int keyBufLength = keyBuffer.getLength();
+            if (keyBuffer == null || keyBufLength == 0)
+                throw new IllegalArgumentException("Key cannot be NULL or of zero length.");
+
+            file_.writeInt(keyBufLength);
+            file_.write(keyBuffer.getData(), 0, keyBufLength);
+
+            int length = buffer.getLength();
+            file_.writeInt(length);
+            file_.write(buffer.getData(), 0, length);
+        }
+
+        public void append(String key, DataOutputBuffer buffer) throws IOException
+        {
+            if (key == null)
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            file_.writeUTF(key);
+            int length = buffer.getLength();
+            file_.writeInt(length);
+            file_.write(buffer.getData(), 0, length);
+        }
+
+        public void append(String key, byte[] value) throws IOException
+        {
+            if (key == null)
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            file_.writeUTF(key);
+            file_.writeInt(value.length);
+            file_.write(value);
+        }
+
+        public void append(String key, long value) throws IOException
+        {
+            if (key == null)
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            file_.writeUTF(key);
+            file_.writeLong(value);
+        }
+
+        /**
+         * Be extremely careful while using this API. This currently
+         * used to write the commit log header in the commit logs.
+         * If not used carefully it could completely screw up reads
+         * of other key/value pairs that are written.
+         *
+         * @param bytes the bytes to write
+         */
+        public long writeDirect(byte[] bytes) throws IOException
+        {
+            file_.write(bytes);
+            return file_.getFilePointer();
+        }
+
+        public void writeLong(long value) throws IOException
+        {
+            file_.writeLong(value);
+        }
+
+        public void close() throws IOException
+        {
+            file_.close();
+        }
+
+        public void close(byte[] footer, int size) throws IOException
+        {
+            file_.writeUTF(SequenceFile.marker_);
+            file_.writeInt(size);
+            file_.write(footer, 0, size);
+        }
+
+        public String getFileName()
+        {
+            return filename_;
+        }
+
+        public long getFileSize() throws IOException
+        {
+            return file_.length();
+        }
+    }
+
+    public static class BufferWriter extends Writer
+    {
+
+        BufferWriter(String filename, int size) throws IOException
+        {
+            super(filename, size);
+        }
+
+        @Override
+        protected void init(String filename) throws IOException
+        {
+            init(filename, 0);
+        }
+
+        @Override
+        protected void init(String filename, int size) throws IOException
+        {
+            File file = new File(filename);
+            file_ = new BufferedRandomAccessFile(file, "rw", size);
+            if (!file.exists())
+            {
+                file.createNewFile();
+            }
+        }
+    }
+
+    public static class ChecksumWriter extends Writer
+    {
+
+        ChecksumWriter(String filename, int size) throws IOException
+        {
+            super(filename, size);
+        }
+
+        @Override
+        protected void init(String filename) throws IOException
+        {
+            init(filename, 0);
+        }
+
+        @Override
+        protected void init(String filename, int size) throws IOException
+        {
+            File file = new File(filename);
+            file_ = new ChecksumRandomAccessFile(file, "rw", size);
+        }
+
+        @Override
+        public void close() throws IOException
+        {
+            super.close();
+            ChecksumManager.close(filename_);
+        }
+    }
+
+    public static class ConcurrentWriter extends AbstractWriter
+    {
+        private FileChannel fc_;
+
+        public ConcurrentWriter(String filename) throws IOException
+        {
+            super(filename);
+            RandomAccessFile raf = new RandomAccessFile(filename, "rw");
+            fc_ = raf.getChannel();
+        }
+
+        public long getCurrentPosition() throws IOException
+        {
+            return fc_.position();
+        }
+
+        public void seek(long position) throws IOException
+        {
+            fc_.position(position);
+        }
+
+        public void append(DataOutputBuffer buffer) throws IOException
+        {
+            int length = buffer.getLength();
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(length);
+            byteBuffer.put(buffer.getData(), 0, length);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public void append(DataOutputBuffer keyBuffer, DataOutputBuffer buffer) throws IOException
+        {
+            int keyBufLength = keyBuffer.getLength();
+            if (keyBuffer == null || keyBufLength == 0)
+                throw new IllegalArgumentException("Key cannot be NULL or of zero length.");
+
+            /* Size allocated "int" for key length + key + "int" for data length + data */
+            int length = buffer.getLength();
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(4 + keyBufLength + 4 + length);
+            byteBuffer.putInt(keyBufLength);
+            byteBuffer.put(keyBuffer.getData(), 0, keyBufLength);
+            byteBuffer.putInt(length);
+            byteBuffer.put(buffer.getData(), 0, length);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public void append(String key, DataOutputBuffer buffer) throws IOException
+        {
+            if (key == null)
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            int length = buffer.getLength();
+            /* Size allocated : utfPrefix_ + key length + "int" for data size + data */
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(SequenceFile.utfPrefix_ + key.length() + 4 + length);
+            SequenceFile.writeUTF(byteBuffer, key);
+            byteBuffer.putInt(length);
+            byteBuffer.put(buffer.getData(), 0, length);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public void append(String key, byte[] value) throws IOException
+        {
+            if (key == null)
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            /* Size allocated key length + "int" for data size + data */
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(utfPrefix_ + key.length() + 4 + value.length);
+            SequenceFile.writeUTF(byteBuffer, key);
+            byteBuffer.putInt(value.length);
+            byteBuffer.put(value);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public void append(String key, long value) throws IOException
+        {
+            if (key == null)
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            /* Size allocated key length + a long */
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(SequenceFile.utfPrefix_ + key.length() + 8);
+            SequenceFile.writeUTF(byteBuffer, key);
+            byteBuffer.putLong(value);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        /*
+         * Be extremely careful while using this API. This currently
+         * used to write the commit log header in the commit logs.
+         * If not used carefully it could completely screw up reads
+         * of other key/value pairs that are written.
+        */
+        public long writeDirect(byte[] bytes) throws IOException
+        {
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(bytes.length);
+            byteBuffer.put(bytes);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+            return fc_.position();
+        }
+
+        public void writeLong(long value) throws IOException
+        {
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(8);
+            byteBuffer.putLong(value);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public void close() throws IOException
+        {
+            fc_.close();
+        }
+
+        public void close(byte[] footer, int size) throws IOException
+        {
+            /* Size is marker length + "int" for size + footer data */
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(utfPrefix_ + SequenceFile.marker_.length() + 4 + footer.length);
+            SequenceFile.writeUTF(byteBuffer, SequenceFile.marker_);
+            byteBuffer.putInt(size);
+            byteBuffer.put(footer);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public String getFileName()
+        {
+            return filename_;
+        }
+
+        public long getFileSize() throws IOException
+        {
+            return fc_.size();
+        }
+    }
+
+    public static class FastConcurrentWriter extends AbstractWriter
+    {
+        private FileChannel fc_;
+        private MappedByteBuffer buffer_;
+
+        public FastConcurrentWriter(String filename, int size) throws IOException
+        {
+            super(filename);
+            fc_ = new RandomAccessFile(filename, "rw").getChannel();
+            buffer_ = fc_.map(FileChannel.MapMode.READ_WRITE, 0, size);
+            buffer_.load();
+        }
+
+        void unmap(final Object buffer)
+        {
+            AccessController.doPrivileged(new PrivilegedAction<MappedByteBuffer>()
+            {
+                public MappedByteBuffer run()
+                {
+                    try
+                    {
+                        Method getCleanerMethod = buffer.getClass().getMethod("cleaner", new Class[0]);
+                        getCleanerMethod.setAccessible(true);
+                        sun.misc.Cleaner cleaner = (sun.misc.Cleaner) getCleanerMethod.invoke(buffer);
+                        cleaner.clean();
+                    }
+                    catch (Throwable e)
+                    {
+                        logger_.warn(LogUtil.throwableToString(e));
+                    }
+                    return null;
+                }
+            });
+        }
+
+
+        public long getCurrentPosition() throws IOException
+        {
+            return buffer_.position();
+        }
+
+        public void seek(long position) throws IOException
+        {
+            buffer_.position((int) position);
+        }
+
+        public void append(DataOutputBuffer buffer) throws IOException
+        {
+            buffer_.put(buffer.getData(), 0, buffer.getLength());
+        }
+
+        public void append(DataOutputBuffer keyBuffer, DataOutputBuffer buffer) throws IOException
+        {
+            int keyBufLength = keyBuffer.getLength();
+            if (keyBuffer == null || keyBufLength == 0)
+                throw new IllegalArgumentException("Key cannot be NULL or of zero length.");
+
+            int length = buffer.getLength();
+            buffer_.putInt(keyBufLength);
+            buffer_.put(keyBuffer.getData(), 0, keyBufLength);
+            buffer_.putInt(length);
+            buffer_.put(buffer.getData(), 0, length);
+        }
+
+        public void append(String key, DataOutputBuffer buffer) throws IOException
+        {
+            if (key == null)
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            int length = buffer.getLength();
+            SequenceFile.writeUTF(buffer_, key);
+            buffer_.putInt(length);
+            buffer_.put(buffer.getData(), 0, length);
+        }
+
+        public void append(String key, byte[] value) throws IOException
+        {
+            if (key == null)
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            SequenceFile.writeUTF(buffer_, key);
+            buffer_.putInt(value.length);
+            buffer_.put(value);
+        }
+
+        public void append(String key, long value) throws IOException
+        {
+            if (key == null)
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            SequenceFile.writeUTF(buffer_, key);
+            buffer_.putLong(value);
+        }
+
+        /*
+         * Be extremely careful while using this API. This currently
+         * used to write the commit log header in the commit logs.
+         * If not used carefully it could completely screw up reads
+         * of other key/value pairs that are written.
+        */
+        public long writeDirect(byte[] bytes) throws IOException
+        {
+            buffer_.put(bytes);
+            return buffer_.position();
+        }
+
+        public void writeLong(long value) throws IOException
+        {
+            buffer_.putLong(value);
+        }
+
+        public void close() throws IOException
+        {
+            buffer_.flip();
+            buffer_.force();
+            unmap(buffer_);
+            fc_.truncate(buffer_.limit());
+        }
+
+        public void close(byte[] footer, int size) throws IOException
+        {
+            SequenceFile.writeUTF(buffer_, SequenceFile.marker_);
+            buffer_.putInt(size);
+            buffer_.put(footer);
+            close();
+        }
+
+        public String getFileName()
+        {
+            return filename_;
+        }
+
+        public long getFileSize() throws IOException
+        {
+            return buffer_.position();
+        }
+    }
+
+    public static abstract class AbstractReader implements IFileReader
+    {
+        private static final short utfPrefix_ = 2;
+        protected RandomAccessFile file_;
+        protected String filename_;
+
+        AbstractReader(String filename)
+        {
+            filename_ = filename;
+        }
+
+        public String getFileName()
+        {
+            return filename_;
+        }
+
+        /**
+         * Return the position of the given key from the block index.
+         *
+         * @param key the key whose offset is to be extracted from the current block index
+         */
+        public long getPositionFromBlockIndex(String key) throws IOException
+        {
+            long position = -1L;
+            /* note the beginning of the block index */
+            long blockIndexPosition = file_.getFilePointer();
+            /* read the block key. */
+            String blockIndexKey = file_.readUTF();
+            if (!blockIndexKey.equals(SSTable.blockIndexKey_))
+                throw new IOException("Unexpected position to be reading the block index from.");
+            /* read the size of the block index */
+            int size = file_.readInt();
+
+            /* Read the entire block index. */
+            byte[] bytes = new byte[size];
+            file_.readFully(bytes);
+
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(bytes, bytes.length);
+            /* Number of keys in the block. */
+            int keys = bufIn.readInt();
+            for (int i = 0; i < keys; ++i)
+            {
+                String keyInBlock = bufIn.readUTF();
+                if (keyInBlock.equals(key))
+                {
+                    position = bufIn.readLong();
+                    break;
+                }
+                else
+                {
+                    /*
+                     * This is not the key we are looking for. So read its position
+                     * and the size of the data associated with it. This was strored
+                     * as the BlockMetadata.
+                    */
+                    bufIn.readLong();
+                    bufIn.readLong();
+                }
+            }
+
+            /* we do this because relative position of the key within a block is stored. */
+            if (position != -1L)
+                position = blockIndexPosition - position;
+            return position;
+        }
+
+        /**
+         * Return the block index metadata for a given key.
+         */
+        public SSTable.BlockMetadata getBlockMetadata(String key) throws IOException
+        {
+            SSTable.BlockMetadata blockMetadata = SSTable.BlockMetadata.NULL;
+            /* read the block key. */
+            String blockIndexKey = file_.readUTF();
+            if (!blockIndexKey.equals(SSTable.blockIndexKey_))
+                throw new IOException("Unexpected position to be reading the block index from.");
+            /* read the size of the block index */
+            int size = file_.readInt();
+
+            /* Read the entire block index. */
+            byte[] bytes = new byte[size];
+            file_.readFully(bytes);
+
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(bytes, bytes.length);
+
+            /* Number of keys in the block. */
+            int keys = bufIn.readInt();
+            for (int i = 0; i < keys; ++i)
+            {
+                if (bufIn.readUTF().equals(key))
+                {
+                    long position = bufIn.readLong();
+                    long dataSize = bufIn.readLong();
+                    blockMetadata = new SSTable.BlockMetadata(position, dataSize);
+                    break;
+                }
+                else
+                {
+                    /*
+                     * This is not the key we are looking for. So read its position
+                     * and the size of the data associated with it. This was strored
+                     * as the BlockMetadata.
+                    */
+                    bufIn.readLong();
+                    bufIn.readLong();
+                }
+            }
+
+            return blockMetadata;
+        }
+
+        /**
+         * This function seeks to the position where the key data is present in the file
+         * in order to get the buffer cache populated with the key-data. This is done as
+         * a hint before the user actually queries the data.
+         *
+         * @param key   the key whose data is being touched
+         * @param fData
+         */
+        public long touch(String key, boolean fData) throws IOException
+        {
+            long bytesRead = -1L;
+            if (isEOF())
+                return bytesRead;
+
+            long startPosition = file_.getFilePointer();
+            String keyInDisk = file_.readUTF();
+            if (keyInDisk != null)
+            {
+                /*
+                 * If key on disk is greater than requested key
+                 * we can bail out since we exploit the property
+                 * of the SSTable format.
+                */
+                if (keyInDisk.compareTo(key) > 0)
+                    return bytesRead;
+
+                /*
+                 * If we found the key then we populate the buffer that
+                 * is passed in. If not then we skip over this key and
+                 * position ourselves to read the next one.
+                */
+                int dataSize = file_.readInt();
+                if (keyInDisk.equals(key))
+                {
+                    /* return 0L to signal the key has been touched. */
+                    bytesRead = 0L;
+                    return bytesRead;
+                }
+                else
+                {
+                    /* skip over data portion */
+                    file_.seek(dataSize + file_.getFilePointer());
+                }
+
+                long endPosition = file_.getFilePointer();
+                bytesRead = endPosition - startPosition;
+            }
+
+            return bytesRead;
+        }
+
+        /**
+         * This method seek the disk head to the block index, finds
+         * the offset of the key within the block and seeks to that
+         * offset.
+         *
+         * @param key     we are interested in.
+         * @param section indicates the location of the block index.
+         * @throws IOException
+         */
+        protected void seekTo(String key, Coordinate section) throws IOException
+        {
+            /* Goto the Block Index */
+            seek(section.end_);
+            long position = getPositionFromBlockIndex(key);
+            if (position == -1)
+                throw new IOException("This key " + key + " does not exist in this file.");
+            seek(position);
+        }
+
+        /**
+         * Defreeze the bloom filter.
+         *
+         * @return bloom filter summarizing the column information
+         * @throws IOException
+         */
+        private BloomFilter defreezeBloomFilter() throws IOException
+        {
+            int size = file_.readInt();
+            byte[] bytes = new byte[size];
+            file_.readFully(bytes);
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(bytes, bytes.length);
+            BloomFilter bf = BloomFilter.serializer().deserialize(bufIn);
+            return bf;
+        }
+
+        /**
+         * Reads the column name indexes if present. If the
+         * indexes are based on time then skip over them.
+         *
+         * @param cfName
+         * @return
+         */
+        private int handleColumnNameIndexes(String cfName, List<IndexHelper.ColumnIndexInfo> columnIndexList) throws IOException
+        {
+            /* check if we have an index */
+            boolean hasColumnIndexes = file_.readBoolean();
+            int totalBytesRead = 1;
+            /* if we do then deserialize the index */
+            if (hasColumnIndexes)
+            {
+                if (DatabaseDescriptor.isNameSortingEnabled(cfName) || DatabaseDescriptor.getColumnFamilyType(cfName).equals("Super"))
+                {
+                    /* read the index */
+                    totalBytesRead += IndexHelper.deserializeIndex(cfName, file_, columnIndexList);
+                }
+                else
+                {
+                    totalBytesRead += IndexHelper.skipIndex(file_);
+                }
+            }
+            return totalBytesRead;
+        }
+
+        /**
+         * Reads the column name indexes if present. If the
+         * indexes are based on time then skip over them.
+         *
+         * @param cfName
+         * @return
+         */
+        private int handleColumnTimeIndexes(String cfName, List<IndexHelper.ColumnIndexInfo> columnIndexList) throws IOException
+        {
+            /* check if we have an index */
+            boolean hasColumnIndexes = file_.readBoolean();
+            int totalBytesRead = 1;
+            /* if we do then deserialize the index */
+            if (hasColumnIndexes)
+            {
+                if (DatabaseDescriptor.isTimeSortingEnabled(cfName))
+                {
+                    /* read the index */
+                    totalBytesRead += IndexHelper.deserializeIndex(cfName, file_, columnIndexList);
+                }
+                else
+                {
+                    totalBytesRead += IndexHelper.skipIndex(file_);
+                }
+            }
+            return totalBytesRead;
+        }
+
+        /**
+         * This method dumps the next key/value into the DataOuputStream
+         * passed in. Always use this method to query for application
+         * specific data as it will have indexes.
+         *
+         * @param key       key we are interested in.
+         * @param bufOut    DataOutputStream that needs to be filled.
+         * @param columnFamilyName name of the columnFamily
+         * @param columnNames columnNames we are interested in
+         * OR
+         * @param timeRange time range we are interested in
+         * @param section   region of the file that needs to be read
+         * @return number of bytes that were read.
+         * @throws IOException
+         */
+        public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, List<String> columnNames, IndexHelper.TimeRange timeRange, Coordinate section) throws IOException
+        {
+            assert !columnFamilyName.contains(":");
+            assert timeRange == null || columnNames == null; // at most one may be non-null
+
+            long bytesRead = -1L;
+            if (isEOF())
+                return bytesRead;
+            seekTo(key, section);
+            /* note the position where the key starts */
+            long startPosition = file_.getFilePointer();
+            String keyInDisk = file_.readUTF();
+            if (keyInDisk != null)
+            {
+                /*
+                 * If key on disk is greater than requested key
+                 * we can bail out since we exploit the property
+                 * of the SSTable format.
+                */
+                if (keyInDisk.compareTo(key) > 0)
+                    return bytesRead;
+
+                /*
+                 * If we found the key then we populate the buffer that
+                 * is passed in. If not then we skip over this key and
+                 * position ourselves to read the next one.
+                */
+                if (keyInDisk.equals(key))
+                {
+                    if (timeRange == null) {
+                        readColumns(key, bufOut, columnFamilyName, columnNames);
+                    } else {
+                        readTimeRange(key, bufOut, columnFamilyName, timeRange);
+                    }
+                }
+                else
+                {
+                    /* skip over data portion */
+                    int dataSize = file_.readInt();
+                    file_.seek(dataSize + file_.getFilePointer());
+                }
+
+                long endPosition = file_.getFilePointer();
+                bytesRead = endPosition - startPosition;
+            }
+
+            return bytesRead;
+        }
+
+        private void readTimeRange(String key, DataOutputBuffer bufOut, String columnFamilyName, IndexHelper.TimeRange timeRange)
+                throws IOException
+        {
+            int dataSize = file_.readInt();
+
+            /* write the key into buffer */
+            bufOut.writeUTF(key);
+
+            int bytesSkipped = IndexHelper.skipBloomFilter(file_);
+            /*
+             * read the correct number of bytes for the column family and
+             * write data into buffer. Substract from dataSize the bloom
+             * filter size.
+            */
+            dataSize -= bytesSkipped;
+            List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
+            /* Read the times indexes if present */
+            int totalBytesRead = handleColumnTimeIndexes(columnFamilyName, columnIndexList);
+            dataSize -= totalBytesRead;
+
+            /* read the column family name */
+            String cfName = file_.readUTF();
+            dataSize -= (utfPrefix_ + cfName.length());
+            
+            /* read local deletion time */
+            int localDeletionTime = file_.readInt();
+            dataSize -=4;
+
+            /* read if this cf is marked for delete */
+            long markedForDeleteAt = file_.readLong();
+            dataSize -= 8;
+
+            /* read the total number of columns */
+            int totalNumCols = file_.readInt();
+            dataSize -= 4;
+
+            /* get the column range we have to read */
+            IndexHelper.ColumnRange columnRange = IndexHelper.getColumnRangeFromTimeIndex(timeRange, columnIndexList, dataSize, totalNumCols);
+
+            Coordinate coordinate = columnRange.coordinate();
+            /* seek to the correct offset to the data, and calculate the data size */
+            file_.skipBytes((int) coordinate.start_);
+            dataSize = (int) (coordinate.end_ - coordinate.start_);
+
+            /*
+             * write the number of columns in the column family we are returning:
+             *  dataSize that we are reading +
+             *  length of column family name +
+             *  one booleanfor deleted or not +
+             *  one int for number of columns
+            */
+            bufOut.writeInt(dataSize + utfPrefix_ + cfName.length() + 4 + 8 + 4);
+            /* write the column family name */
+            bufOut.writeUTF(cfName);
+            /* write local deletion time */
+            bufOut.writeInt(localDeletionTime);
+            /* write if this cf is marked for delete */
+            bufOut.writeLong(markedForDeleteAt);
+            /* write number of columns */
+            bufOut.writeInt(columnRange.count());
+            /* now write the columns */
+            bufOut.write(file_, dataSize);
+        }
+
+        private void readColumns(String key, DataOutputBuffer bufOut, String columnFamilyName, List<String> cNames)
+                throws IOException
+        {
+            int dataSize = file_.readInt();
+
+            /* write the key into buffer */
+            bufOut.writeUTF(key);
+
+            /* if we need to read the all the columns do not read the column indexes */
+            if (cNames == null || cNames.size() == 0)
+            {
+                int bytesSkipped = IndexHelper.skipBloomFilterAndIndex(file_);
+                /*
+                       * read the correct number of bytes for the column family and
+                       * write data into buffer
+                      */
+                dataSize -= bytesSkipped;
+                /* write the data size */
+                bufOut.writeInt(dataSize);
+                /* write the data into buffer, except the boolean we have read */
+                bufOut.write(file_, dataSize);
+            }
+            else
+            {
+                /* Read the bloom filter summarizing the columns */
+                long preBfPos = file_.getFilePointer();
+                BloomFilter bf = defreezeBloomFilter();
+                long postBfPos = file_.getFilePointer();
+                dataSize -= (postBfPos - preBfPos);
+
+                List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
+                /* read the column name indexes if present */
+                int totalBytesRead = handleColumnNameIndexes(columnFamilyName, columnIndexList);
+                dataSize -= totalBytesRead;
+
+                /* read the column family name */
+                String cfName = file_.readUTF();
+                dataSize -= (utfPrefix_ + cfName.length());
+
+                /* read local deletion time */
+                int localDeletionTime = file_.readInt();
+                dataSize -=4;
+
+                /* read if this cf is marked for delete */
+                long markedForDeleteAt = file_.readLong();
+                dataSize -= 8;
+
+                /* read the total number of columns */
+                int totalNumCols = file_.readInt();
+                dataSize -= 4;
+
+                // TODO: this is name sorted - but eventually this should be sorted by the same criteria as the col index
+                /* sort the required list of columns */
+                cNames = new ArrayList<String>(cNames);
+                Collections.sort(cNames);
+                /* get the various column ranges we have to read */
+                List<IndexHelper.ColumnRange> columnRanges = IndexHelper.getMultiColumnRangesFromNameIndex(cNames, columnIndexList, dataSize, totalNumCols);
+
+                /* calculate the data size */
+                int numColsReturned = 0;
+                int dataSizeReturned = 0;
+                for (IndexHelper.ColumnRange columnRange : columnRanges)
+                {
+                    numColsReturned += columnRange.count();
+                    Coordinate coordinate = columnRange.coordinate();
+                    dataSizeReturned += coordinate.end_ - coordinate.start_;
+                }
+
+                /*
+                 * write the number of columns in the column family we are returning:
+                 * 	dataSize that we are reading +
+                 * 	length of column family name +
+                 * 	one booleanfor deleted or not +
+                 * 	one int for number of columns
+                */
+                bufOut.writeInt(dataSizeReturned + utfPrefix_ + cfName.length() + 4 + 8 + 4);
+                /* write the column family name */
+                bufOut.writeUTF(cfName);
+                /* write local deletion time */
+                bufOut.writeInt(localDeletionTime);
+                /* write if this cf is marked for delete */
+                bufOut.writeLong(markedForDeleteAt);
+                /* write number of columns */
+                bufOut.writeInt(numColsReturned);
+                int prevPosition = 0;
+                /* now write all the columns we are required to write */
+                for (IndexHelper.ColumnRange columnRange : columnRanges)
+                {
+                    /* seek to the correct offset to the data */
+                    Coordinate coordinate = columnRange.coordinate();
+                    file_.skipBytes((int) (coordinate.start_ - prevPosition));
+                    bufOut.write(file_, (int) (coordinate.end_ - coordinate.start_));
+                    prevPosition = (int) coordinate.end_;
+                }
+            }
+        }
+
+        /**
+         * This method dumps the next key/value into the DataOuputStream
+         * passed in.
+         *
+         * @param bufOut DataOutputStream that needs to be filled.
+         * @return total number of bytes read/considered
+         */
+        public long next(DataOutputBuffer bufOut) throws IOException
+        {
+            long bytesRead = -1L;
+            if (isEOF())
+                return bytesRead;
+
+            long startPosition = file_.getFilePointer();
+            String key = file_.readUTF();
+            if (key != null)
+            {
+                /* write the key into buffer */
+                bufOut.writeUTF(key);
+                int dataSize = file_.readInt();
+                /* write data size into buffer */
+                bufOut.writeInt(dataSize);
+                /* write the data into buffer */
+                bufOut.write(file_, dataSize);
+                long endPosition = file_.getFilePointer();
+                bytesRead = endPosition - startPosition;
+            }
+
+            /*
+             * If we have read the bloom filter in the data
+             * file we know we are at the end of the file 
+             * and no further key processing is required. So
+             * we return -1 indicating we are at the end of
+             * the file. 
+            */
+            if (key.equals(SequenceFile.marker_))
+                bytesRead = -1L;
+            return bytesRead;
+        }
+
+        /**
+         * This method dumps the next key/value into the DataOuputStream
+         * passed in.
+         *
+         * @param key     - key we are interested in.
+         * @param bufOut  DataOutputStream that needs to be filled.
+         * @param section region of the file that needs to be read
+         * @return total number of bytes read/considered
+         */
+        public long next(String key, DataOutputBuffer bufOut, Coordinate section) throws IOException
+        {
+            long bytesRead = -1L;
+            if (isEOF())
+                return bytesRead;
+
+            seekTo(key, section);
+            /* note the position where the key starts */
+            long startPosition = file_.getFilePointer();
+            String keyInDisk = file_.readUTF();
+            if (keyInDisk != null)
+            {
+                /*
+                 * If key on disk is greater than requested key
+                 * we can bail out since we exploit the property
+                 * of the SSTable format.
+                */
+                if (keyInDisk.compareTo(key) > 0)
+                    return bytesRead;
+
+                /*
+                 * If we found the key then we populate the buffer that
+                 * is passed in. If not then we skip over this key and
+                 * position ourselves to read the next one.
+                */
+                int dataSize = file_.readInt();
+                if (keyInDisk.equals(key))
+                {
+                    /* write the key into buffer */
+                    bufOut.writeUTF(keyInDisk);
+                    /* write data size into buffer */
+                    bufOut.writeInt(dataSize);
+                    /* write the data into buffer */
+                    bufOut.write(file_, dataSize);
+                }
+                else
+                {
+                    /* skip over data portion */
+                    file_.seek(dataSize + file_.getFilePointer());
+                }
+
+                long endPosition = file_.getFilePointer();
+                bytesRead = endPosition - startPosition;
+            }
+
+            return bytesRead;
+        }
+    }
+
+    public static class Reader extends AbstractReader
+    {
+        Reader(String filename) throws IOException
+        {
+            super(filename);
+            init(filename);
+        }
+
+        protected void init(String filename) throws IOException
+        {
+            file_ = new RandomAccessFile(filename, "r");
+        }
+
+        public long getEOF() throws IOException
+        {
+            return file_.length();
+        }
+
+        public long getCurrentPosition() throws IOException
+        {
+            return file_.getFilePointer();
+        }
+
+        public boolean isHealthyFileDescriptor() throws IOException
+        {
+            return file_.getFD().valid();
+        }
+
+        public void seek(long position) throws IOException
+        {
+            file_.seek(position);
+        }
+
+        public boolean isEOF() throws IOException
+        {
+            return (getCurrentPosition() == getEOF());
+        }
+
+        /**
+         * Be extremely careful while using this API. This currently
+         * used to read the commit log header from the commit logs.
+         * Treat this as an internal API.
+         *
+         * @param bytes read from the buffer into the this array
+         */
+        public void readDirect(byte[] bytes) throws IOException
+        {
+            file_.readFully(bytes);
+        }
+
+        public long readLong() throws IOException
+        {
+            return file_.readLong();
+        }
+
+        public void close() throws IOException
+        {
+            file_.close();
+        }
+    }
+
+    public static class BufferReader extends Reader
+    {
+        private int size_;
+
+        BufferReader(String filename, int size) throws IOException
+        {
+            super(filename);
+            size_ = size;
+        }
+
+        protected void init(String filename) throws IOException
+        {
+            file_ = new BufferedRandomAccessFile(filename, "r", size_);
+        }
+    }
+
+    public static class ChecksumReader extends Reader
+    {
+        private int size_;
+
+        ChecksumReader(String filename, int size) throws IOException
+        {
+            super(filename);
+            size_ = size;
+        }
+
+        protected void init(String filename) throws IOException
+        {
+            file_ = new ChecksumRandomAccessFile(filename, "r", size_);
+        }
+    }
+
+    private static Logger logger_ = Logger.getLogger(SequenceFile.class);
+    public static final short utfPrefix_ = 2;
+    public static final String marker_ = "Bloom-Filter";
+
+    public static IFileWriter writer(String filename) throws IOException
+    {
+        return new Writer(filename);
+    }
+
+    public static IFileWriter bufferedWriter(String filename, int size) throws IOException
+    {
+        return new BufferWriter(filename, size);
+    }
+
+    public static IFileWriter fastWriter(String filename, int size) throws IOException
+    {
+        return new FastConcurrentWriter(filename, size);
+    }
+
+    public static IFileReader reader(String filename) throws IOException
+    {
+        return new Reader(filename);
+    }
+
+    public static IFileReader bufferedReader(String filename, int size) throws IOException
+    {
+        return new BufferReader(filename, size);
+    }
+
+    /**
+     * Efficiently writes a UTF8 string to the buffer.
+     * Assuming all Strings that are passed in have length
+     * that can be represented as a short i.e length of the
+     * string is <= 65535
+     *
+     * @param buffer buffer to write the serialize version into
+     * @param str    string to serialize
+     */
+    protected static void writeUTF(ByteBuffer buffer, String str)
+    {
+        int strlen = str.length();
+        int utflen = 0;
+        int c, count = 0;
+
+        /* use charAt instead of copying String to char array */
+        for (int i = 0; i < strlen; i++)
+        {
+            c = str.charAt(i);
+            if ((c >= 0x0001) && (c <= 0x007F))
+            {
+                utflen++;
+            }
+            else if (c > 0x07FF)
+            {
+                utflen += 3;
+            }
+            else
+            {
+                utflen += 2;
+            }
+        }
+
+        byte[] bytearr = new byte[utflen + 2];
+        bytearr[count++] = (byte) ((utflen >>> 8) & 0xFF);
+        bytearr[count++] = (byte) ((utflen >>> 0) & 0xFF);
+
+        int i = 0;
+        for (i = 0; i < strlen; i++)
+        {
+            c = str.charAt(i);
+            if (!((c >= 0x0001) && (c <= 0x007F)))
+                break;
+            bytearr[count++] = (byte) c;
+        }
+
+        for (; i < strlen; i++)
+        {
+            c = str.charAt(i);
+            if ((c >= 0x0001) && (c <= 0x007F))
+            {
+                bytearr[count++] = (byte) c;
+
+            }
+            else if (c > 0x07FF)
+            {
+                bytearr[count++] = (byte) (0xE0 | ((c >> 12) & 0x0F));
+                bytearr[count++] = (byte) (0x80 | ((c >> 6) & 0x3F));
+                bytearr[count++] = (byte) (0x80 | ((c >> 0) & 0x3F));
+            }
+            else
+            {
+                bytearr[count++] = (byte) (0xC0 | ((c >> 6) & 0x1F));
+                bytearr[count++] = (byte) (0x80 | ((c >> 0) & 0x3F));
+            }
+        }
+        buffer.put(bytearr, 0, utflen + 2);
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ColumnFamilyType.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ColumnFamilyType.java
index e69de29b..d4456891 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ColumnFamilyType.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ColumnFamilyType.java
@@ -0,0 +1,188 @@
+//
+// This file was generated by the JavaTM Architecture for XML Binding(JAXB) Reference Implementation, vJAXB 2.1.3 in JDK 1.6 
+// See <a href="http://java.sun.com/xml/jaxb">http://java.sun.com/xml/jaxb</a> 
+// Any modifications to this file will be lost upon recompilation of the source schema. 
+// Generated on: 2007.10.19 at 01:36:41 PM PDT 
+//
+
+
+package org.apache.cassandra.loader;
+
+import java.util.ArrayList;
+import java.util.List;
+import javax.xml.bind.annotation.XmlAccessType;
+import javax.xml.bind.annotation.XmlAccessorType;
+import javax.xml.bind.annotation.XmlAttribute;
+import javax.xml.bind.annotation.XmlElement;
+import javax.xml.bind.annotation.XmlType;
+
+
+/**
+ * <p>Java class for ColumnFamilyType complex type.
+ * 
+ * <p>The following schema fragment specifies the expected content contained within this class.
+ * 
+ * <pre>
+ * &lt;complexType name="ColumnFamilyType">
+ *   &lt;complexContent>
+ *     &lt;restriction base="{http://www.w3.org/2001/XMLSchema}anyType">
+ *       &lt;sequence>
+ *         &lt;element name="Column" type="{}ColumnType" maxOccurs="unbounded"/>
+ *         &lt;element name="SuperColumn" type="{}SuperColumnType"/>
+ *         &lt;element name="Directory" type="{http://www.w3.org/2001/XMLSchema}string"/>
+ *         &lt;element name="Delimiter" type="{http://www.w3.org/2001/XMLSchema}string"/>
+ *       &lt;/sequence>
+ *       &lt;attribute name="Name" type="{http://www.w3.org/2001/XMLSchema}string" />
+ *     &lt;/restriction>
+ *   &lt;/complexContent>
+ * &lt;/complexType>
+ * </pre>
+ * 
+ * 
+ */
+@XmlAccessorType(XmlAccessType.FIELD)
+@XmlType(name = "ColumnFamilyType", propOrder = {
+    "column",
+    "superColumn",
+    "directory",
+    "delimiter"
+})
+public class ColumnFamilyType {
+
+    @XmlElement(name = "Column", required = true, nillable = true)
+    protected List<ColumnType> column;
+    @XmlElement(name = "SuperColumn", required = true, nillable = true)
+    protected SuperColumnType superColumn;
+    @XmlElement(name = "Directory", required = true)
+    protected String directory;
+    @XmlElement(name = "Delimiter", required = true)
+    protected String delimiter;
+    @XmlAttribute(name = "Name")
+    protected String name;
+
+    /**
+     * Gets the value of the column property.
+     * 
+     * <p>
+     * This accessor method returns a reference to the live list,
+     * not a snapshot. Therefore any modification you make to the
+     * returned list will be present inside the JAXB object.
+     * This is why there is not a <CODE>set</CODE> method for the column property.
+     * 
+     * <p>
+     * For example, to add a new item, do as follows:
+     * <pre>
+     *    getColumn().add(newItem);
+     * </pre>
+     * 
+     * 
+     * <p>
+     * Objects of the following type(s) are allowed in the list
+     * {@link ColumnType }
+     * 
+     * 
+     */
+    public List<ColumnType> getColumn() {
+        if (column == null) {
+            column = new ArrayList<ColumnType>();
+        }
+        return this.column;
+    }
+
+    /**
+     * Gets the value of the superColumn property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link SuperColumnType }
+     *     
+     */
+    public SuperColumnType getSuperColumn() {
+        return superColumn;
+    }
+
+    /**
+     * Sets the value of the superColumn property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link SuperColumnType }
+     *     
+     */
+    public void setSuperColumn(SuperColumnType value) {
+        this.superColumn = value;
+    }
+
+    /**
+     * Gets the value of the directory property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link String }
+     *     
+     */
+    public String getDirectory() {
+        return directory;
+    }
+
+    /**
+     * Sets the value of the directory property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link String }
+     *     
+     */
+    public void setDirectory(String value) {
+        this.directory = value;
+    }
+
+    /**
+     * Gets the value of the delimiter property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link String }
+     *     
+     */
+    public String getDelimiter() {
+        return delimiter;
+    }
+
+    /**
+     * Sets the value of the delimiter property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link String }
+     *     
+     */
+    public void setDelimiter(String value) {
+        this.delimiter = value;
+    }
+
+    /**
+     * Gets the value of the name property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link String }
+     *     
+     */
+    public String getName() {
+        return name;
+    }
+
+    /**
+     * Sets the value of the name property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link String }
+     *     
+     */
+    public void setName(String value) {
+        this.name = value;
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ColumnType.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ColumnType.java
index e69de29b..6faed739 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ColumnType.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ColumnType.java
@@ -0,0 +1,152 @@
+//
+// This file was generated by the JavaTM Architecture for XML Binding(JAXB) Reference Implementation, vJAXB 2.1.3 in JDK 1.6 
+// See <a href="http://java.sun.com/xml/jaxb">http://java.sun.com/xml/jaxb</a> 
+// Any modifications to this file will be lost upon recompilation of the source schema. 
+// Generated on: 2007.10.19 at 01:36:41 PM PDT 
+//
+
+
+package org.apache.cassandra.loader;
+
+import javax.xml.bind.annotation.XmlAccessType;
+import javax.xml.bind.annotation.XmlAccessorType;
+import javax.xml.bind.annotation.XmlAttribute;
+import javax.xml.bind.annotation.XmlElement;
+import javax.xml.bind.annotation.XmlType;
+
+
+/**
+ * <p>Java class for ColumnType complex type.
+ * 
+ * <p>The following schema fragment specifies the expected content contained within this class.
+ * 
+ * <pre>
+ * &lt;complexType name="ColumnType">
+ *   &lt;complexContent>
+ *     &lt;restriction base="{http://www.w3.org/2001/XMLSchema}anyType">
+ *       &lt;sequence>
+ *         &lt;element name="Value" type="{}ValueType"/>
+ *         &lt;element name="Timestamp" type="{}TimestampType"/>
+ *       &lt;/sequence>
+ *       &lt;attribute name="Name" type="{http://www.w3.org/2001/XMLSchema}string" />
+ *       &lt;attribute name="Field" type="{http://www.w3.org/2001/XMLSchema}int" />
+ *     &lt;/restriction>
+ *   &lt;/complexContent>
+ * &lt;/complexType>
+ * </pre>
+ * 
+ * 
+ */
+@XmlAccessorType(XmlAccessType.FIELD)
+@XmlType(name = "ColumnType", propOrder = {
+    "value",
+    "timestamp"
+})
+public class ColumnType {
+
+    @XmlElement(name = "Value", required = true)
+    protected ValueType value;
+    @XmlElement(name = "Timestamp", required = true)
+    protected TimestampType timestamp;
+    @XmlAttribute(name = "Name")
+    protected String name;
+    @XmlAttribute(name = "Field")
+    protected Integer field;
+
+    /**
+     * Gets the value of the value property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link ValueType }
+     *     
+     */
+    public ValueType getValue() {
+        return value;
+    }
+
+    /**
+     * Sets the value of the value property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link ValueType }
+     *     
+     */
+    public void setValue(ValueType value) {
+        this.value = value;
+    }
+
+    /**
+     * Gets the value of the timestamp property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link TimestampType }
+     *     
+     */
+    public TimestampType getTimestamp() {
+        return timestamp;
+    }
+
+    /**
+     * Sets the value of the timestamp property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link TimestampType }
+     *     
+     */
+    public void setTimestamp(TimestampType value) {
+        this.timestamp = value;
+    }
+
+    /**
+     * Gets the value of the name property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link String }
+     *     
+     */
+    public String getName() {
+        return name;
+    }
+
+    /**
+     * Sets the value of the name property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link String }
+     *     
+     */
+    public void setName(String value) {
+        this.name = value;
+    }
+
+    /**
+     * Gets the value of the field property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link Integer }
+     *     
+     */
+    public Integer getField() {
+        return field;
+    }
+
+    /**
+     * Sets the value of the field property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link Integer }
+     *     
+     */
+    public void setField(Integer value) {
+        this.field = value;
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/CustomLoader.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/CustomLoader.java
index e69de29b..9afa8523 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/CustomLoader.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/CustomLoader.java
@@ -0,0 +1,206 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.loader;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.StringTokenizer;
+
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+
+
+public class CustomLoader
+{
+    private static Logger logger_ = Logger.getLogger( CustomLoader.class );
+    private StorageService storageService_;
+    private String path_;
+    
+    public CustomLoader(StorageService storageService, String path)
+    {
+        storageService_ = storageService;
+        path_ = path;
+    }
+    /*
+     * This function checks if the local storage endpoint 
+     * is reponsible for storing this key .
+     */
+    boolean checkIfProcessKey(String key)
+    {
+		EndPoint[] endPoints = storageService_.getNStorageEndPoint(key);
+    	EndPoint localEndPoint = StorageService.getLocalStorageEndPoint();
+    	for(EndPoint endPoint : endPoints)
+    	{
+    		if(endPoint.equals(localEndPoint))
+    			return true;
+    	}
+    	return false;
+    }
+    
+    boolean checkUser(String user, String[] list)
+    {
+    	boolean bFound = false;
+    	for(String l:list)
+    	{
+    		if(user.equals(l))
+    		{
+    			bFound = true;
+    		}
+    	}
+    	return bFound;
+    }
+
+
+     void parse(String filepath) throws Throwable
+     {
+    	 try
+    	 {
+	         BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+	                 new FileInputStream(filepath)), 16 * 1024 * 1024);
+	         String line = null;
+	         RowMutation rm = null;
+	         while ((line = bufReader.readLine()) != null)
+	         {
+	 	      	// userid  threadid  folder  date  part-list  author-list  subject  body
+	 	      	String columns[] = line.split("\t");
+	 	      	if(columns.length < 7)
+	 	      		continue;
+	            if( rm == null)
+	            {
+	            	rm = new RowMutation("Mailbox", columns[0]);
+	            }
+	       	    Analyzer analyzer = new StandardAnalyzer();
+	       	    String body = null;
+	       	    if(columns.length > 7 )
+	       	    	body = columns[6]+" "+columns[7];
+	       	    else
+	       	    	body = columns[6];
+	       	    
+	     	    TokenStream ts = analyzer.tokenStream("superColumn", new StringReader(body));
+	     	    Token token = null;
+	     	    token = ts.next();
+	     	    while(token != null)
+	     	    {
+	     	    	if(token.termText() != "")
+	     	    	{
+	     	    		rm.add("MailboxThreadList0:"+token.termText()+":"+columns[1], columns[2].getBytes(), Integer.parseInt(columns[3]) );
+	     	    	}
+	         	    token = ts.next();
+	     	    }
+	     	    rm.add("MailboxMailList0:"+columns[1], columns[2].getBytes(), Integer.parseInt(columns[3]));
+	    		String authors = columns[5];
+	    		String participants = columns[4];
+				if( authors == null)
+					authors = "";
+				if(participants == null)
+					participants = "";
+				String[] authorList = authors.split(":");
+				String[] partList = participants.split(":");
+	            String[] mailersList = null;
+				if(checkUser(columns[0], authorList))
+					mailersList = partList;
+				else
+					mailersList = authorList;
+				for(String mailer : mailersList)
+				{
+					if(!mailer.equals(columns[0]))
+					{
+						rm.add("MailboxUserList0:"+ mailer + ":" +columns[1], columns[2].getBytes(), Integer.parseInt(columns[3]) );
+					}
+				}
+	         }
+	         if(rm != null)
+	         {
+	        	 rm.apply();
+	         }
+		}
+		catch ( Throwable ex ) 
+		{
+          logger_.error( LogUtil.throwableToString(ex) );
+		}
+     }
+
+     void parseFileList(File dir) 
+     {
+ 		int fileCount = dir.list().length;
+ 		String[] dirList = dir.list();
+ 		File[] fileList = dir.listFiles();
+ 		for ( int i = 0 ; i < fileCount ; i++ ) 
+ 		{
+ 			File file = new File(fileList[i].getAbsolutePath());
+ 			if ( file.isDirectory())
+ 			{
+ 				parseFileList(file);
+ 			}
+ 			else 
+ 			{
+ 				try
+ 				{
+					if(checkIfProcessKey(dirList[i]))
+					{
+						parse(fileList[i].getAbsolutePath());
+					}
+ 				}
+ 				catch ( Throwable ex ) 
+ 				{
+ 	                logger_.error( LogUtil.throwableToString(ex) );
+ 				}
+ 			}
+ 		}
+     }
+     
+     
+     
+	
+	/**
+	 * @param args
+	 */
+	public static void main(String[] args) throws Throwable
+	{
+		if(args.length != 1)
+		{
+			System.out.println("Usage: CustomLoader <root path to the data files>");
+		}
+		LogUtil.init();
+        StorageService s = StorageService.instance();
+        s.start();
+        CustomLoader loader = new CustomLoader(s, args[0]);
+        File rootDirectory = new File(args[0]);
+        long start = System.currentTimeMillis();
+        loader.parseFileList(rootDirectory);
+        logger_.info("Done Loading: " + (System.currentTimeMillis() - start)
+                + " ms.");
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/FieldCollection.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/FieldCollection.java
index e69de29b..662f37c0 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/FieldCollection.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/FieldCollection.java
@@ -0,0 +1,76 @@
+//
+// This file was generated by the JavaTM Architecture for XML Binding(JAXB) Reference Implementation, vJAXB 2.1.3 in JDK 1.6 
+// See <a href="http://java.sun.com/xml/jaxb">http://java.sun.com/xml/jaxb</a> 
+// Any modifications to this file will be lost upon recompilation of the source schema. 
+// Generated on: 2007.10.19 at 01:36:41 PM PDT 
+//
+
+
+package org.apache.cassandra.loader;
+
+import java.util.ArrayList;
+import java.util.List;
+import javax.xml.bind.annotation.XmlAccessType;
+import javax.xml.bind.annotation.XmlAccessorType;
+import javax.xml.bind.annotation.XmlElement;
+import javax.xml.bind.annotation.XmlType;
+
+
+/**
+ * <p>Java class for FieldCollection complex type.
+ * 
+ * <p>The following schema fragment specifies the expected content contained within this class.
+ * 
+ * <pre>
+ * &lt;complexType name="FieldCollection">
+ *   &lt;complexContent>
+ *     &lt;restriction base="{http://www.w3.org/2001/XMLSchema}anyType">
+ *       &lt;sequence>
+ *         &lt;element name="Field" type="{http://www.w3.org/2001/XMLSchema}int" maxOccurs="unbounded"/>
+ *       &lt;/sequence>
+ *     &lt;/restriction>
+ *   &lt;/complexContent>
+ * &lt;/complexType>
+ * </pre>
+ * 
+ * 
+ */
+@XmlAccessorType(XmlAccessType.FIELD)
+@XmlType(name = "FieldCollection", propOrder = {
+    "field"
+})
+public class FieldCollection {
+
+    @XmlElement(name = "Field", type = Integer.class)
+    protected List<Integer> field;
+
+    /**
+     * Gets the value of the field property.
+     * 
+     * <p>
+     * This accessor method returns a reference to the live list,
+     * not a snapshot. Therefore any modification you make to the
+     * returned list will be present inside the JAXB object.
+     * This is why there is not a <CODE>set</CODE> method for the field property.
+     * 
+     * <p>
+     * For example, to add a new item, do as follows:
+     * <pre>
+     *    getField().add(newItem);
+     * </pre>
+     * 
+     * 
+     * <p>
+     * Objects of the following type(s) are allowed in the list
+     * {@link Integer }
+     * 
+     * 
+     */
+    public List<Integer> getField() {
+        if (field == null) {
+            field = new ArrayList<Integer>();
+        }
+        return this.field;
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/Importer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/Importer.java
index e69de29b..643312f1 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/Importer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/Importer.java
@@ -0,0 +1,127 @@
+//
+// This file was generated by the JavaTM Architecture for XML Binding(JAXB) Reference Implementation, vJAXB 2.1.3 in JDK 1.6 
+// See <a href="http://java.sun.com/xml/jaxb">http://java.sun.com/xml/jaxb</a> 
+// Any modifications to this file will be lost upon recompilation of the source schema. 
+// Generated on: 2007.10.19 at 01:36:41 PM PDT 
+//
+
+
+package org.apache.cassandra.loader;
+
+import javax.xml.bind.annotation.XmlAccessType;
+import javax.xml.bind.annotation.XmlAccessorType;
+import javax.xml.bind.annotation.XmlElement;
+import javax.xml.bind.annotation.XmlRootElement;
+import javax.xml.bind.annotation.XmlType;
+
+
+/**
+ * <p>Java class for anonymous complex type.
+ * 
+ * <p>The following schema fragment specifies the expected content contained within this class.
+ * 
+ * <pre>
+ * &lt;complexType>
+ *   &lt;complexContent>
+ *     &lt;restriction base="{http://www.w3.org/2001/XMLSchema}anyType">
+ *       &lt;sequence>
+ *         &lt;element name="Table" type="{http://www.w3.org/2001/XMLSchema}string"/>
+ *         &lt;element name="Key" type="{}KeyType"/>
+ *         &lt;element name="ColumnFamily" type="{}ColumnFamilyType"/>
+ *       &lt;/sequence>
+ *     &lt;/restriction>
+ *   &lt;/complexContent>
+ * &lt;/complexType>
+ * </pre>
+ * 
+ * 
+ */
+@XmlAccessorType(XmlAccessType.FIELD)
+@XmlType(name = "", propOrder = {
+    "table",
+    "key",
+    "columnFamily"
+})
+@XmlRootElement(name = "Importer")
+public class Importer {
+
+    @XmlElement(name = "Table", required = true)
+    protected String table;
+    @XmlElement(name = "Key", required = true)
+    protected KeyType key;
+    @XmlElement(name = "ColumnFamily", required = true)
+    protected ColumnFamilyType columnFamily;
+
+    /**
+     * Gets the value of the table property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link String }
+     *     
+     */
+    public String getTable() {
+        return table;
+    }
+
+    /**
+     * Sets the value of the table property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link String }
+     *     
+     */
+    public void setTable(String value) {
+        this.table = value;
+    }
+
+    /**
+     * Gets the value of the key property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link KeyType }
+     *     
+     */
+    public KeyType getKey() {
+        return key;
+    }
+
+    /**
+     * Sets the value of the key property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link KeyType }
+     *     
+     */
+    public void setKey(KeyType value) {
+        this.key = value;
+    }
+
+    /**
+     * Gets the value of the columnFamily property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link ColumnFamilyType }
+     *     
+     */
+    public ColumnFamilyType getColumnFamily() {
+        return columnFamily;
+    }
+
+    /**
+     * Sets the value of the columnFamily property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link ColumnFamilyType }
+     *     
+     */
+    public void setColumnFamily(ColumnFamilyType value) {
+        this.columnFamily = value;
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/KeyType.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/KeyType.java
index e69de29b..cea5a6c5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/KeyType.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/KeyType.java
@@ -0,0 +1,125 @@
+//
+// This file was generated by the JavaTM Architecture for XML Binding(JAXB) Reference Implementation, vJAXB 2.1.3 in JDK 1.6 
+// See <a href="http://java.sun.com/xml/jaxb">http://java.sun.com/xml/jaxb</a> 
+// Any modifications to this file will be lost upon recompilation of the source schema. 
+// Generated on: 2007.10.19 at 01:36:41 PM PDT 
+//
+
+
+package org.apache.cassandra.loader;
+
+import javax.xml.bind.annotation.XmlAccessType;
+import javax.xml.bind.annotation.XmlAccessorType;
+import javax.xml.bind.annotation.XmlElement;
+import javax.xml.bind.annotation.XmlType;
+
+
+/**
+ * <p>Java class for KeyType complex type.
+ * 
+ * <p>The following schema fragment specifies the expected content contained within this class.
+ * 
+ * <pre>
+ * &lt;complexType name="KeyType">
+ *   &lt;complexContent>
+ *     &lt;restriction base="{http://www.w3.org/2001/XMLSchema}anyType">
+ *       &lt;sequence>
+ *         &lt;element name="OptimizeIt" type="{http://www.w3.org/2001/XMLSchema}boolean"/>
+ *         &lt;element name="Combiner" type="{http://www.w3.org/2001/XMLSchema}string"/>
+ *         &lt;element name="Fields" type="{}FieldCollection"/>
+ *       &lt;/sequence>
+ *     &lt;/restriction>
+ *   &lt;/complexContent>
+ * &lt;/complexType>
+ * </pre>
+ * 
+ * 
+ */
+@XmlAccessorType(XmlAccessType.FIELD)
+@XmlType(name = "KeyType", propOrder = {
+    "optimizeIt",
+    "combiner",
+    "fields"
+})
+public class KeyType {
+
+    @XmlElement(name = "OptimizeIt", required = true, type = Boolean.class, nillable = true)
+    protected Boolean optimizeIt;
+    @XmlElement(name = "Combiner", required = true)
+    protected String combiner;
+    @XmlElement(name = "Fields", required = true)
+    protected FieldCollection fields;
+
+    /**
+     * Gets the value of the optimizeIt property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link Boolean }
+     *     
+     */
+    public Boolean isOptimizeIt() {
+        return optimizeIt;
+    }
+
+    /**
+     * Sets the value of the optimizeIt property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link Boolean }
+     *     
+     */
+    public void setOptimizeIt(Boolean value) {
+        this.optimizeIt = value;
+    }
+
+    /**
+     * Gets the value of the combiner property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link String }
+     *     
+     */
+    public String getCombiner() {
+        return combiner;
+    }
+
+    /**
+     * Sets the value of the combiner property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link String }
+     *     
+     */
+    public void setCombiner(String value) {
+        this.combiner = value;
+    }
+
+    /**
+     * Gets the value of the fields property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link FieldCollection }
+     *     
+     */
+    public FieldCollection getFields() {
+        return fields;
+    }
+
+    /**
+     * Sets the value of the fields property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link FieldCollection }
+     *     
+     */
+    public void setFields(FieldCollection value) {
+        this.fields = value;
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/Loader.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/Loader.java
index e69de29b..b109a29f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/Loader.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/Loader.java
@@ -0,0 +1,353 @@
+/**
+ * 
+ */
+package org.apache.cassandra.loader;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.StringTokenizer;
+import javax.xml.bind.JAXBContext;
+import javax.xml.bind.Unmarshaller;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.locator.EndPointSnitch;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.cassandra.utils.*;
+
+
+/**
+ * This class is used to load the storage endpoints with the relevant data
+ * The data should be both what they are responsible for and what should be replicated on the specific
+ * endpoints.
+ * Population is done based on a xml file which should adhere to a schema.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class Loader
+{
+	private static long siesta_ = 60*1000;
+    private static Logger logger_ = Logger.getLogger( Loader.class );
+	private Importer importer_;
+    private StorageService storageService_;
+    
+    public Loader(StorageService storageService)
+    {
+        storageService_ = storageService;
+    }
+    
+    /*
+     * This method loads all the keys into a special column family 
+     * called "RecycleBin". This column family is used for temporary
+     * processing of data and then can be recycled. The idea is that 
+     * after the load is complete we have all the keys in the system.
+     * Now we force a compaction and examine the single Index file 
+     * that is generated to determine how the nodes need to relocate
+     * to be perfectly load balanced.
+     * 
+     *  param @ rootDirectory - rootDirectory at which the parsing begins.
+     *  param @ table - table that will be populated.
+     *  param @ cfName - name of the column that will be populated. This is 
+     *  passed in so that we do not unncessary allocate temporary String objects.
+    */
+    private void preParse(File rootDirectory, String table, String cfName) throws Throwable
+    {        
+        File[] files = rootDirectory.listFiles();
+        
+        for ( File file : files )
+        {
+            if ( file.isDirectory() )
+                preParse(file, table, cfName);
+            else
+            {
+                String fileName = file.getName();
+                RowMutation rm = new RowMutation(table, fileName);
+                rm.add(cfName, fileName.getBytes(), 0);
+                rm.apply();
+            }
+        }
+    }
+    
+    /*
+     * Merges a list of strings with a particular combiner.
+     */
+    String merge( List<String> listFields,  String combiner)
+    {
+    	if(listFields.size() == 0 )
+    		return null;
+    	if(listFields.size() == 1)
+    		return listFields.get(0);
+    	String mergedKey = null;
+    	for(String field: listFields)
+    	{
+    		if(mergedKey == null)
+    		{
+    			mergedKey = field;
+    		}
+    		else
+    		{
+    			mergedKey = mergedKey + combiner + field;
+    		}
+    	}
+    	return mergedKey;
+    	
+    }
+    
+    /*
+     * This function checks if the local storage endpoint 
+     * is reponsible for storing this key .
+     */
+    boolean checkIfProcessKey(String key)
+    {
+		EndPoint[] endPoints = storageService_.getNStorageEndPoint(key);
+    	EndPoint localEndPoint = StorageService.getLocalStorageEndPoint();
+    	for(EndPoint endPoint : endPoints)
+    	{
+    		if(endPoint.equals(localEndPoint))
+    			return true;
+    	}
+    	return false;
+    }
+    
+   /*
+    * This functions parses each file based on delimiters specified in the 
+    * xml file. It also looks at all the parameters specified in teh xml and based
+    * on that populates the internal Row structure.
+    */ 
+    void parse(String filepath) throws Throwable
+    {
+        BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+                new FileInputStream(filepath)), 16 * 1024 * 1024);
+        String line = null;
+        String delimiter_ = new String(",");
+        RowMutation rm = null;
+        Map<String, RowMutation> rms = new HashMap<String, RowMutation>();
+        if(importer_.columnFamily.delimiter != null)
+        {
+        	delimiter_ = importer_.columnFamily.delimiter;
+        }
+        while ((line = bufReader.readLine()) != null)
+        {
+            StringTokenizer st = new StringTokenizer(line, delimiter_);
+            List<String> tokenList = new ArrayList<String>();
+            String key = null;
+            while (st.hasMoreElements())
+            {
+            	tokenList.add((String)st.nextElement());
+            }
+            /* Construct the Key */
+            List<String> keyFields = new ArrayList<String> ();
+            for(int fieldId: importer_.key.fields.field)
+            {
+            	keyFields.add(tokenList.get(fieldId));
+            }
+            key = merge(keyFields, importer_.key.combiner);
+            if(importer_.key.optimizeIt != null && !importer_.key.optimizeIt)
+            {
+	            if(!checkIfProcessKey(key))
+	            {
+	            	continue;
+	            }
+            }
+            rm = rms.get(key);
+            if( rm == null)
+            {
+            	rm = new RowMutation(importer_.table, key);
+            	rms.put(key, rm);
+            }
+            if(importer_.columnFamily.superColumn != null)
+            {
+            	List<String> superColumnList = new ArrayList<String>();
+            	for(int fieldId : importer_.columnFamily.superColumn.fields.field)
+            	{
+            		superColumnList.add(tokenList.get(fieldId));
+            	}
+            	String superColumnName = merge(superColumnList, " ");
+            	superColumnList.clear();
+            	if(importer_.columnFamily.superColumn.tokenize)
+            	{
+            	    Analyzer analyzer = new StandardAnalyzer();
+            	    TokenStream ts = analyzer.tokenStream("superColumn", new StringReader(superColumnName));
+            	    Token token = null;
+            	    token = ts.next();
+            	    while(token != null)
+            	    {
+            	    	superColumnList.add(token.termText());
+                	    token = ts.next();
+            	    }
+            	}
+            	else
+            	{
+            		superColumnList.add(superColumnName);
+            	}
+            	for(String sName : superColumnList)
+            	{
+            		String cfName = importer_.columnFamily.name + ":" + sName;
+    	            if(importer_.columnFamily.column != null)
+    	            {
+    	            	for(ColumnType column : importer_.columnFamily.column )
+    	            	{
+    	            		String cfColumn = cfName +":" + (column.name == null ? tokenList.get(column.field):column.name);
+    	            		rm.add(cfColumn, tokenList.get(column.value.field).getBytes(), Integer.parseInt(tokenList.get(column.timestamp.field)));
+    	            	}
+    	            }
+            		
+            	}
+            	
+            }
+            else
+            {
+	            if(importer_.columnFamily.column != null)
+	            {
+	            	for(ColumnType column : importer_.columnFamily.column )
+	            	{
+	            		String cfColumn = importer_.columnFamily.name +":" + (column.name == null ? tokenList.get(column.field):column.name);
+	            		rm.add(cfColumn, tokenList.get(column.value.field).getBytes(), Integer.parseInt(tokenList.get(column.timestamp.field)));
+	            	}
+	            }
+            }
+        }
+        // Now apply the data for all keys  
+        // TODO : Add checks for large data
+        // size maybe we want to check the 
+        // data size and then apply.
+        Set<String> keys = rms.keySet();
+        for(String pKey : keys)
+        {
+        	rm = rms.get(pKey);
+        	if( rm != null)
+        	{
+        		rm.apply();
+        	}
+        }
+    }
+    
+    
+    void parseFileList(File dir) 
+    {
+		int fileCount = dir.list().length;
+		for ( int i = 0 ; i < fileCount ; i++ ) 
+		{
+			File file = new File(dir.list()[i]);
+			if ( file.isDirectory())
+			{
+				parseFileList(file);
+			}
+			else 
+			{
+				try
+				{
+					if(importer_.key.optimizeIt != null && importer_.key.optimizeIt)
+					{
+						if(checkIfProcessKey(dir.list()[i]))
+						{
+							parse(dir.listFiles()[i].getAbsolutePath());
+						}
+					}
+					else
+					{
+						parse(dir.listFiles()[i].getAbsolutePath());
+					}
+				}
+				catch ( Throwable ex ) 
+				{
+					logger_.error(ex.toString());
+				}
+			}
+		}
+    }
+	
+    void preLoad(File rootDirectory) throws Throwable
+    {
+        String table = DatabaseDescriptor.getTables().get(0);
+        String cfName = Table.recycleBin_ + ":" + "Keys";
+        /* populate just the keys. */
+        preParse(rootDirectory, table, cfName);
+        /* dump the memtables */
+        Table.open(table).flush(false);
+        /* force a compaction of the files. */
+        Table.open(table).forceCompaction(null,null,null);
+        
+        /*
+         * This is a hack to let everyone finish. Just sleep for
+         * a couple of minutes. 
+        */
+        logger_.info("Taking a nap after forcing a compaction ...");
+        Thread.sleep(Loader.siesta_);
+        
+        /* Figure out the keys in the index file to relocate the node */
+        List<String> ssTables = Table.open(table).getAllSSTablesOnDisk();
+        /* Load the indexes into memory */
+        for ( String df : ssTables )
+        {
+        	SSTable ssTable = new SSTable(df, StorageService.getPartitioner());
+        	ssTable.close();
+        }
+        /* We should have only one file since we just compacted. */        
+        List<String> indexedKeys = SSTable.getIndexedKeys();        
+        storageService_.relocate(indexedKeys.toArray( new String[0]) );
+        
+        /*
+         * This is a hack to let everyone relocate and learn about
+         * each other. Just sleep for a couple of minutes. 
+        */
+        logger_.info("Taking a nap after relocating ...");
+        Thread.sleep(Loader.siesta_);  
+        
+        /* 
+         * Do the cleanup necessary. Delete all commit logs and
+         * the SSTables and reset the load state in the StorageService. 
+        */
+        SSTable.delete(ssTables.get(0));
+        logger_.info("Finished all the requisite clean up ...");
+    }
+    
+	void load(String xmlFile) throws Throwable
+	{
+		try
+		{
+			JAXBContext jc = JAXBContext.newInstance(this.getClass().getPackage().getName());			
+			Unmarshaller u = jc.createUnmarshaller();			
+			importer_ = (Importer)u.unmarshal(new FileInputStream( xmlFile ) );
+			String directory = importer_.columnFamily.directory;
+            File rootDirectory = new File(directory);
+            preLoad(rootDirectory);
+			parseFileList(rootDirectory);
+		}
+		catch (Exception e)
+		{
+			logger_.info(LogUtil.throwableToString(e));
+		}
+		
+	}
+
+	/**
+	 * @param args
+	 */
+	public static void main(String[] args) throws Throwable
+	{
+		LogUtil.init();
+        StorageService s = StorageService.instance();
+        s.start();
+		Loader loader = new Loader(s);
+		loader.load("mbox_importer.xml");
+	}
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ObjectFactory.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ObjectFactory.java
index e69de29b..6575ba95 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ObjectFactory.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ObjectFactory.java
@@ -0,0 +1,136 @@
+//
+// This file was generated by the JavaTM Architecture for XML Binding(JAXB) Reference Implementation, vJAXB 2.1.3 in JDK 1.6 
+// See <a href="http://java.sun.com/xml/jaxb">http://java.sun.com/xml/jaxb</a> 
+// Any modifications to this file will be lost upon recompilation of the source schema. 
+// Generated on: 2007.10.19 at 01:36:41 PM PDT 
+//
+
+
+package org.apache.cassandra.loader;
+
+import javax.xml.bind.JAXBElement;
+import javax.xml.bind.annotation.XmlElementDecl;
+import javax.xml.bind.annotation.XmlRegistry;
+import javax.xml.namespace.QName;
+
+
+/**
+ * This object contains factory methods for each 
+ * Java content interface and Java element interface 
+ * generated in the com.facebook.infrastructure.loader package. 
+ * <p>An ObjectFactory allows you to programatically 
+ * construct new instances of the Java representation 
+ * for XML content. The Java representation of XML 
+ * content can consist of schema derived interfaces 
+ * and classes representing the binding of schema 
+ * type definitions, element declarations and model 
+ * groups.  Factory methods for each of these are 
+ * provided in this class.
+ * 
+ */
+@XmlRegistry
+public class ObjectFactory {
+
+    private final static QName _SuperColumn_QNAME = new QName("", "SuperColumn");
+    private final static QName _Table_QNAME = new QName("", "Table");
+    private final static QName _Column_QNAME = new QName("", "Column");
+
+    /**
+     * Create a new ObjectFactory that can be used to create new instances of schema derived classes for package: com.facebook.infrastructure.loader
+     * 
+     */
+    public ObjectFactory() {
+    }
+
+    /**
+     * Create an instance of {@link KeyType }
+     * 
+     */
+    public KeyType createKeyType() {
+        return new KeyType();
+    }
+
+    /**
+     * Create an instance of {@link SuperColumnType }
+     * 
+     */
+    public SuperColumnType createSuperColumnType() {
+        return new SuperColumnType();
+    }
+
+    /**
+     * Create an instance of {@link ColumnType }
+     * 
+     */
+    public ColumnType createColumnType() {
+        return new ColumnType();
+    }
+
+    /**
+     * Create an instance of {@link Importer }
+     * 
+     */
+    public Importer createImporter() {
+        return new Importer();
+    }
+
+    /**
+     * Create an instance of {@link ColumnFamilyType }
+     * 
+     */
+    public ColumnFamilyType createColumnFamilyType() {
+        return new ColumnFamilyType();
+    }
+
+    /**
+     * Create an instance of {@link TimestampType }
+     * 
+     */
+    public TimestampType createTimestampType() {
+        return new TimestampType();
+    }
+
+    /**
+     * Create an instance of {@link FieldCollection }
+     * 
+     */
+    public FieldCollection createFieldCollection() {
+        return new FieldCollection();
+    }
+
+    /**
+     * Create an instance of {@link ValueType }
+     * 
+     */
+    public ValueType createValueType() {
+        return new ValueType();
+    }
+
+    /**
+     * Create an instance of {@link JAXBElement }{@code <}{@link SuperColumnType }{@code >}}
+     * 
+     */
+    @XmlElementDecl(namespace = "", name = "SuperColumn")
+    public JAXBElement<SuperColumnType> createSuperColumn(SuperColumnType value) {
+        return new JAXBElement<SuperColumnType>(_SuperColumn_QNAME, SuperColumnType.class, null, value);
+    }
+
+    /**
+     * Create an instance of {@link JAXBElement }{@code <}{@link String }{@code >}}
+     * 
+     */
+    @XmlElementDecl(namespace = "", name = "Table")
+    public JAXBElement<String> createTable(String value) {
+        return new JAXBElement<String>(_Table_QNAME, String.class, null, value);
+    }
+
+    /**
+     * Create an instance of {@link JAXBElement }{@code <}{@link ColumnType }{@code >}}
+     * 
+     */
+    @XmlElementDecl(namespace = "", name = "Column")
+    public JAXBElement<ColumnType> createColumn(ColumnType value) {
+        return new JAXBElement<ColumnType>(_Column_QNAME, ColumnType.class, null, value);
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/PreLoad.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/PreLoad.java
index e69de29b..4006954c 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/PreLoad.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/PreLoad.java
@@ -0,0 +1,141 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.loader;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class PreLoad
+{
+	
+	private static long siesta_ = 2*60*1000;
+	private static Logger logger_ = Logger.getLogger( Loader.class );
+    private StorageService storageService_;
+	
+	public PreLoad(StorageService storageService)
+    {
+        storageService_ = storageService;
+    }
+    /*
+     * This method loads all the keys into a special column family 
+     * called "RecycleBin". This column family is used for temporary
+     * processing of data and then can be recycled. The idea is that 
+     * after the load is complete we have all the keys in the system.
+     * Now we force a compaction and examine the single Index file 
+     * that is generated to determine how the nodes need to relocate
+     * to be perfectly load balanced.
+     * 
+     *  param @ rootDirectory - rootDirectory at which the parsing begins.
+     *  param @ table - table that will be populated.
+     *  param @ cfName - name of the column that will be populated. This is 
+     *  passed in so that we do not unncessary allocate temporary String objects.
+    */
+    private void preParse(String rootDirectory, String table, String cfName) throws Throwable
+    {        
+        BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+                new FileInputStream(rootDirectory)), 16 * 1024 * 1024);
+        String line = null;
+        while ((line = bufReader.readLine()) != null)
+        {
+                String userId = line;
+                RowMutation rm = new RowMutation(table, userId);
+                rm.add(cfName, userId.getBytes(), 0);
+                rm.apply();
+        }
+    }
+    
+    void run(String userFile) throws Throwable
+    {
+        String table = DatabaseDescriptor.getTables().get(0);
+        String cfName = Table.recycleBin_ + ":" + "Keys";
+        /* populate just the keys. */
+        preParse(userFile, table, cfName);
+        /* dump the memtables */
+        Table.open(table).flush(false);
+        /* force a compaction of the files. */
+        Table.open(table).forceCompaction(null, null,null);
+        
+        /*
+         * This is a hack to let everyone finish. Just sleep for
+         * a couple of minutes. 
+        */
+        logger_.info("Taking a nap after forcing a compaction ...");
+        Thread.sleep(PreLoad.siesta_);
+        
+        /* Figure out the keys in the index file to relocate the node */
+        List<String> ssTables = Table.open(table).getAllSSTablesOnDisk();
+        /* Load the indexes into memory */
+        for ( String df : ssTables )
+        {
+        	SSTable ssTable = new SSTable(df, StorageService.getPartitioner());
+        	ssTable.close();
+        }
+        /* We should have only one file since we just compacted. */        
+        List<String> indexedKeys = SSTable.getIndexedKeys();        
+        storageService_.relocate(indexedKeys.toArray( new String[0]) );
+        
+        /*
+         * This is a hack to let everyone relocate and learn about
+         * each other. Just sleep for a couple of minutes. 
+        */
+        logger_.info("Taking a nap after relocating ...");
+        Thread.sleep(PreLoad.siesta_);  
+        
+        /* 
+         * Do the cleanup necessary. Delete all commit logs and
+         * the SSTables and reset the load state in the StorageService. 
+        */
+        SSTable.delete(ssTables.get(0));
+        logger_.info("Finished all the requisite clean up ...");
+    }
+
+    
+	/**
+	 * @param args
+	 */
+	public static void main(String[] args) throws Throwable
+	{
+		if(args.length != 1)
+		{
+			System.out.println("Usage: PreLoad <Fully qualified path of the file containing user names>");
+		}
+		// TODO Auto-generated method stub
+		LogUtil.init();
+        StorageService s = StorageService.instance();
+        s.start();
+        PreLoad preLoad = new PreLoad(s);
+        preLoad.run(args[0]);
+	}
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/SuperColumnType.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/SuperColumnType.java
index e69de29b..ae3ce864 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/SuperColumnType.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/SuperColumnType.java
@@ -0,0 +1,97 @@
+//
+// This file was generated by the JavaTM Architecture for XML Binding(JAXB) Reference Implementation, vJAXB 2.1.3 in JDK 1.6 
+// See <a href="http://java.sun.com/xml/jaxb">http://java.sun.com/xml/jaxb</a> 
+// Any modifications to this file will be lost upon recompilation of the source schema. 
+// Generated on: 2007.10.19 at 01:36:41 PM PDT 
+//
+
+
+package org.apache.cassandra.loader;
+
+import javax.xml.bind.annotation.XmlAccessType;
+import javax.xml.bind.annotation.XmlAccessorType;
+import javax.xml.bind.annotation.XmlAttribute;
+import javax.xml.bind.annotation.XmlElement;
+import javax.xml.bind.annotation.XmlType;
+
+
+/**
+ * <p>Java class for SuperColumnType complex type.
+ * 
+ * <p>The following schema fragment specifies the expected content contained within this class.
+ * 
+ * <pre>
+ * &lt;complexType name="SuperColumnType">
+ *   &lt;complexContent>
+ *     &lt;restriction base="{http://www.w3.org/2001/XMLSchema}anyType">
+ *       &lt;sequence>
+ *         &lt;element name="Fields" type="{}FieldCollection"/>
+ *       &lt;/sequence>
+ *       &lt;attribute name="Tokenize" type="{http://www.w3.org/2001/XMLSchema}boolean" />
+ *     &lt;/restriction>
+ *   &lt;/complexContent>
+ * &lt;/complexType>
+ * </pre>
+ * 
+ * 
+ */
+@XmlAccessorType(XmlAccessType.FIELD)
+@XmlType(name = "SuperColumnType", propOrder = {
+    "fields"
+})
+public class SuperColumnType {
+
+    @XmlElement(name = "Fields", required = true)
+    protected FieldCollection fields;
+    @XmlAttribute(name = "Tokenize")
+    protected Boolean tokenize;
+
+    /**
+     * Gets the value of the fields property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link FieldCollection }
+     *     
+     */
+    public FieldCollection getFields() {
+        return fields;
+    }
+
+    /**
+     * Sets the value of the fields property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link FieldCollection }
+     *     
+     */
+    public void setFields(FieldCollection value) {
+        this.fields = value;
+    }
+
+    /**
+     * Gets the value of the tokenize property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link Boolean }
+     *     
+     */
+    public Boolean isTokenize() {
+        return tokenize;
+    }
+
+    /**
+     * Sets the value of the tokenize property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link Boolean }
+     *     
+     */
+    public void setTokenize(Boolean value) {
+        this.tokenize = value;
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/TimestampType.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/TimestampType.java
index e69de29b..a3e0bbe4 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/TimestampType.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/TimestampType.java
@@ -0,0 +1,65 @@
+//
+// This file was generated by the JavaTM Architecture for XML Binding(JAXB) Reference Implementation, vJAXB 2.1.3 in JDK 1.6 
+// See <a href="http://java.sun.com/xml/jaxb">http://java.sun.com/xml/jaxb</a> 
+// Any modifications to this file will be lost upon recompilation of the source schema. 
+// Generated on: 2007.10.19 at 01:36:41 PM PDT 
+//
+
+
+package org.apache.cassandra.loader;
+
+import javax.xml.bind.annotation.XmlAccessType;
+import javax.xml.bind.annotation.XmlAccessorType;
+import javax.xml.bind.annotation.XmlAttribute;
+import javax.xml.bind.annotation.XmlType;
+
+
+/**
+ * <p>Java class for TimestampType complex type.
+ * 
+ * <p>The following schema fragment specifies the expected content contained within this class.
+ * 
+ * <pre>
+ * &lt;complexType name="TimestampType">
+ *   &lt;complexContent>
+ *     &lt;restriction base="{http://www.w3.org/2001/XMLSchema}anyType">
+ *       &lt;attribute name="Field" type="{http://www.w3.org/2001/XMLSchema}int" />
+ *     &lt;/restriction>
+ *   &lt;/complexContent>
+ * &lt;/complexType>
+ * </pre>
+ * 
+ * 
+ */
+@XmlAccessorType(XmlAccessType.FIELD)
+@XmlType(name = "TimestampType")
+public class TimestampType {
+
+    @XmlAttribute(name = "Field")
+    protected Integer field;
+
+    /**
+     * Gets the value of the field property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link Integer }
+     *     
+     */
+    public Integer getField() {
+        return field;
+    }
+
+    /**
+     * Sets the value of the field property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link Integer }
+     *     
+     */
+    public void setField(Integer value) {
+        this.field = value;
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ValueType.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ValueType.java
index e69de29b..3dd59a7e 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ValueType.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/loader/ValueType.java
@@ -0,0 +1,65 @@
+//
+// This file was generated by the JavaTM Architecture for XML Binding(JAXB) Reference Implementation, vJAXB 2.1.3 in JDK 1.6 
+// See <a href="http://java.sun.com/xml/jaxb">http://java.sun.com/xml/jaxb</a> 
+// Any modifications to this file will be lost upon recompilation of the source schema. 
+// Generated on: 2007.10.19 at 01:36:41 PM PDT 
+//
+
+
+package org.apache.cassandra.loader;
+
+import javax.xml.bind.annotation.XmlAccessType;
+import javax.xml.bind.annotation.XmlAccessorType;
+import javax.xml.bind.annotation.XmlAttribute;
+import javax.xml.bind.annotation.XmlType;
+
+
+/**
+ * <p>Java class for ValueType complex type.
+ * 
+ * <p>The following schema fragment specifies the expected content contained within this class.
+ * 
+ * <pre>
+ * &lt;complexType name="ValueType">
+ *   &lt;complexContent>
+ *     &lt;restriction base="{http://www.w3.org/2001/XMLSchema}anyType">
+ *       &lt;attribute name="Field" type="{http://www.w3.org/2001/XMLSchema}int" />
+ *     &lt;/restriction>
+ *   &lt;/complexContent>
+ * &lt;/complexType>
+ * </pre>
+ * 
+ * 
+ */
+@XmlAccessorType(XmlAccessType.FIELD)
+@XmlType(name = "ValueType")
+public class ValueType {
+
+    @XmlAttribute(name = "Field")
+    protected Integer field;
+
+    /**
+     * Gets the value of the field property.
+     * 
+     * @return
+     *     possible object is
+     *     {@link Integer }
+     *     
+     */
+    public Integer getField() {
+        return field;
+    }
+
+    /**
+     * Sets the value of the field property.
+     * 
+     * @param value
+     *     allowed object is
+     *     {@link Integer }
+     *     
+     */
+    public void setField(Integer value) {
+        this.field = value;
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/AbstractStrategy.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/AbstractStrategy.java
index e69de29b..fba6fd3a 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/AbstractStrategy.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/AbstractStrategy.java
@@ -0,0 +1,117 @@
+package org.apache.cassandra.locator;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * This class contains a helper method that will be used by
+ * all abstraction that implement the IReplicaPlacementStrategy
+ * interface.
+*/
+public abstract class AbstractStrategy implements IReplicaPlacementStrategy
+{
+    protected static Logger logger_ = Logger.getLogger(AbstractStrategy.class);
+
+    protected TokenMetadata tokenMetadata_;
+    protected IPartitioner partitioner_;
+    protected int replicas_;
+    protected int storagePort_;
+
+    AbstractStrategy(TokenMetadata tokenMetadata, IPartitioner partitioner, int replicas, int storagePort)
+    {
+        tokenMetadata_ = tokenMetadata;
+        partitioner_ = partitioner;
+        replicas_ = replicas;
+        storagePort_ = storagePort;
+    }
+
+    /*
+     * This method changes the ports of the endpoints from
+     * the control port to the storage ports.
+    */
+    protected void retrofitPorts(List<EndPoint> eps)
+    {
+        for ( EndPoint ep : eps )
+        {
+            ep.setPort(storagePort_);
+        }
+    }
+
+    protected EndPoint getNextAvailableEndPoint(EndPoint startPoint, List<EndPoint> topN, List<EndPoint> liveNodes)
+    {
+        EndPoint endPoint = null;
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        Token token = tokenMetadata_.getToken(startPoint);
+        int index = Collections.binarySearch(tokens, token);
+        if(index < 0)
+        {
+            index = (index + 1) * (-1);
+            if (index >= tokens.size())
+                index = 0;
+        }
+        int totalNodes = tokens.size();
+        int startIndex = (index+1)%totalNodes;
+        for (int i = startIndex, count = 1; count < totalNodes ; ++count, i = (i+1)%totalNodes)
+        {
+            EndPoint tmpEndPoint = tokenToEndPointMap.get(tokens.get(i));
+            if(FailureDetector.instance().isAlive(tmpEndPoint) && !topN.contains(tmpEndPoint) && !liveNodes.contains(tmpEndPoint))
+            {
+                endPoint = tmpEndPoint;
+                break;
+            }
+        }
+        return endPoint;
+    }
+
+    /*
+     * This method returns the hint map. The key is the endpoint
+     * on which the data is being placed and the value is the
+     * endpoint which is in the top N.
+     * Get the map of top N to the live nodes currently.
+     */
+    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(Token token)
+    {
+        List<EndPoint> liveList = new ArrayList<EndPoint>();
+        Map<EndPoint, EndPoint> map = new HashMap<EndPoint, EndPoint>();
+        EndPoint[] topN = getStorageEndPoints( token );
+
+        for( int i = 0 ; i < topN.length ; i++)
+        {
+            if( FailureDetector.instance().isAlive(topN[i]))
+            {
+                map.put(topN[i], topN[i]);
+                liveList.add(topN[i]) ;
+            }
+            else
+            {
+                EndPoint endPoint = getNextAvailableEndPoint(topN[i], Arrays.asList(topN), liveList);
+                if(endPoint != null)
+                {
+                    map.put(endPoint, topN[i]);
+                    liveList.add(endPoint) ;
+                }
+                else
+                {
+                    // log a warning , maybe throw an exception
+                    logger_.warn("Unable to find a live Endpoint we might be out of live nodes , This is dangerous !!!!");
+                }
+            }
+        }
+        return map;
+    }
+
+    public abstract EndPoint[] getStorageEndPoints(Token token);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/EndPointSnitch.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/EndPointSnitch.java
index e69de29b..72dfbc2d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/EndPointSnitch.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/EndPointSnitch.java
@@ -0,0 +1,61 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.net.*;
+
+import org.apache.cassandra.net.EndPoint;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class EndPointSnitch implements IEndPointSnitch
+{
+    public boolean isOnSameRack(EndPoint host, EndPoint host2) throws UnknownHostException
+    {
+        /*
+         * Look at the IP Address of the two hosts. Compare 
+         * the 3rd octet. If they are the same then the hosts
+         * are in the same rack else different racks. 
+        */        
+        byte[] ip = getIPAddress(host.getHost());
+        byte[] ip2 = getIPAddress(host2.getHost());
+        
+        return ( ip[2] == ip2[2] );
+    }
+    
+    public boolean isInSameDataCenter(EndPoint host, EndPoint host2) throws UnknownHostException
+    {
+        /*
+         * Look at the IP Address of the two hosts. Compare 
+         * the 2nd octet. If they are the same then the hosts
+         * are in the same datacenter else different datacenter. 
+        */
+        byte[] ip = getIPAddress(host.getHost());
+        byte[] ip2 = getIPAddress(host2.getHost());
+        
+        return ( ip[1] == ip2[1] );
+    }
+    
+    private byte[] getIPAddress(String host) throws UnknownHostException
+    {
+        InetAddress ia = InetAddress.getByName(host);         
+        return ia.getAddress();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/IEndPointSnitch.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/IEndPointSnitch.java
index e69de29b..3dd1f5a5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/IEndPointSnitch.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/IEndPointSnitch.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.net.UnknownHostException;
+
+import org.apache.cassandra.net.EndPoint;
+
+
+/**
+ * This interface helps determine location of node in the data center relative to another node.
+ * Give a node A and another node B it can tell if A and B are on the same rack or in the same
+ * data center.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IEndPointSnitch
+{
+    /**
+     * Helps determine if 2 nodes are in the same rack in the data center.
+     * @param host a specified endpoint
+     * @param host2 another specified endpoint
+     * @return true if on the same rack false otherwise
+     * @throws UnknownHostException
+     */
+    public boolean isOnSameRack(EndPoint host, EndPoint host2) throws UnknownHostException;
+    
+    /**
+     * Helps determine if 2 nodes are in the same data center.
+     * @param host a specified endpoint
+     * @param host2 another specified endpoint
+     * @return true if in the same data center false otherwise
+     * @throws UnknownHostException
+     */
+    public boolean isInSameDataCenter(EndPoint host, EndPoint host2) throws UnknownHostException;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/IReplicaPlacementStrategy.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/IReplicaPlacementStrategy.java
index e69de29b..3cd1bd50 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/IReplicaPlacementStrategy.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/IReplicaPlacementStrategy.java
@@ -0,0 +1,40 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.Map;
+import java.math.BigInteger;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.net.EndPoint;
+
+
+/*
+ * This interface has two implementations. One which
+ * does not respect rack or datacenter awareness and
+ * the other which does.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public interface IReplicaPlacementStrategy
+{
+	public EndPoint[] getStorageEndPoints(Token token);
+    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys);
+    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap);
+    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(Token token);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/RackAwareStrategy.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/RackAwareStrategy.java
index e69de29b..6f0529f5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/RackAwareStrategy.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/RackAwareStrategy.java
@@ -0,0 +1,123 @@
+package org.apache.cassandra.locator;
+
+import java.net.UnknownHostException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+
+/*
+ * This class returns the nodes responsible for a given
+ * key but does respects rack awareness. It makes a best
+ * effort to get a node from a different data center and
+ * a node in a different rack in the same datacenter as
+ * the primary.
+ */
+public class RackAwareStrategy extends AbstractStrategy
+{
+    public RackAwareStrategy(TokenMetadata tokenMetadata, IPartitioner partitioner, int replicas, int storagePort)
+    {
+        super(tokenMetadata, partitioner, replicas, storagePort);
+    }
+
+    public EndPoint[] getStorageEndPoints(Token token)
+    {
+        int startIndex;
+        List<EndPoint> list = new ArrayList<EndPoint>();
+        boolean bDataCenter = false;
+        boolean bOtherRack = false;
+        int foundCount = 0;
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        int index = Collections.binarySearch(tokens, token);
+        if(index < 0)
+        {
+            index = (index + 1) * (-1);
+            if (index >= tokens.size())
+                index = 0;
+        }
+        int totalNodes = tokens.size();
+        // Add the node at the index by default
+        list.add(tokenToEndPointMap.get(tokens.get(index)));
+        foundCount++;
+        if( replicas_ == 1 )
+        {
+            return list.toArray(new EndPoint[list.size()]);
+        }
+        startIndex = (index + 1)%totalNodes;
+        IEndPointSnitch endPointSnitch = StorageService.instance().getEndPointSnitch();
+        
+        for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i+1)%totalNodes)
+        {
+            try
+            {
+                // First try to find one in a different data center
+                if(!endPointSnitch.isInSameDataCenter(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))))
+                {
+                    // If we have already found something in a diff datacenter no need to find another
+                    if( !bDataCenter )
+                    {
+                        list.add(tokenToEndPointMap.get(tokens.get(i)));
+                        bDataCenter = true;
+                        foundCount++;
+                    }
+                    continue;
+                }
+                // Now  try to find one on a different rack
+                if(!endPointSnitch.isOnSameRack(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))) &&
+                        endPointSnitch.isInSameDataCenter(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))))
+                {
+                    // If we have already found something in a diff rack no need to find another
+                    if( !bOtherRack )
+                    {
+                        list.add(tokenToEndPointMap.get(tokens.get(i)));
+                        bOtherRack = true;
+                        foundCount++;
+                    }
+                }
+            }
+            catch (UnknownHostException e)
+            {
+                logger_.debug(LogUtil.throwableToString(e));
+            }
+
+        }
+        // If we found N number of nodes we are good. This loop wil just exit. Otherwise just
+        // loop through the list and add until we have N nodes.
+        for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i+1)%totalNodes)
+        {
+            if( ! list.contains(tokenToEndPointMap.get(tokens.get(i))))
+            {
+                list.add(tokenToEndPointMap.get(tokens.get(i)));
+                foundCount++;
+            }
+        }
+        retrofitPorts(list);
+        return list.toArray(new EndPoint[list.size()]);
+    }
+    
+    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys)
+    {
+    	Map<String, EndPoint[]> results = new HashMap<String, EndPoint[]>();
+
+        for ( String key : keys )
+        {
+            results.put(key, getStorageEndPoints(partitioner_.getInitialToken(key)));
+        }
+
+        return results;
+    }
+
+    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap)
+    {
+        throw new UnsupportedOperationException("This operation is not currently supported");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/RackUnawareStrategy.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/RackUnawareStrategy.java
index e69de29b..092f9ee4 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/RackUnawareStrategy.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/RackUnawareStrategy.java
@@ -0,0 +1,75 @@
+package org.apache.cassandra.locator;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * This class returns the nodes responsible for a given
+ * key but does not respect rack awareness. Basically
+ * returns the 3 nodes that lie right next to each other
+ * on the ring.
+ */
+public class RackUnawareStrategy extends AbstractStrategy
+{
+    public RackUnawareStrategy(TokenMetadata tokenMetadata, IPartitioner partitioner, int replicas, int storagePort)
+    {
+        super(tokenMetadata, partitioner, replicas, storagePort);
+    }
+
+    public EndPoint[] getStorageEndPoints(Token token)
+    {
+        return getStorageEndPoints(token, tokenMetadata_.cloneTokenEndPointMap());            
+    }
+    
+    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap)
+    {
+        int startIndex;
+        List<EndPoint> list = new ArrayList<EndPoint>();
+        int foundCount = 0;
+        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        int index = Collections.binarySearch(tokens, token);
+        if(index < 0)
+        {
+            index = (index + 1) * (-1);
+            if (index >= tokens.size())
+                index = 0;
+        }
+        int totalNodes = tokens.size();
+        // Add the node at the index by default
+        list.add(tokenToEndPointMap.get(tokens.get(index)));
+        foundCount++;
+        startIndex = (index + 1)%totalNodes;
+        // If we found N number of nodes we are good. This loop will just exit. Otherwise just
+        // loop through the list and add until we have N nodes.
+        for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i+1)%totalNodes)
+        {
+            if( ! list.contains(tokenToEndPointMap.get(tokens.get(i))))
+            {
+                list.add(tokenToEndPointMap.get(tokens.get(i)));
+                foundCount++;
+            }
+        }
+        retrofitPorts(list);
+        return list.toArray(new EndPoint[list.size()]);
+    }
+            
+    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys)
+    {
+    	Map<String, EndPoint[]> results = new HashMap<String, EndPoint[]>();
+
+        for ( String key : keys )
+        {
+            results.put(key, getStorageEndPoints(partitioner_.getInitialToken(key)));
+        }
+
+        return results;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/TokenMetadata.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/TokenMetadata.java
index e69de29b..17ef95ae 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/TokenMetadata.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/locator/TokenMetadata.java
@@ -0,0 +1,173 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.net.EndPoint;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TokenMetadata
+{
+    /* Maintains token to endpoint map of every node in the cluster. */
+    private Map<Token, EndPoint> tokenToEndPointMap_ = new HashMap<Token, EndPoint>();
+    /* Maintains a reverse index of endpoint to token in the cluster. */
+    private Map<EndPoint, Token> endPointToTokenMap_ = new HashMap<EndPoint, Token>();
+    
+    /* Use this lock for manipulating the token map */
+    private final ReadWriteLock lock_ = new ReentrantReadWriteLock(true);
+
+    public TokenMetadata()
+    {
+    }
+
+    private TokenMetadata(Map<Token, EndPoint> tokenToEndPointMap, Map<EndPoint, Token> endPointToTokenMap)
+    {
+        tokenToEndPointMap_ = tokenToEndPointMap;
+        endPointToTokenMap_ = endPointToTokenMap;
+    }
+    
+    public TokenMetadata cloneMe()
+    {
+        return new TokenMetadata(cloneTokenEndPointMap(), cloneEndPointTokenMap());
+    }
+    
+    /**
+     * Update the two maps in an safe mode. 
+    */
+    public void update(Token token, EndPoint endpoint)
+    {
+        lock_.writeLock().lock();
+        try
+        {            
+            Token oldToken = endPointToTokenMap_.get(endpoint);
+            if ( oldToken != null )
+                tokenToEndPointMap_.remove(oldToken);
+            tokenToEndPointMap_.put(token, endpoint);
+            endPointToTokenMap_.put(endpoint, token);
+        }
+        finally
+        {
+            lock_.writeLock().unlock();
+        }
+    }
+    
+    /**
+     * Remove the entries in the two maps.
+     * @param endpoint
+     */
+    public void remove(EndPoint endpoint)
+    {
+        lock_.writeLock().lock();
+        try
+        {            
+            Token oldToken = endPointToTokenMap_.get(endpoint);
+            if ( oldToken != null )
+                tokenToEndPointMap_.remove(oldToken);            
+            endPointToTokenMap_.remove(endpoint);
+        }
+        finally
+        {
+            lock_.writeLock().unlock();
+        }
+    }
+    
+    public Token getToken(EndPoint endpoint)
+    {
+        lock_.readLock().lock();
+        try
+        {
+            return endPointToTokenMap_.get(endpoint);
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    }
+    
+    public boolean isKnownEndPoint(EndPoint ep)
+    {
+        lock_.readLock().lock();
+        try
+        {
+            return endPointToTokenMap_.containsKey(ep);
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    }
+    
+    /*
+     * Returns a safe clone of tokenToEndPointMap_.
+    */
+    public Map<Token, EndPoint> cloneTokenEndPointMap()
+    {
+        lock_.readLock().lock();
+        try
+        {            
+            return new HashMap<Token, EndPoint>( tokenToEndPointMap_ );
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    }
+    
+    /*
+     * Returns a safe clone of endPointTokenMap_.
+    */
+    public Map<EndPoint, Token> cloneEndPointTokenMap()
+    {
+        lock_.readLock().lock();
+        try
+        {            
+            return new HashMap<EndPoint, Token>( endPointToTokenMap_ );
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder();
+        Set<EndPoint> eps = endPointToTokenMap_.keySet();
+        
+        for ( EndPoint ep : eps )
+        {
+            sb.append(ep);
+            sb.append(":");
+            sb.append(endPointToTokenMap_.get(ep));
+            sb.append(System.getProperty("line.separator"));
+        }
+        
+        return sb.toString();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/AsyncResult.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/AsyncResult.java
index e69de29b..ded451e3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/AsyncResult.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/AsyncResult.java
@@ -0,0 +1,135 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.util.List;
+import java.util.Hashtable;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.service.QuorumResponseHandler;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class AsyncResult implements IAsyncResult
+{
+    private static Logger logger_ = Logger.getLogger( AsyncResult.class );
+    private Object[] result_ = new Object[0];    
+    private AtomicBoolean done_ = new AtomicBoolean(false);
+    private Lock lock_ = new ReentrantLock();
+    private Condition condition_;
+
+    public AsyncResult()
+    {        
+        condition_ = lock_.newCondition();
+    }    
+    
+    public Object[] get()
+    {
+        lock_.lock();
+        try
+        {
+            if ( !done_.get() )
+            {
+                condition_.await();                    
+            }
+        }
+        catch ( InterruptedException ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+        finally
+        {
+            lock_.unlock();            
+        }        
+        return result_;
+    }
+    
+    public boolean isDone()
+    {
+        return done_.get();
+    }
+    
+    public Object[] get(long timeout, TimeUnit tu) throws TimeoutException
+    {
+        lock_.lock();
+        try
+        {            
+            boolean bVal = true;
+            try
+            {
+                if ( !done_.get() )
+                {                    
+                    bVal = condition_.await(timeout, tu);
+                }
+            }
+            catch ( InterruptedException ex )
+            {
+                logger_.warn( LogUtil.throwableToString(ex) );
+            }
+            
+            if ( !bVal && !done_.get() )
+            {                                           
+                throw new TimeoutException("Operation timed out.");
+            }
+        }
+        finally
+        {
+            lock_.unlock();      
+        }
+        return result_;
+    }
+    
+    public List<Object[]> multiget()
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
+    }
+    
+    public List<Object[]> multiget(long timeout, TimeUnit tu) throws TimeoutException
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
+    }
+    
+    public void result(Message response)
+    {        
+        try
+        {
+            lock_.lock();
+            if ( !done_.get() )
+            {                
+                result_ = response.getMessageBody();
+                done_.set(true);
+                condition_.signal();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }        
+    }    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/CompactEndPointSerializationHelper.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/CompactEndPointSerializationHelper.java
index e69de29b..dd2b84b6 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/CompactEndPointSerializationHelper.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/CompactEndPointSerializationHelper.java
@@ -0,0 +1,50 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.*;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class CompactEndPointSerializationHelper
+{
+    public static void serialize(EndPoint endPoint, DataOutputStream dos) throws IOException
+    {        
+        dos.write(EndPoint.toBytes(endPoint));
+    }
+    
+    public static EndPoint deserialize(DataInputStream dis) throws IOException
+    {     
+        byte[] bytes = new byte[6];
+        dis.readFully(bytes, 0, bytes.length);
+        return EndPoint.fromBytes(bytes);       
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {
+        EndPoint ep = new EndPoint(7000);
+        byte[] bytes = EndPoint.toBytes(ep);
+        System.out.println(bytes.length);
+        EndPoint ep2 = EndPoint.fromBytes(bytes);
+        System.out.println(ep2);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ConnectionStatistics.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ConnectionStatistics.java
index e69de29b..91b3a485 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ConnectionStatistics.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ConnectionStatistics.java
@@ -0,0 +1,78 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ConnectionStatistics
+{
+    private String localHost;
+    private int localPort;
+    private String remoteHost;
+    private int remotePort;
+    private int totalConnections;
+    private int connectionsInUse;
+
+    ConnectionStatistics(EndPoint localEp, EndPoint remoteEp, int tc, int ciu)
+    {
+        localHost = localEp.getHost();
+        localPort = localEp.getPort();
+        remoteHost = remoteEp.getHost();
+        remotePort = remoteEp.getPort();
+        totalConnections = tc;
+        connectionsInUse = ciu;
+    }
+    
+    public String getLocalHost()
+    {
+        return localHost;
+    }
+    
+    public int getLocalPort()
+    {
+        return localPort;
+    }
+    
+    public String getRemoteHost()
+    {
+        return remoteHost;
+    }
+    
+    public int getRemotePort()
+    {
+        return remotePort;
+    }
+    
+    public int getTotalConnections()
+    {
+        return totalConnections;
+    }
+    
+    public int getConnectionInUse()
+    {
+        return connectionsInUse;
+    }
+
+    public String toString()
+    {
+        return localHost + ":" + localPort + "->" + remoteHost + ":" + remotePort + " Total Connections open : " + totalConnections + " Connections in use : " + connectionsInUse;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/EndPoint.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/EndPoint.java
index e69de29b..03461d31 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/EndPoint.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/EndPoint.java
@@ -0,0 +1,173 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.net.*;
+import java.nio.ByteBuffer;
+import java.nio.CharBuffer;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class EndPoint implements Serializable, Comparable<EndPoint>
+{
+    // logging and profiling.
+    private static Logger logger_ = Logger.getLogger(EndPoint.class);
+    private static final long serialVersionUID = -4962625949179835907L;
+    private static Map<CharBuffer, String> hostNames_ = new HashMap<CharBuffer, String>();
+    protected static final int randomPort_ = 5555;
+    public static EndPoint randomLocalEndPoint_;
+    
+    static
+    {
+        try
+        {
+            randomLocalEndPoint_ = new EndPoint(FBUtilities.getHostAddress(), EndPoint.randomPort_);
+        }        
+        catch ( IOException ex )
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }
+    }
+
+    private String host_;
+    private int port_;
+
+    private transient InetSocketAddress ia_;
+
+    public EndPoint(String host, int port)
+    {
+        assert host.matches("\\d+\\.\\d+\\.\\d+\\.\\d+") : host;
+        host_ = host;
+        port_ = port;
+    }
+
+    // create a local endpoint id
+    public EndPoint(int port)
+    {
+        try
+        {
+            host_ = FBUtilities.getHostAddress();
+        }
+        catch (UnknownHostException e)
+        {
+            throw new RuntimeException(e);
+        }
+        port_ = port;
+    }
+
+    public String getHost()
+    {
+        return host_;
+    }
+
+    public int getPort()
+    {
+        return port_;
+    }
+
+    public void setPort(int port)
+    {
+        port_ = port;
+    }
+
+    public InetSocketAddress getInetAddress()
+    {
+        if (ia_ == null || ia_.isUnresolved())
+        {
+            ia_ = new InetSocketAddress(host_, port_);
+        }
+        return ia_;
+    }
+
+    public boolean equals(Object o)
+    {
+        if (!(o instanceof EndPoint))
+            return false;
+
+        EndPoint rhs = (EndPoint) o;
+        return (host_.equals(rhs.host_) && port_ == rhs.port_);
+    }
+
+    public int hashCode()
+    {
+        return (host_ + port_).hashCode();
+    }
+
+    public int compareTo(EndPoint rhs)
+    {
+        return host_.compareTo(rhs.host_);
+    }
+
+    public String toString()
+    {
+        return (host_ + ":" + port_);
+    }
+
+    public static EndPoint fromString(String str)
+    {
+        String[] values = str.split(":");
+        return new EndPoint(values[0], Integer.parseInt(values[1]));
+    }
+
+    public static byte[] toBytes(EndPoint ep)
+    {
+        ByteBuffer buffer = ByteBuffer.allocate(6);
+        byte[] iaBytes = ep.getInetAddress().getAddress().getAddress();
+        buffer.put(iaBytes);
+        buffer.put(MessagingService.toByteArray((short) ep.getPort()));
+        buffer.flip();
+        return buffer.array();
+    }
+
+    public static EndPoint fromBytes(byte[] bytes)
+    {
+        ByteBuffer buffer = ByteBuffer.allocate(4);
+        System.arraycopy(bytes, 0, buffer.array(), 0, 4);
+        byte[] portBytes = new byte[2];
+        System.arraycopy(bytes, 4, portBytes, 0, portBytes.length);
+        try
+        {
+            CharBuffer charBuffer = buffer.asCharBuffer();
+            String host = hostNames_.get(charBuffer);
+            if (host == null)
+            {               
+                host = InetAddress.getByAddress(buffer.array()).getHostAddress();              
+                hostNames_.put(charBuffer, host);
+            }
+            int port = (int) MessagingService.byteArrayToShort(portBytes);
+            return new EndPoint(host, port);
+        }
+        catch (UnknownHostException e)
+        {
+            throw new IllegalArgumentException(e);
+        }
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/FileStreamTask.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/FileStreamTask.java
index e69de29b..a8dcac23 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/FileStreamTask.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/FileStreamTask.java
@@ -0,0 +1,84 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.*;
+import java.net.SocketException;
+
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class FileStreamTask implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger( FileStreamTask.class );
+    
+    private String file_;
+    private long startPosition_;
+    private long total_;
+    private EndPoint from_;
+    private EndPoint to_;
+    
+    FileStreamTask(String file, long startPosition, long total, EndPoint from, EndPoint to)
+    {
+        file_ = file;
+        startPosition_ = startPosition;
+        total_ = total;
+        from_ = from;
+        to_ = to;
+    }
+    
+    public void run()
+    {
+        TcpConnection connection = null;
+        try
+        {                        
+            connection = new TcpConnection(from_, to_);
+            File file = new File(file_);             
+            connection.stream(file, startPosition_, total_);
+            MessagingService.setStreamingMode(false);
+            logger_.debug("Done streaming " + file);
+        }            
+        catch ( SocketException se )
+        {                        
+            logger_.info(LogUtil.throwableToString(se));
+        }
+        catch ( IOException e )
+        {
+            logConnectAndIOException(e, connection);
+        }
+        catch (Throwable th)
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }        
+    }
+    
+    private void logConnectAndIOException(IOException ex, TcpConnection connection)
+    {                    
+        if ( connection != null )
+        {
+            connection.errorClose();
+        }
+        logger_.info(LogUtil.throwableToString(ex));
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/Header.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/Header.java
index e69de29b..21272432 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/Header.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/Header.java
@@ -0,0 +1,182 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.utils.GuidGenerator;
+
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Header implements java.io.Serializable
+{
+    static final long serialVersionUID = -3194851946523170022L;
+    private static ICompactSerializer<Header> serializer_;
+    private static AtomicInteger idGen_ = new AtomicInteger(0);
+    
+    static
+    {
+        serializer_ = new HeaderSerializer();        
+    }
+    
+    static ICompactSerializer<Header> serializer()
+    {
+        return serializer_;
+    }
+
+    private EndPoint from_;
+    private String type_;
+    private String verb_;
+    private String messageId_;
+    protected Map<String, byte[]> details_ = new Hashtable<String, byte[]>();
+    
+    Header(String id, EndPoint from, String messageType, String verb)
+    {
+        messageId_ = id;
+        from_ = from;
+        type_ = messageType;
+        verb_ = verb;        
+    }
+    
+    Header(String id, EndPoint from, String messageType, String verb, Map<String, byte[]> details)
+    {
+        this(id, from, messageType, verb);
+        details_ = details;
+    }
+
+    Header(EndPoint from, String messageType, String verb)
+    {
+        messageId_ = Integer.toString(idGen_.incrementAndGet());
+        from_ = from;
+        type_ = messageType;
+        verb_ = verb;
+    }        
+
+    EndPoint getFrom()
+    {
+        return from_;
+    }
+
+    String getMessageType()
+    {
+        return type_;
+    }
+
+    String getVerb()
+    {
+        return verb_;
+    }
+
+    String getMessageId()
+    {
+        return messageId_;
+    }
+
+    void setMessageId(String id)
+    {
+        messageId_ = id;
+    }
+    
+    void setMessageType(String type)
+    {
+        type_ = type;
+    }
+    
+    void setMessageVerb(String verb)
+    {
+        verb_ = verb;
+    }
+    
+    byte[] getDetail(Object key)
+    {
+        return details_.get(key);
+    }
+    
+    void removeDetail(Object key)
+    {
+        details_.remove(key);
+    }
+    
+    void addDetail(String key, byte[] value)
+    {
+        details_.put(key, value);
+    }
+    
+    Map<String, byte[]> getDetails()
+    {
+        return details_;
+    }
+}
+
+class HeaderSerializer implements ICompactSerializer<Header>
+{
+    public void serialize(Header t, DataOutputStream dos) throws IOException
+    {           
+        dos.writeUTF(t.getMessageId());
+        CompactEndPointSerializationHelper.serialize(t.getFrom(), dos);
+        dos.writeUTF(t.getMessageType());
+        dos.writeUTF( t.getVerb() );
+        
+        /* Serialize the message header */
+        int size = t.details_.size();
+        dos.writeInt(size);
+        Set<String> keys = t.details_.keySet();
+        
+        for( String key : keys )
+        {
+            dos.writeUTF(key);
+            byte[] value = t.details_.get(key);
+            dos.writeInt(value.length);
+            dos.write(value);
+        }
+    }
+
+    public Header deserialize(DataInputStream dis) throws IOException
+    {
+        String id = dis.readUTF();
+        EndPoint from = CompactEndPointSerializationHelper.deserialize(dis);
+        String type = dis.readUTF();
+        String verb = dis.readUTF();
+        
+        /* Deserializing the message header */
+        int size = dis.readInt();
+        Map<String, byte[]> details = new Hashtable<String, byte[]>(size);
+        for ( int i = 0; i < size; ++i )
+        {
+            String key = dis.readUTF();
+            int length = dis.readInt();
+            byte[] bytes = new byte[length];
+            dis.readFully(bytes);
+            details.put(key, bytes);
+        }
+        
+        return new Header(id, from, type, verb, details);
+    }
+}
+
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/HeaderTypes.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/HeaderTypes.java
index e69de29b..8de3cb3c 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/HeaderTypes.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/HeaderTypes.java
@@ -0,0 +1,28 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class HeaderTypes 
+{
+    public final static String TASK_PROFILE_CHAIN = "TASK_PROFILE_CHAIN";
+    public static String TASK_ID = "TASK_ID";
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IAsyncCallback.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IAsyncCallback.java
index e69de29b..7fb8d990 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IAsyncCallback.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IAsyncCallback.java
@@ -0,0 +1,38 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IAsyncCallback 
+{
+	/**
+	 * @param msg responses to be returned
+	 */
+	public void response(Message msg);
+    
+    /**
+     * Attach some application specific context to the
+     * callback.
+     * @param o application specific context
+     */
+    public void attachContext(Object o);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IAsyncResult.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IAsyncResult.java
index e69de29b..d162f316 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IAsyncResult.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IAsyncResult.java
@@ -0,0 +1,73 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IAsyncResult
+{    
+    /**
+     * This is used to check if the task has been completed
+     * 
+     * @return true if the task has been completed and false otherwise.
+     */
+    public boolean isDone();
+    
+    /**
+     * Returns the result for the task that was submitted.
+     * @return the result wrapped in an Object[]
+    */
+    public Object[] get();    
+    
+    /**
+     * Same operation as the above get() but allows the calling
+     * thread to specify a timeout.
+     * @param timeout the maximum time to wait
+     * @param tu the time unit of the timeout argument
+     * @return the result wrapped in an Object[]
+    */
+    public Object[] get(long timeout, TimeUnit tu) throws TimeoutException;
+    
+    /**
+     * Returns the result for all tasks that was submitted.
+     * @return the list of results wrapped in an Object[]
+    */
+    public List<Object[]> multiget();
+    
+    /**
+     * Same operation as the above get() but allows the calling
+     * thread to specify a timeout.
+     * @param timeout the maximum time to wait
+     * @param tu the time unit of the timeout argument
+     * @return the result wrapped in an Object[]
+    */
+    public List<Object[]> multiget(long timeout, TimeUnit tu) throws TimeoutException;
+    
+    /**
+     * Store the result obtained for the submitted task.
+     * @param result the response message
+     */
+    public void result(Message result);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IMessagingService.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IMessagingService.java
index e69de29b..f69d28f0 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IMessagingService.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IMessagingService.java
@@ -0,0 +1,179 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+
+import javax.xml.bind.JAXBException;
+
+import org.apache.cassandra.concurrent.IStage;
+
+
+/**
+ * An IMessagingService provides the methods for sending messages to remote
+ * endpoints. IMessagingService enables the sending of request-response style
+ * messages and fire-forget style messages.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IMessagingService
+{   
+	/**
+     * Register a verb and the corresponding verb handler with the
+     * Messaging Service.
+     * @param type name of the verb.     
+     * @param verbHandler handler for the specified verb
+     */
+    public void registerVerbHandlers(String type, IVerbHandler verbHandler);
+    
+    /**
+     * Deregister all verbhandlers corresponding to localEndPoint.
+     * @param localEndPoint
+     */
+    public void deregisterAllVerbHandlers(EndPoint localEndPoint);
+    
+    /**
+     * Deregister a verbhandler corresponding to the verb from the
+     * Messaging Service.
+     * @param type name of the verb.      
+     */
+    public void deregisterVerbHandlers(String type);
+    
+    /**
+     * Listen on the specified port.
+     * @param ep EndPoint whose port to listen on.
+     * @param isHttp specify if the port is an Http port.     
+     */
+    public void listen(EndPoint ep, boolean isHttp) throws IOException;
+    
+    /**
+     * Listen on the specified port.
+     * @param ep EndPoint whose port to listen on.     
+     */
+    public void listenUDP(EndPoint ep);
+    
+    /**
+     * Send a message to a given endpoint. 
+     * @param message message to be sent.
+     * @param to endpoint to which the message needs to be sent
+     * @return an reference to an IAsyncResult which can be queried for the
+     * response
+     */
+    public IAsyncResult sendRR(Message message, EndPoint to);
+
+    /**
+     * Send a message to the given set of endpoints and informs the MessagingService
+     * to wait for at least <code>howManyResults</code> responses to determine success
+     * of failure.
+     * @param message message to be sent.
+     * @param to endpoints to which the message needs to be sent
+     * @param cb callback interface which is used to pass the responses
+     * @return an reference to message id used to match with the result
+     */
+    public String sendRR(Message message, EndPoint[] to, IAsyncCallback cb);
+    
+    /**
+     * Send a message to a given endpoint. This method specifies a callback
+     * which is invoked with the actual response.
+     * @param message message to be sent.
+     * @param to endpoint to which the message needs to be sent
+     * @param cb callback interface which is used to pass the responses or
+     *           suggest that a timeout occured to the invoker of the send().
+     *           suggest that a timeout occured to the invoker of the send().
+     * @return an reference to message id used to match with the result
+     */
+    public String sendRR(Message message, EndPoint to, IAsyncCallback cb);
+
+    /**
+     * Send a message to a given endpoint. The ith element in the <code>messages</code>
+     * array is sent to the ith element in the <code>to</code> array.This method assumes
+     * there is a one-one mapping between the <code>messages</code> array and
+     * the <code>to</code> array. Otherwise an  IllegalArgumentException will be thrown.
+     * This method also informs the MessagingService to wait for at least
+     * <code>howManyResults</code> responses to determine success of failure.
+     * @param messages messages to be sent.
+     * @param to endpoints to which the message needs to be sent
+     * @param cb callback interface which is used to pass the responses or
+     *           suggest that a timeout occured to the invoker of the send().
+     * @return an reference to message id used to match with the result
+     */
+    public String sendRR(Message[] messages, EndPoint[] to, IAsyncCallback cb);
+    
+    /**
+     * Send a message to a given endpoint. The ith element in the <code>messages</code>
+     * array is sent to the ith element in the <code>to</code> array.This method assumes
+     * there is a one-one mapping between the <code>messages</code> array and
+     * the <code>to</code> array. Otherwise an  IllegalArgumentException will be thrown.
+     * This method also informs the MessagingService to wait for at least
+     * <code>howManyResults</code> responses to determine success of failure.
+     * @param messages messages to be sent.
+     * @param to endpoints to which the message needs to be sent
+     * @return an reference to IAsyncResult
+     */
+    public IAsyncResult sendRR(Message[] messages, EndPoint[] to);
+    
+    /**
+     * Send a message to a given endpoint. The ith element in the <code>messages</code>
+     * array is sent to the ith element in the <code>to</code> array.This method assumes
+     * there is a one-one mapping between the <code>messages</code> array and
+     * the <code>to</code> array. Otherwise an  IllegalArgumentException will be thrown.
+     * The idea is that multi-groups of messages are grouped as one logical message
+     * whose results are harnessed via the <i>IAsyncResult</i>
+     * @param messages groups of grouped messages.
+     * @param to destination for the groups of messages
+     * @param cb the callback handler to be invoked for the responses
+     * @return the group id which is basically useless - it is only returned for API's
+     *         to look compatible.
+     */
+    public String sendRR(Message[][] messages, EndPoint[][] to, IAsyncCallback cb);
+
+    /**
+     * Send a message to a given endpoint. This method adheres to the fire and forget
+     * style messaging.
+     * @param message messages to be sent.
+     * @param to endpoint to which the message needs to be sent
+     */
+    public void sendOneWay(Message message, EndPoint to);
+        
+    /**
+     * Send a message to a given endpoint. This method adheres to the fire and forget
+     * style messaging.
+     * @param message messages to be sent.
+     * @param to endpoint to which the message needs to be sent
+     */
+    public void sendUdpOneWay(Message message, EndPoint to);
+    
+    /**
+     * Stream a file from source to destination. This is highly optimized
+     * to not hold any of the contents of the file in memory.
+     * @param file name of file to stream.
+     * param start position inside the file
+     * param total number of bytes to stream
+     * param to endpoint to which we need to stream the file.
+    */
+    public void stream(String file, long startPosition, long total, EndPoint from, EndPoint to);
+
+    /**
+     * This method returns the verb handler associated with the registered
+     * verb. If no handler has been registered then null is returned.
+     * @param verb for which the verb handler is sought
+     * @return a reference to IVerbHandler which is the handler for the specified verb
+     */
+    public IVerbHandler getVerbHandler(String verb);    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IVerbHandler.java
index e69de29b..bd8da0c2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/IVerbHandler.java
@@ -0,0 +1,39 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * IVerbHandler provides the method that all verb handlers need to implement.
+ * The concrete implementation of this interface would provide the functionality
+ * for a given verb.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IVerbHandler
+{
+    /**
+     * This method delivers a message to the implementing class (if the implementing
+     * class was registered by a call to MessagingService.registerVerbHandlers).
+     * Note that the caller should not be holding any locks when calling this method
+     * because the implementation may be synchronized.
+     * 
+     * @param message - incoming message that needs handling.     
+     */
+    public void doVerb(Message message);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/Message.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/Message.java
index e69de29b..304803c9 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/Message.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/Message.java
@@ -0,0 +1,219 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.lang.reflect.Array;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.*;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Message implements java.io.Serializable
+{    
+    static final long serialVersionUID = 6329198792470413221L;
+    private static ICompactSerializer<Message> serializer_;
+    
+    static
+    {
+        serializer_ = new MessageSerializer();        
+    }
+    
+    public static ICompactSerializer<Message> serializer()
+    {
+        return serializer_;
+    }
+    
+    Header header_;
+    private Object[] body_ = new Object[0];
+    
+    protected Message(String id, EndPoint from, String messageType, String verb, Object... body)
+    {
+        this(new Header(id, from, messageType, verb), body);
+    }
+    
+    protected Message(Header header, Object... body)
+    {
+        header_ = header;
+        body_ = body;
+    }
+
+    public Message(EndPoint from, String messageType, String verb, Object... body)
+    {
+        this(new Header(from, messageType, verb), body);
+    }    
+    
+    public byte[] getHeader(Object key)
+    {
+        return header_.getDetail(key);
+    }
+    
+    public void removeHeader(Object key)
+    {
+        header_.removeDetail(key);
+    }
+    
+    public void setMessageType(String type)
+    {
+        header_.setMessageType(type);
+    }
+
+    public void setMessageVerb(String verb)
+    {
+        header_.setMessageVerb(verb);
+    }
+
+    public void addHeader(String key, byte[] value)
+    {
+        header_.addDetail(key, value);
+    }
+    
+    public Map<String, byte[]> getHeaders()
+    {
+        return header_.getDetails();
+    }
+
+    public Object[] getMessageBody()
+    {
+        return body_;
+    }
+    
+    public void setMessageBody(Object[] body)
+    {
+        body_ = body;
+    }
+
+    public EndPoint getFrom()
+    {
+        return header_.getFrom();
+    }
+
+    public String getMessageType()
+    {
+        return header_.getMessageType();
+    }
+
+    public String getVerb()
+    {
+        return header_.getVerb();
+    }
+
+    public String getMessageId()
+    {
+        return header_.getMessageId();
+    }
+    
+    public Class[] getTypes()
+    {
+        List<Class> types = new ArrayList<Class>();
+        
+        for ( int i = 0; i < body_.length; ++i )
+        {
+            if ( body_[i].getClass().isArray() )
+            {
+                int size = Array.getLength(body_[i]);
+                if ( size > 0 )
+                {
+                    types.add( Array.get( body_[i], 0).getClass() );
+                }
+            }
+            else
+            {
+                types.add(body_[i].getClass());
+            }
+        }
+        
+        return types.toArray( new Class[0] );
+    }    
+
+    void setMessageId(String id)
+    {
+        header_.setMessageId(id);
+    }    
+
+    public Message getReply(EndPoint from, Object... args)
+    {        
+        Message response = new Message(getMessageId(),
+                                       from,
+                                       MessagingService.responseStage_,
+                                       MessagingService.responseVerbHandler_,
+                                       args);
+        return response;
+    }
+    
+    public String toString()
+    {
+        StringBuffer sbuf = new StringBuffer("");
+        String separator = System.getProperty("line.separator");
+        sbuf.append("ID:" + getMessageId());
+        sbuf.append(separator);
+        sbuf.append("FROM:" + getFrom());
+        sbuf.append(separator);
+        sbuf.append("TYPE:" + getMessageType());
+        sbuf.append(separator);
+        sbuf.append("VERB:" + getVerb());
+        sbuf.append(separator);
+        sbuf.append("BODY TYPE:" + getBodyTypes());        
+        sbuf.append(separator);
+        return sbuf.toString();
+    }
+    
+    private String getBodyTypes()
+    {
+        StringBuffer sbuf = new StringBuffer("");
+        Class[] types = getTypes();
+        for ( int i = 0; i < types.length; ++i )
+        {
+            sbuf.append(types[i].getName());
+            sbuf.append(" ");         
+        }
+        return sbuf.toString();
+    }    
+}
+
+class MessageSerializer implements ICompactSerializer<Message>
+{
+    public void serialize(Message t, DataOutputStream dos) throws IOException
+    {
+        Header.serializer().serialize( t.header_, dos);
+        byte[] bytes = (byte[])t.getMessageBody()[0];
+        dos.writeInt(bytes.length);
+        dos.write(bytes);
+    }
+
+    public Message deserialize(DataInputStream dis) throws IOException
+    {
+        Header header = Header.serializer().deserialize(dis);
+        int size = dis.readInt();
+        byte[] bytes = new byte[size];
+        dis.readFully(bytes);
+        // return new Message(header.getMessageId(), header.getFrom(), header.getMessageType(), header.getVerb(), new Object[]{bytes});
+        return new Message(header, new Object[]{bytes});
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageDeliveryTask.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageDeliveryTask.java
index e69de29b..6f97e20a 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageDeliveryTask.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageDeliveryTask.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import org.apache.cassandra.continuations.Suspendable;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MessageDeliveryTask implements Runnable
+{
+    private Message message_;
+    private static Logger logger_ = Logger.getLogger(MessageDeliveryTask.class);    
+    
+    public MessageDeliveryTask(Message message)
+    {
+        message_ = message;    
+    }
+    
+    public void run()
+    { 
+        try
+        {            
+            String verb = message_.getVerb();                               
+            IVerbHandler verbHandler = MessagingService.getMessagingInstance().getVerbHandler(verb);           
+            if ( verbHandler != null )
+            {
+                verbHandler.doVerb(message_);        
+            }
+        }
+        catch (Throwable th)
+        {
+            logger_.warn( LogUtil.throwableToString(th) );
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageDeserializationTask.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageDeserializationTask.java
index e69de29b..4ffbfb16 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageDeserializationTask.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageDeserializationTask.java
@@ -0,0 +1,71 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.net.io.FastSerializer;
+import org.apache.cassandra.net.io.ISerializer;
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class MessageDeserializationTask implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger(MessageDeserializationTask.class); 
+    private static ISerializer serializer_ = new FastSerializer();
+    private int serializerType_;
+    private byte[] bytes_ = new byte[0];    
+    
+    MessageDeserializationTask(int serializerType, byte[] bytes)
+    {
+        serializerType_ = serializerType;
+        bytes_ = bytes;        
+    }
+    
+    public void run()
+    {
+    	/* For DEBUG only. Printing queue length */   
+    	DebuggableThreadPoolExecutor es = (DebuggableThreadPoolExecutor)MessagingService.getDeserilizationExecutor();
+        logger_.debug( "Message Deserialization Task: " + (es.getTaskCount() - es.getCompletedTaskCount()) );
+        /* END DEBUG */
+        try
+        {                        
+            Message message = (Message)serializer_.deserialize(bytes_);                                                           
+            
+            if ( message != null )
+            {
+                message = SinkManager.processServerMessageSink(message);
+                MessagingService.receive(message);                                                                                                    
+            }
+        }
+        catch ( IOException ex )
+        {            
+            logger_.warn(LogUtil.throwableToString(ex));              
+        }
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageSerializationTask.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageSerializationTask.java
index e69de29b..43127636 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageSerializationTask.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessageSerializationTask.java
@@ -0,0 +1,106 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.net.SocketException;
+
+import org.apache.cassandra.concurrent.Context;
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadLocalContext;
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class MessageSerializationTask implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger(MessageSerializationTask.class);
+    private Message message_;
+    private EndPoint to_;    
+    
+    public MessageSerializationTask(Message message, EndPoint to)
+    {
+        message_ = message;
+        to_ = to;        
+    }
+    
+    public Message getMessage()
+    {
+        return message_;
+    }
+
+    public void run()
+    {        
+    	/* For DEBUG only. Printing queue length */   
+    	DebuggableThreadPoolExecutor es = (DebuggableThreadPoolExecutor)MessagingService.getWriteExecutor();
+        logger_.debug( "Message Serialization Task: " + (es.getTaskCount() - es.getCompletedTaskCount()) );
+        /* END DEBUG */
+        
+        /* Adding the message to be serialized in the TLS. For accessing in the afterExecute() */
+        Context ctx = new Context();
+        ctx.put(this.getClass().getName(), message_);
+        ThreadLocalContext.put(ctx);
+        
+        TcpConnection connection = null;
+        try
+        {
+            Message message = SinkManager.processClientMessageSink(message_);
+            if(null == message) 
+                return;
+            connection = MessagingService.getConnection(message_.getFrom(), to_);
+            connection.write(message);            
+        }            
+        catch ( SocketException se )
+        {            
+            // Shutting down the entire pool. May be too conservative an approach.
+            MessagingService.getConnectionPool(message_.getFrom(), to_).shutdown();
+            logger_.warn(LogUtil.throwableToString(se));
+        }
+        catch ( IOException e )
+        {
+            logConnectAndIOException(e, connection);
+        }
+        catch (Throwable th)
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }
+        finally
+        {
+            if ( connection != null )
+            {
+                connection.close();
+            }            
+        }
+    }
+    
+    private void logConnectAndIOException(IOException ex, TcpConnection connection)
+    {                    
+        if ( connection != null )
+        {
+            connection.errorClose();
+        }
+        logger_.warn(LogUtil.throwableToString(ex));
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingConfig.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingConfig.java
index e69de29b..6da286b9 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingConfig.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingConfig.java
@@ -0,0 +1,96 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MessagingConfig
+{
+    // The expected time for one message round trip.  It does not reflect message processing
+    // time at the receiver.
+    private static int expectedRoundTripTime_ = 400;
+    private static int numberOfPorts_ = 2;
+    private static int threadCount_ = 4;
+
+    public static int getMessagingThreadCount()
+    {
+        return threadCount_;
+    }
+
+    public static void setMessagingThreadCount(int threadCount)
+    {
+        threadCount_ = threadCount;
+    }
+
+    public static void setExpectedRoundTripTime(int roundTripTimeMillis) {
+    	if(roundTripTimeMillis > 0 )
+    		expectedRoundTripTime_ = roundTripTimeMillis;
+    }
+
+    public static int getExpectedRoundTripTime()
+    {
+        return expectedRoundTripTime_;
+    }
+
+    public static int getConnectionPoolInitialSize()
+    {
+        return ConnectionPoolConfiguration.initialSize_;
+    }
+
+    public static int getConnectionPoolGrowthFactor()
+    {
+        return ConnectionPoolConfiguration.growthFactor_;
+    }
+
+    public static int getConnectionPoolMaxSize()
+    {
+        return ConnectionPoolConfiguration.maxSize_;
+    }
+
+    public static int getConnectionPoolWaitTimeout()
+    {
+        return ConnectionPoolConfiguration.waitTimeout_;
+    }
+
+    public static int getConnectionPoolMonitorInterval()
+    {
+        return ConnectionPoolConfiguration.monitorInterval_;
+    }
+
+    public static void setNumberOfPorts(int n)
+    {
+        numberOfPorts_ = n;
+    }
+
+    public static int getNumberOfPorts()
+    {
+        return numberOfPorts_;
+    }
+}
+
+class ConnectionPoolConfiguration
+{
+    public static int initialSize_ = 1;
+    public static int growthFactor_ = 1;
+    public static int maxSize_ = 1;
+    public static int waitTimeout_ = 10;
+    public static int monitorInterval_ = 300;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
index e69de29b..30190925 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
@@ -0,0 +1,795 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.*;
+import java.lang.management.ManagementFactory;
+import java.net.*;
+import java.security.MessageDigest;
+import java.util.*;
+import java.nio.ByteBuffer;
+import java.util.concurrent.*;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.ReentrantLock;
+import java.nio.channels.*;
+import org.apache.cassandra.concurrent.*;
+import org.apache.cassandra.net.io.*;
+import org.apache.cassandra.utils.*;
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+import javax.xml.bind.*;
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.IStage;
+import org.apache.cassandra.concurrent.MultiThreadedStage;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.http.HttpConnectionHandler;
+import org.apache.cassandra.net.io.SerializerType;
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.utils.Cachetable;
+import org.apache.cassandra.utils.GuidGenerator;
+import org.apache.cassandra.utils.HashingSchemes;
+import org.apache.cassandra.utils.ICachetable;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MessagingService implements IMessagingService, MessagingServiceMBean
+{
+    private static boolean debugOn_ = false;   
+    
+    private static int version_ = 1;
+    //TODO: make this parameter dynamic somehow.  Not sure if config is appropriate.
+    private static SerializerType serializerType_ = SerializerType.BINARY;
+    
+    private static byte[] protocol_ = new byte[16];
+    /* Verb Handler for the Response */
+    public static final String responseVerbHandler_ = "RESPONSE";
+    /* Stage for responses. */
+    public static final String responseStage_ = "RESPONSE-STAGE";
+    private enum ReservedVerbs_ {RESPONSE};
+    
+    private static Map<String, String> reservedVerbs_ = new Hashtable<String, String>();
+    /* Indicate if we are currently streaming data to another node or receiving streaming data */
+    private static AtomicBoolean isStreaming_ = new AtomicBoolean(false);
+    
+    /* This records all the results mapped by message Id */
+    private static ICachetable<String, IAsyncCallback> callbackMap_;
+    private static ICachetable<String, IAsyncResult> taskCompletionMap_;
+    
+    /* Manages the table of endpoints it is listening on */
+    private static Set<EndPoint> endPoints_;
+    
+    /* List of sockets we are listening on */
+    private static Map<EndPoint, SelectionKey> listenSockets_ = new HashMap<EndPoint, SelectionKey>();
+    
+    /* Lookup table for registering message handlers based on the verb. */
+    private static Map<String, IVerbHandler> verbHandlers_;
+    
+    private static Map<String, MulticastSocket> mCastMembership_ = new HashMap<String, MulticastSocket>();
+    
+    /* Thread pool to handle messaging read activities of Socket and default stage */
+    private static ExecutorService messageDeserializationExecutor_;
+    
+    /* Thread pool to handle messaging write activities */
+    private static ExecutorService messageSerializerExecutor_;
+    
+    /* Thread pool to handle deserialization of messages read from the socket. */
+    private static ExecutorService messageDeserializerExecutor_;
+    
+    /* Thread pool to handle messaging write activities */
+    private static ExecutorService streamExecutor_;
+    
+    private final static ReentrantLock lock_ = new ReentrantLock();
+    private static Map<String, TcpConnectionManager> poolTable_ = new Hashtable<String, TcpConnectionManager>();
+    
+    private static boolean bShutdown_ = false;
+    
+    private static Logger logger_ = Logger.getLogger(MessagingService.class);
+    
+    private static IMessagingService messagingService_ = new MessagingService();
+    
+    public static boolean isDebugOn()
+    {
+        return debugOn_;
+    }
+    
+    public static void debugOn(boolean on)
+    {
+        debugOn_ = on;
+    }
+    
+    public static SerializerType getSerializerType()
+    {
+        return serializerType_;
+    }
+    
+    public synchronized static void serializerType(String type)
+    { 
+        if ( type.equalsIgnoreCase("binary") )
+        {
+            serializerType_ = SerializerType.BINARY;
+        }
+        else if ( type.equalsIgnoreCase("java") )
+        {
+            serializerType_ = SerializerType.JAVA;
+        }
+        else if ( type.equalsIgnoreCase("xml") )
+        {
+            serializerType_ = SerializerType.XML;
+        }
+    }
+    
+    public static int getVersion()
+    {
+        return version_;
+    }
+    
+    public static void setVersion(int version)
+    {
+        version_ = version;
+    }
+    
+    public static IMessagingService getMessagingInstance()
+    {   
+    	if ( bShutdown_ )
+    	{
+            lock_.lock();
+            try
+            {
+                if ( bShutdown_ )
+                {
+            		messagingService_ = new MessagingService();
+            		bShutdown_ = false;
+                }
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+    	}
+        return messagingService_;
+    }
+    
+    public Object clone() throws CloneNotSupportedException
+    {
+        //Prevents the singleton from being cloned
+        throw new CloneNotSupportedException();
+    }
+
+    protected MessagingService()
+    {        
+        for ( ReservedVerbs_ verbs : ReservedVerbs_.values() )
+        {
+            reservedVerbs_.put(verbs.toString(), verbs.toString());
+        }
+        verbHandlers_ = new HashMap<String, IVerbHandler>();        
+        endPoints_ = new HashSet<EndPoint>();
+        /*
+         * Leave callbacks in the cachetable long enough that any related messages will arrive
+         * before the callback is evicted from the table. The concurrency level is set at 128
+         * which is the sum of the threads in the pool that adds shit into the table and the 
+         * pool that retrives the callback from here.
+        */ 
+        int maxSize = MessagingConfig.getMessagingThreadCount();
+        callbackMap_ = new Cachetable<String, IAsyncCallback>( 2 * DatabaseDescriptor.getRpcTimeout() );
+        taskCompletionMap_ = new Cachetable<String, IAsyncResult>( 2 * DatabaseDescriptor.getRpcTimeout() );        
+        
+        messageDeserializationExecutor_ = new DebuggableThreadPoolExecutor( maxSize,
+                maxSize,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl("MESSAGING-SERVICE-POOL")
+                );
+                
+        messageSerializerExecutor_ = new DebuggableThreadPoolExecutor( maxSize,
+                maxSize,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl("MESSAGE-SERIALIZER-POOL")
+                ); 
+        
+        messageDeserializerExecutor_ = new DebuggableThreadPoolExecutor( maxSize,
+                maxSize,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl("MESSAGE-DESERIALIZER-POOL")
+                ); 
+        
+        streamExecutor_ = new DebuggableThreadPoolExecutor( 1,
+                1,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl("MESSAGE-STREAMING-POOL")
+                ); 
+                
+        protocol_ = hash(HashingSchemes.MD5, "FB-MESSAGING".getBytes());        
+        /* register the response verb handler */
+        registerVerbHandlers(MessagingService.responseVerbHandler_, new ResponseVerbHandler());
+        /* register stage for response */
+        StageManager.registerStage(MessagingService.responseStage_, new MultiThreadedStage("RESPONSE-STAGE", maxSize) );
+    }
+    
+    public byte[] hash(String type, byte data[])
+    {
+        byte result[] = null;
+        try
+        {
+            MessageDigest messageDigest = MessageDigest.getInstance(type);
+            result = messageDigest.digest(data);
+        }
+        catch(Exception e)
+        {
+            LogUtil.getLogger(MessagingService.class.getName()).debug(LogUtil.throwableToString(e));
+        }
+        return result;
+    }
+    
+    public long getMessagingSerializerTaskCount()
+    {
+        DebuggableThreadPoolExecutor dstp = (DebuggableThreadPoolExecutor)messageSerializerExecutor_;        
+        return dstp.getTaskCount() - dstp.getCompletedTaskCount();
+    }
+    
+    public long getMessagingReceiverTaskCount()
+    {
+        DebuggableThreadPoolExecutor dstp = (DebuggableThreadPoolExecutor)messageDeserializationExecutor_;        
+        return dstp.getTaskCount() - dstp.getCompletedTaskCount(); 
+    }
+    
+    public void listen(EndPoint localEp, boolean isHttp) throws IOException
+    {        
+        ServerSocketChannel serverChannel = ServerSocketChannel.open();
+        ServerSocket ss = serverChannel.socket();            
+        ss.bind(localEp.getInetAddress());
+        serverChannel.configureBlocking(false);
+        
+        SelectionKeyHandler handler = null;
+        if ( isHttp )
+        {                
+            handler = new HttpConnectionHandler();
+        }
+        else
+        {
+            handler = new TcpConnectionHandler(localEp);
+        }
+        
+        SelectionKey key = SelectorManager.getSelectorManager().register(serverChannel, handler, SelectionKey.OP_ACCEPT);          
+        endPoints_.add(localEp);            
+        listenSockets_.put(localEp, key);             
+    }
+    
+    public void listenUDP(EndPoint localEp)
+    {
+        UdpConnection connection = new UdpConnection();
+        logger_.debug("Starting to listen on " + localEp);
+        try
+        {
+            connection.init(localEp.getPort());
+            endPoints_.add(localEp);     
+        }
+        catch ( IOException e )
+        {
+            logger_.warn(LogUtil.throwableToString(e));
+        }
+    }
+    
+    public static TcpConnectionManager getConnectionPool(EndPoint from, EndPoint to)
+    {
+        String key = from + ":" + to;
+        TcpConnectionManager cp = poolTable_.get(key);
+        if( cp == null )
+        {
+            lock_.lock();
+            try
+            {
+                cp = poolTable_.get(key);
+                if (cp == null )
+                {
+                    cp = new TcpConnectionManager(MessagingConfig.getConnectionPoolInitialSize(), 
+                            MessagingConfig.getConnectionPoolGrowthFactor(), 
+                            MessagingConfig.getConnectionPoolMaxSize(), from, to);
+                    poolTable_.put(key, cp);
+                }
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return cp;
+    }
+
+    public static ConnectionStatistics[] getPoolStatistics()
+    {
+        Set<ConnectionStatistics> stats = new HashSet<ConnectionStatistics>();        
+        Iterator<TcpConnectionManager> it = poolTable_.values().iterator();
+        while ( it.hasNext() )
+        {
+            TcpConnectionManager cp = it.next();
+            ConnectionStatistics cs = new ConnectionStatistics(cp.getLocalEndPoint(), cp.getRemoteEndPoint(), cp.getPoolSize(), cp.getConnectionsInUse());
+            stats.add( cs );
+        }
+        return stats.toArray(new ConnectionStatistics[0]);
+    }
+    
+    public static TcpConnection getConnection(EndPoint from, EndPoint to) throws IOException
+    {
+        return getConnectionPool(from, to).getConnection();
+    }
+    
+    private void checkForReservedVerb(String type)
+    {
+    	if ( reservedVerbs_.get(type) != null && verbHandlers_.get(type) != null )
+    	{
+    		throw new IllegalArgumentException( type + " is a reserved verb handler. Scram!");
+    	}
+    }     
+    
+    public void registerVerbHandlers(String type, IVerbHandler verbHandler)
+    {
+    	checkForReservedVerb(type);
+    	verbHandlers_.put(type, verbHandler);
+    }
+    
+    public void deregisterAllVerbHandlers(EndPoint localEndPoint)
+    {
+        Iterator keys = verbHandlers_.keySet().iterator();
+        String key = null;
+        
+        /*
+         * endpoint specific verbhandlers can be distinguished because
+         * their key's contain the name of the endpoint. 
+         */
+        while(keys.hasNext())
+        {
+            key = (String)keys.next();
+            if (key.contains(localEndPoint.toString()))
+                keys.remove();
+        }
+    }
+    
+    public void deregisterVerbHandlers(String type)
+    {
+        verbHandlers_.remove(type);
+    }
+
+    public IVerbHandler getVerbHandler(String type)
+    {
+        IVerbHandler handler = (IVerbHandler)verbHandlers_.get(type);
+        return handler;
+    }
+
+    public String sendRR(Message message, EndPoint[] to, IAsyncCallback cb)
+    {
+        String messageId = message.getMessageId();                        
+        callbackMap_.put(messageId, cb);
+        for ( int i = 0; i < to.length; ++i )
+        {
+            sendOneWay(message, to[i]);
+        }
+        return messageId;
+    }
+    
+    public String sendRR(Message message, EndPoint to, IAsyncCallback cb)
+    {        
+        String messageId = message.getMessageId();
+        callbackMap_.put(messageId, cb);
+        sendOneWay(message, to);
+        return messageId;
+    }
+
+    public String sendRR(Message[] messages, EndPoint[] to, IAsyncCallback cb)
+    {
+        if ( messages.length != to.length )
+        {
+            throw new IllegalArgumentException("Number of messages and the number of endpoints need to be same.");
+        }
+        String groupId = GuidGenerator.guid();
+        callbackMap_.put(groupId, cb);
+        for ( int i = 0; i < messages.length; ++i )
+        {
+            messages[i].setMessageId(groupId);
+            sendOneWay(messages[i], to[i]);
+        }
+        return groupId;
+    } 
+    
+    public IAsyncResult sendRR(Message[] messages, EndPoint[] to)
+    {
+        if ( messages.length != to.length )
+        {
+            throw new IllegalArgumentException("Number of messages and the number of endpoints need to be same.");
+        }
+        
+        IAsyncResult iar = new MultiAsyncResult(messages.length);
+        String groupId = GuidGenerator.guid();
+        taskCompletionMap_.put(groupId, iar);
+        for ( int i = 0; i < messages.length; ++i )
+        {
+            messages[i].setMessageId(groupId);
+            sendOneWay(messages[i], to[i]);
+        }
+        
+        return iar;
+    }
+    
+    public String sendRR(Message[][] messages, EndPoint[][] to, IAsyncCallback cb)
+    {
+        if ( messages.length != to.length )
+        {
+            throw new IllegalArgumentException("Number of messages and the number of endpoints need to be same.");
+        }
+        
+        int length = messages.length;
+        String[] gids = new String[length];
+        /* Generate the requisite GUID's */
+        for ( int i = 0; i < length; ++i )
+        {
+            gids[i] = GuidGenerator.guid();
+        }
+        /* attach this context to the callback */
+        cb.attachContext(gids);
+        for ( int i = 0; i < length; ++i )
+        {
+            callbackMap_.put(gids[i], cb);
+            for ( int j = 0; j < messages[i].length; ++j )
+            {
+                messages[i][j].setMessageId(gids[i]);
+                sendOneWay(messages[i][j], to[i][j]);
+            }            
+        }      
+        return gids[0];
+    }
+
+    /*
+        Use this version for fire and forget style messaging.
+    */
+    public void sendOneWay(Message message, EndPoint to)
+    {        
+        // do local deliveries        
+        if ( message.getFrom().equals(to) )
+        {            
+            MessagingService.receive(message);
+            return;
+        }
+        
+        Runnable tcpWriteEvent = new MessageSerializationTask(message, to);
+        messageSerializerExecutor_.execute(tcpWriteEvent);    
+    }
+    
+    public IAsyncResult sendRR(Message message, EndPoint to)
+    {
+        IAsyncResult iar = new AsyncResult();
+        taskCompletionMap_.put(message.getMessageId(), iar);
+        sendOneWay(message, to);
+        return iar;
+    }
+    
+    public void sendUdpOneWay(Message message, EndPoint to)
+    {
+        EndPoint from = message.getFrom();              
+        if (message.getFrom().equals(to)) {
+            MessagingService.receive(message);
+            return;
+        }
+        
+        UdpConnection connection = null;
+        try
+        {
+            connection = new UdpConnection(); 
+            connection.init();            
+            connection.write(message, to);            
+        }            
+        catch ( IOException e )
+        {               
+            logger_.warn(LogUtil.throwableToString(e));
+        } 
+        finally
+        {
+            if ( connection != null )
+                connection.close();
+        }
+    }
+    
+    public void stream(String file, long startPosition, long total, EndPoint from, EndPoint to)
+    {
+        isStreaming_.set(true);
+        /* Streaming asynchronously on streamExector_ threads. */
+        Runnable streamingTask = new FileStreamTask(file, startPosition, total, from, to);
+        streamExecutor_.execute(streamingTask);
+    }
+    
+    /*
+     * Does the application determine if we are currently streaming data.
+     * This would imply either streaming to a receiver, receiving streamed
+     * data or both. 
+    */
+    public static boolean isStreaming()
+    {
+        return isStreaming_.get();
+    }
+    
+    public static void setStreamingMode(boolean bVal)
+    {
+        isStreaming_.set(bVal);
+    }
+    
+    public static void shutdown()
+    {
+        logger_.info("Shutting down ...");
+        synchronized ( MessagingService.class )
+        {          
+            /* Stop listening on any socket */            
+            for( SelectionKey skey : listenSockets_.values() )
+            {
+                skey.cancel();
+                try
+                {
+                    skey.channel().close();
+                }
+                catch (IOException e) {}
+            }
+            listenSockets_.clear();
+            
+            /* Shutdown the threads in the EventQueue's */            
+            messageDeserializationExecutor_.shutdownNow();            
+            messageSerializerExecutor_.shutdownNow();
+            messageDeserializerExecutor_.shutdownNow();
+            streamExecutor_.shutdownNow();
+            
+            /* shut down the cachetables */
+            taskCompletionMap_.shutdown();
+            callbackMap_.shutdown();                        
+                        
+            /* Interrupt the selector manager thread */
+            SelectorManager.getSelectorManager().interrupt();
+            
+            poolTable_.clear();            
+            verbHandlers_.clear();                                    
+            bShutdown_ = true;
+        }
+        logger_.debug("Shutdown invocation complete.");
+    }
+
+    public static void receive(Message message)
+    {        
+        enqueueRunnable(message.getMessageType(), new MessageDeliveryTask(message));
+    }
+    
+    public static boolean isLocalEndPoint(EndPoint ep)
+    {
+        return ( endPoints_.contains(ep) );
+    }
+        
+    private static void enqueueRunnable(String stageName, Runnable runnable){
+        
+        IStage stage = StageManager.getStage(stageName);   
+        
+        if ( stage != null )
+        {
+            logger_.info("Running on stage " + stage.getName());
+            stage.execute(runnable);
+        } 
+        else
+        {
+            logger_.info("Running on default stage - beware");
+            messageSerializerExecutor_.execute(runnable);
+        }
+    }    
+    
+    public static IAsyncCallback getRegisteredCallback(String key)
+    {
+        return callbackMap_.get(key);
+    }
+    
+    public static void removeRegisteredCallback(String key)
+    {
+        callbackMap_.remove(key);
+    }
+    
+    public static IAsyncResult getAsyncResult(String key)
+    {
+        return taskCompletionMap_.remove(key);
+    }
+    
+    public static void removeAsyncResult(String key)
+    {
+        taskCompletionMap_.remove(key);
+    }
+
+    public static byte[] getProtocol()
+    {
+        return protocol_;
+    }
+    
+    public static ExecutorService getReadExecutor()
+    {
+        return messageDeserializationExecutor_;
+    }
+    
+    public static ExecutorService getWriteExecutor()
+    {
+        return messageSerializerExecutor_;
+    }
+    
+    public static ExecutorService getDeserilizationExecutor()
+    {
+        return messageDeserializerExecutor_;
+    }
+
+    public static boolean isProtocolValid(byte[] protocol)
+    {
+        return isEqual(protocol_, protocol);
+    }
+    
+    public static boolean isEqual(byte digestA[], byte digestB[])
+    {
+        return MessageDigest.isEqual(digestA, digestB);
+    }
+
+    public static byte[] toByteArray(int i)
+    {
+        byte bytes[] = new byte[4];
+        bytes[0] = (byte)(i >>> 24 & 0xff);
+        bytes[1] = (byte)(i >>> 16 & 0xff);
+        bytes[2] = (byte)(i >>> 8 & 0xff);
+        bytes[3] = (byte)(i & 0xff);
+        return bytes;
+    }
+    
+    public static byte[] toByteArray(short s)
+    {
+        byte bytes[] = new byte[2];
+        bytes[0] = (byte)(s >>> 8 & 0xff);
+        bytes[1] = (byte)(s & 0xff);
+        return bytes;
+    }
+    
+    public static short byteArrayToShort(byte bytes[])
+    {
+        return byteArrayToShort(bytes, 0);
+    }
+    
+    public static short byteArrayToShort(byte bytes[], int offset)
+    {
+        if(bytes.length - offset < 2)
+            throw new IllegalArgumentException("A short must be 2 bytes in size.");
+        short n = 0;
+        for(int i = 0; i < 2; i++)
+        {
+            n <<= 8;
+            n |= bytes[offset + i] & 0xff;
+        }
+
+        return n;
+    }
+
+    public static int getBits(int x, int p, int n)
+    {
+        return x >>> (p + 1) - n & ~(-1 << n);
+    }
+    
+    public static int byteArrayToInt(byte bytes[])
+    {
+        return byteArrayToInt(bytes, 0);
+    }
+
+    public static int byteArrayToInt(byte bytes[], int offset)
+    {
+        if(bytes.length - offset < 4)
+            throw new IllegalArgumentException("An integer must be 4 bytes in size.");
+        int n = 0;
+        for(int i = 0; i < 4; i++)
+        {
+            n <<= 8;
+            n |= bytes[offset + i] & 0xff;
+        }
+
+        return n;
+    }
+    
+    public static ByteBuffer packIt(byte[] bytes, boolean compress, boolean stream, boolean listening)
+    {
+        byte[] size = toByteArray(bytes.length);
+        /* 
+             Setting up the protocol header. This is 4 bytes long
+             represented as an integer. The first 2 bits indicate
+             the serializer type. The 3rd bit indicates if compression
+             is turned on or off. It is turned off by default. The 4th
+             bit indicates if we are in streaming mode. It is turned off
+             by default. The 5th bit is used to indicate that the sender
+             is not listening on any well defined port. This implies the 
+             receiver needs to cache the connection using the port on the 
+             socket. The following 3 bits are reserved for future use. 
+             The next 8 bits indicate a version number. Remaining 15 bits 
+             are not used currently.            
+        */
+        int n = 0;
+        // Setting up the serializer bit
+        n |= serializerType_.ordinal();
+        // set compression bit.
+        if ( compress )
+            n |= 4;
+        
+        // set streaming bit
+        if ( stream )
+            n |= 8;
+        
+        // set listening 5th bit
+        if ( listening )
+            n |= 16;
+        
+        // Setting up the version bit 
+        n |= (version_ << 8);               
+        /* Finished the protocol header setup */
+               
+        byte[] header = toByteArray(n);
+        ByteBuffer buffer = ByteBuffer.allocate(16 + header.length + size.length + bytes.length);
+        buffer.put(protocol_);
+        buffer.put(header);
+        buffer.put(size);
+        buffer.put(bytes);
+        buffer.flip();
+        return buffer;
+    }
+        
+    public static ByteBuffer constructStreamHeader(boolean compress, boolean stream)
+    {
+        /* 
+        Setting up the protocol header. This is 4 bytes long
+        represented as an integer. The first 2 bits indicate
+        the serializer type. The 3rd bit indicates if compression
+        is turned on or off. It is turned off by default. The 4th
+        bit indicates if we are in streaming mode. It is turned off
+        by default. The following 4 bits are reserved for future use. 
+        The next 8 bits indicate a version number. Remaining 15 bits 
+        are not used currently.            
+        */
+        int n = 0;
+        // Setting up the serializer bit
+        n |= serializerType_.ordinal();
+        // set compression bit.
+        if ( compress )
+            n |= 4;
+       
+        // set streaming bit
+        if ( stream )
+            n |= 8;
+       
+        // Setting up the version bit 
+        n |= (version_ << 8);              
+        /* Finished the protocol header setup */
+              
+        byte[] header = toByteArray(n);
+        ByteBuffer buffer = ByteBuffer.allocate(16 + header.length);
+        buffer.put(protocol_);
+        buffer.put(header);
+        buffer.flip();
+        return buffer;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingServiceMBean.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingServiceMBean.java
index e69de29b..69c47bd8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingServiceMBean.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MessagingServiceMBean.java
@@ -0,0 +1,29 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface MessagingServiceMBean
+{   
+    public long getMessagingSerializerTaskCount();
+    public long getMessagingReceiverTaskCount();
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MultiAsyncResult.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MultiAsyncResult.java
index e69de29b..6918705f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MultiAsyncResult.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/MultiAsyncResult.java
@@ -0,0 +1,133 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+public class MultiAsyncResult implements IAsyncResult
+{
+    private static Logger logger_ = Logger.getLogger( AsyncResult.class );
+    private int expectedResults_;
+    private List<Object[]> result_ = new ArrayList<Object[]>();    
+    private AtomicBoolean done_ = new AtomicBoolean(false);
+    private Lock lock_ = new ReentrantLock();
+    private Condition condition_;
+    
+    MultiAsyncResult(int expectedResults)
+    {
+        expectedResults_ = expectedResults;
+        condition_ = lock_.newCondition();
+    }
+    
+    public Object[] get()
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
+    }
+    
+    public Object[] get(long timeout, TimeUnit tu) throws TimeoutException
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
+    }
+    
+    public List<Object[]> multiget()
+    {
+        lock_.lock();
+        try
+        {
+            if ( !done_.get() )
+            {
+                condition_.await();                    
+            }
+        }
+        catch ( InterruptedException ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+        finally
+        {
+            lock_.unlock();            
+        }        
+        return result_;
+    }
+    
+    public boolean isDone()
+    {
+        return done_.get();
+    }
+    
+    public List<Object[]> multiget(long timeout, TimeUnit tu) throws TimeoutException
+    {
+        lock_.lock();
+        try
+        {            
+            boolean bVal = true;
+            try
+            {
+                if ( !done_.get() )
+                {                    
+                    bVal = condition_.await(timeout, tu);
+                }
+            }
+            catch ( InterruptedException ex )
+            {
+                logger_.warn( LogUtil.throwableToString(ex) );
+            }
+            
+            if ( !bVal && !done_.get() )
+            {                                           
+                throw new TimeoutException("Operation timed out.");
+            }
+        }
+        finally
+        {
+            lock_.unlock();      
+        }
+        return result_;
+    }
+    
+    public void result(Message result)
+    {        
+        try
+        {
+            lock_.lock();
+            if ( !done_.get() )
+            {
+                result_.add(result.getMessageBody());
+                if ( result_.size() == expectedResults_ )
+                {
+                    done_.set(true);
+                    condition_.signal();
+                }
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }        
+    }    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ProtocolHeader.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ProtocolHeader.java
index e69de29b..621770ab 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ProtocolHeader.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ProtocolHeader.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class ProtocolHeader
+{
+    public static final String SERIALIZER = "SERIALIZER";
+    public static final String COMPRESSION = "COMPRESSION";
+    public static final String VERSION = "VERSION";
+    
+    public int serializerType_;
+    public boolean isCompressed_;
+    public boolean isStreamingMode_;
+    public boolean isListening_;
+    public int version_;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ResponseVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ResponseVerbHandler.java
index e69de29b..55a48bae 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ResponseVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/ResponseVerbHandler.java
@@ -0,0 +1,50 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class ResponseVerbHandler implements IVerbHandler
+{
+    private static final Logger logger_ = Logger.getLogger( ResponseVerbHandler.class );
+    
+    public void doVerb(Message message)
+    {     
+        String messageId = message.getMessageId();        
+        IAsyncCallback cb = MessagingService.getRegisteredCallback(messageId);
+        if ( cb != null )
+        {
+            logger_.info("Processing response on a callback from " + message.getFrom());
+            cb.response(message);
+        }
+        else
+        {            
+            IAsyncResult ar = MessagingService.getAsyncResult(messageId);
+            if ( ar != null )
+            {
+                logger_.info("Processing response on an async result from " + message.getFrom());
+                ar.result(message);
+            }
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/SelectionKeyHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/SelectionKeyHandler.java
index e69de29b..7cc53837 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/SelectionKeyHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/SelectionKeyHandler.java
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.nio.channels.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SelectionKeyHandler 
+{
+    /**
+     * Method which is called when the key becomes acceptable.
+     *
+     * @param key The key which is acceptable.
+     */
+    public void accept(SelectionKey key)
+    {
+         throw new UnsupportedOperationException("accept() cannot be called on " + getClass().getName() + "!");
+    }
+    
+    /**
+     * Method which is called when the key becomes connectable.
+     *
+     * @param key The key which is connectable.
+     */
+    public void connect(SelectionKey key)
+    {
+        throw new UnsupportedOperationException("connect() cannot be called on " + getClass().getName() + "!");
+    }
+    
+    /**
+     * Method which is called when the key becomes readable.
+     *
+     * @param key The key which is readable.
+     */
+    public void read(SelectionKey key)
+    {
+        throw new UnsupportedOperationException("read() cannot be called on " + getClass().getName() + "!");
+    }
+    
+    /**
+     * Method which is called when the key becomes writable.
+     *
+     * @param key The key which is writable.
+     */
+    public void write(SelectionKey key)
+    {
+        throw new UnsupportedOperationException("write() cannot be called on " + getClass().getName() + "!");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/SelectorManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/SelectorManager.java
index e69de29b..c63087a5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/SelectorManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/SelectorManager.java
@@ -0,0 +1,172 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.nio.channels.SelectableChannel;
+import java.nio.channels.SelectionKey;
+import java.nio.channels.Selector;
+
+import org.apache.log4j.Logger;
+
+public class SelectorManager extends Thread
+{
+    private static final Logger logger = Logger.getLogger(SelectorManager.class); 
+
+    // the underlying selector used
+    protected Selector selector;
+
+    // The static selector manager which is used by all applications
+    private static SelectorManager manager;
+    
+    // The static UDP selector manager which is used by all applications
+    private static SelectorManager udpManager;
+
+    private SelectorManager(String name)
+    {
+        super(name);
+
+        try
+        {
+            selector = Selector.open();
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+
+        setDaemon(false);
+    }
+
+    /**
+     * Registers a new channel with the selector, and attaches the given
+     * SelectionKeyHandler as the handler for the newly created key. Operations
+     * which the hanlder is interested in will be called as available.
+     * 
+     * @param channel
+     *            The channel to regster with the selector
+     * @param handler
+     *            The handler to use for the callbacks
+     * @param ops
+     *            The initial interest operations
+     * @return The SelectionKey which uniquely identifies this channel
+     * @exception IOException if the channel is closed
+     */
+    public SelectionKey register(SelectableChannel channel,
+            SelectionKeyHandler handler, int ops) throws IOException
+    {
+        if ((channel == null) || (handler == null))
+        {
+            throw new NullPointerException();
+        }
+
+        return channel.register(selector, ops, handler);
+    }      
+
+    /**
+     * This method starts the socket manager listening for events. It is
+     * designed to be started when this thread's start() method is invoked.
+     */
+    public void run()
+    {
+        while (true)
+        {
+            try
+            {
+                selector.select(100);
+                doProcess();
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+
+    protected void doProcess() throws IOException
+    {
+        SelectionKey[] keys = selector.selectedKeys().toArray(new SelectionKey[0]);
+
+        for (SelectionKey key : keys)
+        {
+            selector.selectedKeys().remove(key);
+
+            synchronized (key)
+            {
+                SelectionKeyHandler skh = (SelectionKeyHandler) key.attachment();
+
+                if (skh != null)
+                {
+                    // accept
+                    if (key.isValid() && key.isAcceptable())
+                    {
+                        skh.accept(key);
+                    }
+
+                    // connect
+                    if (key.isValid() && key.isConnectable())
+                    {
+                        skh.connect(key);
+                    }
+
+                    // read
+                    if (key.isValid() && key.isReadable())
+                    {
+                        skh.read(key);
+                    }
+
+                    // write
+                    if (key.isValid() && key.isWritable())
+                    {
+                        skh.write(key);
+                    }
+                }
+            }
+        }
+    }
+
+    /**
+     * Returns the SelectorManager applications should use.
+     * 
+     * @return The SelectorManager which applications should use
+     */
+    public static SelectorManager getSelectorManager()
+    {
+        synchronized (SelectorManager.class)
+        {
+            if (manager == null)
+            {
+                manager = new SelectorManager("TCP Selector Manager");
+            }            
+        }
+        return manager;
+    }
+    
+    public static SelectorManager getUdpSelectorManager()
+    {
+        synchronized (SelectorManager.class)
+        {
+            if (udpManager == null)
+            {
+                udpManager = new SelectorManager("UDP Selector Manager");
+            }            
+        }
+        return udpManager;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnection.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnection.java
index e69de29b..5a5205fd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnection.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnection.java
@@ -0,0 +1,553 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.*;
+import java.net.InetSocketAddress;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.channels.SelectionKey;
+import java.nio.channels.SocketChannel;
+import java.util.*;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.io.FastSerializer;
+import org.apache.cassandra.net.io.ISerializer;
+import org.apache.cassandra.net.io.ProtocolState;
+import org.apache.cassandra.net.io.StartState;
+import org.apache.cassandra.net.io.TcpReader;
+import org.apache.cassandra.net.io.TcpReader.TcpReaderState;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.net.io.*;
+import org.apache.cassandra.net.sink.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TcpConnection extends SelectionKeyHandler implements Comparable
+{
+    // logging and profiling.
+    private static Logger logger_ = Logger.getLogger(TcpConnection.class);  
+    private static ISerializer serializer_ = new FastSerializer();
+    private SocketChannel socketChannel_;
+    private SelectionKey key_;
+    private TcpConnectionManager pool_;
+    private boolean isIncoming_ = false;       
+    private TcpReader tcpReader_;    
+    private ReadWorkItem readWork_ = new ReadWorkItem(); 
+    private List<ByteBuffer> pendingWrites_ = new Vector<ByteBuffer>();  
+    private EndPoint localEp_;
+    private EndPoint remoteEp_;
+    boolean inUse_ = false;
+         
+    /* 
+     * Added for streaming support. We need the boolean
+     * to indicate that this connection is used for 
+     * streaming. The Condition and the Lock are used
+     * to signal the stream() that it can continue
+     * streaming when the socket becomes writable.
+    */
+    private boolean bStream_ = false;
+    private Lock lock_;
+    private Condition condition_;
+    
+    // used from getConnection - outgoing
+    TcpConnection(TcpConnectionManager pool, EndPoint from, EndPoint to) throws IOException
+    {          
+        socketChannel_ = SocketChannel.open();            
+        socketChannel_.configureBlocking(false);        
+        pool_ = pool;
+        
+        localEp_ = from;
+        remoteEp_ = to;
+        
+        if ( !socketChannel_.connect( remoteEp_.getInetAddress() ) )
+        {
+            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_CONNECT);
+        }
+        else
+        {
+            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
+        }
+    }
+    
+    /*
+     * Used for streaming purposes has no pooling semantics.
+    */
+    TcpConnection(EndPoint from, EndPoint to) throws IOException
+    {
+        socketChannel_ = SocketChannel.open();               
+        socketChannel_.configureBlocking(false);       
+        
+        localEp_ = from;
+        remoteEp_ = to;
+        
+        if ( !socketChannel_.connect( remoteEp_.getInetAddress() ) )
+        {
+            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_CONNECT);
+        }
+        else
+        {
+            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
+        }
+        bStream_ = true;
+        lock_ = new ReentrantLock();
+        condition_ = lock_.newCondition();
+    }
+    
+    /*
+     * This method is invoked by the TcpConnectionHandler to accept incoming TCP connections.
+     * Accept the connection and then register interest for reads.
+    */
+    static void acceptConnection(SocketChannel socketChannel, EndPoint localEp, boolean isIncoming) throws IOException
+    {
+        TcpConnection tcpConnection = new TcpConnection(socketChannel, localEp, true);
+        tcpConnection.registerReadInterest();
+    }
+    
+    private void registerReadInterest() throws IOException
+    {
+        key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
+    }
+    
+    // used for incoming connections
+    TcpConnection(SocketChannel socketChannel, EndPoint localEp, boolean isIncoming) throws IOException
+    {       
+        socketChannel_ = socketChannel;
+        socketChannel_.configureBlocking(false);                           
+        isIncoming_ = isIncoming;
+        localEp_ = localEp;
+    }
+    
+    EndPoint getLocalEp()
+    {
+        return localEp_;
+    }
+    
+    public void setLocalEp(EndPoint localEp)
+    {
+        localEp_ = localEp;
+    }
+
+    public EndPoint getEndPoint() 
+    {
+        return remoteEp_;
+    }
+    
+    public boolean isIncoming()
+    {
+        return isIncoming_;
+    }    
+    
+    public SocketChannel getSocketChannel()
+    {
+        return socketChannel_;
+    }    
+    
+    public void write(Message message) throws IOException
+    {           
+        byte[] data = serializer_.serialize(message);        
+        if ( data.length > 0 )
+        {    
+            boolean listening = ( message.getFrom().equals(EndPoint.randomLocalEndPoint_) ) ? false : true;
+            ByteBuffer buffer = MessagingService.packIt( data , false, false, listening);   
+            synchronized(this)
+            {
+                if (!pendingWrites_.isEmpty() || !socketChannel_.isConnected())
+                {                     
+                    pendingWrites_.add(buffer);                
+                    return;
+                }
+                
+                logger_.debug("Sending packets of size " + data.length);            
+                socketChannel_.write(buffer);                
+                
+                if (buffer.remaining() > 0) 
+                {                   
+                    pendingWrites_.add(buffer);
+                    key_.interestOps(key_.interestOps() | SelectionKey.OP_WRITE);
+                }
+            }
+        }
+    }
+    
+    public void stream(File file, long startPosition, long endPosition) throws IOException
+    {
+        if ( !bStream_ )
+            throw new IllegalStateException("Cannot stream since we are not set up to stream data.");
+                
+        lock_.lock();        
+        try
+        {            
+            /* transfer 64MB in each attempt */
+            int limit = 64*1024*1024;  
+            long total = endPosition - startPosition;
+            /* keeps track of total number of bytes transferred */
+            long bytesWritten = 0L;                          
+            RandomAccessFile raf = new RandomAccessFile(file, "r");            
+            FileChannel fc = raf.getChannel();            
+            
+            /* 
+             * If the connection is not yet established then wait for
+             * the timeout period of 2 seconds. Attempt to reconnect 3 times and then 
+             * bail with an IOException.
+            */
+            long waitTime = 2;
+            int retry = 0;
+            while (!socketChannel_.isConnected())
+            {
+                if ( retry == 3 )
+                    throw new IOException("Unable to connect to " + remoteEp_ + " after " + retry + " attempts.");
+                waitToContinueStreaming(waitTime, TimeUnit.SECONDS);
+                ++retry;
+            }
+            
+            while ( bytesWritten < total )
+            {                                
+                if ( startPosition == 0 )
+                {
+                    ByteBuffer buffer = MessagingService.constructStreamHeader(false, true);                      
+                    socketChannel_.write(buffer);
+                    handleIncompleteWrite(buffer);
+                }
+                
+                /* returns the number of bytes transferred from file to the socket */
+                long bytesTransferred = fc.transferTo(startPosition, limit, socketChannel_);
+                logger_.debug("Bytes transferred " + bytesTransferred);                
+                bytesWritten += bytesTransferred;
+                startPosition += bytesTransferred; 
+                /*
+                 * If the number of bytes transferred is less than intended 
+                 * then we need to wait till socket becomes writeable again. 
+                */
+                if ( bytesTransferred < limit && bytesWritten != total )
+                {                    
+                    key_.interestOps(key_.interestOps() | SelectionKey.OP_WRITE);
+                    waitToContinueStreaming();
+                }
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }        
+    }
+    
+    private void handleIncompleteWrite(ByteBuffer buffer)
+    {
+        if (buffer.remaining() > 0) 
+        {            
+            pendingWrites_.add(buffer);
+            key_.interestOps(key_.interestOps() | SelectionKey.OP_WRITE);
+            waitToContinueStreaming();
+        }     
+    }
+    
+    private void waitToContinueStreaming()
+    {
+        try
+        {
+            condition_.await();
+        }
+        catch ( InterruptedException ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+    }
+    
+    private void waitToContinueStreaming(long waitTime, TimeUnit tu)
+    {
+        try
+        {
+            condition_.await(waitTime, tu);
+        }
+        catch ( InterruptedException ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+    }
+    
+    private void resumeStreaming()
+    {
+        /* if not in streaming mode do nothing */
+        if ( !bStream_ )
+            return;
+        
+        lock_.lock();
+        try
+        {
+            condition_.signal();
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    public void close()
+    {
+        inUse_ = false;
+        if ( pool_.contains(this) )
+            pool_.decUsed();               
+    }
+
+    public boolean isConnected()
+    {
+        return socketChannel_.isConnected();
+    }
+    
+    public boolean equals(Object o)
+    {
+        if ( !(o instanceof TcpConnection) )
+            return false;
+        
+        TcpConnection rhs = (TcpConnection)o;        
+        if ( localEp_.equals(rhs.localEp_) && remoteEp_.equals(rhs.remoteEp_) )
+            return true;
+        else
+            return false;
+    }
+    
+    public int hashCode()
+    {
+        return (localEp_ + ":" + remoteEp_).hashCode();
+    }
+
+    public String toString()
+    {        
+        return socketChannel_.toString();
+    }
+    
+    void closeSocket()
+    {
+        logger_.warn("Closing down connection " + socketChannel_ + " with " + pendingWrites_.size() + " writes remaining.");            
+        if ( pool_ != null )
+        {
+            pool_.removeConnection(this);
+        }
+        cancel(key_);
+        pendingWrites_.clear();
+    }
+    
+    void errorClose() 
+    {        
+        logger_.warn("Closing down connection " + socketChannel_);
+        pendingWrites_.clear();
+        cancel(key_);
+        pendingWrites_.clear();        
+        if ( pool_ != null )
+        {
+            pool_.removeConnection(this);            
+        }
+    }
+    
+    private void cancel(SelectionKey key)
+    {
+        if ( key != null )
+        {
+            key.cancel();
+            try
+            {
+                key.channel().close();
+            }
+            catch (IOException e) {}
+        }
+    }
+    
+    // called in the selector thread
+    public void connect(SelectionKey key)
+    {       
+        key.interestOps(key.interestOps() & (~SelectionKey.OP_CONNECT));
+        try
+        {
+            if (socketChannel_.finishConnect())
+            {
+                key.interestOps(key.interestOps() | SelectionKey.OP_READ);
+                
+                // this will flush the pending                
+                if (!pendingWrites_.isEmpty()) 
+                {
+                    key_.interestOps(key_.interestOps() | SelectionKey.OP_WRITE);
+                }
+                resumeStreaming();
+            } 
+            else 
+            {  
+                logger_.warn("Closing connection because socket channel could not finishConnect.");;
+                errorClose();
+            }
+        } 
+        catch(IOException e) 
+        {               
+            logger_.warn("Encountered IOException on connection: "  + socketChannel_);
+            logger_.warn( LogUtil.throwableToString(e) );
+            errorClose();
+        }
+    }
+    
+    // called in the selector thread
+    public void write(SelectionKey key)
+    {   
+        key.interestOps( key.interestOps() & ( ~SelectionKey.OP_WRITE ) );                
+        doPendingWrites();
+        /*
+         * This is executed only if we are in streaming mode.
+         * Idea is that we read a chunk of data from a source
+         * and wait to read the next from the source until we 
+         * are siganlled to do so from here. 
+        */
+         resumeStreaming();        
+    }
+    
+    void doPendingWrites()
+    {
+        try
+        {                     
+            while(!pendingWrites_.isEmpty()) 
+            {
+                ByteBuffer buffer = pendingWrites_.get(0);
+                socketChannel_.write(buffer);                    
+                if (buffer.remaining() > 0) 
+                {   
+                    break;
+                }               
+                pendingWrites_.remove(0);                    
+            } 
+            
+        }
+        catch(IOException ex)
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+            // This is to fix the wierd Linux bug with NIO.
+            errorClose();
+        }
+        finally
+        {    
+            synchronized(this)
+            {
+                if (!pendingWrites_.isEmpty())
+                {                    
+                    key_.interestOps(key_.interestOps() | SelectionKey.OP_WRITE);
+                }
+            }
+        }
+    }
+    
+    // called in the selector thread
+    public void read(SelectionKey key)
+    {
+        key.interestOps( key.interestOps() & ( ~SelectionKey.OP_READ ) );
+        // publish this event onto to the TCPReadEvent Queue.
+        MessagingService.getReadExecutor().execute(readWork_);
+    }
+    
+    class ReadWorkItem implements Runnable
+    {                 
+        // called from the TCP READ thread pool
+        public void run()
+        {                         
+            if ( tcpReader_ == null )
+            {
+                tcpReader_ = new TcpReader(TcpConnection.this);    
+                StartState nextState = tcpReader_.getSocketState(TcpReader.TcpReaderState.PREAMBLE);
+                if ( nextState == null )
+                {
+                    nextState = new ProtocolState(tcpReader_);
+                    tcpReader_.putSocketState(TcpReader.TcpReaderState.PREAMBLE, nextState);
+                }
+                tcpReader_.morphState(nextState);
+            }
+            
+            try
+            {           
+                byte[] bytes = new byte[0];
+                while ( (bytes = tcpReader_.read()).length > 0 )
+                {                       
+                    ProtocolHeader pH = tcpReader_.getProtocolHeader();                    
+                    if ( !pH.isStreamingMode_ )
+                    {
+                        /* first message received */
+                        if (remoteEp_ == null)
+                        {             
+                            int port = ( pH.isListening_ ) ? DatabaseDescriptor.getStoragePort() : EndPoint.randomPort_;
+                            remoteEp_ = new EndPoint( socketChannel_.socket().getInetAddress().getHostAddress(), port );                            
+                            // put connection into pool if possible
+                            pool_ = MessagingService.getConnectionPool(localEp_, remoteEp_);                            
+                            pool_.addToPool(TcpConnection.this);                            
+                        }
+                        
+                        /* Deserialize and handle the message */
+                        MessagingService.getDeserilizationExecutor().submit( new MessageDeserializationTask(pH.serializerType_, bytes) );                                                  
+                        tcpReader_.resetState();
+                    }
+                    else
+                    {
+                        MessagingService.setStreamingMode(false);
+                        /* Close this socket connection  used for streaming */
+                        closeSocket();
+                    }                    
+                }
+            }
+            catch ( IOException ex )
+            {                   
+                handleException(ex);
+            }
+            catch ( Throwable th )
+            {
+                handleException(th);
+            }
+            finally
+            {
+                key_.interestOps(key_.interestOps() | SelectionKey.OP_READ);
+            }
+        }
+        
+        private void handleException(Throwable th)
+        {
+            logger_.warn("Problem reading from socket connected to : " + socketChannel_);
+            logger_.warn(LogUtil.throwableToString(th));
+            // This is to fix the wierd Linux bug with NIO.
+            errorClose();
+        }
+    }
+    
+    public int pending()
+    {
+        return pendingWrites_.size();
+    }
+    
+    public int compareTo(Object o)
+    {
+        if (o instanceof TcpConnection) 
+        {
+            return pendingWrites_.size() - ((TcpConnection) o).pendingWrites_.size();            
+        }
+                    
+        throw new IllegalArgumentException();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnectionHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnectionHandler.java
index e69de29b..cd065eb6 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnectionHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnectionHandler.java
@@ -0,0 +1,60 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.nio.channels.*;
+import java.io.IOException;
+import java.net.*;
+
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TcpConnectionHandler extends SelectionKeyHandler
+{
+    private static Logger logger_ = Logger.getLogger(TcpConnectionHandler.class);
+    EndPoint localEp_;    
+    
+    public TcpConnectionHandler(EndPoint localEp) 
+    {
+        localEp_ = localEp;
+    }
+
+    public void accept(SelectionKey key)
+    {
+        try 
+        {            
+            ServerSocketChannel serverChannel = (ServerSocketChannel)key.channel();
+            SocketChannel client = serverChannel.accept();
+            
+            if ( client != null )
+            {                        
+                //new TcpConnection(client, localEp_, true);
+                TcpConnection.acceptConnection(client, localEp_, true);                
+            }            
+        } 
+        catch(IOException e) 
+        {
+            logger_.warn(LogUtil.throwableToString(e));
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnectionManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnectionManager.java
index e69de29b..8b5f5d7b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnectionManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/TcpConnectionManager.java
@@ -0,0 +1,217 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.*;
+import java.util.concurrent.locks.*;
+
+import org.apache.log4j.Logger;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class TcpConnectionManager
+{
+    private Lock lock_ = new ReentrantLock();
+    private List<TcpConnection> allConnections_;
+    private EndPoint localEp_;
+    private EndPoint remoteEp_;
+    private int initialSize_;
+    private int growthFactor_;
+    private int maxSize_;
+    private long lastTimeUsed_;
+    private boolean isShut_;
+    
+    private int inUse_;
+
+    TcpConnectionManager(int initialSize, int growthFactor, int maxSize, EndPoint localEp, EndPoint remoteEp)
+    {
+        initialSize_ = initialSize;
+        growthFactor_ = growthFactor;
+        maxSize_ = maxSize;
+        localEp_ = localEp;
+        remoteEp_ = remoteEp;     
+        isShut_ = false;                
+        lastTimeUsed_ = System.currentTimeMillis();        
+        allConnections_ = new Vector<TcpConnection>(); 
+    }
+    
+    TcpConnection getConnection() throws IOException
+    {
+        lock_.lock();
+        try
+        {
+            if (allConnections_.isEmpty()) 
+            {                
+                TcpConnection conn = new TcpConnection(this, localEp_, remoteEp_);
+                addToPool(conn);
+                conn.inUse_ = true;
+                incUsed();
+                return conn;
+            }
+            
+            TcpConnection least = getLeastLoaded();
+            
+            if ( (least != null && least.pending() == 0) || allConnections_.size() == maxSize_) {
+                least.inUse_ = true;
+                incUsed();
+                return least;
+            }
+                                    
+            TcpConnection connection = new TcpConnection(this, localEp_, remoteEp_);
+            if ( connection != null && !contains(connection) )
+            {
+                addToPool(connection);
+                connection.inUse_ = true;
+                incUsed();
+                return connection;
+            }
+            else
+            {
+                if ( connection != null )
+                {                
+                    connection.closeSocket();
+                }
+                return getLeastLoaded();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    protected TcpConnection getLeastLoaded() 
+    {  
+        TcpConnection connection = null;
+        lock_.lock();
+        try
+        {
+            Collections.sort(allConnections_);
+            connection = (allConnections_.size() > 0 ) ? allConnections_.get(0) : null;
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+        return connection;
+    }
+    
+    void removeConnection(TcpConnection connection)
+    {
+        allConnections_.remove(connection);        
+    }
+    
+    void incUsed()
+    {
+        inUse_++;
+    }
+    
+    void decUsed()
+    {        
+        inUse_--;
+    }
+    
+    int getConnectionsInUse()
+    {
+        return inUse_;
+    }
+
+    void addToPool(TcpConnection connection)
+    { 
+        
+        if ( contains(connection) )
+            return;
+        
+        lock_.lock();
+        try
+        {
+            if ( allConnections_.size() < maxSize_ )
+            {                 
+                allConnections_.add(connection);                
+            }
+            else
+            {                
+                connection.closeSocket();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    void shutdown()
+    {    
+        lock_.lock();
+        try
+        {
+            while ( allConnections_.size() > 0 )
+            {
+                TcpConnection connection = allConnections_.remove(0);                        
+                connection.closeSocket();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+        isShut_ = true;
+    }
+
+    int getPoolSize()
+    {
+        return allConnections_.size();
+    }
+
+    EndPoint getLocalEndPoint()
+    {
+        return localEp_;
+    }
+    
+    EndPoint getRemoteEndPoint()
+    {
+        return remoteEp_;
+    }
+    
+    int getPendingWrites()
+    {
+        int total = 0;
+        lock_.lock();
+        try
+        {
+            for ( TcpConnection connection : allConnections_ )
+            {
+                total += connection.pending();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+        return total;
+    }
+    
+    boolean contains(TcpConnection connection)
+    {
+        return allConnections_.contains(connection);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/UdpConnection.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/UdpConnection.java
index e69de29b..103615fe 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/UdpConnection.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/UdpConnection.java
@@ -0,0 +1,166 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.net.SocketAddress;
+import java.nio.*;
+import java.nio.channels.*;
+import java.util.*;
+import java.util.concurrent.*;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.net.io.ProtocolState;
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.utils.BasicUtilities;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.concurrent.*;
+import org.apache.cassandra.utils.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class UdpConnection extends SelectionKeyHandler
+{
+    private static Logger logger_ = Logger.getLogger(UdpConnection.class);
+    private static final int BUFFER_SIZE = 4096;
+    private static final int protocol_ = 0xBADBEEF;
+    
+    private DatagramChannel socketChannel_;
+    private SelectionKey key_;    
+    private EndPoint localEndPoint_;
+    
+    public void init() throws IOException
+    {
+        socketChannel_ = DatagramChannel.open();
+        socketChannel_.socket().setReuseAddress(true);
+        socketChannel_.configureBlocking(false);        
+    }
+    
+    public void init(int port) throws IOException
+    {
+        // TODO: get TCP port from config and add one.
+        localEndPoint_ = new EndPoint(port);
+        socketChannel_ = DatagramChannel.open();
+        socketChannel_.socket().bind(localEndPoint_.getInetAddress());
+        socketChannel_.socket().setReuseAddress(true);
+        socketChannel_.configureBlocking(false);        
+        key_ = SelectorManager.getUdpSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
+    }
+    
+    public boolean write(Message message, EndPoint to) throws IOException
+    {
+        boolean bVal = true;                       
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        Message.serializer().serialize(message, dos);        
+        byte[] data = bos.toByteArray();
+        if ( data.length > 0 )
+        {  
+            logger_.debug("Size of Gossip packet " + data.length);
+            byte[] protocol = BasicUtilities.intToByteArray(protocol_);
+            ByteBuffer buffer = ByteBuffer.allocate(data.length + protocol.length);
+            buffer.put( protocol );
+            buffer.put(data);
+            buffer.flip();
+            
+            int n  = socketChannel_.send(buffer, to.getInetAddress());
+            if ( n == 0 )
+            {
+                bVal = false;
+            }
+        }
+        return bVal;
+    }
+    
+    void close()
+    {
+        try
+        {
+            if ( socketChannel_ != null )
+                socketChannel_.close();
+        }
+        catch ( IOException ex )
+        {
+            logger_.error( LogUtil.throwableToString(ex) );
+        }
+    }
+    
+    public DatagramChannel getDatagramChannel()
+    {
+        return socketChannel_;
+    }
+    
+    private byte[] gobbleHeaderAndExtractBody(ByteBuffer buffer)
+    {
+        byte[] body = new byte[0];        
+        byte[] protocol = new byte[4];
+        buffer = buffer.get(protocol, 0, protocol.length);
+        int value = BasicUtilities.byteArrayToInt(protocol);
+        
+        if ( protocol_ != value )
+        {
+            logger_.info("Invalid protocol header in the incoming message " + value);
+            return body;
+        }
+        body = new byte[buffer.remaining()];
+        buffer.get(body, 0, body.length);       
+        return body;
+    }
+    
+    public void read(SelectionKey key)
+    {        
+        key.interestOps( key.interestOps() & (~SelectionKey.OP_READ) );
+        ByteBuffer buffer = ByteBuffer.allocate(BUFFER_SIZE);
+        try
+        {
+            SocketAddress sa = socketChannel_.receive(buffer);
+            if ( sa == null )
+            {
+                logger_.debug("*** No datagram packet was available to be read ***");
+                return;
+            }            
+            buffer.flip();
+            
+            byte[] bytes = gobbleHeaderAndExtractBody(buffer);
+            if ( bytes.length > 0 )
+            {
+                DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+                Message message = Message.serializer().deserialize(dis);                
+                if ( message != null )
+                {                                        
+                    MessagingService.receive(message);
+                }
+            }
+        }
+        catch ( IOException ioe )
+        {
+            logger_.warn(LogUtil.throwableToString(ioe));
+        }
+        finally
+        {
+            key.interestOps( key_.interestOps() | SelectionKey.OP_READ );
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/ColumnFamilyFormatter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/ColumnFamilyFormatter.java
index e69de29b..7e836da8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/ColumnFamilyFormatter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/ColumnFamilyFormatter.java
@@ -0,0 +1,93 @@
+package org.apache.cassandra.net.http;
+
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.SuperColumn;
+
+
+public class ColumnFamilyFormatter extends HTMLFormatter
+{
+	public ColumnFamilyFormatter()
+	{
+		super();
+	}
+
+	public ColumnFamilyFormatter(StringBuilder sb)
+	{
+		super(sb);
+	}
+
+    public void printKeyColumnFamily(StringBuilder sb, String sKey, ColumnFamily cf)
+    {
+    	// print the key
+		sb.append("Key = " + sKey + "<br>");
+
+		// print the column familie
+		printColumnFamily(sb, cf);
+    }
+
+    public void printColumnFamily(StringBuilder sb, ColumnFamily cf)
+    {
+    	// first print the column family specific data
+    	sb.append("ColumnFamily = " + cf.name() + "<br>");
+
+    	Collection<IColumn> cols = cf.getAllColumns();
+    	if (cf.isSuper())
+    	{
+    		printSuperColumns(sb, cols);
+    	}
+    	else
+    	{
+    		printSimpleColumns(sb, cols);
+    	}
+    }
+
+    public void printSuperColumns(StringBuilder sb, Collection<IColumn> cols)
+    {
+		// print the super column summary
+		sb.append("Number of super columns = " + cols.size() + "<br>");
+
+		startTable();
+		for(IColumn col : cols)
+		{
+	        addHeader(col.name());
+			startRow();
+			Collection<IColumn> simpleCols = ((SuperColumn)col).getSubColumns();
+			printSimpleColumns(sb, simpleCols);
+			endRow();
+		}
+		endTable();
+    }
+
+    public void printSimpleColumns(StringBuilder sb, Collection<IColumn> cols)
+    {
+		int numColumns = cols.size();
+		String[] columnNames = new String[numColumns];
+		String[] columnValues = new String[numColumns];
+
+		// print the simple column summary
+		//sb.append("Number of simple columns = " + cols.size() + "<br>");
+
+		int i = 0;
+		for(IColumn col : cols)
+    	{
+			columnNames[i] = col.name();
+			columnValues[i] = new String(col.value());
+			++i;
+    	}
+
+		startTable();
+        addHeaders(columnNames);
+		startRow();
+		for(i = 0; i <  numColumns; ++i)
+		{
+			addCol(columnValues[i]);
+		}
+		endRow();
+		endTable();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HTMLFormatter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HTMLFormatter.java
index e69de29b..1eb2bad8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HTMLFormatter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HTMLFormatter.java
@@ -0,0 +1,347 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * Helper function to write some basic HTML.
+ */
+
+package org.apache.cassandra.net.http;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+import java.util.List;
+import java.util.Set;
+
+/**
+ *
+ * @author kranganathan
+ */
+public class HTMLFormatter
+{
+    protected StringBuilder sb_ = null;
+    private boolean writeBody_;
+
+    public HTMLFormatter()
+    {
+        sb_ = new StringBuilder();
+    }
+
+    public HTMLFormatter(StringBuilder sb)
+    {
+        sb_ = sb;
+    }
+
+    public void startBody()
+    {
+    	startBody(false, "", true, true);
+    }
+
+    public void startBody(boolean writeJSCallback, String jsCallbackFunction, boolean writeCSS, boolean writeBody)
+    {
+    	writeBody_ = writeBody;
+
+        sb_.append("<html>\n");
+        if(writeCSS || writeJSCallback)
+        {
+	        sb_.append("<head>\n");
+	        if(writeJSCallback)
+	        	addJSCallback(jsCallbackFunction);
+	        if(writeCSS)
+	        	addCSS();
+	        sb_.append("</head>\n");
+        }
+
+        if(writeBody)
+        {
+        	sb_.append("<body bgcolor=black>\n");
+        }
+    }
+
+    public void endBody()
+    {
+    	if(writeBody_)
+    	{
+    		sb_.append("</body>\n");
+    	}
+        sb_.append("</html>\n");
+    }
+
+    public void appendLine(String s)
+    {
+        sb_.append(s);
+        sb_.append("<br>\n");
+    }
+
+    public void append(String s)
+    {
+        sb_.append(s);
+    }
+
+    public void addJScript(String jscript)
+    {
+    	append("<script language=\"text/javascript\">\n");
+    	append(jscript + "\n");
+    	append("</script>\n");
+    }
+
+    public void startTable()
+    {
+        sb_.append("<table>\n");
+    }
+
+    public void addHeaders(String[] sTableHeaders)
+    {
+        sb_.append("<tr style=\"border: 2px solid #333333\"	>\n");
+        for (int i = 0; i < sTableHeaders.length; ++i)
+        {
+            sb_.append("<th><div class=\"tmenubar\">");
+            sb_.append("<b>" + sTableHeaders[i] + "</b>");
+            sb_.append("</div></th>\n");
+        }
+        sb_.append("\n</tr>\n\n");
+    }
+
+    public void addHeader(String sTableHeader)
+    {
+        sb_.append("<tr style=\"border: 2px solid #333333\"	>\n");
+        sb_.append("<th><div class=\"tmenubar\">");
+        sb_.append("<b>" + sTableHeader + "</b>");
+        sb_.append("</div></th>\n");
+        sb_.append("\n</tr>\n\n");
+    }
+
+    public void startRow()
+    {
+        sb_.append("<tr style=\"border: 2px solid #333333\">\n");
+    }
+
+    public void addCol(String sData)
+    {
+        sb_.append("<td style=\"border: 2px solid #333333\">");
+        sb_.append(sData);
+        sb_.append("</td>");
+    }
+
+    public void endRow()
+    {
+        sb_.append("</tr>\n");
+    }
+
+    public void endTable()
+    {
+        sb_.append("</table>\n");
+    }
+
+    public void addCombobox(Set<String> comboBoxEntries, String htmlElementName)
+    {
+    	addCombobox(comboBoxEntries, htmlElementName, -1);
+    }
+
+    public void addCombobox(Set<String> comboBoxEntries, String htmlElementName, int defaultSelected)
+    {
+    	sb_.append("  <select name=" + htmlElementName + " size=1>\n");
+    	if(defaultSelected == -1)
+    	{
+    		sb_.append("    <option value=\"\" SELECTED>Select an option \n");
+    	}
+
+    	int i = 0;
+    	for(String colFamName : comboBoxEntries)
+    	{
+    		if(defaultSelected == i)
+    		{
+    			sb_.append("    <option value=\"" + colFamName + "\" SELECTED>" + colFamName + "\n");
+    		}
+    		else
+    		{
+    			sb_.append("    <option value=\"" + colFamName + "\">" + colFamName + "\n");
+    		}
+    	}
+    	sb_.append("  </select>\n");
+    }
+
+    public void addDivElement(String divId, String value)
+    {
+    	sb_.append("<div id = \"" + divId + "\">");
+    	if(value != null)
+    		sb_.append(value);
+    	sb_.append("</div>\n");
+    }
+
+    public void createTable(String[] sTableHeaders, String[][] sTable)
+    {
+        if (sTable == null || sTable.length == 0)
+            return;
+
+        sb_.append("<table style=\"border: 2px solid #333333\">\n");
+
+        sb_.append("<tr style=\"border: 2px solid #333333\">\n");
+        for (int i = 0; i < sTableHeaders.length; ++i)
+        {
+            sb_.append("<td style=\"border: 2px solid #333333\">");
+            sb_.append("<b>" + sTableHeaders[i] + "</b>");
+            sb_.append("</td>\n");
+        }
+        sb_.append("\n</tr>\n\n");
+
+        for (int i = 0; i < sTable.length; ++i)
+        {
+            sb_.append("<tr style=\"border: 2px solid #333333\">\n");
+            for (int j = 0; j < sTable[i].length; ++j)
+            {
+                sb_.append("<td style=\"border: 2px solid #333333\">");
+                sb_.append(sTable[i][j]);
+                sb_.append("</td>\n");
+            }
+            sb_.append("\n</tr>\n\n");
+        }
+        sb_.append("</table>\n");
+    }
+
+    public void addJSCallback(String jsCallbackFunction)
+    {
+    	sb_.append("<script type=\"text/javascript\">\n");
+
+    	addJSForTabs();
+
+    	sb_.append(jsCallbackFunction +"\n");
+    	sb_.append("</script>\n");
+    }
+
+    public void addCSS()
+    {
+        sb_.append("<style type=\"text/css\">\n");
+        sb_.append("body\n");
+        sb_.append("{\n");
+        sb_.append("  color:white;\n");
+        sb_.append("  font-family:Arial Unicode MS,Verdana, Arial, Sans-serif;\n");
+        sb_.append("  font-size:10pt;\n");
+        sb_.append("}\n");
+
+        sb_.append(".tmenubar\n");
+        sb_.append("{\n");
+        sb_.append("  background-color:green;\n");
+        sb_.append("  font-family:Verdana, Arial, Sans-serif;\n");
+        sb_.append("  font-size:10pt;\n");
+        sb_.append("  font-weight:bold;\n");
+        sb_.append("}\n");
+
+        sb_.append("th\n");
+        sb_.append("{\n");
+        sb_.append(" 	 color:white;\n");
+        sb_.append("}\n");
+
+        sb_.append("td\n");
+        sb_.append("{\n");
+        sb_.append(" 	 color:white;\n");
+        sb_.append("}\n");
+        sb_.append("a:link {color:#CAF99B;font-size:10pt;font-weight:bold;font-family:Arial Unicode MS,Lucida-grande,Verdana}\n");
+        sb_.append("a:visited {color:red}\n");
+        sb_.append("a:hover{color:yellow;font-size:10pt;font-weight:bold;font-family:Arial Unicode MS,Lucida-grande,Verdana;background-color:green}\n");
+
+        addCSSForTabs();
+
+        sb_.append("</style>\n");
+
+    }
+
+    public void addCSSForTabs()
+    {
+    	sb_.append("#header ul {\n");
+    	sb_.append("	list-style: none;\n");
+    	sb_.append("	padding: 0;\n");
+    	sb_.append("	margin: 0;\n");
+    	sb_.append("	}\n");
+    	sb_.append("\n");
+    	sb_.append("#header li {\n");
+    	sb_.append("	float: left;\n");
+    	sb_.append("	border: 1px solid #bbb;\n");
+    	sb_.append("	border-bottom-width: 0;\n");
+    	sb_.append("	margin: 0;\n");
+    	sb_.append("}\n");
+    	sb_.append("\n");
+    	sb_.append("#header a {\n");
+    	sb_.append("	text-decoration: none;\n");
+    	sb_.append("	display: block;\n");
+    	sb_.append("	background: #eee;\n");
+    	sb_.append("	padding: 0.24em 1em;\n");
+    	sb_.append("	color: #00c;\n");
+    	sb_.append("	width: 8em;\n");
+    	sb_.append("	text-align: center;\n");
+    	sb_.append("	}\n");
+    	sb_.append("\n");
+    	sb_.append("#header a:hover {\n");
+    	sb_.append("	background: #ddf;\n");
+    	sb_.append("}\n");
+    	sb_.append("\n");
+    	sb_.append("#header #selected {\n");
+    	sb_.append("	border-color: black;\n");
+    	sb_.append("}\n");
+    	sb_.append("\n");
+    	sb_.append("#header #selected a {\n");
+    	sb_.append("	position: relative;\n");
+    	sb_.append("	top: 1px;\n");
+    	sb_.append("	background: white;\n");
+    	sb_.append("	color: black;\n");
+    	sb_.append("	font-weight: bold;\n");
+    	sb_.append("}\n");
+    	sb_.append("\n");
+    	sb_.append("#content {\n");
+    	sb_.append("	border: 1px solid black;\n");
+    	sb_.append("	visibility:hidden;\n");
+    	sb_.append("	position:absolute;\n");
+    	sb_.append("	top:200;\n");
+    	sb_.append("	clear: both;\n");
+    	sb_.append("	padding: 0 1em;\n");
+    	sb_.append("}\n");
+    	sb_.append("\n");
+    	sb_.append("h1 {\n");
+    	sb_.append("	margin: 0;\n");
+    	sb_.append("	padding: 0 0 1em 0;\n");
+    	sb_.append("}\n");
+    }
+
+    public void addJSForTabs()
+    {
+    	sb_.append("var curSelectedDivId = \"one\";\n");
+    	sb_.append("\n");
+    	sb_.append("function selectTab(tabDivId)\n");
+    	sb_.append("{\n");
+    	sb_.append("	var x = document.getElementsByName(curSelectedDivId);\n");
+    	sb_.append("	if(x[1])\n");
+    	sb_.append("		x[1].style.visibility=\"hidden\";\n");
+    	sb_.append("	if(x[0])\n");
+    	sb_.append("		x[0].id=curSelectedDivId;\n");
+    	sb_.append("\n");
+    	sb_.append("\n");
+    	sb_.append("	var y = document.getElementsByName(tabDivId);\n");
+    	sb_.append("	if(y[1])\n");
+    	sb_.append("		y[1].style.visibility=\"visible\";\n");
+    	sb_.append("	if(y[0])\n");
+    	sb_.append("		y[0].id = \"selected\";\n");
+    	sb_.append("\n");
+    	sb_.append("	curSelectedDivId = tabDivId;\n");
+    	sb_.append("}\n");
+    }
+
+    public String toString()
+    {
+        return sb_.toString();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpConnection.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpConnection.java
index e69de29b..ed53ce40 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpConnection.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpConnection.java
@@ -0,0 +1,432 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * This class accepts a client connection and parses http data from it.
+ */
+
+// TODO: shouldClose_ is not used correctly. It should be used to close the socket? When?
+
+package org.apache.cassandra.net.http;
+
+import java.util.*;
+import java.net.*;
+import java.io.*;
+import java.nio.*;
+import java.nio.channels.SelectionKey;
+import java.nio.channels.SocketChannel;
+import org.apache.cassandra.service.*;
+import org.apache.cassandra.concurrent.SingleThreadedStage;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.SelectionKeyHandler;
+import org.apache.cassandra.net.SelectorManager;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ *
+ * @author kranganathan
+ */
+public class HttpConnection extends SelectionKeyHandler implements HttpStartLineParser.Callback, HttpHeaderParser.Callback
+{
+    private static Logger logger_ = Logger.getLogger(StorageService.class);
+    public static final String httpRequestVerbHandler_ = "HTTP-REQUEST-VERB-HANDLER";
+    public static final String httpStage_ = "HTTP-STAGE";
+
+    /*
+     * These are the callbacks into who ever intends
+     * to listen on the client socket.
+     */
+    public interface HttpConnectionListener
+    {
+        public void onRequest(HttpRequest httpRequest);
+        public void onResponse(HttpResponse httpResponse);
+    }
+
+    enum HttpMessageType
+    {
+        UNKNOWN,
+        REQUEST,
+        RESPONSE
+    }
+
+    enum ParseState
+    {
+        IN_NEW,
+        IN_START,
+        IN_HEADERS, IN_BODY
+    }
+
+    private ParseState parseState_ = ParseState.IN_NEW;
+    private long parseStartTime_ = 0;
+    private HttpMessageType currentMsgType_ = HttpMessageType.UNKNOWN;
+    private int contentLength_ = 0;
+    private List<ByteBuffer> bodyBuffers_ = new LinkedList<ByteBuffer>();
+    private boolean shouldClose_ = false;
+    private String defaultContentType_ = "text/html";
+    private HttpRequest currentRequest_ = null;
+    private HttpResponse currentResponse_ = null;
+    private HttpStartLineParser startLineParser_ = new HttpStartLineParser(this);
+    private HttpHeaderParser headerParser_ = new HttpHeaderParser(this);
+    /* Selection Key associated with this HTTP Connection */
+    private SelectionKey httpKey_;
+    /* SocketChannel associated with this HTTP Connection */
+    private SocketChannel httpChannel_;
+    /* HTTPReader instance associated with this HTTP Connection */
+    private HTTPReader httpReader_ = new HTTPReader();
+
+    /*
+     * This abstraction starts reading the data that comes in
+     * on a HTTP request. It accumulates the bytes read into
+     * a buffer and passes the buffer to the HTTP parser.
+    */
+
+    class HTTPReader implements Runnable
+    {
+        /* We read 256 bytes at a time from a HTTP connection */
+        private static final int bufferSize_ = 256;
+
+        /*
+         * Read buffers from the input stream into the byte buffer.
+         */
+        public void run()
+        {
+            ByteBuffer readBuffer = ByteBuffer.allocate(HTTPReader.bufferSize_);
+            try
+            {
+                int bytesRead = httpChannel_.read(readBuffer);
+                readBuffer.flip();
+                if ( readBuffer.remaining() > 0 )
+                    HttpConnection.this.parse(readBuffer);
+            }
+            catch ( IOException ex )
+            {
+                logger_.warn(LogUtil.throwableToString(ex));
+            }
+        }
+    }
+
+    public static class HttpRequestMessage
+    {
+        private HttpRequest httpRequest_;
+        private HttpConnection httpConnection_;
+
+        HttpRequestMessage(HttpRequest httpRequest, HttpConnection httpConnection)
+        {
+            httpRequest_ = httpRequest;
+            httpConnection_ = httpConnection;
+        }
+
+        public HttpRequest getHttpRequest()
+        {
+            return httpRequest_;
+        }
+
+        public HttpConnection getHttpConnection()
+        {
+            return httpConnection_;
+        }
+    }
+
+    /*
+     *  Read called on the Selector thread. This is called
+     *  when there is some HTTP request that needs to be
+     *  processed.
+    */
+    public void read(SelectionKey key)
+    {
+        if ( httpKey_ == null )
+        {
+            httpKey_ = key;
+            httpChannel_ = (SocketChannel)key.channel();
+        }
+        /* deregister interest for read */
+        key.interestOps( key.interestOps() & ( ~SelectionKey.OP_READ ) );
+        /* Add a task to process the HTTP request */
+        MessagingService.getReadExecutor().execute(httpReader_);
+    }
+    private void resetParserState()
+    {
+        startLineParser_.resetParserState();
+        headerParser_.resetParserState();
+        parseState_ = ParseState.IN_NEW;
+        contentLength_ = 0;
+        bodyBuffers_ = new LinkedList<ByteBuffer>();
+        currentMsgType_ = HttpMessageType.UNKNOWN;
+        currentRequest_ = null;
+        currentResponse_ = null;
+    }
+
+    public void close()
+    {        
+        logger_.info("Closing HTTP socket ...");
+        if ( httpKey_ != null )
+        {
+            httpKey_.cancel();
+            try
+            {
+                httpKey_.channel().close();
+            }
+            catch (IOException e) {}
+        }
+    }
+
+    /*
+     * Process the HTTP commands sent from the client. Reads
+     * the socket and parses the HTTP request.
+    */
+    public void parse(ByteBuffer bb)
+    {
+        try
+        {
+            logger_.debug("Processing http requests from socket ...");
+            switch (parseState_)
+            {
+                case IN_NEW:
+                    parseState_ = ParseState.IN_START;
+                    parseStartTime_ = System.currentTimeMillis();
+
+                // fall through
+                case IN_START:
+                    if (startLineParser_.onMoreBytesNew(bb) == false)
+                    {
+                        break;
+                    }
+                    else
+                    {
+                        /* Already done through the callback */
+                        parseState_ = ParseState.IN_HEADERS;
+                    }
+
+                // fall through
+                case IN_HEADERS:
+                    if (headerParser_.onMoreBytesNew(bb) == false)
+                    {
+
+                        break; // need more bytes
+                    }
+                    else
+                    {
+                        String len;
+                        if (currentMsgType_ == HttpMessageType.REQUEST)
+                        {
+                            len = currentRequest_.getHeader(HttpProtocolConstants.CONTENT_LENGTH);
+
+                            // find if we should close method
+                            if (currentRequest_.getVersion().equalsIgnoreCase("HTTP/1.1"))
+                            {
+                                /*
+                                 * Scan all of the headers for close messages
+                                 */
+                                String val = currentRequest_.getHeader(HttpProtocolConstants.CONNECTION);
+
+                                if (val != null && val.equalsIgnoreCase(HttpProtocolConstants.CLOSE))
+                                {
+                                    shouldClose_ = true;
+                                }
+                            } else if (currentRequest_.getVersion().equalsIgnoreCase("HTTP/1.0"))
+                            {
+                                /* By default no keep-alive */
+                                shouldClose_ = true;
+
+                                /*
+                                 * Scan all of the headers for keep-alive
+                                 * messages
+                                 */
+                                String val = currentRequest_.getHeader(HttpProtocolConstants.CONNECTION);
+
+                                if (val != null && val.equalsIgnoreCase(HttpProtocolConstants.KEEP_ALIVE))
+                                {
+                                    shouldClose_ = false;
+                                }
+                            } else
+                            {
+                                /* Assume 0.9 */
+                                shouldClose_ = true;
+                            }
+                        }
+                        else if (currentMsgType_ == HttpMessageType.RESPONSE)
+                        {
+                            len = currentResponse_.getHeader(HttpProtocolConstants.CONTENT_LENGTH);
+
+                        // TODO: pay attention to keep-alive and
+                        // close headers
+                        }
+                        else
+                        {
+                            logger_.warn("in HttpConnection::processInput_() Message type is not set");
+                            return;
+                        }
+
+                        if (len != null)
+                        {
+                            try
+                            {
+                                if(len == null || len.equals(""))
+                                    contentLength_ = 0;
+                                else
+                                    contentLength_ = Integer.parseInt(len);
+                            }
+                            catch (NumberFormatException ex)
+                            {
+                                throw new HttpParsingException();
+                            }
+                        }
+                        parseState_ = ParseState.IN_BODY;
+                    }
+
+                // fall through
+                case IN_BODY:
+                    boolean done = false;
+
+                    if (contentLength_ > 0)
+                    {
+                        if (bb.remaining() > contentLength_)
+                        {
+                            int newLimit = bb.position() + contentLength_;
+                            bodyBuffers_.add(((ByteBuffer) bb.duplicate().limit(newLimit)).slice());
+                            bb.position(newLimit);
+                            contentLength_ = 0;
+                        }
+                        else
+                        {
+                            contentLength_ -= bb.remaining();
+                            bodyBuffers_.add(bb.duplicate());
+                            bb.position(bb.limit());
+                        }
+                    }
+
+                if (contentLength_ == 0)
+                {
+                    done = true;
+                }
+
+                if (done)
+                {
+                    if (currentMsgType_ == HttpMessageType.REQUEST)
+                    {
+                        //currentRequest_.setParseTime(env_.getCurrentTime() - parseStartTime_);
+                        currentRequest_.setBody(bodyBuffers_);
+
+                        if (currentRequest_.getHeader("Content-Type") == null)
+                        {
+                            currentRequest_.addHeader("Content-Type", defaultContentType_);
+                        }
+
+                        handleRequest(currentRequest_);
+                    }
+                    else if (currentMsgType_ == HttpMessageType.RESPONSE)
+                    {
+                        logger_.info("Holy shit! We are not supposed to be here - ever !!!");
+                    }
+                    else
+                    {
+                        logger_.error("Http message type is still" +
+                                " unset after we finish parsing the body?");
+                    }
+
+                    resetParserState();
+                }
+            }
+
+        }
+        catch (final Throwable e)
+        {
+            logger_.warn(LogUtil.throwableToString(e));
+            //close();
+        }
+        finally
+        {
+            httpKey_.interestOps(httpKey_.interestOps() | SelectionKey.OP_READ);
+        }
+    }
+
+    public void write(ByteBuffer buffer)
+    {
+        /*
+         * TODO: Make this a non blocking write.
+        */
+        try
+        {
+            while ( buffer.remaining() > 0 )
+            {
+                httpChannel_.write(buffer);
+            }
+            close();
+        }
+        catch ( IOException ex )
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }
+    }
+
+    private void handleRequest(HttpRequest request)
+    {
+        HttpConnection.HttpRequestMessage httpRequestMessage = new HttpConnection.HttpRequestMessage(request, this);
+        Message httpMessage = new Message(null, HttpConnection.httpStage_, HttpConnection.httpRequestVerbHandler_, new Object[]{httpRequestMessage});
+        MessagingService.receive(httpMessage);
+    }
+
+    // HttpStartLineParser.Callback interface implementation
+    public void onStartLine(String method, String path, String query, String version)
+    {
+        logger_.debug("Startline method=" + method + " path=" + path + " query=" + query + " version=" + version);
+
+        if (method.startsWith("HTTP"))
+        {
+                // response
+                currentMsgType_ = HttpMessageType.RESPONSE;
+                currentResponse_ = new HttpResponse();
+                currentResponse_.setStartLine(method, path, version);
+        }
+        else
+        {
+                // request
+                currentMsgType_ = HttpMessageType.REQUEST;
+                currentRequest_ = new HttpRequest();
+                currentRequest_.setStartLine(method, path, query, version);
+        }
+    }
+
+    // HttpHeaderParser.Callback interface implementation
+    public void onHeader(String name, String value)
+    {
+        if (currentMsgType_ == HttpMessageType.REQUEST)
+        {
+                currentRequest_.addHeader(name, value);
+        }
+        else if (currentMsgType_ == HttpMessageType.RESPONSE)
+        {
+                currentResponse_.addHeader(name, value);
+        }
+        else
+        {
+            logger_.warn("Unknown message type -- HttpConnection::onHeader()");
+        }
+
+        logger_.debug(name + " : " + value);
+    }
+}
+
+
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpConnectionHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpConnectionHandler.java
index e69de29b..9cfb79a2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpConnectionHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpConnectionHandler.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.http;
+
+import java.io.IOException;
+import java.nio.channels.SelectionKey;
+import java.nio.channels.ServerSocketChannel;
+import java.nio.channels.SocketChannel;
+
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.net.SelectionKeyHandler;
+import org.apache.cassandra.net.SelectorManager;
+import org.apache.log4j.Logger;
+
+public class HttpConnectionHandler extends SelectionKeyHandler
+{
+    private static Logger logger_ = Logger.getLogger(HttpConnectionHandler.class);
+    
+    public void accept(SelectionKey key)
+    {
+        try
+        {
+            ServerSocketChannel serverChannel = (ServerSocketChannel)key.channel();
+            SocketChannel client = serverChannel.accept();
+            if ( client != null )
+            {
+                client.configureBlocking(false);            
+                SelectionKeyHandler handler = new HttpConnection();
+                SelectorManager.getSelectorManager().register(client, handler, SelectionKey.OP_READ);
+            }
+        } 
+        catch(IOException e) 
+        {
+            logger_.warn(e);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpHeaderParser.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpHeaderParser.java
index e69de29b..82f23064 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpHeaderParser.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpHeaderParser.java
@@ -0,0 +1,378 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * To change this template, choose Tools | Templates
+ * and open the template in the editor.
+ */
+package org.apache.cassandra.net.http;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.ByteBuffer;
+
+/**
+ *
+ * @author kranganathan
+ */
+/**
+ * A parser for HTTP header lines.  
+ * 
+ */
+public class HttpHeaderParser
+{
+
+    private Callback callback_;
+
+    public interface Callback
+    {
+
+        public void onHeader(String key, String value);
+    }
+
+    public HttpHeaderParser(Callback cb)
+    {
+        callback_ = cb;
+    }
+
+    enum HeaderParseState
+    {
+        // we are at the very beginning of the line
+        START_OF_HEADER_LINE,
+        // are at line beginning, read '\r' but ran out of bytes in this round
+        START_OF_HEADER_LINE_WITH_READ_SLASH_R,
+        // we are in the process of parsing a header key
+        IN_HEADER_KEY,
+        // eat whitespace after the ':' but before the value
+        PRE_HEADER_VALUE_WHITESPACE,
+        // we are in the process of parsing a header value
+        IN_HEADER_VALUE,
+        // were in IN_HEADER_VALUE and read '\r' but ran out of more bytes
+        IN_HEADER_VALUE_WITH_READ_SLASH_R,
+        /*
+         * got \r\n in the header value.  now consider whether its a multilined 
+         * value.  For example,
+         *
+         * HeaderKey: HeaderValue\r\n this is still part of the value\r\n
+         * 
+         * is a valid HTTP header line with value 
+         *
+         * HeaderValue\r\n this is still part of the value
+         *
+         * NOTE: while all whitespace should generally be condensed into a 
+         * single space by the HTTP standard, we will just preserve all of the
+         * whitespace for now
+         * 
+         * TODO: consider replacing all whitespace with a single space
+         * 
+         * TODO: this parser doesn't correctly preserve the \r\n, should it?
+         */
+        CHECKING_END_OF_VALUE,
+        // we are just about to reset the state of the header parser
+        TO_RESET
+    }
+
+    // the current state of the parser
+    private HeaderParseState parseState_ = HeaderParseState.TO_RESET;
+    // incrementally build up this HTTP header key as we read it
+    private StringBuilder headerKey_ = new StringBuilder(32);
+
+    // incrementally build up this HTTP header value as we read it
+    private StringBuilder headerValue_ = new StringBuilder(64);
+
+    public void resetParserState()
+    {
+        headerKey_.setLength(0);
+        headerValue_.setLength(0);
+        parseState_ = HeaderParseState.START_OF_HEADER_LINE;
+    }
+
+    private void finishCurrentHeader_()
+    {
+        if (callback_ != null)
+        {
+            callback_.onHeader(headerKey_.toString().trim(), headerValue_
+                    .toString().trim());
+        }
+        resetParserState();
+    }
+
+    public boolean onMoreBytes(InputStream in) throws IOException
+    {
+        int got;
+
+        if (parseState_ == HeaderParseState.TO_RESET)
+        {
+            resetParserState();
+        }
+
+        while (in.available() > 0)
+        {
+            in.mark(1);
+            got = in.read();
+
+            switch (parseState_)
+            {
+
+            case START_OF_HEADER_LINE:
+                switch (got)
+                {
+                case '\r':
+                    if (in.available() > 0)
+                    {
+                        in.mark(1);
+                        got = in.read();
+
+                        if (got == '\n')
+                        {
+                            parseState_ = HeaderParseState.TO_RESET;
+                            return true;
+                        } // TODO: determine whether this \r-eating is valid
+                        else
+                        {
+                            in.reset();
+                        }
+                    } // wait for more data to make this decision
+                    else
+                    {
+                        in.reset();
+                        return false;
+                    }
+                    break;
+
+                default:
+                    in.reset();
+                    parseState_ = HeaderParseState.IN_HEADER_KEY;
+                    break;
+                }
+                break;
+
+            case IN_HEADER_KEY:
+                switch (got)
+                {
+                case ':':
+                    parseState_ = HeaderParseState.PRE_HEADER_VALUE_WHITESPACE;
+                    break;
+                // TODO: find out: whether to eat whitespace before a : 
+                default:
+                    headerKey_.append((char) got);
+                    break;
+                }
+                break;
+
+            case PRE_HEADER_VALUE_WHITESPACE:
+                switch (got)
+                {
+                case ' ':
+                case '\t':
+                    break;
+                default:
+                    in.reset();
+                    parseState_ = HeaderParseState.IN_HEADER_VALUE;
+                    break;
+                }
+                break;
+
+            case IN_HEADER_VALUE:
+                switch (got)
+                {
+                case '\r':
+                    if (in.available() > 0)
+                    {
+                        in.mark(1);
+                        got = in.read();
+
+                        if (got == '\n')
+                        {
+                            parseState_ = HeaderParseState.CHECKING_END_OF_VALUE;
+                            break;
+                        } // TODO: determine whether this \r-eating is valid
+                        else
+                        {
+                            in.reset();
+                        }
+                    }
+                    else
+                    {
+                        in.reset();
+                        return false;
+                    }
+                    break;
+                default:
+                    headerValue_.append((char) got);
+                    break;
+                }
+                break;
+
+            case CHECKING_END_OF_VALUE:
+                switch (got)
+                {
+                case ' ':
+                case '\t':
+                    in.reset();
+                    parseState_ = HeaderParseState.IN_HEADER_VALUE;
+                    break;
+                default:
+                    in.reset();
+                    finishCurrentHeader_();
+                }
+                break;
+            default:
+                assert false;
+                parseState_ = HeaderParseState.START_OF_HEADER_LINE;
+                break;
+            }
+        }
+
+        return false;
+    }
+
+    public boolean onMoreBytesNew(ByteBuffer buffer) throws IOException
+    {
+
+        int got;
+        int limit = buffer.limit();
+        int pos = buffer.position();
+
+        if (parseState_ == HeaderParseState.TO_RESET)
+        {
+            resetParserState();
+        }
+
+        while (pos < limit)
+        {
+            switch (parseState_)
+            {
+
+            case START_OF_HEADER_LINE:
+                if ((got = buffer.get(pos)) != '\r')
+                {
+                    parseState_ = HeaderParseState.IN_HEADER_KEY;
+                    break;
+                }
+                else
+                {
+                    pos++;
+                    if (pos == limit) // Need more bytes
+                    {
+                        buffer.position(pos);
+                        parseState_ = HeaderParseState.START_OF_HEADER_LINE_WITH_READ_SLASH_R;
+                        return false;
+                    }
+                }
+            // fall through
+
+            case START_OF_HEADER_LINE_WITH_READ_SLASH_R:
+                // Processed "...\r\n\r\n" - headers are complete
+                if (((char) buffer.get(pos)) == '\n')
+                {
+                    buffer.position(++pos);
+                    parseState_ = HeaderParseState.TO_RESET;
+                    return true;
+                } // TODO: determine whether this \r-eating is valid
+                else
+                {
+                    parseState_ = HeaderParseState.IN_HEADER_KEY;
+                }
+            //fall through
+
+            case IN_HEADER_KEY:
+                // TODO: find out: whether to eat whitespace before a :
+                while (pos < limit && (got = buffer.get(pos)) != ':')
+                {
+                    headerKey_.append((char) got);
+                    pos++;
+                }
+                if (pos < limit)
+                {
+                    pos++; //eating ':'
+                    parseState_ = HeaderParseState.PRE_HEADER_VALUE_WHITESPACE;
+                }
+                break;
+
+            case PRE_HEADER_VALUE_WHITESPACE:
+                while ((((got = buffer.get(pos)) == ' ') || (got == '\t'))
+                        && (++pos < limit))
+                {
+                    ;
+                }
+                if (pos < limit)
+                {
+                    parseState_ = HeaderParseState.IN_HEADER_VALUE;
+                }
+                break;
+
+            case IN_HEADER_VALUE:
+                while (pos < limit && (got = buffer.get(pos)) != '\r')
+                {
+                    headerValue_.append((char) got);
+                    pos++;
+                }
+                if (pos == limit)
+                {
+                    break;
+                }
+
+                pos++;
+                if (pos == limit)
+                {
+                    parseState_ = HeaderParseState.IN_HEADER_VALUE_WITH_READ_SLASH_R;
+                    break;
+                    //buffer.position(pos);
+                    //return false;
+                }
+            // fall through
+
+            case IN_HEADER_VALUE_WITH_READ_SLASH_R:
+                if (((char) buffer.get(pos)) == '\n')
+                {
+                    parseState_ = HeaderParseState.CHECKING_END_OF_VALUE;
+                    pos++;
+                } // TODO: determine whether this \r-eating is valid
+                else
+                {
+                    parseState_ = HeaderParseState.IN_HEADER_VALUE;
+                }
+                break;
+
+            case CHECKING_END_OF_VALUE:
+                switch ((char) buffer.get(pos))
+                {
+                case ' ':
+                case '\t':
+                    parseState_ = HeaderParseState.IN_HEADER_VALUE;
+                    break;
+
+                default:
+                    // Processed "headerKey headerValue\r\n"
+                    finishCurrentHeader_();
+                }
+                break;
+
+            default:
+                assert false;
+                parseState_ = HeaderParseState.START_OF_HEADER_LINE;
+                break;
+            }
+
+        }
+        // Need to read more bytes - get next buffer
+        buffer.position(pos);
+        return false;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpParsingException.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpParsingException.java
index e69de29b..135cd2e8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpParsingException.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpParsingException.java
@@ -0,0 +1,38 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * To change this template, choose Tools | Templates
+ * and open the template in the editor.
+ */
+
+package org.apache.cassandra.net.http;
+
+import java.io.IOException;
+
+/**
+ *
+ * @author kranganathan
+ */
+
+public class HttpParsingException extends IOException
+{
+    private static final long serialVersionUID = 1L;
+}
+
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpProtocolConstants.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpProtocolConstants.java
index e69de29b..349dd58f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpProtocolConstants.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpProtocolConstants.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * To change this template, choose Tools | Templates
+ * and open the template in the editor.
+ */
+
+package org.apache.cassandra.net.http;
+
+/**
+ *
+ * @author kranganathan
+ */
+public interface HttpProtocolConstants
+{
+    static final String CONNECTION = "Connection";
+    static final String CONTENT_LENGTH = "Content-Length";
+    static final String CLOSE = "close";
+    static final String KEEP_ALIVE = "Keep-Alive";
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpRequest.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpRequest.java
index e69de29b..7a6a7ab6 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpRequest.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpRequest.java
@@ -0,0 +1,210 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * Encapsulates a HTTP request.
+ */
+
+package org.apache.cassandra.net.http;
+
+import java.util.*;
+import java.io.*;
+import java.net.URLDecoder;
+import java.nio.*;
+
+/**
+ *
+ * @author kranganathan
+ */
+public class HttpRequest
+{
+    private Map<String, String> headersMap_ = new HashMap<String, String>();
+    private Map<String, String> paramsMap_ = new HashMap<String, String>();
+    private String sBody_ = "";
+    private String method_;
+    private String path_;
+    private String query_;
+    private String version_;
+
+    /*
+     * Returns the type of method - GET, POST, etc.
+    */
+    public String getMethod()
+    {
+        return method_;
+    }
+
+    /*
+     * Gets the request path referenced by the request.
+     * For example, if the URL is of the form:
+     *  http://somedomain:PORT/some/path?param=value
+     * this function will return
+     *  "/some/path"
+     */
+    public String getPath()
+    {
+        return path_;
+    }
+
+    /*
+     * Gets the query in the request.
+     * For example, if the URL is of the form:
+     *  http://somedomain:PORT/some/path?param=value
+     * this function will return
+     *  "/some/path"
+     */
+    public String getQuery()
+    {
+        return query_;
+    }
+
+    /*
+     * Returns the supported HTTP protocol version.
+     */
+    public String getVersion()
+    {
+        return "HTTP/1.1";
+    }
+
+    /*
+     * This function add to the map of header name-values
+     * in the HTTP request.
+     */
+    public void addHeader(String name, String value)
+    {
+        headersMap_.put(name, value);
+    }
+
+    /*
+     * For a gives name, returns the value if it was in the
+     * headers. Returns the empty string otherwise.
+     */
+    public String getHeader(String name)
+    {
+        if(headersMap_.get(name) == null)
+            return "";
+        return headersMap_.get(name).toString();
+    }
+
+    public void setParameter(String name, String value)
+    {
+    	// first decode the data then store it in the map using the standard UTF-8 encoding
+    	String decodedValue = value;
+    	try
+    	{
+    		decodedValue = URLDecoder.decode(value, "UTF-8");
+    	}
+    	catch (Exception e)
+    	{
+    		// do nothing
+    	}
+        paramsMap_.put(name, decodedValue);
+    }
+
+    /*
+     * This function get the parameters from the body of the HTTP message.
+     * Returns the value for the parameter if one exists or returns null.
+     *
+z    * For example, if the body is of the form:
+     *  a=b&c=d
+     * this function will:
+     * return "b" when called as getParameter("a")
+     */
+    public String getParameter(String name)
+    {
+        if(paramsMap_.get(name) != null)
+            return paramsMap_.get(name).toString();
+        return null;
+    }
+
+    /*
+     * Get the string representation of the byte buffers passed in and put
+     * them in sBody_ variable. Then parse all the parameters in the body.
+     */
+    public void setBody(List<ByteBuffer> bodyBuffers)
+    {
+        if(bodyBuffers == null)
+            return;
+        try
+        {
+            // get the byte buffers that the body should be composed of
+            // and collect them in the string builder
+            StringBuilder sb = new StringBuilder();
+            for(int i = 0; i < bodyBuffers.size(); ++i)
+            {
+                ByteBuffer bb = bodyBuffers.get(i);
+                if(bb.remaining() <= 0)
+                {
+                    continue;
+                }
+                byte[] byteStr = new byte[bb.remaining()];
+                bb.get(byteStr);
+                String s = new String(byteStr);
+                sb.append(s);
+            }
+
+            // add the string to the body
+            if(sb.toString() != null)
+            {
+                sBody_ += sb.toString();
+            }
+
+            // once we are done with the body, parse the parameters
+            String[] sParamValues = sBody_.split("&");
+            for(int i = 0; i < sParamValues.length; ++i)
+            {
+                String[] paramVal = sParamValues[i].split("=");
+                if ( paramVal[0] != null && paramVal[1] != null )
+                {
+                    setParameter(paramVal[0], paramVal[1]);
+                }
+            }
+        }
+        catch(Exception e)
+        {
+        }
+    }
+
+    public String getBody()
+    {
+        return sBody_;
+    }
+
+    public void setStartLine(String method, String path, String query, String version)
+    {
+        method_ = method;
+        path_ = path;
+        query_ = query;
+        version_ = version;
+    }
+
+    public String toString()
+    {
+        StringWriter sw = new StringWriter();
+        PrintWriter pw = new PrintWriter(sw);
+
+        pw.println("HttpRequest-------->");
+        pw.println("method = " + method_ + ", path = " + path_ + ", query = " + query_ + ", version = " + version_);
+        pw.println("Headers: " + headersMap_.toString());
+        pw.println("Body: " + sBody_);
+        pw.println("<--------HttpRequest");
+
+        return sw.toString();
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpResponse.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpResponse.java
index e69de29b..d988fa5d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpResponse.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpResponse.java
@@ -0,0 +1,103 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * To change this template, choose Tools | Templates
+ * and open the template in the editor.
+ */
+
+package org.apache.cassandra.net.http;
+
+import java.util.*;
+import java.io.*;
+import java.nio.*;
+
+/**
+ *
+ * @author kranganathan
+ */
+public class HttpResponse
+{
+    private Map<String, String> headersMap_ = new HashMap<String, String>();;
+    private String sBody_ = null;
+    private String method_ = null;
+    private String path_ = null;
+    private String version_ = null;
+
+
+    public String getMethod()
+    {
+        return method_;
+    }
+    
+    public String getPath()
+    {
+        return path_;
+    }
+    
+    public String getVersion()
+    {
+        return "HTTP/1.1";
+    }
+
+    public void addHeader(String name, String value)
+    {
+        headersMap_.put(name, value);
+    }
+
+    public String getHeader(String name)
+    {
+        return headersMap_.get(name).toString();
+    }
+
+    public void setBody(List<ByteBuffer> bodyBuffers)
+    {
+        StringBuffer sb = new StringBuffer();
+        while(bodyBuffers.size() > 0)
+        {
+            sb.append(bodyBuffers.remove(0).asCharBuffer().toString());
+        }
+        sBody_ = sb.toString();
+    }
+    
+    public String getBody()
+    {
+        return sBody_;
+    }
+    
+    public void setStartLine(String method, String path, String version)
+    {
+        method_ = method;
+        path_ = path;
+        version_ = version;
+    }    
+
+    public String toString()
+    {
+        StringWriter sw = new StringWriter();
+        PrintWriter pw = new PrintWriter(sw);
+        
+        pw.println("HttpResponse-------->");
+        pw.println("method = " + method_ + ", path = " + path_ + ", version = " + version_);
+        pw.println("Headers: " + headersMap_.toString());
+        pw.println("Body: " + sBody_);
+        pw.println("<--------HttpResponse");
+        
+        return sw.toString();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpStartLineParser.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpStartLineParser.java
index e69de29b..3719e3fe 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpStartLineParser.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpStartLineParser.java
@@ -0,0 +1,389 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * To change this template, choose Tools | Templates
+ * and open the template in the editor.
+ */
+
+package org.apache.cassandra.net.http;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.ByteBuffer;
+
+/**
+ *
+ * @author kranganathan
+ */
+public class HttpStartLineParser
+{
+    private Callback callback_;
+
+    public interface Callback
+    {
+        void onStartLine(String method, String path, String query, String version);
+    };
+    
+    public HttpStartLineParser(Callback cb)
+    {
+        callback_ = cb;
+    }
+    
+    private enum StartLineParseState
+    {
+        EATING_WHITESPACE,
+        READING_METHOD,
+        READING_PATH,
+        READING_QUERY,
+        DECODING_FIRST_CHAR,
+        DECODING_SECOND_CHAR,
+        READING_VERSION,
+        CHECKING_EOL,
+        TO_RESET
+    }
+    
+    private StartLineParseState parseState_ = StartLineParseState.TO_RESET;
+    private StartLineParseState nextState_;
+    private StringBuilder httpMethod_ = new StringBuilder(32);
+    private StringBuilder httpPath_ = new StringBuilder();
+    private StringBuilder httpQuery_ = new StringBuilder(32);
+    private StringBuilder httpVersion_ = new StringBuilder();
+    
+    // we will encode things of the form %{2 digit hex number} and this is a 
+    // temporary holder for the leftmost digit's value as the second digit is
+    // being read
+    private int encodedValue_;
+    // this is a pointer to one of httpMethod_, httpPath_, httpQuery_, 
+    // httpVersion_ so that the encoded value can be appended to the correct 
+    // buffer
+    private StringBuilder encodeTo_;
+
+    public void resetParserState() {
+        httpMethod_.setLength(0);
+        httpPath_.setLength(0);
+        httpQuery_.setLength(0);
+        httpVersion_.setLength(0);
+        
+        parseState_ = StartLineParseState.EATING_WHITESPACE;
+        nextState_ = StartLineParseState.READING_METHOD;
+    }
+    
+    private void finishLine_()
+    {
+        if (callback_ != null) 
+        {
+            callback_.onStartLine(
+                    httpMethod_.toString(), 
+                    httpPath_.toString(), 
+                    httpQuery_.toString(), 
+                    httpVersion_.toString()
+                    );
+        }
+    }
+    
+    private static int decodeHex(int hex)
+    {
+        if (hex >= '0' && hex <= '9')
+        {
+            return hex-'0';
+        }
+        else if (hex >= 'a' && hex <= 'f')
+        {
+            return hex-'a'+10;
+        }
+        else if (hex >= 'A' && hex <= 'F')
+        {
+            return hex-'A'+10;
+        }
+        else
+        {
+            return 0;
+        }
+    }
+    
+    public boolean onMoreBytes(InputStream in) throws HttpParsingException, IOException
+    {
+        int got;
+
+        if (parseState_ == StartLineParseState.TO_RESET)
+        {
+            resetParserState();
+        }
+
+        while (in.available() > 0)
+        {
+            in.mark(1);
+            got = in.read();
+			
+            switch (parseState_)
+            {
+                case EATING_WHITESPACE:
+                        switch (got)
+                        {
+                            case ' ':
+                                        break;
+                            default:
+                                        in.reset();
+                                        parseState_ = nextState_;
+                                        break;
+                        }
+                        break;
+                    
+                case READING_METHOD:
+                        switch (got)
+                        {
+                            case ' ':
+                                    parseState_ = StartLineParseState.EATING_WHITESPACE;
+                                    nextState_ = StartLineParseState.READING_PATH;
+                                    break;
+                            default:
+                                    httpMethod_.append((char) got);
+                                    break;
+                        }
+                        break;
+                        
+                case READING_PATH:
+                        switch (got)
+                        {
+                            case '\r':
+                                    parseState_ = StartLineParseState.CHECKING_EOL;
+                                    break;
+                            case '%':
+                                    encodeTo_ = httpPath_;
+                                    nextState_ = parseState_;
+                                    parseState_ = StartLineParseState.DECODING_FIRST_CHAR;
+                                    break;
+                            case ' ':
+                                    parseState_ = StartLineParseState.EATING_WHITESPACE;
+                                    nextState_ = StartLineParseState.READING_VERSION;
+                                    break;
+                            case '?':
+                                    parseState_ = StartLineParseState.READING_QUERY;
+                                    break;
+                            default:
+                                    httpPath_.append((char) got);
+                                    break;
+                        }
+                        break;
+                            
+                case READING_QUERY:
+                        switch (got)
+                        {
+                            case '\r':
+                                    parseState_ = StartLineParseState.CHECKING_EOL;
+                                    break;
+                            case '%':
+                                    encodeTo_ = httpQuery_;
+                                    nextState_ = parseState_;
+                                    parseState_ = StartLineParseState.DECODING_FIRST_CHAR;
+                                    break;
+                            case ' ':
+                                    parseState_ = StartLineParseState.EATING_WHITESPACE;
+                                    nextState_ = StartLineParseState.READING_VERSION;
+                                    break;
+                            case '+':
+                                    httpQuery_.append(' ');
+                                    break;
+                            default:
+                                    httpQuery_.append((char) got);
+                                    break;
+                        }
+                        break;
+                            
+                case DECODING_FIRST_CHAR:
+                        encodedValue_ = decodeHex(got) * 16;
+                        parseState_ = StartLineParseState.DECODING_SECOND_CHAR;
+                        break;
+
+                case DECODING_SECOND_CHAR:
+                        encodeTo_.append((char) (decodeHex(got) + encodedValue_));
+                        parseState_ = nextState_;
+                        break;
+                                
+                case READING_VERSION:
+                        switch (got)
+                        {
+                            case '\r':
+                                    parseState_ = StartLineParseState.CHECKING_EOL;
+                                    break;
+                            default:
+                                    httpVersion_.append((char) got);
+                                    break;
+                        }
+                        break;
+                        
+                case CHECKING_EOL:
+                        switch (got)
+                        {
+                            case '\n':
+                                    finishLine_();
+                                    parseState_ = StartLineParseState.TO_RESET;
+                                    return true;
+                            default:
+                                    throw new HttpParsingException();
+                        }
+                        
+                default:
+                        throw new HttpParsingException();
+            }
+        }
+        
+        return false;
+    }
+    
+    public boolean onMoreBytesNew(ByteBuffer buffer) throws HttpParsingException, IOException
+    {
+        int got;
+        int limit = buffer.limit();
+        int pos = buffer.position();
+    	
+        if (parseState_ == StartLineParseState.TO_RESET)
+        {
+            resetParserState();
+        }
+
+        while(pos < limit) 
+        {
+            switch(parseState_)
+            {	
+                case EATING_WHITESPACE:
+                        while((char)buffer.get(pos) == ' ' && ++pos < limit);
+                        if(pos < limit)
+                            parseState_ = nextState_;
+                        break;
+
+                case READING_METHOD:
+                        while(pos < limit && (got = buffer.get(pos)) != ' ') 
+                        {
+                            httpMethod_.append((char)got);
+                            pos++;
+                        }
+
+                        if(pos < limit)
+                        {
+                            parseState_ = StartLineParseState.EATING_WHITESPACE;
+                            nextState_ = StartLineParseState.READING_PATH;
+                        }
+                        break;
+
+                case READING_PATH:
+                        while(pos < limit && parseState_ == StartLineParseState.READING_PATH) 
+                        {
+                            got = buffer.get(pos++);
+
+                            switch (got)
+                            {
+                                case '\r':
+                                        parseState_ = StartLineParseState.CHECKING_EOL;
+                                        break;
+                                case '%':
+                                        encodeTo_ = httpPath_;
+                                        nextState_ = parseState_;
+                                        parseState_ = StartLineParseState.DECODING_FIRST_CHAR;
+                                        break;
+                                case ' ':
+                                        parseState_ = StartLineParseState.EATING_WHITESPACE;
+                                        nextState_ = StartLineParseState.READING_VERSION;
+                                        break;
+                                case '?':
+                                        parseState_ = StartLineParseState.READING_QUERY;
+                                        break;
+                                default:
+                                        httpPath_.append((char) got);
+                                        break;
+                            }
+                        }
+                        break;
+
+                case READING_QUERY:
+                        while(pos < limit && parseState_ == StartLineParseState.READING_QUERY) 
+                        {
+                            got = buffer.get(pos++);
+
+                            switch (got)
+                            {
+                                case '\r':
+                                        parseState_ = StartLineParseState.CHECKING_EOL;
+                                        break;
+                                case '%':
+                                        encodeTo_ = httpQuery_;
+                                        nextState_ = parseState_;
+                                        parseState_ = StartLineParseState.DECODING_FIRST_CHAR;
+                                        break;
+                                case ' ':
+                                        parseState_ = StartLineParseState.EATING_WHITESPACE;
+                                        nextState_ = StartLineParseState.READING_VERSION;
+                                        break;
+                                case '+':
+                                        httpQuery_.append(' ');
+                                        break;
+                                default:
+                                        httpQuery_.append((char) got);
+                                        break;
+                            }
+                        }
+                        break;
+
+                case DECODING_FIRST_CHAR:
+                        got = (int)buffer.get(pos++);
+                        encodedValue_ = decodeHex(got) * 16;
+                        parseState_ = StartLineParseState.DECODING_SECOND_CHAR;
+                        break;
+
+                case DECODING_SECOND_CHAR:
+                        got = (int)buffer.get(pos++);
+                        encodeTo_.append((char) (decodeHex(got) + encodedValue_));
+                        parseState_ = nextState_;
+                        break;
+
+                case READING_VERSION:
+                        while(pos < limit && (got = buffer.get(pos)) != '\r' )
+                        {
+                            httpVersion_.append((char)got);
+                            pos++;
+                        }
+                        if(pos < limit) 
+                        {
+                            parseState_ = StartLineParseState.CHECKING_EOL;
+                            pos++; // skipping '\r'
+                        }
+                        break;
+
+                case CHECKING_EOL:
+                        switch (buffer.get(pos++))
+                        {
+                            case '\n':
+                                    finishLine_();
+                                    parseState_ = StartLineParseState.TO_RESET;
+                                    buffer.position(pos);
+                                    return true;  //could have reached limit here
+                            default:
+                                    throw new HttpParsingException();
+                        }
+                        
+                default:
+                        throw new HttpParsingException();
+            }
+        } 
+
+        buffer.position(pos);
+        return false;
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpWriteResponse.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpWriteResponse.java
index e69de29b..bc184146 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpWriteResponse.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/http/HttpWriteResponse.java
@@ -0,0 +1,76 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * This class writes the HTTP 1.1 responses back to the client.
+ */
+
+package org.apache.cassandra.net.http;
+
+import java.net.*;
+import java.nio.ByteBuffer;
+import java.io.*;
+
+/**
+ *
+ * @author kranganathan
+ */
+public class HttpWriteResponse
+{
+    private HttpRequest httpRequest_ = null;
+    private StringBuilder body_ = new StringBuilder();
+
+    public HttpWriteResponse(HttpRequest httpRequest)
+    {
+        httpRequest_ = httpRequest;
+    }
+
+    public void println(String responseLine)
+    {
+        if(responseLine != null)
+        {
+            body_.append(responseLine);
+            body_.append( System.getProperty("line.separator"));
+        }
+    }
+
+    public ByteBuffer flush() throws Exception
+    {
+        StringBuilder sb = new StringBuilder();
+        // write out the HTTP response headers first
+        sb.append(httpRequest_.getVersion() + " 200 OK\r\n");
+        sb.append("Content-Type: text/html\r\n");
+        if(body_.length() > 0)
+        	sb.append("Content-Length: " + body_.length() + "\r\n");
+        sb.append("Cache-Control: no-cache\r\n");
+        sb.append("Pragma: no-cache\r\n");
+
+        // terminate the headers
+        sb.append("\r\n");
+
+        // now write out the HTTP response body
+        if(body_.length() > 0)
+            sb.append(body_.toString());
+
+        // terminate the body
+        //sb.append("\r\n");
+        //sb.append("\r\n");
+        ByteBuffer buffer = ByteBuffer.wrap(sb.toString().getBytes());
+        return buffer;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentLengthState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentLengthState.java
index e69de29b..93b2f786 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentLengthState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentLengthState.java
@@ -0,0 +1,67 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+
+import java.nio.ByteBuffer;
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+
+import org.apache.cassandra.utils.FBUtilities;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class ContentLengthState extends StartState
+{
+    private ByteBuffer buffer_;
+
+    ContentLengthState(TcpReader stream)
+    {
+        super(stream);
+        buffer_ = ByteBuffer.allocate(4);
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        return doRead(buffer_);
+    }
+
+    public void morphState() throws IOException
+    {
+        int size = FBUtilities.byteArrayToInt(buffer_.array());        
+        StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.CONTENT);
+        if ( nextState == null )
+        {
+            nextState = new ContentState(stream_, size);
+            stream_.putSocketState( TcpReader.TcpReaderState.CONTENT, nextState );
+        }
+        else
+        {               
+            nextState.setContextData(size);
+        }
+        stream_.morphState( nextState );
+        buffer_.clear();
+    }
+    
+    public void setContextData(Object data)
+    {
+        throw new UnsupportedOperationException("This method is not supported in the ContentLengthState");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentState.java
index e69de29b..7c479824 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentState.java
@@ -0,0 +1,84 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.nio.ByteBuffer;
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class ContentState extends StartState
+{
+    private ByteBuffer buffer_;   
+    private int length_;
+
+    ContentState(TcpReader stream, int length)
+    {
+        super(stream);
+        length_ = length; 
+        buffer_ = ByteBuffer.allocate(length_);
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {          
+        return doRead(buffer_);
+    }
+
+    public void morphState() throws IOException
+    {        
+        StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.DONE);
+        if ( nextState == null )
+        {
+            nextState = new DoneState(stream_, toBytes());
+            stream_.putSocketState( TcpReader.TcpReaderState.DONE, nextState );
+        }
+        else
+        {            
+            nextState.setContextData(toBytes());
+        }
+        stream_.morphState( nextState );               
+    }
+    
+    private byte[] toBytes()
+    {
+        buffer_.position(0); 
+        /*
+        ByteBuffer slice = buffer_.slice();        
+        return slice.array();
+        */  
+        byte[] bytes = new byte[length_];
+        buffer_.get(bytes, 0, length_);
+        return bytes;
+    }
+    
+    public void setContextData(Object data)
+    {
+        Integer value = (Integer)data;
+        length_ = value;               
+        buffer_.clear();
+        if ( buffer_.capacity() < length_ )
+            buffer_ = ByteBuffer.allocate(length_);
+        else
+        {            
+            buffer_.limit(length_);
+        }        
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentStreamState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentStreamState.java
index e69de29b..99f6ffad 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentStreamState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ContentStreamState.java
@@ -0,0 +1,136 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.net.InetSocketAddress;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.channels.SocketChannel;
+import java.io.*;
+
+import org.apache.cassandra.db.Table;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+
+class ContentStreamState extends StartState
+{       
+    private static Logger logger_ = Logger.getLogger(ContentStreamState.class);
+    private static long count_ = 64*1024*1024;
+    /* Return this byte array to exit event loop */
+    private static byte[] bytes_ = new byte[1];
+    private long bytesRead_ = 0L;
+    private FileChannel fc_;
+    private StreamContextManager.StreamContext streamContext_;
+    private StreamContextManager.StreamStatus streamStatus_;
+    
+    ContentStreamState(TcpReader stream)
+    {
+        super(stream); 
+        SocketChannel socketChannel = stream.getStream();
+        InetSocketAddress remoteAddress = (InetSocketAddress)socketChannel.socket().getRemoteSocketAddress();
+        String remoteHost = remoteAddress.getHostName();        
+        streamContext_ = StreamContextManager.getStreamContext(remoteHost);   
+        streamStatus_ = StreamContextManager.getStreamStatus(remoteHost);
+    }
+    
+    private void createFileChannel() throws IOException
+    {
+        if ( fc_ == null )
+        {
+            logger_.debug("Creating file for " + streamContext_.getTargetFile());
+            FileOutputStream fos = new FileOutputStream( streamContext_.getTargetFile(), true );
+            fc_ = fos.getChannel();            
+        }
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        SocketChannel socketChannel = stream_.getStream();
+        InetSocketAddress remoteAddress = (InetSocketAddress)socketChannel.socket().getRemoteSocketAddress();
+        String remoteHost = remoteAddress.getHostName();  
+        createFileChannel();
+        if ( streamContext_ != null )
+        {  
+            try
+            {
+                bytesRead_ += fc_.transferFrom(socketChannel, bytesRead_, ContentStreamState.count_);
+                if ( bytesRead_ != streamContext_.getExpectedBytes() )
+                    throw new ReadNotCompleteException("Specified number of bytes have not been read from the Socket Channel");
+            }
+            catch ( IOException ex )
+            {
+                /* Ask the source node to re-stream this file. */
+                streamStatus_.setAction(StreamContextManager.StreamCompletionAction.STREAM);                
+                handleStreamCompletion(remoteHost);
+                /* Delete the orphaned file. */
+                File file = new File(streamContext_.getTargetFile());
+                file.delete();
+                throw ex;
+            }
+            if ( bytesRead_ == streamContext_.getExpectedBytes() )
+            {       
+                logger_.debug("Removing stream context " + streamContext_);                 
+                handleStreamCompletion(remoteHost);                              
+                bytesRead_ = 0L;
+                fc_.close();
+                morphState();
+            }                            
+        }
+        
+        return new byte[0];
+    }
+    
+    private void handleStreamCompletion(String remoteHost) throws IOException
+    {
+        /* 
+         * Streaming is complete. If all the data that has to be received inform the sender via 
+         * the stream completion callback so that the source may perform the requisite cleanup. 
+        */
+        IStreamComplete streamComplete = StreamContextManager.getStreamCompletionHandler(remoteHost);
+        if ( streamComplete != null )
+        {                    
+            streamComplete.onStreamCompletion(remoteHost, streamContext_, streamStatus_);                    
+        }
+    }
+
+    public void morphState() throws IOException
+    {        
+        /* We instantiate an array of size 1 so that we can exit the event loop of the read. */                
+        StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.DONE);
+        if ( nextState == null )
+        {
+            nextState = new DoneState(stream_, ContentStreamState.bytes_);
+            stream_.putSocketState( TcpReader.TcpReaderState.DONE, nextState );
+        }
+        else
+        {
+            nextState.setContextData(ContentStreamState.bytes_);
+        }
+        stream_.morphState( nextState );  
+    }
+    
+    public void setContextData(Object data)
+    {
+        throw new UnsupportedOperationException("This method is not supported in the ContentStreamState");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/DoneState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/DoneState.java
index e69de29b..c03587f8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/DoneState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/DoneState.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class DoneState extends StartState
+{
+    private byte[] bytes_ = new byte[0];
+
+    DoneState(TcpReader stream, byte[] bytes)
+    {
+        super(stream);
+        bytes_ = bytes;
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        morphState();
+        return bytes_;
+    }
+
+    public void morphState() throws IOException
+    {                       
+        stream_.morphState(null);
+    }
+    
+    public void setContextData(Object data)
+    {                
+        bytes_ = (byte[])data;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/FastSerializer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/FastSerializer.java
index e69de29b..43b14b56 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/FastSerializer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/FastSerializer.java
@@ -0,0 +1,46 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.IOException;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.net.Message;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class FastSerializer implements ISerializer
+{ 
+    public byte[] serialize(Message message) throws IOException
+    {
+        DataOutputBuffer buffer = new DataOutputBuffer();
+        Message.serializer().serialize(message, buffer);
+        return buffer.getData();
+    }
+    
+    public Message deserialize(byte[] bytes) throws IOException
+    {
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(bytes, bytes.length);
+        return Message.serializer().deserialize(bufIn);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ISerializer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ISerializer.java
index e69de29b..c25843a5 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ISerializer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ISerializer.java
@@ -0,0 +1,32 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.IOException;
+
+import org.apache.cassandra.net.Message;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface ISerializer
+{
+    public byte[] serialize(Message message) throws IOException;
+    public Message deserialize(byte[] bytes) throws IOException;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/IStreamComplete.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/IStreamComplete.java
index e69de29b..7fcbdd16 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/IStreamComplete.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/IStreamComplete.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.IOException;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IStreamComplete
+{
+    /*
+     * This callback if registered with the StreamContextManager is 
+     * called when the stream from a host is completely handled. 
+    */
+    public void onStreamCompletion(String from, StreamContextManager.StreamContext streamContext, StreamContextManager.StreamStatus streamStatus) throws IOException;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ProtocolHeaderState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ProtocolHeaderState.java
index e69de29b..87aafd6b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ProtocolHeaderState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ProtocolHeaderState.java
@@ -0,0 +1,103 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import org.apache.cassandra.utils.*;
+import java.nio.ByteBuffer;
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+import org.apache.cassandra.net.MessagingService;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ProtocolHeaderState extends StartState
+{
+    private ByteBuffer buffer_;
+
+    public ProtocolHeaderState(TcpReader stream)
+    {
+        super(stream);
+        buffer_ = ByteBuffer.allocate(4);
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        return doRead(buffer_);
+    }
+
+    public void morphState() throws IOException
+    {
+        byte[] protocolHeader = buffer_.array();
+        int pH = MessagingService.byteArrayToInt(protocolHeader);
+        
+        int type = MessagingService.getBits(pH, 1, 2);
+        stream_.getProtocolHeader().serializerType_ = type;
+        
+        int stream = MessagingService.getBits(pH, 3, 1);
+        stream_.getProtocolHeader().isStreamingMode_ = (stream == 1) ? true : false;
+        
+        if ( stream_.getProtocolHeader().isStreamingMode_ )
+            MessagingService.setStreamingMode(true);
+        
+        int listening = MessagingService.getBits(pH, 4, 1);
+        stream_.getProtocolHeader().isListening_ = (listening == 1) ? true : false;
+        
+        int version = MessagingService.getBits(pH, 15, 8);
+        stream_.getProtocolHeader().version_ = version;
+        
+        if ( version <= MessagingService.getVersion() )
+        {
+            if ( stream_.getProtocolHeader().isStreamingMode_ )
+            { 
+                StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.CONTENT_STREAM);
+                if ( nextState == null )
+                {
+                    nextState = new ContentStreamState(stream_);
+                    stream_.putSocketState( TcpReader.TcpReaderState.CONTENT_STREAM, nextState );
+                }
+                stream_.morphState( nextState );
+                buffer_.clear();
+            }
+            else
+            {
+                StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.CONTENT_LENGTH);
+                if ( nextState == null )
+                {
+                    nextState = new ContentLengthState(stream_);
+                    stream_.putSocketState( TcpReader.TcpReaderState.CONTENT_LENGTH, nextState );
+                }                
+                stream_.morphState( nextState );   
+                buffer_.clear();
+            }            
+        }
+        else
+        {
+            throw new IOException("Invalid version in message. Scram.");
+        }
+    }
+    
+    public void setContextData(Object data)
+    {
+        throw new UnsupportedOperationException("This method is not supported in the ProtocolHeaderState");
+    }
+}
+
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ProtocolState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ProtocolState.java
index e69de29b..1e4ba09d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ProtocolState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ProtocolState.java
@@ -0,0 +1,71 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import org.apache.cassandra.utils.*;
+import java.nio.ByteBuffer;
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+import org.apache.cassandra.net.MessagingService;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ProtocolState extends StartState
+{
+    private ByteBuffer buffer_;
+
+    public ProtocolState(TcpReader stream)
+    {
+        super(stream);
+        buffer_ = ByteBuffer.allocate(16);
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        return doRead(buffer_);
+    }
+
+    public void morphState() throws IOException
+    {
+        byte[] protocol = buffer_.array();
+        if ( MessagingService.isProtocolValid(protocol) )
+        {            
+            StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.PROTOCOL);
+            if ( nextState == null )
+            {
+                nextState = new ProtocolHeaderState(stream_);
+                stream_.putSocketState( TcpReader.TcpReaderState.PROTOCOL, nextState );
+            }
+            stream_.morphState( nextState ); 
+            buffer_.clear();
+        }
+        else
+        {
+            throw new IOException("Invalid protocol header. The preamble seems to be messed up.");
+        }
+    }
+    
+    public void setContextData(Object data)
+    {
+        throw new UnsupportedOperationException("This method is not supported in the ProtocolState");
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ReadNotCompleteException.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ReadNotCompleteException.java
index e69de29b..865a6361 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ReadNotCompleteException.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/ReadNotCompleteException.java
@@ -0,0 +1,34 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+/**
+ * Created by IntelliJ IDEA.
+ * User: lakshman
+ * Date: Aug 22, 2005
+ * Time: 11:37:31 AM
+ * To change this template use File | Settings | File Templates.
+ */
+public class ReadNotCompleteException extends Exception
+{
+    ReadNotCompleteException(String message)
+    {
+        super(message);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/SerializerAttribute.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/SerializerAttribute.java
index e69de29b..cb6e869f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/SerializerAttribute.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/SerializerAttribute.java
@@ -0,0 +1,27 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.lang.annotation.*;
+
+@Retention(RetentionPolicy.RUNTIME)
+public @interface SerializerAttribute
+{
+    SerializerType value();
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/SerializerType.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/SerializerType.java
index e69de29b..6bb328fc 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/SerializerType.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/SerializerType.java
@@ -0,0 +1,27 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+public enum SerializerType
+{
+    BINARY,
+    JAVA,
+    XML,
+    JSON
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/StartState.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/StartState.java
index e69de29b..7ed72580 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/StartState.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/StartState.java
@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.nio.channels.SocketChannel;
+import java.nio.ByteBuffer;
+import java.io.IOException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public abstract class StartState
+{
+    protected TcpReader stream_;
+
+    public StartState(TcpReader stream)
+    {
+        stream_ = stream;
+    }
+
+    public abstract byte[] read() throws IOException, ReadNotCompleteException;
+    public abstract void morphState() throws IOException;
+    public abstract void setContextData(Object data);
+
+    protected byte[] doRead(ByteBuffer buffer) throws IOException, ReadNotCompleteException
+    {        
+        SocketChannel socketChannel = stream_.getStream();
+        int bytesRead = socketChannel.read(buffer);     
+        if ( bytesRead == -1 && buffer.remaining() > 0 )
+        {            
+            throw new IOException("Reached an EOL or something bizzare occured. Reading from: " + socketChannel.socket().getInetAddress() + " BufferSizeRemaining: " + buffer.remaining());
+        }
+        if ( buffer.remaining() == 0 )
+        {
+            morphState();
+        }
+        else
+        {            
+            throw new ReadNotCompleteException("Specified number of bytes have not been read from the Socket Channel");
+        }
+        return new byte[0];
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/StreamContextManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/StreamContextManager.java
index e69de29b..a3e6f408 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/StreamContextManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/StreamContextManager.java
@@ -0,0 +1,325 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.*;
+import java.util.concurrent.LinkedBlockingQueue;
+import javax.xml.bind.annotation.XmlElement;
+
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.dht.BootstrapInitiateMessage;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class StreamContextManager
+{
+    private static Logger logger_ = Logger.getLogger(StreamContextManager.class);
+    
+    public static enum StreamCompletionAction
+    {
+        DELETE,
+        STREAM
+    }
+    
+    public static class StreamContext implements Serializable
+    {
+        private static Logger logger_ = Logger.getLogger(StreamContextManager.StreamContext.class);
+        private static ICompactSerializer<StreamContext> serializer_;
+        
+        static
+        {
+            serializer_ = new StreamContextSerializer();
+        }
+        
+        public static ICompactSerializer<StreamContext> serializer()
+        {
+            return serializer_;
+        }
+                
+        private String targetFile_;        
+        private long expectedBytes_;                     
+        
+        public StreamContext(String targetFile, long expectedBytes)
+        {
+            targetFile_ = targetFile;
+            expectedBytes_ = expectedBytes;         
+        }                
+                
+        public String getTargetFile()
+        {
+            return targetFile_;
+        }
+        
+        public void setTargetFile(String file)
+        {
+            targetFile_ = file;
+        }
+        
+        public long getExpectedBytes()
+        {
+            return expectedBytes_;
+        }
+                
+        public boolean equals(Object o)
+        {
+            if ( !(o instanceof StreamContext) )
+                return false;
+            
+            StreamContext rhs = (StreamContext)o;
+            return targetFile_.equals(rhs.targetFile_);
+        }
+        
+        public int hashCode()
+        {
+            return toString().hashCode();
+        }
+        
+        public String toString()
+        {
+            return targetFile_ + ":" + expectedBytes_;
+        }
+    }
+    
+    public static class StreamContextSerializer implements ICompactSerializer<StreamContext>
+    {
+        public void serialize(StreamContextManager.StreamContext sc, DataOutputStream dos) throws IOException
+        {
+            dos.writeUTF(sc.targetFile_);
+            dos.writeLong(sc.expectedBytes_);            
+        }
+        
+        public StreamContextManager.StreamContext deserialize(DataInputStream dis) throws IOException
+        {
+            String targetFile = dis.readUTF();
+            long expectedBytes = dis.readLong();           
+            return new StreamContext(targetFile, expectedBytes);
+        }
+    }
+    
+    public static class StreamStatus
+    {
+        private static ICompactSerializer<StreamStatus> serializer_;
+        
+        static 
+        {
+            serializer_ = new StreamStatusSerializer();
+        }
+        
+        public static ICompactSerializer<StreamStatus> serializer()
+        {
+            return serializer_;
+        }
+            
+        private String file_;               
+        private long expectedBytes_;                
+        private StreamCompletionAction action_;
+                
+        public StreamStatus(String file, long expectedBytes)
+        {
+            file_ = file;
+            expectedBytes_ = expectedBytes;
+            action_ = StreamContextManager.StreamCompletionAction.DELETE;
+        }
+        
+        public String getFile()
+        {
+            return file_;
+        }
+        
+        public long getExpectedBytes()
+        {
+            return expectedBytes_;
+        }
+        
+        void setAction(StreamContextManager.StreamCompletionAction action)
+        {
+            action_ = action;
+        }
+        
+        public StreamContextManager.StreamCompletionAction getAction()
+        {
+            return action_;
+        }
+    }
+    
+    public static class StreamStatusSerializer implements ICompactSerializer<StreamStatus>
+    {
+        public void serialize(StreamStatus streamStatus, DataOutputStream dos) throws IOException
+        {
+            dos.writeUTF(streamStatus.getFile());
+            dos.writeLong(streamStatus.getExpectedBytes());
+            dos.writeInt(streamStatus.getAction().ordinal());
+        }
+        
+        public StreamStatus deserialize(DataInputStream dis) throws IOException
+        {
+            String targetFile = dis.readUTF();
+            long expectedBytes = dis.readLong();
+            StreamStatus streamStatus = new StreamStatus(targetFile, expectedBytes);
+            
+            int ordinal = dis.readInt();                        
+            if ( ordinal == StreamCompletionAction.DELETE.ordinal() )
+            {
+                streamStatus.setAction(StreamCompletionAction.DELETE);
+            }
+            else if ( ordinal == StreamCompletionAction.STREAM.ordinal() )
+            {
+                streamStatus.setAction(StreamCompletionAction.STREAM);
+            }
+            
+            return streamStatus;
+        }
+    }
+    
+    public static class StreamStatusMessage implements Serializable
+    {
+        private static ICompactSerializer<StreamStatusMessage> serializer_;
+        
+        static 
+        {
+            serializer_ = new StreamStatusMessageSerializer();
+        }
+        
+        public static ICompactSerializer<StreamStatusMessage> serializer()
+        {
+            return serializer_;
+        }
+        
+        public static Message makeStreamStatusMessage(StreamStatusMessage streamStatusMessage) throws IOException
+        {
+            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+            DataOutputStream dos = new DataOutputStream( bos );
+            StreamStatusMessage.serializer().serialize(streamStatusMessage, dos);
+            return new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapTerminateVerbHandler_, new Object[]{bos.toByteArray()});
+        }
+        
+        protected StreamContextManager.StreamStatus streamStatus_;
+        
+        public StreamStatusMessage(StreamContextManager.StreamStatus streamStatus)
+        {
+            streamStatus_ = streamStatus;
+        }
+        
+        public StreamContextManager.StreamStatus getStreamStatus()
+        {
+            return streamStatus_;
+        }
+    }
+    
+    public static class StreamStatusMessageSerializer implements ICompactSerializer<StreamStatusMessage>
+    {
+        public void serialize(StreamStatusMessage streamStatusMessage, DataOutputStream dos) throws IOException
+        {
+            StreamStatus.serializer().serialize(streamStatusMessage.streamStatus_, dos);            
+        }
+        
+        public StreamStatusMessage deserialize(DataInputStream dis) throws IOException
+        {            
+            StreamContextManager.StreamStatus streamStatus = StreamStatus.serializer().deserialize(dis);         
+            return new StreamStatusMessage(streamStatus);
+        }
+    }
+        
+    /* Maintain a stream context per host that is the source of the stream */
+    public static Map<String, List<StreamContext>> ctxBag_ = new Hashtable<String, List<StreamContext>>();  
+    /* Maintain in this map the status of the streams that need to be sent back to the source */
+    public static Map<String, List<StreamStatus>> streamStatusBag_ = new Hashtable<String, List<StreamStatus>>();
+    /* Maintains a callback handler per endpoint to notify the app that a stream from a given endpoint has been handled */
+    public static Map<String, IStreamComplete> streamNotificationHandlers_ = new HashMap<String, IStreamComplete>();
+    
+    public synchronized static StreamContext getStreamContext(String key)
+    {        
+        List<StreamContext> context = ctxBag_.get(key);
+        if ( context == null )
+            throw new IllegalStateException("Streaming context has not been set.");
+        StreamContext streamContext = context.remove(0);        
+        if ( context.isEmpty() )
+            ctxBag_.remove(key);
+        return streamContext;
+    }
+    
+    public synchronized static StreamStatus getStreamStatus(String key)
+    {
+        List<StreamStatus> status = streamStatusBag_.get(key);
+        if ( status == null )
+            throw new IllegalStateException("Streaming status has not been set.");
+        StreamStatus streamStatus = status.remove(0);        
+        if ( status.isEmpty() )
+            streamStatusBag_.remove(key);
+        return streamStatus;
+    }
+    
+    /*
+     * This method helps determine if the StreamCompletionHandler needs
+     * to be invoked for the data being streamed from a source. 
+    */
+    public synchronized static boolean isDone(String key)
+    {
+        return (ctxBag_.get(key) == null);
+    }
+    
+    public synchronized static IStreamComplete getStreamCompletionHandler(String key)
+    {
+        return streamNotificationHandlers_.get(key);
+    }
+    
+    public synchronized static void removeStreamCompletionHandler(String key)
+    {
+        streamNotificationHandlers_.remove(key);
+    }
+    
+    public synchronized static void registerStreamCompletionHandler(String key, IStreamComplete streamComplete)
+    {
+        streamNotificationHandlers_.put(key, streamComplete);
+    }
+    
+    public synchronized static void addStreamContext(String key, StreamContext streamContext, StreamStatus streamStatus)
+    {
+        /* Record the stream context */
+        List<StreamContext> context = ctxBag_.get(key);        
+        if ( context == null )
+        {
+            context = new ArrayList<StreamContext>();
+            ctxBag_.put(key, context);
+        }
+        context.add(streamContext);
+        
+        /* Record the stream status for this stream context */
+        List<StreamStatus> status = streamStatusBag_.get(key);
+        if ( status == null )
+        {
+            status = new ArrayList<StreamStatus>();
+            streamStatusBag_.put(key, status);
+        }
+        status.add( streamStatus );
+    }        
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/TcpReader.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/TcpReader.java
index e69de29b..a979cc94 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/TcpReader.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/io/TcpReader.java
@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.IOException;
+import java.nio.channels.SocketChannel;
+import java.util.*;
+
+import org.apache.cassandra.net.ProtocolHeader;
+import org.apache.cassandra.net.TcpConnection;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TcpReader
+{
+    public static enum TcpReaderState
+    {
+        START,
+        PREAMBLE,
+        PROTOCOL,
+        CONTENT_LENGTH,
+        CONTENT,
+        CONTENT_STREAM,
+        DONE
+    }
+    
+    private Map<TcpReaderState, StartState> stateMap_ = new HashMap<TcpReaderState, StartState>();
+    private TcpConnection connection_;
+    private StartState socketState_;
+    private ProtocolHeader protocolHeader_;
+    
+    public TcpReader(TcpConnection connection)
+    {
+        connection_ = connection;        
+    }
+    
+    public StartState getSocketState(TcpReaderState state)
+    {
+        return stateMap_.get(state);
+    }
+    
+    public void putSocketState(TcpReaderState state, StartState socketState)
+    {
+        stateMap_.put(state, socketState);
+    } 
+    
+    public void resetState()
+    {
+        StartState nextState = stateMap_.get(TcpReaderState.PREAMBLE);
+        if ( nextState == null )
+        {
+            nextState = new ProtocolState(this);
+            stateMap_.put(TcpReaderState.PREAMBLE, nextState);
+        }
+        socketState_ = nextState;
+    }
+    
+    public void morphState(StartState state)
+    {        
+        socketState_ = state;
+        if ( protocolHeader_ == null )
+            protocolHeader_ = new ProtocolHeader();
+    }
+    
+    public ProtocolHeader getProtocolHeader()
+    {
+        return protocolHeader_;
+    }
+    
+    public SocketChannel getStream()
+    {
+        return connection_.getSocketChannel();
+    }
+    
+    public byte[] read() throws IOException
+    {
+        byte[] bytes = new byte[0];      
+        while ( socketState_ != null )
+        {
+            try
+            {                                                                      
+                bytes = socketState_.read();
+            }
+            catch ( ReadNotCompleteException e )
+            {                
+                break;
+            }
+        }
+        return bytes;
+    }    
+    
+    public static void main(String[] args) throws Throwable
+    {
+        Map<TcpReaderState, StartState> stateMap = new HashMap<TcpReaderState, StartState>();
+        stateMap.put(TcpReaderState.CONTENT, new ContentState(null, 10));
+        stateMap.put(TcpReaderState.START, new ProtocolState(null));
+        stateMap.put(TcpReaderState.CONTENT_LENGTH, new ContentLengthState(null));
+        
+        StartState state = stateMap.get(TcpReaderState.CONTENT);
+        System.out.println( state.getClass().getName() );
+        state = stateMap.get(TcpReaderState.CONTENT_LENGTH);
+        System.out.println( state.getClass().getName() );
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/sink/IMessageSink.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/sink/IMessageSink.java
index e69de29b..ba7c83c0 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/sink/IMessageSink.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/sink/IMessageSink.java
@@ -0,0 +1,30 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.sink;
+
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IMessageSink
+{
+    public Message handleMessage(Message message);    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/sink/SinkManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/sink/SinkManager.java
index e69de29b..ab71bdc2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/sink/SinkManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/net/sink/SinkManager.java
@@ -0,0 +1,78 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.sink;
+
+import java.util.*;
+import java.io.IOException;
+
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SinkManager
+{
+    private static LinkedList<IMessageSink> messageSinks_ = new LinkedList<IMessageSink>();
+
+    public static boolean isInitialized()
+    {
+        return ( messageSinks_.size() > 0 );
+    }
+
+    public static void addMessageSink(IMessageSink ms)
+    {
+        messageSinks_.addLast(ms);
+    }
+    
+    public static void clearSinks(){
+        messageSinks_.clear();
+    }
+
+    public static Message processClientMessageSink(Message message)
+    {
+        ListIterator<IMessageSink> li = messageSinks_.listIterator();
+        while ( li.hasNext() )
+        {
+            IMessageSink ms = li.next();
+            message = ms.handleMessage(message);
+            if ( message == null )
+            {
+                return null;
+            }
+        }
+        return message;
+    }
+
+    public static Message processServerMessageSink(Message message)
+    {
+        ListIterator<IMessageSink> li = messageSinks_.listIterator(messageSinks_.size());
+        while ( li.hasPrevious() )
+        {
+            IMessageSink ms = li.previous();
+            message = ms.handleMessage(message);
+            if ( message == null )
+            {
+                return null;
+            }
+        }
+        return message;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/procedures/GroovyScriptRunner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/procedures/GroovyScriptRunner.java
index e69de29b..e3a45484 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/procedures/GroovyScriptRunner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/procedures/GroovyScriptRunner.java
@@ -0,0 +1,13 @@
+package org.apache.cassandra.procedures;
+
+import groovy.lang.GroovyShell;
+
+public class GroovyScriptRunner
+{
+	private static GroovyShell groovyShell_ = new GroovyShell();
+
+	public static String evaluateString(String script)
+	{        
+		 return groovyShell_.evaluate(script).toString();
+	}
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/CassandraDaemon.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/CassandraDaemon.java
index e69de29b..9bd59d2b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/CassandraDaemon.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/CassandraDaemon.java
@@ -0,0 +1,147 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.File;
+import java.io.IOException;
+import java.net.InetSocketAddress;
+
+import org.apache.log4j.Logger;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.protocol.TProtocolFactory;
+import org.apache.thrift.server.TThreadPoolServer;
+import org.apache.thrift.transport.TServerSocket;
+import org.apache.thrift.transport.TTransportException;
+import org.apache.thrift.transport.TTransportFactory;
+import org.apache.thrift.TProcessorFactory;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.FBUtilities;
+
+/**
+ * This class supports two methods for creating a Cassandra node daemon, 
+ * invoking the class's main method, and using the jsvc wrapper from 
+ * commons-daemon, (for more information on using this class with the 
+ * jsvc wrapper, see the 
+ * <a href="http://commons.apache.org/daemon/jsvc.html">Commons Daemon</a>
+ * documentation).
+ * 
+ * @author Eric Evans <eevans@racklabs.com>
+ * 
+ */
+
+public class CassandraDaemon
+{
+    private static Logger logger = Logger.getLogger(CassandraDaemon.class);
+    private TThreadPoolServer serverEngine;
+
+    private void setup() throws IOException, TTransportException
+    {
+        int listenPort = DatabaseDescriptor.getThriftPort();
+        
+        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler()
+        {
+            public void uncaughtException(Thread t, Throwable e)
+            {
+                logger.error("Fatal exception in thread " + t, e);
+            }
+        });
+        
+        CassandraServer peerStorageServer = new CassandraServer();
+        peerStorageServer.start();
+        Cassandra.Processor processor = new Cassandra.Processor(peerStorageServer);
+
+        // Transport
+        TServerSocket tServerSocket = new TServerSocket(new InetSocketAddress(FBUtilities.getHostAddress(), listenPort));
+
+        // Protocol factory
+        TProtocolFactory tProtocolFactory = new TBinaryProtocol.Factory();
+
+        // ThreadPool Server
+        TThreadPoolServer.Options options = new TThreadPoolServer.Options();
+        options.minWorkerThreads = 64;
+        serverEngine = new TThreadPoolServer(new TProcessorFactory(processor),
+                                             tServerSocket,
+                                             new TTransportFactory(),
+                                             new TTransportFactory(),
+                                             tProtocolFactory,
+                                             tProtocolFactory,
+                                             options);
+    }
+
+    /** hook for JSVC */
+    public void init(String[] args) throws IOException, TTransportException
+    {  
+        setup();
+    }
+
+    /** hook for JSVC */
+    public void start()
+    {
+        logger.info("Cassandra starting up...");
+        serverEngine.serve();
+    }
+
+    /** hook for JSVC */
+    public void stop()
+    {
+        logger.info("Cassandra shutting down...");
+        serverEngine.stop();
+    }
+    
+    
+    /** hook for JSVC */
+    public void destroy()
+    {        
+    }
+    
+    public static void main(String[] args)
+    {
+        CassandraDaemon daemon = new CassandraDaemon();
+        String pidFile = System.getProperty("cassandra-pidfile");
+        
+        try
+        {   
+            daemon.setup();
+
+            if (pidFile != null)
+            {
+                new File(pidFile).deleteOnExit();
+            }
+
+            if (System.getProperty("cassandra-foreground") == null)
+            {
+                System.out.close();
+                System.err.close();
+            }
+
+            daemon.start();
+        }
+        catch (Exception e)
+        {
+            String msg = "Exception encountered during startup.";
+            logger.error(msg, e);
+
+            // try to warn user on stdout too, if we haven't already detached
+            System.out.println(msg);
+            e.printStackTrace();
+
+            System.exit(3);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/CassandraServer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/CassandraServer.java
index e69de29b..51a00981 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/CassandraServer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/CassandraServer.java
@@ -0,0 +1,604 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.BufferedInputStream;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Arrays;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.cql.common.CqlResult;
+import org.apache.cassandra.cql.driver.CqlDriver;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.ColumnReadCommand;
+import org.apache.cassandra.db.ColumnsSinceReadCommand;
+import org.apache.cassandra.db.SliceByNamesReadCommand;
+import org.apache.cassandra.db.SliceReadCommand;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.db.ColumnFamilyNotDefinedException;
+import org.apache.cassandra.db.TableNotDefinedException;
+import org.apache.cassandra.db.RangeCommand;
+import org.apache.cassandra.db.RangeReply;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.IAsyncResult;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.dht.OrderPreservingPartitioner;
+import org.apache.thrift.TException;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class CassandraServer implements Cassandra.Iface
+{
+	private static Logger logger_ = Logger.getLogger(CassandraServer.class);
+
+    private final static List<column_t> EMPTY_COLUMNS = Arrays.asList();
+    private final static List<superColumn_t> EMPTY_SUPERCOLUMNS = Arrays.asList();
+
+    /*
+      * Handle to the storage service to interact with the other machines in the
+      * cluster.
+      */
+	protected StorageService storageService;
+
+	protected CassandraServer()
+	{
+		storageService = StorageService.instance();
+	}
+
+	/*
+	 * The start function initializes the server and start's listening on the
+	 * specified port.
+	 */
+	public void start() throws IOException
+    {
+		LogUtil.init();
+		//LogUtil.setLogLevel("com.facebook", "DEBUG");
+		// Start the storage service
+		storageService.start();
+	}
+	
+	private void validateCommand(String key, String tablename, String... columnFamilyNames) throws InvalidRequestException
+	{
+        if (key.isEmpty())
+        {
+            throw new InvalidRequestException("Key may not be empty");
+        }
+		if ( !DatabaseDescriptor.getTables().contains(tablename) )
+		{
+			throw new TableNotDefinedException("Table " + tablename + " does not exist in this schema.");
+		}
+        Table table = Table.open(tablename);
+        for (String cfName : columnFamilyNames)
+        {
+            if (!table.getColumnFamilies().contains(cfName))
+            {
+                throw new ColumnFamilyNotDefinedException("Column Family " + cfName + " is invalid.");
+            }
+        }
+	}
+    
+	protected ColumnFamily readColumnFamily(ReadCommand command) throws InvalidRequestException
+    {
+        String cfName = command.getColumnFamilyName();
+        validateCommand(command.key, command.table, cfName);
+
+        Row row;
+        try
+        {
+            row = StorageProxy.readProtocol(command, StorageService.ConsistencyLevel.WEAK);
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+        catch (TimeoutException e)
+        {
+            throw new RuntimeException(e);
+        }
+
+        if (row == null)
+        {
+            return null;
+        }
+        return row.getColumnFamily(cfName);
+	}
+
+    public List<column_t> thriftifyColumns(Collection<IColumn> columns)
+    {
+        if (columns == null || columns.isEmpty())
+        {
+            return EMPTY_COLUMNS;
+        }
+
+        ArrayList<column_t> thriftColumns = new ArrayList<column_t>(columns.size());
+        for (IColumn column : columns)
+        {
+            if (column.isMarkedForDelete())
+            {
+                continue;
+            }
+            column_t thrift_column = new column_t(column.name(), column.value(), column.timestamp());
+            thriftColumns.add(thrift_column);
+        }
+
+        return thriftColumns;
+    }
+
+    public List<column_t> get_columns_since(String tablename, String key, String columnFamily_column, long timeStamp) throws InvalidRequestException
+    {
+        long startTime = System.currentTimeMillis();
+        try
+        {
+            ColumnFamily cfamily = readColumnFamily(new ColumnsSinceReadCommand(tablename, key, columnFamily_column, timeStamp));
+            String[] values = RowMutation.getColumnAndColumnFamily(columnFamily_column);
+            if (cfamily == null)
+            {
+                return EMPTY_COLUMNS;
+            }
+            Collection<IColumn> columns = null;
+            if( values.length > 1 )
+            {
+                // this is the super column case
+                IColumn column = cfamily.getColumn(values[1]);
+                if(column != null)
+                    columns = column.getSubColumns();
+            }
+            else
+            {
+                columns = cfamily.getAllColumns();
+            }
+            return thriftifyColumns(columns);
+        }
+        finally
+        {
+            logger_.debug("get_slice2: " + (System.currentTimeMillis() - startTime) + " ms.");
+        }
+	}
+	
+
+    public List<column_t> get_slice_by_names(String tablename, String key, String columnFamily, List<String> columnNames) throws InvalidRequestException
+    {
+        long startTime = System.currentTimeMillis();
+        try
+        {
+            ColumnFamily cfamily = readColumnFamily(new SliceByNamesReadCommand(tablename, key, columnFamily, columnNames));
+            if (cfamily == null)
+            {
+                return EMPTY_COLUMNS;
+            }
+            Collection<IColumn> columns = null;
+            columns = cfamily.getAllColumns();
+            return thriftifyColumns(columns);
+        }
+        finally
+        {
+            logger_.debug("get_slice2: " + (System.currentTimeMillis() - startTime) + " ms.");
+        }
+    }
+    
+    public List<column_t> get_slice(String tablename, String key, String columnFamily_column, int start, int count) throws InvalidRequestException
+    {
+        long startTime = System.currentTimeMillis();
+		try
+		{
+	        String[] values = RowMutation.getColumnAndColumnFamily(columnFamily_column);
+            ColumnFamily cfamily = readColumnFamily(new SliceReadCommand(tablename, key, columnFamily_column, start, count));
+            if (cfamily == null)
+			{
+                return EMPTY_COLUMNS;
+			}
+			Collection<IColumn> columns = null;
+			if( values.length > 1 )
+			{
+				// this is the super column case 
+				IColumn column = cfamily.getColumn(values[1]);
+				if(column != null)
+					columns = column.getSubColumns();
+			}
+			else
+			{
+				columns = cfamily.getAllColumns();
+			}
+            return thriftifyColumns(columns);
+		}
+        finally
+        {
+            logger_.debug("get_slice2: " + (System.currentTimeMillis() - startTime) + " ms.");
+        }
+	}
+    
+    public column_t get_column(String tablename, String key, String columnFamily_column) throws NotFoundException, InvalidRequestException
+    {
+        String[] values = RowMutation.getColumnAndColumnFamily(columnFamily_column);
+        if (values.length < 2)
+        {
+            throw new InvalidRequestException("get_column requires both parts of columnfamily:column");
+        }
+        ColumnFamily cfamily = readColumnFamily(new ColumnReadCommand(tablename, key, columnFamily_column));
+        if (cfamily == null)
+        {
+            throw new NotFoundException();
+        }
+        Collection<IColumn> columns = null;
+        if( values.length > 2 )
+        {
+            // this is the super column case
+            IColumn column = cfamily.getColumn(values[1]);
+            if(column != null)
+                columns = column.getSubColumns();
+        }
+        else
+        {
+            columns = cfamily.getAllColumns();
+        }
+        if (columns == null || columns.size() == 0)
+        {
+            throw new NotFoundException();
+        }
+
+        assert columns.size() == 1;
+        IColumn column = columns.iterator().next();
+        if (column.isMarkedForDelete())
+        {
+            throw new NotFoundException();
+        }
+
+        return new column_t(column.name(), column.value(), column.timestamp());
+    }
+    
+
+    public int get_column_count(String tablename, String key, String columnFamily_column) throws InvalidRequestException
+    {
+        String[] values = RowMutation.getColumnAndColumnFamily(columnFamily_column);
+        ColumnFamily cfamily = readColumnFamily(new SliceReadCommand(tablename, key, columnFamily_column, -1, Integer.MAX_VALUE));
+        if (cfamily == null)
+        {
+            return 0;
+        }
+        Collection<IColumn> columns = null;
+        if( values.length > 1 )
+        {
+            // this is the super column case
+            IColumn column = cfamily.getColumn(values[1]);
+            if(column != null)
+                columns = column.getSubColumns();
+        }
+        else
+        {
+            columns = cfamily.getAllColumns();
+        }
+        if (columns == null || columns.size() == 0)
+        {
+            return 0;
+        }
+        return columns.size();
+	}
+
+    public void insert(String tablename, String key, String columnFamily_column, byte[] cellData, long timestamp)
+	{
+        RowMutation rm = new RowMutation(tablename, key.trim());
+        rm.add(columnFamily_column, cellData, timestamp);
+        try
+        {
+            validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0]));
+        }
+        catch (InvalidRequestException e)
+        {
+            throw new RuntimeException(e);
+        }
+        StorageProxy.insert(rm);
+	}
+    
+    public boolean insert_blocking(String tablename, String key, String columnFamily_column, byte[] cellData, long timestamp) throws InvalidRequestException
+    {
+        RowMutation rm = new RowMutation(tablename, key.trim());
+        rm.add(columnFamily_column, cellData, timestamp);
+        validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0]));
+        return StorageProxy.insertBlocking(rm);
+    }
+
+    public boolean batch_insert_blocking(batch_mutation_t batchMutation) throws InvalidRequestException
+    {
+        logger_.debug("batch_insert_blocking");
+        RowMutation rm = RowMutation.getRowMutation(batchMutation);
+        validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0]));
+        return StorageProxy.insertBlocking(rm);
+    }
+
+	public void batch_insert(batch_mutation_t batchMutation)
+    {
+        logger_.debug("batch_insert");
+        RowMutation rm = RowMutation.getRowMutation(batchMutation);
+        try
+        {
+            validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0]));
+        }
+        catch (InvalidRequestException e)
+        {
+            // it would be confusing to declare an exception in thrift that can't be returned to the client
+            throw new RuntimeException(e);
+        }
+        StorageProxy.insert(rm);
+	}
+
+    public boolean remove(String tablename, String key, String columnFamily_column, long timestamp, boolean block) throws InvalidRequestException
+    {
+        logger_.debug("remove");
+        RowMutation rm = new RowMutation(tablename, key.trim());
+        rm.delete(columnFamily_column, timestamp);
+        validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0]));
+        if (block) {
+            return StorageProxy.insertBlocking(rm);
+        } else {
+            StorageProxy.insert(rm);
+            return true;
+        }
+	}
+
+    public List<superColumn_t> get_slice_super_by_names(String tablename, String key, String columnFamily, List<String> superColumnNames) throws InvalidRequestException
+    {
+        long startTime = System.currentTimeMillis();
+		try
+		{
+			ColumnFamily cfamily = readColumnFamily(new SliceByNamesReadCommand(tablename, key, columnFamily, superColumnNames));
+			if (cfamily == null)
+			{
+                return EMPTY_SUPERCOLUMNS;
+			}
+			Collection<IColumn> columns = null;
+			columns = cfamily.getAllColumns();
+            return thriftifySuperColumns(columns);
+		}
+        finally
+        {
+            logger_.debug("get_slice2: " + (System.currentTimeMillis() - startTime) + " ms.");
+        }
+    }
+
+    private List<superColumn_t> thriftifySuperColumns(Collection<IColumn> columns)
+    {
+        if (columns == null || columns.isEmpty())
+        {
+            return EMPTY_SUPERCOLUMNS;
+        }
+
+        ArrayList<superColumn_t> thriftSuperColumns = new ArrayList<superColumn_t>(columns.size());
+        for (IColumn column : columns)
+        {
+            List<column_t> subcolumns = thriftifyColumns(column.getSubColumns());
+            if (subcolumns.isEmpty())
+            {
+                continue;
+            }
+            thriftSuperColumns.add(new superColumn_t(column.name(), subcolumns));
+        }
+
+        return thriftSuperColumns;
+    }
+
+    public List<superColumn_t> get_slice_super(String tablename, String key, String columnFamily_superColumnName, int start, int count) throws InvalidRequestException
+    {
+        ColumnFamily cfamily = readColumnFamily(new SliceReadCommand(tablename, key, columnFamily_superColumnName, start, count));
+        if (cfamily == null)
+        {
+            return EMPTY_SUPERCOLUMNS;
+        }
+        Collection<IColumn> columns = cfamily.getAllColumns();
+        return thriftifySuperColumns(columns);
+    }
+    
+    public superColumn_t get_superColumn(String tablename, String key, String columnFamily_column) throws InvalidRequestException, NotFoundException
+    {
+        ColumnFamily cfamily = readColumnFamily(new ColumnReadCommand(tablename, key, columnFamily_column));
+        if (cfamily == null)
+        {
+            throw new NotFoundException();
+        }
+        Collection<IColumn> columns = cfamily.getAllColumns();
+        if (columns == null || columns.size() == 0)
+        {
+            throw new NotFoundException();
+        }
+
+        assert columns.size() == 1;
+        IColumn column = columns.iterator().next();
+        if (column.getSubColumns().size() == 0)
+        {
+            throw new NotFoundException();
+        }
+
+        return new superColumn_t(column.name(), thriftifyColumns(column.getSubColumns()));
+    }
+    
+    public boolean batch_insert_superColumn_blocking(batch_mutation_super_t batchMutationSuper) throws InvalidRequestException
+    {
+        logger_.debug("batch_insert_SuperColumn_blocking");
+        RowMutation rm = RowMutation.getRowMutation(batchMutationSuper);
+        validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0]));
+        return StorageProxy.insertBlocking(rm);
+    }
+
+    public void batch_insert_superColumn(batch_mutation_super_t batchMutationSuper)
+    {
+        logger_.debug("batch_insert_SuperColumn");
+        RowMutation rm = RowMutation.getRowMutation(batchMutationSuper);
+        try
+        {
+            validateCommand(rm.key(), rm.table(), rm.columnFamilyNames().toArray(new String[0]));
+        }
+        catch (InvalidRequestException e)
+        {
+            // it would be confusing to declare an exception in thrift that can't be returned to the client
+            throw new RuntimeException(e);
+        }
+        StorageProxy.insert(rm);
+    }
+
+    public String getStringProperty(String propertyName)
+    {
+        if (propertyName.equals("cluster name"))
+        {
+            return DatabaseDescriptor.getClusterName();
+        }
+        else if (propertyName.equals("config file"))
+        {
+            String filename = DatabaseDescriptor.getConfigFileName();
+            try
+            {
+                StringBuffer fileData = new StringBuffer(8192);
+                BufferedInputStream stream = new BufferedInputStream(new FileInputStream(filename));
+                byte[] buf = new byte[1024];
+                int numRead;
+                while( (numRead = stream.read(buf)) != -1)
+                {
+                    String str = new String(buf, 0, numRead);
+                    fileData.append(str);
+                }
+                stream.close();
+                return fileData.toString();
+            }
+            catch (IOException e)
+            {
+                return "file not found!";
+            }
+        }
+        else if (propertyName.equals("version"))
+        {
+            return "1";
+        }
+        else
+        {
+            return "?";
+        }
+    }
+
+    public List<String> getStringListProperty(String propertyName)
+    {
+        if (propertyName.equals("tables"))
+        {
+            return DatabaseDescriptor.getTables();        
+        }
+        else
+        {
+            return new ArrayList<String>();
+        }
+    }
+
+    public String describeTable(String tableName)
+    {
+        String desc = "";
+        Map<String, CFMetaData> tableMetaData = DatabaseDescriptor.getTableMetaData(tableName);
+
+        if (tableMetaData == null)
+        {
+            return "Table " + tableName +  " not found.";
+        }
+
+        Iterator iter = tableMetaData.entrySet().iterator();
+        while (iter.hasNext())
+        {
+            Map.Entry<String, CFMetaData> pairs = (Map.Entry<String, CFMetaData>)iter.next();
+            desc = desc + pairs.getValue().pretty() + "-----\n";
+        }
+        return desc;
+    }
+
+    public CqlResult_t executeQuery(String query) throws TException
+    {
+        CqlResult_t result = new CqlResult_t();
+
+        CqlResult cqlResult = CqlDriver.executeQuery(query);
+        
+        // convert CQL result type to Thrift specific return type
+        if (cqlResult != null)
+        {
+            result.errorTxt = cqlResult.errorTxt;
+            result.resultSet = cqlResult.resultSet;
+            result.errorCode = cqlResult.errorCode;
+        }
+        return result;
+    }
+
+    public List<String> get_key_range(String tablename, String startWith, String stopAt, int maxResults) throws InvalidRequestException
+    {
+        logger_.debug("get_range");
+
+        if (!(StorageService.getPartitioner() instanceof OrderPreservingPartitioner))
+        {
+            throw new InvalidRequestException("range queries may only be performed against an order-preserving partitioner");
+        }
+
+        try
+        {
+            Message message = new RangeCommand(tablename, startWith, stopAt, maxResults).getMessage();
+            EndPoint endPoint = StorageService.instance().findSuitableEndPoint(startWith);
+            IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(message, endPoint);
+
+            // read response
+            // TODO send more requests if we need to span multiple nodes
+            // double the usual timeout since range requests are expensive
+            byte[] responseBody = (byte[])iar.get(2 * DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS)[0];
+            return RangeReply.read(responseBody).keys;
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /*
+     * This method is used to ensure that all keys
+     * prior to the specified key, as dtermined by
+     * the SSTable index bucket it falls in, are in
+     * buffer cache.  
+    */
+    public void touch (String key , boolean fData) 
+    {
+    	try
+    	{
+    		StorageProxy.touchProtocol(DatabaseDescriptor.getTables().get(0), key, fData, StorageService.ConsistencyLevel.WEAK);
+    	}
+    	catch ( Exception e)
+    	{
+			logger_.info( LogUtil.throwableToString(e) );
+    	}
+	}
+    
+    // main method moved to CassandraDaemon
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ConsistencyManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ConsistencyManager.java
index e69de29b..79473ec2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ConsistencyManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ConsistencyManager.java
@@ -0,0 +1,182 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.ReadResponse;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IAsyncCallback;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.Cachetable;
+import org.apache.cassandra.utils.ICacheExpungeHook;
+import org.apache.cassandra.utils.ICachetable;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+class ConsistencyManager implements Runnable
+{
+	private static Logger logger_ = Logger.getLogger(ConsistencyManager.class);
+	
+	class DigestResponseHandler implements IAsyncCallback
+	{
+		List<Message> responses_ = new ArrayList<Message>();
+		
+		public void response(Message msg)
+		{
+			logger_.debug("Received reponse : " + msg.toString());
+			responses_.add(msg);
+			if ( responses_.size() == ConsistencyManager.this.replicas_.size() )
+				handleDigestResponses();
+		}
+        
+        public void attachContext(Object o)
+        {
+            throw new UnsupportedOperationException("This operation is not currently supported.");
+        }
+		
+		private void handleDigestResponses()
+		{
+			DataInputBuffer bufIn = new DataInputBuffer();
+			logger_.debug("Handle Digest reponses");
+			for( Message response : responses_ )
+			{
+				byte[] body = (byte[])response.getMessageBody()[0];            
+	            bufIn.reset(body, body.length);
+	            try
+	            {	               
+	                ReadResponse result = ReadResponse.serializer().deserialize(bufIn);
+	                byte[] digest = result.digest();
+	                if( !Arrays.equals(row_.digest(), digest) )
+					{
+	                	doReadRepair();
+	                	break;
+					}
+	            }
+	            catch( IOException ex )
+	            {
+	            	logger_.info(LogUtil.throwableToString(ex));
+	            }
+			}
+		}
+		
+		private void doReadRepair() throws IOException
+		{
+			IResponseResolver<Row> readResponseResolver = new ReadResponseResolver();
+            /* Add the local storage endpoint to the replicas_ list */
+            replicas_.add(StorageService.getLocalStorageEndPoint());
+			IAsyncCallback responseHandler = new DataRepairHandler(ConsistencyManager.this.replicas_.size(), readResponseResolver);	
+			String table = DatabaseDescriptor.getTables().get(0);
+            ReadCommand readCommand = constructReadMessage(false);
+			// ReadMessage readMessage = new ReadMessage(table, row_.key(), columnFamily_);
+            Message message = readCommand.makeReadMessage();
+			MessagingService.getMessagingInstance().sendRR(message, replicas_.toArray( new EndPoint[0] ), responseHandler);			
+		}
+	}
+	
+	class DataRepairHandler implements IAsyncCallback, ICacheExpungeHook<String, String>
+	{
+		private List<Message> responses_ = new ArrayList<Message>();
+		private IResponseResolver<Row> readResponseResolver_;
+		private int majority_;
+		
+		DataRepairHandler(int responseCount, IResponseResolver<Row> readResponseResolver)
+		{
+			readResponseResolver_ = readResponseResolver;
+			majority_ = (responseCount >> 1) + 1;  
+		}
+		
+		public void response(Message message)
+		{
+			logger_.debug("Received responses in DataRepairHandler : " + message.toString());
+			responses_.add(message);
+			if ( responses_.size() == majority_ )
+			{
+				String messageId = message.getMessageId();
+				readRepairTable_.put(messageId, messageId, this);				
+			}
+		}
+        
+        public void attachContext(Object o)
+        {
+            throw new UnsupportedOperationException("This operation is not currently supported.");
+        }
+		
+		public void callMe(String key, String value)
+		{
+			handleResponses();
+		}
+		
+		private void handleResponses()
+		{
+			try
+			{
+				readResponseResolver_.resolve(new ArrayList<Message>(responses_));
+			}
+			catch ( DigestMismatchException ex )
+			{
+				logger_.info("We should not be coming here under any circumstances ...");
+				logger_.info(LogUtil.throwableToString(ex));
+			}
+		}
+	}
+	private static long scheduledTimeMillis_ = 600;
+	private static ICachetable<String, String> readRepairTable_ = new Cachetable<String, String>(scheduledTimeMillis_);
+	private Row row_;
+	protected List<EndPoint> replicas_;
+	
+	private ReadCommand readCommand_;
+	
+    public ConsistencyManager(Row row_, List<EndPoint> replicas_, ReadCommand readCommand)
+    {
+        this.readCommand_ = readCommand;
+    }
+
+	public void run()
+	{
+		logger_.debug(" Run the consistency checks for " + readCommand_.getColumnFamilyName());		
+        ReadCommand readCommandDigestOnly = constructReadMessage(true);
+		try
+		{
+			Message messageDigestOnly = readCommandDigestOnly.makeReadMessage();
+			IAsyncCallback digestResponseHandler = new DigestResponseHandler();
+			MessagingService.getMessagingInstance().sendRR(messageDigestOnly, replicas_.toArray(new EndPoint[0]), digestResponseHandler);
+		}
+		catch ( IOException ex )
+		{
+			logger_.info(LogUtil.throwableToString(ex));
+		}
+	}
+    
+    private ReadCommand constructReadMessage(boolean isDigestQuery)
+    {
+        ReadCommand readCommand = readCommand_.copy();
+        readCommand.setDigestQuery(isDigestQuery);
+        return readCommand;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/Constants.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/Constants.java
index e69de29b..1171df76 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/Constants.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/Constants.java
@@ -0,0 +1,18 @@
+/**
+ * Autogenerated by Thrift
+ *
+ * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
+ */
+package org.apache.cassandra.service;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Set;
+import java.util.HashSet;
+import org.apache.thrift.*;
+
+public class Constants {
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/DigestMismatchException.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/DigestMismatchException.java
index e69de29b..16465848 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/DigestMismatchException.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/DigestMismatchException.java
@@ -0,0 +1,30 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class DigestMismatchException extends Exception
+{
+	public DigestMismatchException(String message)
+	{
+		super(message);
+	}
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/HttpRequestVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/HttpRequestVerbHandler.java
index e69de29b..5f5e0689 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/HttpRequestVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/HttpRequestVerbHandler.java
@@ -0,0 +1,723 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.lang.management.ManagementFactory;
+import java.lang.management.MemoryMXBean;
+import java.lang.management.MemoryUsage;
+import java.lang.management.RuntimeMXBean;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.CalloutDeployMessage;
+import org.apache.cassandra.db.CalloutManager;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.http.ColumnFamilyFormatter;
+import org.apache.cassandra.net.http.HTMLFormatter;
+import org.apache.cassandra.net.http.HttpConnection;
+import org.apache.cassandra.net.http.HttpRequest;
+import org.apache.cassandra.net.http.HttpWriteResponse;
+import org.apache.cassandra.procedures.GroovyScriptRunner;
+import org.apache.cassandra.utils.LogUtil;
+
+/*
+ * This class handles the incoming HTTP request after
+ * it has been parsed.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class HttpRequestVerbHandler implements IVerbHandler
+{
+    private static final Logger logger_ = Logger.getLogger(HttpRequestVerbHandler.class);
+    /* These are the list of actions supported */
+    private static final String DETAILS = "details";
+    private static final String LOADME = "loadme";
+    private static final String KILLME = "killme";
+    private static final String COMPACTME = "compactme";
+    private static final String LB_HEALTH_CHECK = "lb_health_check";
+    private static final String LB_HEALTH_CHECK_RESPONSE = "I-AM-ALIVE";
+    private static final String QUERY = "query";
+    private static final String INSERT = "insert";
+    private static final String SCRIPT = "script";
+    private static final String QUERYRESULTSDIV = "queryResultsDiv";
+    private static final String INSERTRESULTSDIV = "insertResultsDiv";
+    private static final String SCRIPTRESULTSDIV = "insertResultsDiv";
+    private static final String JS_UPDATE_QUERY_FUNCTION = "updateQueryResults";
+    private static final String JS_UPDATE_INSERT_FUNCTION = "updateInsertResults";
+
+    private StorageService storageService_;
+
+    public HttpRequestVerbHandler(StorageService storageService)
+    {
+        storageService_ = storageService;
+    }
+
+    public void doVerb(Message message)
+    {
+        HttpConnection.HttpRequestMessage httpRequestMessage = (HttpConnection.HttpRequestMessage)message.getMessageBody()[0];
+        try
+        {
+            HttpRequest httpRequest = httpRequestMessage.getHttpRequest();
+            HttpWriteResponse httpServerResponse = new HttpWriteResponse(httpRequest);
+            if(httpRequest.getMethod().toUpperCase().equals("GET"))
+            {
+                // handle the get request type
+                doGet(httpRequest, httpServerResponse);
+            }
+            else if(httpRequest.getMethod().toUpperCase().equals("POST"))
+            {
+                // handle the POST request type
+                doPost(httpRequest, httpServerResponse);
+            }
+
+            // write the response we have constructed into the socket
+            ByteBuffer buffer = httpServerResponse.flush();
+            httpRequestMessage.getHttpConnection().write(buffer);
+        }
+        catch(Exception e)
+        {
+            logger_.warn(LogUtil.throwableToString(e));
+        }
+    }
+
+    private void doGet(HttpRequest httpRequest, HttpWriteResponse httpResponse)
+    {
+        boolean fServeSummary = true;
+        HTMLFormatter formatter = new HTMLFormatter();
+        String query = httpRequest.getQuery();
+        /*
+         * we do not care about the path for most requests except those
+         * from the load balancer
+         */
+        String path = httpRequest.getPath();
+        /* for the health checks, just return the string only */
+        if(path.contains(LB_HEALTH_CHECK))
+        {
+        	httpResponse.println(handleLBHealthCheck());
+            return;
+        }
+
+        formatter.startBody(true, getJSFunctions(), true, true);
+        formatter.appendLine("<h1><font color=\"white\"> Cluster map </font></h1>");
+
+        StringBuilder sbResult = new StringBuilder();
+        do
+        {
+            if(query.contains(DETAILS))
+            {
+                fServeSummary = false;
+                sbResult.append(handleNodeDetails());
+                break;
+            }
+            else if(query.contains(LOADME))
+            {
+                sbResult.append(handleLoadMe());
+                break;
+            }
+            else if(query.contains(KILLME))
+            {
+                sbResult.append(handleKillMe());
+                break;
+            }
+            else if(query.contains(COMPACTME))
+            {
+                sbResult.append(handleCompactMe());
+                break;
+            }
+        }
+        while(false);
+
+        //formatter.appendLine("<br>-------END DEBUG INFO-------<br><br>");
+
+        if(fServeSummary)
+        {
+            formatter.appendLine(handlePageDisplay(null, null, null));
+        }
+
+        formatter.appendLine("<br>");
+
+        if(sbResult.toString() != null)
+        {
+            formatter.appendLine(sbResult.toString());
+        }
+
+        formatter.endBody();
+        httpResponse.println(formatter.toString());
+    }
+
+    /*
+     * As a result of the POST query, we currently only send back some
+     * javascript that updates the data in some place on the browser.
+    */
+    private void doPost(HttpRequest httpRequest, HttpWriteResponse httpResponse)
+    {
+        String query = httpRequest.getQuery();
+
+        HTMLFormatter formatter = new HTMLFormatter();
+        formatter.startBody(true, getJSFunctions(), true, true);
+        formatter.appendLine("<h1><font color=\"white\"> Cluster map </font></h1>");
+
+        // write a shell for adding some javascript to do in-place updates
+        StringBuilder sbResult = new StringBuilder();
+        do
+        {
+            if(query.contains(QUERY))
+            {
+                sbResult.append(handleQuery(httpRequest));
+                break;
+            }
+            else if(query.contains(INSERT))
+            {
+                sbResult.append(handleInsert(httpRequest));
+                break;
+            }
+            else if(query.contains(SCRIPT))
+            {
+                sbResult.append(handleScript(httpRequest));
+                break;
+            }
+        }
+        while(false);
+
+        if(sbResult.toString() != null)
+        {
+            formatter.appendLine(sbResult.toString());
+        }
+
+        formatter.endBody();
+
+    	httpResponse.println(formatter.toString());
+    }
+
+    private String handleNodeDetails()
+    {
+        HTMLFormatter formatter = new HTMLFormatter();
+
+        formatter.appendLine("Token: " + storageService_.getToken());
+        RuntimeMXBean runtimeMxBean = ManagementFactory.getRuntimeMXBean();
+        formatter.appendLine("Up time (in seconds): " + (runtimeMxBean.getUptime()/1000));
+
+        MemoryMXBean memoryMxBean = ManagementFactory.getMemoryMXBean();
+        MemoryUsage memUsage = memoryMxBean.getHeapMemoryUsage();
+        java.text.DecimalFormat df = new java.text.DecimalFormat("#0.00");
+        String smemUsed = df.format((double)memUsage.getUsed()/(1024 * 1024));
+        String smemMax = df.format((double)memUsage.getMax()/(1024 * 1024));
+        formatter.appendLine("Heap memory usage (in MB): " + smemUsed + "/" + smemMax);
+
+        formatter.appendLine("<br>");
+        formatter.appendLine("<br>");
+
+        /*
+         * Display DB statatics if we have something to show.
+        */
+        displayDBStatistics(formatter, df);
+
+        formatter.appendLine("<button onClick=\"window.location='" + StorageService.getHostUrl() + "?" + LOADME + "=T'\">Load Me</button>");
+        formatter.appendLine("<button onClick=\"window.location='" + StorageService.getHostUrl() + "?" + COMPACTME + "=T'\">Compact Me</button>");
+        formatter.appendLine("<button onClick=\"window.location='" + StorageService.getHostUrl() + "?" + KILLME + "=T'\">Kill Me</button>");
+
+        formatter.appendLine("<br>");
+        formatter.appendLine("<br><a href='" + StorageService.getHostUrl() + "'>Back to live nodes list" + "</a>");
+
+        return formatter.toString();
+    }
+
+    private void displayDBStatistics(HTMLFormatter formatter, java.text.DecimalFormat df)
+    {
+        String tableStats = Table.open( DatabaseDescriptor.getTables().get(0) ).tableStats("\n<br>\n", df);
+
+        if ( tableStats.length() == 0 )
+            return;
+
+        formatter.appendLine("DB statistics:");
+        formatter.appendLine("<br>");
+        formatter.appendLine("<br>");
+
+        formatter.appendLine(tableStats);
+        formatter.appendLine("<br>");
+        formatter.appendLine("<br>");
+    }
+
+    private String handlePageDisplay(String queryFormData, String insertFormData, String scriptFormData)
+    {
+    	StringBuilder sb = new StringBuilder();
+		sb.append("\n<div id=\"header\"> \n");
+		sb.append("<ul>\n");
+		sb.append("	<li name=\"one\" onclick=\"javascript:selectTab('one')\"><a href=\"#\">Cluster</a></li>\n");
+		sb.append("	<li name=\"two\" onclick=\"javascript:selectTab('two')\"><a href=\"#\">SQL</a></li>\n");
+		sb.append("	<li name=\"three\" onclick=\"javascript:selectTab('three')\"><a href=\"#\">Ring</a></li>\n");
+		sb.append("</ul>\n");
+		sb.append("</div>\n\n");
+
+		sb.append("<div name=\"one\" id=\"content\"> <!-- start tab one -->\n\n");
+        sb.append(serveSummary());
+        sb.append("</div> <!-- finish tab one -->\n\n");
+
+        sb.append("<div name=\"two\" id=\"content\"> <!-- start tab two -->\n\n");
+        sb.append(serveInsertForm(insertFormData));
+        sb.append(serveQueryForm(queryFormData));
+        sb.append(serveGroovyForm(scriptFormData));
+        sb.append("</div> <!-- finish tab two -->\n\n");
+
+        sb.append("<div name=\"three\" id=\"content\"> <!-- start tab three -->\n\n");
+        sb.append(serveRingView());
+        sb.append("</div> <!-- finish tab three -->\n\n");
+
+        sb.append("\n<script type=\"text/javascript\">\n");
+        if(queryFormData != null || insertFormData != null || scriptFormData != null)
+        	sb.append("selectTab(\"two\");\n");
+        else
+        	sb.append("selectTab(\"one\");\n");
+
+        sb.append("</script>\n");
+
+        return (sb.toString() == null)?"":sb.toString();
+    }
+
+    /*
+     * Serve the summary of the current node.
+     */
+    private String serveSummary()
+    {
+        HTMLFormatter formatter = new HTMLFormatter();
+
+        Set<EndPoint> liveNodeList = Gossiper.instance().getAllMembers();
+        // we want this set of live nodes sorted based on the hostname
+        EndPoint[] liveNodes = liveNodeList.toArray(new EndPoint[0]);
+        Arrays.sort(liveNodes);
+
+        String[] sHeaders = {"Node No.", "Host:Port", "Status", "Leader", "Load Info", "Token", "Generation No."};
+        formatter.startTable();
+        formatter.addHeaders(sHeaders);
+        int iNodeNumber = 0;
+        for( EndPoint curNode : liveNodes )
+        {
+            formatter.startRow();
+            ++iNodeNumber;
+
+            // Node No.
+            formatter.addCol("" + iNodeNumber);
+            // Host:Port
+            formatter.addCol("<a href='http://" + curNode.getHost() + ":" + DatabaseDescriptor.getHttpPort() + "/home?" + DETAILS + "=T'>" + curNode.getHost() + ":" + curNode.getPort() + "</a>");
+            //Status
+            String status = ( FailureDetector.instance().isAlive(curNode) ) ? "Up" : "Down";
+            formatter.addCol(status);
+            //Leader
+            boolean isLeader = StorageService.instance().isLeader(curNode);
+            formatter.addCol(Boolean.toString(isLeader));
+            //Load Info
+            String loadInfo = getLoadInfo(curNode);
+            formatter.addCol(loadInfo);
+            // Token
+            if(curNode == null)
+                formatter.addCol("NULL!");
+            else
+                formatter.addCol(storageService_.getToken(curNode));
+            // Generation Number
+            formatter.addCol(Integer.toString(Gossiper.instance().getCurrentGenerationNumber(curNode)));
+
+            formatter.endRow();
+        }
+
+        formatter.endTable();
+
+        return formatter.toString();
+    }
+
+    private String serveRingView()
+    {
+        HTMLFormatter formatter = new HTMLFormatter();
+        String[] sHeaders = {"Range No.", "Range", "N1", "N2", "N3"};
+        formatter.startTable();
+        formatter.addHeaders(sHeaders);
+
+        Map<Range, List<EndPoint>> oldRangeToEndPointMap = StorageService.instance().getRangeToEndPointMap();
+        Set<Range> rangeSet = oldRangeToEndPointMap.keySet();
+
+        int iNodeNumber = 0;
+        for ( Range range : rangeSet )
+        {
+        	formatter.startRow();
+            ++iNodeNumber;
+
+            // Range No.
+            formatter.addCol("" + iNodeNumber);
+
+            // Range
+            formatter.addCol("(" + range.left() + ",<br>" + range.right() + "]");
+
+            List<EndPoint> replicas = oldRangeToEndPointMap.get(range);
+            for ( EndPoint replica : replicas )
+            {
+            	// N1 N2 N3
+            	formatter.addCol(replica.toString());
+            }
+
+            formatter.endRow();
+        }
+
+        formatter.endTable();
+
+        return formatter.toString();
+    }
+
+    private String getLoadInfo(EndPoint ep)
+    {
+        if ( StorageService.getLocalControlEndPoint().equals(ep) )
+        {
+            return StorageService.instance().getLoadInfo();
+        }
+        else
+        {
+            return StorageService.instance().getLoadInfo(ep);
+        }
+    }
+
+    /*
+     * Returns the HTML code for a form to query data from the db cluster.
+     */
+    private String serveQueryForm(String queryResult)
+    {
+        HTMLFormatter formatter = new HTMLFormatter();
+        formatter.appendLine("<BR><fieldset><legend>Query the cluster</legend>");
+        formatter.appendLine("<FORM action=\"" + StorageService.getHostUrl() + "/home?" + QUERY + "=T\" method=\"post\">");
+
+        // get the list of column families
+        Table table = Table.open("Mailbox");
+        Set<String> columnFamilyComboBoxSet = table.getColumnFamilies();
+
+        formatter.append("select from ");
+        formatter.addCombobox(columnFamilyComboBoxSet, "columnfamily", 0);
+        formatter.append(" : <INPUT name=columnName>");
+        formatter.appendLine(" where key = <INPUT name=key>");
+        formatter.appendLine("<BR>");
+        formatter.appendLine("<INPUT type=\"submit\" value=\"Send\"> <INPUT type=\"reset\">");
+
+        formatter.appendLine("</FORM>");
+        formatter.addDivElement(QUERYRESULTSDIV, queryResult);
+        formatter.appendLine("</fieldset><BR>");
+
+        return formatter.toString();
+    }
+
+    /*
+     * Returns the HTML code for a form to to run custom code on the cluster.
+     */
+    private String serveGroovyForm(String scriptResult)
+    {
+        HTMLFormatter formatter = new HTMLFormatter();
+        formatter.appendLine("<BR><fieldset><legend>Run custom code on the cluster</legend>");
+        formatter.appendLine("<FORM action=\"" + StorageService.getHostUrl() + "/home?" + SCRIPT + "=T\" method=\"post\">");
+        formatter.append(" Callout name : <INPUT name=calloutName>");
+        formatter.appendLine("<BR>");
+        formatter.append("Groovy code to run on the server:<br>");
+        formatter.append("<textarea name=scriptTextArea rows=\"10\" cols=\"100\"></textarea>");
+        formatter.appendLine("<BR>");
+        formatter.appendLine("<INPUT name=deploy type=\"submit\" value=\"Deploy\"> <INPUT name=execute type=\"submit\" value=\"Execute\"> <INPUT name=reset type=\"reset\">");
+
+        formatter.appendLine("</FORM>");
+        formatter.addDivElement(SCRIPTRESULTSDIV, scriptResult);
+        formatter.appendLine("</fieldset><BR>");
+
+        return formatter.toString();
+    }
+
+    /*
+     * Returns the HTML code for a form to insert data into the db cluster.
+     */
+    private String serveInsertForm(String insertResult)
+    {
+        HTMLFormatter formatter = new HTMLFormatter();
+        formatter.appendLine("<BR><fieldset>\n<legend>Insert data into the cluster</legend>\n");
+        formatter.appendLine("<FORM action=\"" + StorageService.getHostUrl() + "/home?" + INSERT + "=T\" method=\"post\">");
+
+        // get the list of column families
+        Table table = Table.open("Mailbox");
+        Set<String> columnFamilyComboBoxSet = table.getColumnFamilies();
+
+        formatter.append("insert into ");
+        formatter.addCombobox(columnFamilyComboBoxSet, "columnfamily", 0);
+        formatter.append(" : <INPUT name=columnName>");
+        formatter.append(" data = <INPUT name=data>");
+        formatter.appendLine(" where key = <INPUT name=key>\n");
+        formatter.appendLine("<BR>\n");
+        formatter.appendLine("<INPUT type=\"submit\" value=\"Send\"> <INPUT type=\"reset\">\n");
+
+        formatter.appendLine("</FORM>\n");
+        formatter.addDivElement(INSERTRESULTSDIV, insertResult);
+        formatter.appendLine("</fieldset>\n<BR>\n");
+
+        return formatter.toString();
+    }
+
+    /*
+     * Handle the query of some data from the client.
+     */
+    private String handleQuery(HttpRequest httpRequest)
+    {
+    	boolean fQuerySuccess = false;
+    	String sRetVal = "";
+
+    	// get the various values for this HTTP request
+    	String sColumnFamily = httpRequest.getParameter("columnfamily");
+    	String sColumn = httpRequest.getParameter("columnName");
+    	String sKey = httpRequest.getParameter("key");
+
+    	// get the table name
+    	String sTableName = DatabaseDescriptor.getTables().get(0);
+
+    	StringBuilder sb = new StringBuilder();
+    	ColumnFamilyFormatter cformatter = new ColumnFamilyFormatter(sb);
+
+        try
+        {
+	    	Table table = Table.open(sTableName);
+	    	String queryFor = sColumnFamily;
+	    	if(sColumn != null && !"*".equals(sColumn))
+    		{
+	    		queryFor += ":" + sColumn;
+    		}
+	        ColumnFamily cf = table.get(sKey, queryFor);
+
+	        if (cf == null)
+	        {
+	            sRetVal = "Key [" + sKey + "], column family [" + sColumnFamily + "] not found.";
+	        }
+	        else
+	        {
+		        cformatter.printKeyColumnFamily(sb, sKey, cf);
+	        	fQuerySuccess = true;
+	        	sRetVal = sb.toString();
+	        }
+        }
+        catch (Exception e)
+        {
+        	// write failed - return the reason
+        	sRetVal = e.getMessage();
+        }
+
+        if(fQuerySuccess)
+        	sRetVal = "Success: " + sRetVal;
+        else
+        	sRetVal = "Error: " + sRetVal;
+
+        return handlePageDisplay(sRetVal, null, null);
+    }
+
+    /*
+     * Handle the query of some data from the client.
+     */
+    private String handleInsert(HttpRequest httpRequest)
+    {
+    	boolean fInsertSuccess = false;
+    	String sRetVal = "";
+
+    	// get the various values for this HTTP request
+    	String sColumnFamily = httpRequest.getParameter("columnfamily");
+    	String sColumn = httpRequest.getParameter("columnName");
+    	String sKey = httpRequest.getParameter("key");
+    	String sDataToInsert = httpRequest.getParameter("data");
+
+    	// get the table name
+    	String sTableName = DatabaseDescriptor.getTables().get(0);
+
+        try
+        {
+        	// do the insert first
+            RowMutation rm = new RowMutation(sTableName, sKey);
+            rm.add(sColumnFamily + ":" + sColumn, sDataToInsert.getBytes(), 0);
+            rm.apply();
+
+            fInsertSuccess = true;
+	        sRetVal = "columnfamily=" + httpRequest.getParameter("columnfamily") + " key=" + httpRequest.getParameter("key") + " data=" + httpRequest.getParameter("data");
+        }
+        catch (Exception e)
+        {
+        	// write failed - return the reason
+        	sRetVal = e.getMessage();
+        }
+        System.out.println("Write done ...");
+
+        if(fInsertSuccess)
+        	sRetVal = "The insert was successful : " + sRetVal;
+        else
+        	sRetVal = "The insert was failed : " + sRetVal;
+
+        return handlePageDisplay(null, sRetVal, null);
+    }
+
+    /*
+     * Handle the script to be run on the server.
+     */
+    private String handleScript(HttpRequest httpRequest)
+    {
+    	boolean fQuerySuccess = false;
+    	String sRetVal = "";
+
+    	// get the various values for this HTTP request
+        String callout = httpRequest.getParameter("calloutName");        
+    	String script = httpRequest.getParameter("scriptTextArea");
+        String deploy = httpRequest.getParameter("deploy");
+        String execute = httpRequest.getParameter("execute");
+    	try
+    	{
+            if ( deploy != null )
+            {
+                if ( callout != null && script != null )
+                {
+                    doDeploy(callout, script);
+                    sRetVal = "Finished deployment of callouts ...";
+                }
+            }
+            if ( execute != null )
+            {
+                if ( script != null )
+                {
+            		sRetVal = GroovyScriptRunner.evaluateString(script);            		
+                }
+            }
+            fQuerySuccess = true;
+    	}
+    	catch(Throwable t)
+    	{
+    		sRetVal = t.getMessage();
+    		logger_.warn(LogUtil.throwableToString(t));
+    	}
+
+        if(fQuerySuccess)
+        	sRetVal = "Result: Success<br>\nReturn value: <br>\n" + sRetVal;
+        else
+        	sRetVal = "Result: Error<br>\nError: <br>\n" + sRetVal;
+
+        return handlePageDisplay(null, null, sRetVal);
+    }
+    
+    private void doDeploy(String callout, String script)
+    {
+        Set<EndPoint> allMbrs = Gossiper.instance().getAllMembers();                
+        /* Send the script to all mbrs to deploy it locally. */
+        CalloutDeployMessage cdMessage = new CalloutDeployMessage(callout, script);
+        try
+        {
+            Message message = CalloutDeployMessage.getCalloutDeployMessage(cdMessage);
+            for ( EndPoint mbr : allMbrs )
+            {
+                if ( mbr.equals( StorageService.getLocalControlEndPoint() ) )
+                {
+                    /* Deploy locally */
+                    CalloutManager.instance().addCallout(callout, script);
+                }
+                else
+                {
+                    EndPoint to = new EndPoint(mbr.getHost(), DatabaseDescriptor.getStoragePort());
+                    logger_.debug("Deploying the script to " + mbr);
+                    MessagingService.getMessagingInstance().sendOneWay(message, to);
+                }
+            }
+        }
+        catch ( IOException ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+    }
+
+    private String getJSFunctions()
+    {
+        StringBuilder sb = new StringBuilder();
+        sb.append("function " + JS_UPDATE_QUERY_FUNCTION + "(text)\n");
+        sb.append("{\n");
+        sb.append("    obj = document.getElementById(\"" + QUERYRESULTSDIV + "\");\n");
+        sb.append("    if(obj)\n");
+        sb.append("        obj.innerHTML = text;\n");
+        sb.append("}\n");
+        sb.append("\n");
+        sb.append("function " + JS_UPDATE_INSERT_FUNCTION + "(text)\n");
+        sb.append("{\n");
+        sb.append("    obj = document.getElementById(\"" + INSERTRESULTSDIV + "\");\n");
+        sb.append("    if(obj)\n");
+        sb.append("        obj.innerHTML = text;\n");
+        sb.append("}\n");
+        sb.append("\n");
+
+        return sb.toString();
+    }
+
+    /*
+     * Load the current node with data.
+     */
+    private String handleLoadMe()
+    {
+        return "Loading...";
+    }
+
+    private String handleCompactMe()
+    {
+        Table table = Table.open(DatabaseDescriptor.getTables().get(0));
+        table.forceCompaction();
+        return "Compacting ...";
+    }
+
+    private String handleLBHealthCheck()
+    {
+    	if(StorageService.instance().isShutdown())
+    		return "";
+    	return LB_HEALTH_CHECK_RESPONSE;
+    }
+
+    /*
+     * Kill the current node.
+     */
+    private String handleKillMe()
+    {
+    	if(StorageService.instance().isShutdown())
+    		return "Already scheduled for being shutdown";
+    	/*
+    	 * The storage service will wait for a period of time to let the
+    	 * VIP know that we are shutting down, then will perform an actual
+    	 * shutdown on a separate thread.
+    	 */
+        String status = "Service has been killed";
+        try
+        {
+            StorageService.instance().killMe();
+        }
+        catch( Throwable th )
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+            status = "Failed to kill service.";
+        }
+    	return status;
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/IComponentShutdown.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/IComponentShutdown.java
index e69de29b..43994561 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/IComponentShutdown.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/IComponentShutdown.java
@@ -0,0 +1,24 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+public interface IComponentShutdown
+{
+	public void shutdown();
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/IResponseResolver.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/IResponseResolver.java
index e69de29b..47ed69dd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/IResponseResolver.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/IResponseResolver.java
@@ -0,0 +1,42 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.util.List;
+
+import org.apache.cassandra.net.Message;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IResponseResolver<T> {
+
+	/*
+	 * This Method resolves the responses that are passed in . for example : if
+	 * its write response then all we get is true or false return values which
+	 * implies if the writes were successful but for reads its more complicated
+	 * you need to look at the responses and then based on differences schedule
+	 * repairs . Hence you need to derive a response resolver based on your
+	 * needs from this interface.
+	 */
+	public T resolve(List<Message> responses) throws DigestMismatchException;
+	public boolean isDataPresent(List<Message> responses);
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LeaderElector.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LeaderElector.java
index e69de29b..70efe3c1 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LeaderElector.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LeaderElector.java
@@ -0,0 +1,272 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.EndPointState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.gms.IEndPointStateChangeSubscriber;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.zookeeper.*;
+import org.apache.zookeeper.ZooDefs.Ids;
+import org.apache.zookeeper.data.Stat;
+
+class LeaderElector implements IEndPointStateChangeSubscriber
+{
+    private static Logger logger_ = Logger.getLogger(LeaderElector.class);
+    protected static final String leaderState_ = "LEADER";
+    private static LeaderElector instance_ = null;
+    private static Lock createLock_ = new ReentrantLock();
+    
+    /*
+     * Factory method that gets an instance of the StorageService
+     * class.
+    */
+    public static LeaderElector instance()
+    {
+        if ( instance_ == null )
+        {
+            LeaderElector.createLock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                {
+                    instance_ = new LeaderElector();
+                }
+            }
+            finally
+            {
+                createLock_.unlock();
+            }
+        }
+        return instance_;
+    }
+    
+    /* The elected leader. */
+    private AtomicReference<EndPoint> leader_;
+    private Condition condition_;
+    private ExecutorService leaderElectionService_ = new DebuggableThreadPoolExecutor(1,
+            1,
+            Integer.MAX_VALUE,
+            TimeUnit.SECONDS,
+            new LinkedBlockingQueue<Runnable>(),
+            new ThreadFactoryImpl("LEADER-ELECTOR")
+            );
+    
+    private class LeaderDeathMonitor implements Runnable
+    {
+        private String pathCreated_;
+        
+        LeaderDeathMonitor(String pathCreated)
+        {
+            pathCreated_ = pathCreated;
+        }
+        
+        public void run()
+        {            
+            ZooKeeper zk = StorageService.instance().getZooKeeperHandle();
+            String path = "/Cassandra/" + DatabaseDescriptor.getClusterName() + "/Leader";
+            try
+            {
+                String createPath = path + "/L-";                                
+                LeaderElector.createLock_.lock();
+                while( true )
+                {
+                    /* Get all znodes under the Leader znode */
+                    List<String> values = zk.getChildren(path, false);
+                    SortedMap<Integer, String> suffixToZnode = getSuffixToZnodeMapping(values);
+                    String value = suffixToZnode.get( suffixToZnode.firstKey() );
+                    /*
+                     * Get the first znode and if it is the 
+                     * pathCreated created above then the data
+                     * in that znode is the leader's identity. 
+                    */
+                    if ( leader_ == null )
+                    {
+                        leader_ = new AtomicReference<EndPoint>( EndPoint.fromBytes( zk.getData(path + "/" + value, false, null) ) );
+                    }
+                    else
+                    {
+                        leader_.set( EndPoint.fromBytes( zk.getData(path + "/" + value, false, null) ) );
+                        /* Disseminate the state as to who the leader is. */
+                        onLeaderElection();
+                    }
+                    logger_.debug("Elected leader is " + leader_ + " @ znode " + ( path + "/" + value ) );                                     
+                    /* We need only the last portion of this znode */
+                    int index = getLocalSuffix();                   
+                    if ( index > suffixToZnode.firstKey() )
+                    {
+                        String pathToCheck = path + "/" + getImmediatelyPrecedingZnode(suffixToZnode, index);
+                        Stat stat = zk.exists(pathToCheck, true);
+                        if ( stat != null )
+                        {
+                            logger_.debug("Awaiting my turn ...");
+                            condition_.await();
+                            logger_.debug("Checking to see if leader is around ...");
+                        }
+                    }
+                    else
+                    {
+                        break;
+                    }
+                }
+            }
+            catch ( InterruptedException ex )
+            {
+                logger_.warn(LogUtil.throwableToString(ex));
+            }
+            catch ( IOException ex )
+            {
+                logger_.warn(LogUtil.throwableToString(ex));
+            }
+            catch ( KeeperException ex )
+            {
+                logger_.warn(LogUtil.throwableToString(ex));
+            }
+            finally
+            {
+                LeaderElector.createLock_.unlock();
+            }
+        }
+        
+        private SortedMap<Integer, String> getSuffixToZnodeMapping(List<String> values)
+        {
+            SortedMap<Integer, String> suffixZNodeMap = new TreeMap<Integer, String>();
+            for ( String znode : values )
+            {
+                String[] peices = znode.split("-");
+                suffixZNodeMap.put(Integer.parseInt( peices[1] ), znode);
+            }
+            return suffixZNodeMap;
+        }
+        
+        private String getImmediatelyPrecedingZnode(SortedMap<Integer, String> suffixToZnode, int index)
+        {
+            List<Integer> suffixes = new ArrayList<Integer>( suffixToZnode.keySet() );            
+            Collections.sort(suffixes);
+            int position = Collections.binarySearch(suffixes, index);
+            return suffixToZnode.get( suffixes.get( position - 1 ) );
+        }
+        
+        /**
+         * If the local node's leader related znode is L-3
+         * this method will return 3.
+         * @return suffix portion of L-3.
+         */
+        private int getLocalSuffix()
+        {
+            String[] peices = pathCreated_.split("/");
+            String leaderPeice = peices[peices.length - 1];
+            String[] leaderPeices = leaderPeice.split("-");
+            return Integer.parseInt( leaderPeices[1] );
+        }
+    }
+    
+    private LeaderElector()
+    {
+        condition_ = LeaderElector.createLock_.newCondition();
+    }
+    
+    /**
+     * Use to inform interested parties about the change in the state
+     * for specified endpoint
+     * 
+     * @param endpoint endpoint for which the state change occured.
+     * @param epState state that actually changed for the above endpoint.
+     */
+    public void onChange(EndPoint endpoint, EndPointState epState)
+    {        
+        /* node identifier for this endpoint on the identifier space */
+        ApplicationState leaderState = epState.getApplicationState(LeaderElector.leaderState_);
+        if (leaderState != null && !leader_.equals(endpoint))
+        {
+            EndPoint leader = EndPoint.fromString( leaderState.getState() );
+            logger_.debug("New leader in the cluster is " + leader);
+            leader_.set(endpoint);
+        }
+    }
+    
+    void start() throws Throwable
+    {
+        /* Register with the Gossiper for EndPointState notifications */
+        Gossiper.instance().register(this);
+        logger_.debug("Starting the leader election process ...");
+        ZooKeeper zk = StorageService.instance().getZooKeeperHandle();
+        String path = "/Cassandra/" + DatabaseDescriptor.getClusterName() + "/Leader";
+        String createPath = path + "/L-";
+        
+        /* Create the znodes under the Leader znode */       
+        logger_.debug("Attempting to create znode " + createPath);
+        String pathCreated = zk.create(createPath, EndPoint.toBytes( StorageService.getLocalControlEndPoint() ), Ids.OPEN_ACL_UNSAFE, (CreateMode.EPHEMERAL_SEQUENTIAL) );             
+        logger_.debug("Created znode under leader znode " + pathCreated);            
+        leaderElectionService_.submit(new LeaderDeathMonitor(pathCreated));
+    }
+    
+    void signal()
+    {
+        logger_.debug("Signalling others to check on leader ...");
+        try
+        {
+            LeaderElector.createLock_.lock();
+            condition_.signal();
+        }
+        finally
+        {
+            LeaderElector.createLock_.unlock();
+        }
+    }
+    
+    EndPoint getLeader()
+    {
+        return (leader_ != null ) ? leader_.get() : StorageService.getLocalStorageEndPoint();
+    }
+    
+    private void onLeaderElection() throws InterruptedException, IOException
+    {
+        /*
+         * If the local node is the leader then not only does he 
+         * diseminate the information but also starts the M/R job
+         * tracker. Non leader nodes start the M/R task tracker 
+         * thereby initializing the M/R subsystem.
+        */
+        if ( StorageService.instance().isLeader(leader_.get()) )
+        {
+            Gossiper.instance().addApplicationState(LeaderElector.leaderState_, new ApplicationState(leader_.toString()));              
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LoadDisseminator.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LoadDisseminator.java
index e69de29b..25403cce 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LoadDisseminator.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LoadDisseminator.java
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.service;
+
+import java.util.TimerTask;
+
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+class LoadDisseminator extends TimerTask
+{
+    private final static Logger logger_ = Logger.getLogger(LoadDisseminator.class);
+    protected final static String loadInfo_= "LOAD-INFORMATION";
+    
+    public void run()
+    {
+        try
+        {
+            long diskSpace = FileUtils.getUsedDiskSpace();                
+            String diskUtilization = FileUtils.stringifyFileSize(diskSpace);
+            logger_.debug("Disseminating load info ...");
+            Gossiper.instance().addApplicationState(LoadDisseminator.loadInfo_, new ApplicationState(diskUtilization));
+        }
+        catch ( Throwable ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LoadInfo.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LoadInfo.java
index e69de29b..a8d2385c 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LoadInfo.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/LoadInfo.java
@@ -0,0 +1,64 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.util.Comparator;
+
+import org.apache.cassandra.utils.FileUtils;
+
+
+class LoadInfo
+{
+    protected static class DiskSpaceComparator implements Comparator<LoadInfo>
+    {
+        public int compare(LoadInfo li, LoadInfo li2)
+        {
+            if ( li == null || li2 == null )
+                throw new IllegalArgumentException("Cannot pass in values that are NULL.");
+            
+            double space = FileUtils.stringToFileSize(li.diskSpace_);
+            double space2 = FileUtils.stringToFileSize(li2.diskSpace_);
+            return (int)(space - space2);
+        }
+    }
+        
+    private String diskSpace_;
+    
+    LoadInfo(long diskSpace)
+    {       
+        diskSpace_ = FileUtils.stringifyFileSize(diskSpace);
+    }
+    
+    LoadInfo(String loadInfo)
+    {
+        diskSpace_ = loadInfo;
+    }
+    
+    String diskSpace()
+    {
+        return diskSpace_;
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder("");       
+        sb.append(diskSpace_);
+        return sb.toString();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/MultiQuorumResponseHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/MultiQuorumResponseHandler.java
index e69de29b..5a9fb5b2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/MultiQuorumResponseHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/MultiQuorumResponseHandler.java
@@ -0,0 +1,251 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.*;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IAsyncCallback;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MultiQuorumResponseHandler implements IAsyncCallback
+{ 
+    private static Logger logger_ = Logger.getLogger( QuorumResponseHandler.class );
+    private Lock lock_ = new ReentrantLock();
+    private Condition condition_;
+    /* This maps the keys to the original data read messages */
+    private Map<String, ReadCommand> readMessages_ = new HashMap<String, ReadCommand>();
+    /* This maps the key to its set of replicas */
+    private Map<String, EndPoint[]> endpoints_ = new HashMap<String, EndPoint[]>();
+    /* This maps the groupId to the individual callback for the set of messages */
+    private Map<String, SingleQuorumResponseHandler> handlers_ = new HashMap<String, SingleQuorumResponseHandler>();
+    /* This should hold all the responses for the keys */
+    private List<Row> responses_ = new ArrayList<Row>();
+    private AtomicBoolean done_ = new AtomicBoolean(false);
+    
+    /**
+     * This is used to handle the responses from the individual messages
+     * that are sent out to the replicas.
+     * @author alakshman
+     *
+    */
+    private class SingleQuorumResponseHandler implements IAsyncCallback
+    {
+        private Lock lock_ = new ReentrantLock();
+        private IResponseResolver<Row> responseResolver_;
+        private List<Message> responses_ = new ArrayList<Message>();
+        
+        SingleQuorumResponseHandler(IResponseResolver<Row> responseResolver)
+        {
+            responseResolver_ = responseResolver;
+        }
+        
+        public void attachContext(Object o)
+        {
+            throw new UnsupportedOperationException("This operation is not supported in this implementation");
+        }
+        
+        public void response(Message response)
+        {
+            lock_.lock();
+            try
+            {
+                responses_.add(response);
+                int majority = (DatabaseDescriptor.getReplicationFactor() >> 1) + 1;                            
+                if ( responses_.size() >= majority && responseResolver_.isDataPresent(responses_))
+                {
+                    onCompletion();               
+                }
+            }
+            catch ( IOException ex )
+            {
+                logger_.info( LogUtil.throwableToString(ex) );
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        
+        private void onCompletion() throws IOException
+        {
+            try
+            {
+                Row row = responseResolver_.resolve(responses_);
+                MultiQuorumResponseHandler.this.onCompleteResponse(row);
+            }
+            catch ( DigestMismatchException ex )
+            {
+                /* 
+                 * The DigestMismatchException has the key for which the mismatch
+                 * occured bundled in it as context 
+                */
+                String key = ex.getMessage();
+                onDigestMismatch(key);
+            }
+        }
+        
+        /**
+         * This method is invoked on a digest match. We pass in the key
+         * in order to retrieve the appropriate data message that needs
+         * to be sent out to the replicas. 
+         * 
+         * @param key for which the mismatch occured.
+        */
+        private void onDigestMismatch(String key) throws IOException
+        {
+            if ( DatabaseDescriptor.getConsistencyCheck())
+            {                                
+                ReadCommand readCommand = readMessages_.get(key);
+                readCommand.setDigestQuery(false);
+                Message messageRepair = readCommand.makeReadMessage();
+                EndPoint[] endpoints = MultiQuorumResponseHandler.this.endpoints_.get(readCommand.key);
+                Message[][] messages = new Message[][]{ {messageRepair, messageRepair, messageRepair} };
+                EndPoint[][] epList = new EndPoint[][]{ endpoints };
+                MessagingService.getMessagingInstance().sendRR(messages, epList, MultiQuorumResponseHandler.this);                
+            }
+        }
+    }
+    
+    public MultiQuorumResponseHandler(Map<String, ReadCommand> readMessages, Map<String, EndPoint[]> endpoints)
+    {        
+        condition_ = lock_.newCondition();
+        readMessages_ = readMessages;
+        endpoints_ = endpoints;
+    }
+    
+    public Row[] get() throws TimeoutException
+    {
+        long startTime = System.currentTimeMillis();
+        lock_.lock();
+        try
+        {            
+            boolean bVal = true;            
+            try
+            {
+                if ( !done_.get() )
+                {                   
+                    bVal = condition_.await(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
+                }
+            }
+            catch ( InterruptedException ex )
+            {
+                logger_.debug( LogUtil.throwableToString(ex) );
+            }
+            
+            if ( !bVal && !done_.get() )
+            {
+                StringBuilder sb = new StringBuilder("");
+                for ( Row row : responses_ )
+                {
+                    sb.append(row.key());
+                    sb.append(":");
+                }                
+                throw new TimeoutException("Operation timed out - received only " +  responses_.size() + " responses from " + sb.toString() + " .");
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+        
+        logger_.info("MultiQuorumResponseHandler: " + (System.currentTimeMillis() - startTime) + " ms.");
+        return responses_.toArray( new Row[0] );
+    }
+    
+    /**
+     * Invoked when a complete response has been obtained
+     * for one of the sub-groups a.k.a keys for the query 
+     * has been performed.
+     * 
+     * @param row obtained as a result of the response.
+     */
+    void onCompleteResponse(Row row)
+    {        
+        if ( !done_.get() )
+        {
+            responses_.add(row);
+            if ( responses_.size() == readMessages_.size() )
+            {
+                done_.set(true);
+                condition_.signal();                
+            }
+        }
+    }
+    
+    /**
+     * The handler of the response message that has been
+     * sent by one of the replicas for one of the keys.
+     * 
+     * @param message the reponse message for one of the
+     *        message that we sent out.
+     */
+    public void response(Message message)
+    {
+        lock_.lock();
+        try
+        {
+            SingleQuorumResponseHandler handler = handlers_.get(message.getMessageId());
+            handler.response(message);
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    /**
+     * The context that is passed in for the query of
+     * multiple keys in the system. For each message 
+     * id in the context register a callback handler 
+     * for the same. This is done so that all responses
+     * for a given key use the same callback handler.
+     * 
+     * @param o the context which is an array of strings
+     *        corresponding to the message id's for each
+     *        key.
+     */
+    public void attachContext(Object o)
+    {
+        String[] gids = (String[])o;
+        for ( String gid : gids )
+        {
+            IResponseResolver<Row> responseResolver = new ReadResponseResolver();
+            SingleQuorumResponseHandler handler = new SingleQuorumResponseHandler(responseResolver);
+            handlers_.put(gid, handler);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/QuorumResponseHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/QuorumResponseHandler.java
index e69de29b..004a9673 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/QuorumResponseHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/QuorumResponseHandler.java
@@ -0,0 +1,130 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.*;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.IAsyncCallback;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class QuorumResponseHandler<T> implements IAsyncCallback
+{
+    private static Logger logger_ = Logger.getLogger( QuorumResponseHandler.class );
+    private Lock lock_ = new ReentrantLock();
+    private Condition condition_;
+    private int responseCount_;
+    private List<Message> responses_ = new ArrayList<Message>();
+    private IResponseResolver<T> responseResolver_;
+    private AtomicBoolean done_ = new AtomicBoolean(false);
+    
+    public QuorumResponseHandler(int responseCount, IResponseResolver<T> responseResolver)
+    {        
+        condition_ = lock_.newCondition();
+        responseCount_ = responseCount;
+        responseResolver_ =  responseResolver;
+    }
+    
+    public void  setResponseCount(int responseCount)
+    {
+        responseCount_ = responseCount;
+    }
+    
+    public T get() throws TimeoutException, DigestMismatchException
+    {
+        long startTime = System.currentTimeMillis();
+    	lock_.lock();
+        try
+        {            
+            boolean bVal = true;            
+            try
+            {
+            	if ( !done_.get() )
+                {            		
+            		bVal = condition_.await(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
+                }
+            }
+            catch ( InterruptedException ex )
+            {
+                logger_.debug( LogUtil.throwableToString(ex) );
+            }
+            
+            if ( !bVal && !done_.get() )
+            {
+                StringBuilder sb = new StringBuilder("");
+                for ( Message message : responses_ )
+                {
+                    sb.append(message.getFrom());                    
+                }                
+                throw new TimeoutException("Operation timed out - received only " +  responses_.size() + " responses from " + sb.toString() + " .");
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+            for(Message response : responses_)
+            {
+            	MessagingService.removeRegisteredCallback( response.getMessageId() );
+            }
+        }
+        logger_.info("QuorumResponseHandler: " + (System.currentTimeMillis() - startTime)
+                + " ms.");
+
+    	return responseResolver_.resolve( responses_);
+    }
+    
+    public void response(Message message)
+    {
+        lock_.lock();
+        try
+        {
+        	int majority = (responseCount_ >> 1) + 1;            
+            if ( !done_.get() )
+            {
+            	responses_.add( message );
+            	if ( responses_.size() >= majority && responseResolver_.isDataPresent(responses_))
+            	{
+            		done_.set(true);
+            		condition_.signal();            	
+            	}
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    public void attachContext(Object o)
+    {
+        throw new UnsupportedOperationException("This operation is not supported in this version of the callback handler");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/RangeVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/RangeVerbHandler.java
index e69de29b..a5eb4e45 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/RangeVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/RangeVerbHandler.java
@@ -0,0 +1,33 @@
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.RangeCommand;
+import org.apache.cassandra.db.RangeReply;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+
+public class RangeVerbHandler implements IVerbHandler
+{
+    public void doVerb(Message message)
+    {
+        List<String> keys;
+        try
+        {
+            RangeCommand command = RangeCommand.read(message);
+            Table table = Table.open(command.table);
+            keys = table.getKeyRange(command.startWith, command.stopAt, command.maxResults);
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+
+        Message response = new RangeReply(keys).getReply(message);
+        MessagingService.getMessagingInstance().sendOneWay(response, message.getFrom());
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ReadRepairManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ReadRepairManager.java
index e69de29b..5f7e70ce 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ReadRepairManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ReadRepairManager.java
@@ -0,0 +1,125 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.concurrent.locks.*;
+
+import org.apache.cassandra.db.Column;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.RowMutationMessage;
+import org.apache.cassandra.db.SuperColumn;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Header;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.Cachetable;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.ICacheExpungeHook;
+import org.apache.cassandra.utils.ICachetable;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+
+/*
+ * This class manages the read repairs . This is a singleton class
+ * it basically uses the cache table construct to schedule writes that have to be 
+ * made for read repairs. 
+ * A cachetable is created which wakes up every n  milliseconds specified by 
+ * expirationTimeInMillis and calls a global hook fn on pending entries 
+ * This fn basically sends the message to the appropriate servers to update them
+ * with the latest changes.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+class ReadRepairManager
+{
+    private static Logger logger_ = Logger.getLogger(ReadRepairManager.class);
+	private static final long expirationTimeInMillis = 2000;
+	private static Lock lock_ = new ReentrantLock();
+	private static ReadRepairManager self_ = null;
+
+	/*
+	 * This is the internal class which actually
+	 * implements the global hook fn called by the readrepair manager
+	 */
+	static class ReadRepairPerformer implements
+			ICacheExpungeHook<String, Message>
+	{
+		/*
+		 * The hook fn which takes the end point and the row mutation that 
+		 * needs to be sent to the end point in order 
+		 * to perform read repair.
+		 */
+		public void callMe(String target,
+				Message message)
+		{
+			String[] pieces = FBUtilities.strip(target, ":");
+			EndPoint to = new EndPoint(pieces[0], Integer.parseInt(pieces[1]));
+			MessagingService.getMessagingInstance().sendOneWay(message, to);			
+		}
+
+	}
+
+	private ICachetable<String, Message> readRepairTable_ = new Cachetable<String, Message>(expirationTimeInMillis, new ReadRepairManager.ReadRepairPerformer());
+
+	protected ReadRepairManager()
+	{
+
+	}
+
+	public  static ReadRepairManager instance()
+	{
+		if (self_ == null)
+		{
+            lock_.lock();
+            try
+            {
+                if ( self_ == null )
+                    self_ = new ReadRepairManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+		}
+		return self_;
+	}
+
+	/*
+	 * Schedules a read repair.
+	 * @param target endpoint on whcih the read repair should happen
+	 * @param rowMutationMessage the row mutation message that has the repaired row.
+	 */
+	public void schedule(EndPoint target, RowMutationMessage rowMutationMessage)
+	{
+        try
+        {
+            Message message = RowMutationMessage.makeRowMutationMessage(rowMutationMessage, StorageService.readRepairVerbHandler_);
+    		String key = target + ":" + message.getMessageId();
+    		readRepairTable_.put(key, message);
+        }
+        catch ( IOException ex )
+        {
+            logger_.error(LogUtil.throwableToString(ex));
+        }
+	}
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ReadResponseResolver.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ReadResponseResolver.java
index e69de29b..f5ccbb31 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ReadResponseResolver.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ReadResponseResolver.java
@@ -0,0 +1,176 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.ReadResponse;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.RowMutationMessage;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+/**
+ * This class is used by all read functions and is called by the Quorum 
+ * when atleast a few of the servers ( few is specified in Quorum)
+ * have sent the response . The resolve fn then schedules read repair 
+ * and resolution of read data from the various servers.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class ReadResponseResolver implements IResponseResolver<Row>
+{
+	private static Logger logger_ = Logger.getLogger(WriteResponseResolver.class);
+
+	/*
+	 * This method for resolving read data should look at the timestamps of each
+	 * of the columns that are read and should pick up columns with the latest
+	 * timestamp. For those columns where the timestamp is not the latest a
+	 * repair request should be scheduled.
+	 * 
+	 */
+	public Row resolve(List<Message> responses) throws DigestMismatchException
+	{
+        long startTime = System.currentTimeMillis();
+		Row retRow = null;
+		List<Row> rowList = new ArrayList<Row>();
+		List<EndPoint> endPoints = new ArrayList<EndPoint>();
+		String key = null;
+		String table = null;
+		byte[] digest = new byte[0];
+		boolean isDigestQuery = false;
+        
+        /*
+		 * Populate the list of rows from each of the messages
+		 * Check to see if there is a digest query. If a digest 
+         * query exists then we need to compare the digest with 
+         * the digest of the data that is received.
+        */
+        DataInputBuffer bufIn = new DataInputBuffer();
+		for (Message response : responses)
+		{					            
+            byte[] body = (byte[])response.getMessageBody()[0];            
+            bufIn.reset(body, body.length);
+            try
+            {
+                long start = System.currentTimeMillis();
+                ReadResponse result = ReadResponse.serializer().deserialize(bufIn);
+                logger_.debug( "Response deserialization time : " + (System.currentTimeMillis() - start) + " ms.");
+    			if(!result.isDigestQuery())
+    			{
+    				rowList.add(result.row());
+    				endPoints.add(response.getFrom());
+    				key = result.row().key();
+    				table = result.table();
+    			}
+    			else
+    			{
+    				digest = result.digest();
+    				isDigestQuery = true;
+    			}
+            }
+            catch( IOException ex )
+            {
+                logger_.info(LogUtil.throwableToString(ex));
+            }
+		}
+		// If there was a digest query compare it with all the data digests 
+		// If there is a mismatch then throw an exception so that read repair can happen.
+		if(isDigestQuery)
+		{
+			for(Row row: rowList)
+			{
+				if( !Arrays.equals(row.digest(), digest) )
+				{
+                    /* Wrap the key as the context in this exception */
+					throw new DigestMismatchException(row.key());
+				}
+			}
+		}
+		
+        /* If the rowList is empty then we had some exception above. */
+        if ( rowList.size() == 0 )
+        {
+            return retRow;
+        }
+        
+        /* Now calculate the resolved row */
+		retRow = new Row(key);		
+		for (int i = 0 ; i < rowList.size(); i++)
+		{
+			retRow.repair(rowList.get(i));			
+		}
+
+        // At  this point  we have the return row .
+		// Now we need to calculate the differnce 
+		// so that we can schedule read repairs 
+		for (int i = 0 ; i < rowList.size(); i++)
+		{
+			// since retRow is the resolved row it can be used as the super set
+			Row diffRow = rowList.get(i).diff(retRow);
+			if(diffRow == null) // no repair needs to happen
+				continue;
+			// create the row mutation message based on the diff and schedule a read repair 
+			RowMutation rowMutation = new RowMutation(table, key);            			
+	        for (ColumnFamily cf : diffRow.getColumnFamilies())
+	        {
+	            rowMutation.add(cf);
+	        }
+            RowMutationMessage rowMutationMessage = new RowMutationMessage(rowMutation);
+	        ReadRepairManager.instance().schedule(endPoints.get(i),rowMutationMessage);
+		}
+        logger_.info("resolve: " + (System.currentTimeMillis() - startTime) + " ms.");
+		return retRow;
+	}
+
+	public boolean isDataPresent(List<Message> responses)
+	{
+		boolean isDataPresent = false;
+		for (Message response : responses)
+		{
+            byte[] body = (byte[])response.getMessageBody()[0];
+			DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+            try
+            {
+    			ReadResponse result = ReadResponse.serializer().deserialize(bufIn);
+    			if(!result.isDigestQuery())
+    			{
+    				isDataPresent = true;
+    			}
+                bufIn.close();
+            }
+            catch(IOException ex)
+            {
+                logger_.info(LogUtil.throwableToString(ex));
+            }                        
+		}
+		return isDataPresent;
+	}
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageLoadBalancer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageLoadBalancer.java
index e69de29b..31449b5b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageLoadBalancer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageLoadBalancer.java
@@ -0,0 +1,410 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.SingleThreadedStage;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.EndPointState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.gms.IEndPointStateChangeSubscriber;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+
+/*
+ * The load balancing algorithm here is an implementation of
+ * the algorithm as described in the paper "Scalable range query
+ * processing for large-scale distributed database applications".
+ * This class keeps track of load information across the system.
+ * It registers itself with the Gossiper for ApplicationState namely
+ * load information i.e number of requests processed w.r.t distinct
+ * keys at an Endpoint. Monitor load infomation for a 5 minute
+ * interval and then do load balancing operations if necessary.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+final class StorageLoadBalancer implements IEndPointStateChangeSubscriber, IComponentShutdown
+{
+    class LoadBalancer implements Runnable
+    {
+        LoadBalancer()
+        {
+            /* Copy the entries in loadInfo_ into loadInfo2_ and use it for all calculations */
+            loadInfo2_.putAll(loadInfo_);
+        }
+
+        /**
+         * Obtain a node which is a potential target. Start with
+         * the neighbours i.e either successor or predecessor.
+         * Send the target a MoveMessage. If the node cannot be
+         * relocated on the ring then we pick another candidate for
+         * relocation.
+        */        
+        public void run()
+        {
+            /*
+            int threshold = (int)(StorageLoadBalancer.ratio_ * averageSystemLoad());
+            int myLoad = localLoad();            
+            EndPoint predecessor = storageService_.getPredecessor(StorageService.getLocalStorageEndPoint());
+            logger_.debug("Trying to relocate the predecessor " + predecessor);
+            boolean value = tryThisNode(myLoad, threshold, predecessor);
+            if ( !value )
+            {
+                loadInfo2_.remove(predecessor);
+                EndPoint successor = storageService_.getSuccessor(StorageService.getLocalStorageEndPoint());
+                logger_.debug("Trying to relocate the successor " + successor);
+                value = tryThisNode(myLoad, threshold, successor);
+                if ( !value )
+                {
+                    loadInfo2_.remove(successor);
+                    while ( !loadInfo2_.isEmpty() )
+                    {
+                        EndPoint target = findARandomLightNode();
+                        if ( target != null )
+                        {
+                            logger_.debug("Trying to relocate the random node " + target);
+                            value = tryThisNode(myLoad, threshold, target);
+                            if ( !value )
+                            {
+                                loadInfo2_.remove(target);
+                            }
+                            else
+                            {
+                                break;
+                            }
+                        }
+                        else
+                        {
+                            // No light nodes available - this is NOT good.
+                            logger_.warn("Not even a single lightly loaded node is available ...");
+                            break;
+                        }
+                    }
+
+                    loadInfo2_.clear();                    
+                     // If we are here and no node was available to
+                     // perform load balance with we need to report and bail.                    
+                    if ( !value )
+                    {
+                        logger_.warn("Load Balancing operations weren't performed for this node");
+                    }
+                }                
+            }
+            */        
+        }
+
+        /*
+        private boolean tryThisNode(int myLoad, int threshold, EndPoint target)
+        {
+            boolean value = false;
+            LoadInfo li = loadInfo2_.get(target);
+            int pLoad = li.count();
+            if ( ((myLoad + pLoad) >> 1) <= threshold )
+            {
+                //calculate the number of keys to be transferred
+                int keyCount = ( (myLoad - pLoad) >> 1 );
+                logger_.debug("Number of keys we attempt to transfer to " + target + " " + keyCount);
+                // Determine the token that the target should join at.         
+                BigInteger targetToken = BootstrapAndLbHelper.getTokenBasedOnPrimaryCount(keyCount);
+                // Send a MoveMessage and see if this node is relocateable
+                MoveMessage moveMessage = new MoveMessage(targetToken);
+                Message message = new Message(StorageService.getLocalStorageEndPoint(), StorageLoadBalancer.lbStage_, StorageLoadBalancer.moveMessageVerbHandler_, new Object[]{moveMessage});
+                logger_.debug("Sending a move message to " + target);
+                IAsyncResult result = MessagingService.getMessagingInstance().sendRR(message, target);
+                value = (Boolean)result.get()[0];
+                logger_.debug("Response for query to relocate " + target + " is " + value);
+            }
+            return value;
+        }
+        */
+    }
+
+    class MoveMessageVerbHandler implements IVerbHandler
+    {
+        public void doVerb(Message message)
+        {
+            Message reply = message.getReply(StorageService.getLocalStorageEndPoint(), new Object[]{isMoveable_.get()});
+            MessagingService.getMessagingInstance().sendOneWay(reply, message.getFrom());
+            if ( isMoveable_.get() )
+            {
+                MoveMessage moveMessage = (MoveMessage)message.getMessageBody()[0];
+                /* Start the leave operation and join the ring at the position specified */
+                isMoveable_.set(false);
+            }
+        }
+    }
+
+    private static final Logger logger_ = Logger.getLogger(StorageLoadBalancer.class);
+    private static final String lbStage_ = "LOAD-BALANCER-STAGE";
+    private static final String moveMessageVerbHandler_ = "MOVE-MESSAGE-VERB-HANDLER";
+    /* time to delay in minutes the actual load balance procedure if heavily loaded */
+    private static final int delay_ = 5;
+    /* Ratio of highest loaded node and the average load. */
+    private static final double ratio_ = 1.5;
+
+    private StorageService storageService_;
+    /* this indicates whether this node is already helping someone else */
+    private AtomicBoolean isMoveable_ = new AtomicBoolean(false);
+    private Map<EndPoint, LoadInfo> loadInfo_ = new HashMap<EndPoint, LoadInfo>();
+    /* This map is a clone of the one above and is used for various calculations during LB operation */
+    private Map<EndPoint, LoadInfo> loadInfo2_ = new HashMap<EndPoint, LoadInfo>();
+    /* This thread pool is used for initiating load balancing operations */
+    private ScheduledThreadPoolExecutor lb_ = new DebuggableScheduledThreadPoolExecutor(
+            1,
+            new ThreadFactoryImpl("LB-OPERATIONS")
+            );
+    /* This thread pool is used by target node to leave the ring. */
+    private ExecutorService lbOperations_ = new DebuggableThreadPoolExecutor(1,
+            1,
+            Integer.MAX_VALUE,
+            TimeUnit.SECONDS,
+            new LinkedBlockingQueue<Runnable>(),
+            new ThreadFactoryImpl("LB-TARGET")
+            );
+
+    StorageLoadBalancer(StorageService storageService)
+    {
+        storageService_ = storageService;
+        /* register the load balancer stage */
+        StageManager.registerStage(StorageLoadBalancer.lbStage_, new SingleThreadedStage(StorageLoadBalancer.lbStage_));
+        /* register the load balancer verb handler */
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageLoadBalancer.moveMessageVerbHandler_, new MoveMessageVerbHandler());
+        /* register with the StorageService */
+        storageService_.registerComponentForShutdown(this);
+    }
+
+    public void start()
+    {
+        /* Register with the Gossiper for EndPointState notifications */
+        Gossiper.instance().register(this);
+    }
+
+    public void shutdown()
+    {
+        lbOperations_.shutdownNow();
+        lb_.shutdownNow();
+    }
+
+    public void onChange(EndPoint endpoint, EndPointState epState)
+    {
+        logger_.debug("CHANGE IN STATE FOR @ StorageLoadBalancer " + endpoint);
+        // load information for this specified endpoint for load balancing 
+        ApplicationState loadInfoState = epState.getApplicationState(LoadDisseminator.loadInfo_);
+        if ( loadInfoState != null )
+        {
+            String lInfoState = loadInfoState.getState();
+            LoadInfo lInfo = new LoadInfo(lInfoState);
+            loadInfo_.put(endpoint, lInfo);
+            
+            /*
+            int currentLoad = Integer.parseInt(loadInfoState.getState());
+            // update load information for this endpoint
+            loadInfo_.put(endpoint, currentLoad);
+
+            // clone load information to perform calculations
+            loadInfo2_.putAll(loadInfo_);
+            // Perform the analysis for load balance operations
+            if ( isHeavyNode() )
+            {
+                logger_.debug(StorageService.getLocalStorageEndPoint() + " is a heavy node with load " + localLoad());
+                // lb_.schedule( new LoadBalancer(), StorageLoadBalancer.delay_, TimeUnit.MINUTES );
+            }
+            */
+        }       
+    }
+
+    /*
+     * Load information associated with a given endpoint.
+    */
+    LoadInfo getLoad(EndPoint ep)
+    {
+        LoadInfo li = loadInfo_.get(ep);        
+        return li;        
+    }
+
+    /*
+    private boolean isMoveable()
+    {
+        if ( !isMoveable_.get() )
+            return false;
+        int myload = localLoad();
+        EndPoint successor = storageService_.getSuccessor(StorageService.getLocalStorageEndPoint());
+        LoadInfo li = loadInfo2_.get(successor);
+        // "load" is NULL means that the successor node has not
+        // yet gossiped its load information. We should return
+        // false in this case since we want to err on the side
+        // of caution.
+        if ( li == null )
+            return false;
+        else
+        {            
+            if ( ( myload + li.count() ) > StorageLoadBalancer.ratio_*averageSystemLoad() )
+                return false;
+            else
+                return true;
+        }
+    }
+    */
+
+    /*
+    private int localLoad()
+    {
+        LoadInfo value = loadInfo2_.get(StorageService.getLocalStorageEndPoint());
+        return (value == null) ? 0 : value.count();
+    }
+    */
+
+    /*
+    private int averageSystemLoad()
+    {
+        int nodeCount = loadInfo2_.size();
+        Set<EndPoint> nodes = loadInfo2_.keySet();
+
+        int systemLoad = 0;
+        for ( EndPoint node : nodes )
+        {
+            LoadInfo load = loadInfo2_.get(node);
+            if ( load != null )
+                systemLoad += load.count();
+        }
+        int averageLoad = (nodeCount > 0) ? (systemLoad / nodeCount) : 0;
+        logger_.debug("Average system load should be " + averageLoad);
+        return averageLoad;
+    }
+    */
+    
+    /*
+    private boolean isHeavyNode()
+    {
+        return ( localLoad() > ( StorageLoadBalancer.ratio_ * averageSystemLoad() ) );
+    }
+    */
+    
+    /*
+    private boolean isMoveable(EndPoint target)
+    {
+        int threshold = (int)(StorageLoadBalancer.ratio_ * averageSystemLoad());
+        if ( isANeighbour(target) )
+        {
+            // If the target is a neighbour then it is
+            // moveable if its
+            LoadInfo load = loadInfo2_.get(target);
+            if ( load == null )
+                return false;
+            else
+            {
+                int myload = localLoad();
+                int avgLoad = (load.count() + myload) >> 1;
+                if ( avgLoad <= threshold )
+                    return true;
+                else
+                    return false;
+            }
+        }
+        else
+        {
+            EndPoint successor = storageService_.getSuccessor(target);
+            LoadInfo sLoad = loadInfo2_.get(successor);
+            LoadInfo targetLoad = loadInfo2_.get(target);
+            if ( (sLoad.count() + targetLoad.count()) > threshold )
+                return false;
+            else
+                return true;
+        }
+    }
+    */
+
+    private boolean isANeighbour(EndPoint neighbour)
+    {
+        EndPoint predecessor = storageService_.getPredecessor(StorageService.getLocalStorageEndPoint());
+        if ( predecessor.equals(neighbour) )
+            return true;
+
+        EndPoint successor = storageService_.getSuccessor(StorageService.getLocalStorageEndPoint());
+        if ( successor.equals(neighbour) )
+            return true;
+
+        return false;
+    }
+
+    /*
+     * Determine the nodes that are lightly loaded. Choose at
+     * random one of the lightly loaded nodes and use them as
+     * a potential target for load balance.
+    */
+    /*
+    private EndPoint findARandomLightNode()
+    {
+        List<EndPoint> potentialCandidates = new ArrayList<EndPoint>();
+        Set<EndPoint> allTargets = loadInfo2_.keySet();
+        int avgLoad =  averageSystemLoad();
+
+        for( EndPoint target : allTargets )
+        {
+            LoadInfo load = loadInfo2_.get(target);
+            if ( load.count() < avgLoad )
+                potentialCandidates.add(target);
+        }
+
+        if ( potentialCandidates.size() > 0 )
+        {
+            Random random = new Random();
+            int index = random.nextInt(potentialCandidates.size());
+            return potentialCandidates.get(index);
+        }
+        return null;
+    }
+    */
+}
+
+class MoveMessage implements Serializable
+{
+    private Token targetToken_;
+
+    private MoveMessage()
+    {
+    }
+
+    MoveMessage(Token targetToken)
+    {
+        targetToken_ = targetToken;
+    }
+
+    Token getTargetToken()
+    {
+        return targetToken_;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
index e69de29b..b9bc2eaa 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
@@ -0,0 +1,628 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+import org.apache.commons.lang.StringUtils;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.ReadResponse;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.db.TouchMessage;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IAsyncResult;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+public class StorageProxy
+{
+    private static Logger logger_ = Logger.getLogger(StorageProxy.class);    
+    
+    /**
+     * This method is responsible for creating Message to be
+     * sent over the wire to N replicas where some of the replicas
+     * may be hints.
+     */
+    private static Map<EndPoint, Message> createWriteMessages(RowMutation rm, Map<EndPoint, EndPoint> endpointMap) throws IOException
+    {
+		Map<EndPoint, Message> messageMap = new HashMap<EndPoint, Message>();
+		Message message = rm.makeRowMutationMessage();
+
+		for (Map.Entry<EndPoint, EndPoint> entry : endpointMap.entrySet())
+		{
+            EndPoint target = entry.getKey();
+            EndPoint hint = entry.getValue();
+            if ( !target.equals(hint) )
+			{
+				Message hintedMessage = rm.makeRowMutationMessage();
+				hintedMessage.addHeader(RowMutation.HINT, EndPoint.toBytes(hint) );
+				logger_.debug("Sending the hint of " + target.getHost() + " to " + hint.getHost());
+				messageMap.put(target, hintedMessage);
+			}
+			else
+			{
+				messageMap.put(target, message);
+			}
+		}
+		return messageMap;
+    }
+    
+    /**
+     * Use this method to have this RowMutation applied
+     * across all replicas. This method will take care
+     * of the possibility of a replica being down and hint
+     * the data across to some other replica. 
+     * @param rm the mutation to be applied across the replicas
+    */
+    public static void insert(RowMutation rm)
+	{
+        // TODO check for valid Table, ColumnFamily
+
+        /*
+         * Get the N nodes from storage service where the data needs to be
+         * replicated
+         * Construct a message for write
+         * Send them asynchronously to the replicas.
+        */
+
+		try
+		{
+			Map<EndPoint, EndPoint> endpointMap = StorageService.instance().getNStorageEndPointMap(rm.key());
+			// TODO: throw a thrift exception if we do not have N nodes
+			Map<EndPoint, Message> messageMap = createWriteMessages(rm, endpointMap);
+            logger_.debug("insert writing to [" + StringUtils.join(messageMap.keySet(), ", ") + "]");
+			for (Map.Entry<EndPoint, Message> entry : messageMap.entrySet())
+			{
+				MessagingService.getMessagingInstance().sendOneWay(entry.getValue(), entry.getKey());
+			}
+		}
+        catch (Exception e)
+        {
+            logger_.error( LogUtil.throwableToString(e) );
+        }
+        return;
+    }
+
+    public static boolean insertBlocking(RowMutation rm)
+    {
+        try
+        {
+            Message message = rm.makeRowMutationMessage();
+
+            IResponseResolver<Boolean> writeResponseResolver = new WriteResponseResolver();
+            QuorumResponseHandler<Boolean> quorumResponseHandler = new QuorumResponseHandler<Boolean>(
+                    DatabaseDescriptor.getReplicationFactor(),
+                    writeResponseResolver);
+            EndPoint[] endpoints = StorageService.instance().getNStorageEndPoint(rm.key());
+            logger_.debug("insertBlocking writing to [" + StringUtils.join(endpoints, ", ") + "]");
+            // TODO: throw a thrift exception if we do not have N nodes
+
+            MessagingService.getMessagingInstance().sendRR(message, endpoints, quorumResponseHandler);
+            return quorumResponseHandler.get();
+
+            // TODO: if the result is false that means the writes to all the
+            // servers failed hence we need to throw an exception or return an
+            // error back to the client so that it can take appropriate action.
+        }
+        catch (Exception e)
+        {
+            logger_.error( LogUtil.throwableToString(e) );
+            return false;
+        }
+    }
+    
+    private static Map<String, Message> constructMessages(Map<String, ReadCommand> readMessages) throws IOException
+    {
+        Map<String, Message> messages = new HashMap<String, Message>();
+        Set<String> keys = readMessages.keySet();        
+        for ( String key : keys )
+        {
+            Message message = readMessages.get(key).makeReadMessage();
+            messages.put(key, message);
+        }        
+        return messages;
+    }
+    
+    private static IAsyncResult dispatchMessages(Map<String, EndPoint> endPoints, Map<String, Message> messages)
+    {
+        Set<String> keys = endPoints.keySet();
+        EndPoint[] eps = new EndPoint[keys.size()];
+        Message[] msgs  = new Message[keys.size()];
+        
+        int i = 0;
+        for ( String key : keys )
+        {
+            eps[i] = endPoints.get(key);
+            msgs[i] = messages.get(key);
+            ++i;
+        }
+        
+        IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(msgs, eps);
+        return iar;
+    }
+    
+    /**
+     * This is an implementation for the multiget version. 
+     * @param readMessages map of key --> ReadMessage to be sent
+     * @return map of key --> Row
+     * @throws IOException
+     * @throws TimeoutException
+     */
+    public static Map<String, Row> doReadProtocol(Map<String, ReadCommand> readMessages) throws IOException,TimeoutException
+    {
+        Map<String, Row> rows = new HashMap<String, Row>();
+        Set<String> keys = readMessages.keySet();
+        /* Find all the suitable endpoints for the keys */
+        Map<String, EndPoint> endPoints = StorageService.instance().findSuitableEndPoints(keys.toArray( new String[0] ));
+        /* Construct the messages to be sent out */
+        Map<String, Message> messages = constructMessages(readMessages);
+        /* Dispatch the messages to the respective endpoints */
+        IAsyncResult iar = dispatchMessages(endPoints, messages);        
+        List<Object[]> results = iar.multiget(2*DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
+        
+        for ( Object[] result : results )
+        {
+            byte[] body = (byte[])result[0];
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+            ReadResponse response = ReadResponse.serializer().deserialize(bufIn);
+            Row row = response.row();
+            rows.put(row.key(), row);
+        }        
+        return rows;
+    }
+
+    /**
+     * Read the data from one replica.  If there is no reply, read the data from another.  In the event we get
+     * the data we perform consistency checks and figure out if any repairs need to be done to the replicas.
+     * @param command the read to perform
+     * @return the row associated with command.key
+     * @throws Exception
+     */
+    private static Row weakReadRemote(ReadCommand command) throws IOException
+    {
+        EndPoint endPoint = StorageService.instance().findSuitableEndPoint(command.key);
+        assert endPoint != null;
+        logger_.debug("weakreadremote reading " + command + " from " + endPoint);
+        Message message = command.makeReadMessage();
+        message.addHeader(ReadCommand.DO_REPAIR, ReadCommand.DO_REPAIR.getBytes());
+        IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(message, endPoint);
+        byte[] body;
+        try
+        {
+            Object[] result = iar.get(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
+            body = (byte[])result[0];
+        }
+        catch (TimeoutException e)
+        {
+            throw new RuntimeException(e);
+            // TODO retry to a different endpoint
+        }
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(body, body.length);
+        ReadResponse response = ReadResponse.serializer().deserialize(bufIn);
+        return response.row();
+    }
+
+    static void touch_local(String tablename, String key, boolean fData ) throws IOException
+    {
+		Table table = Table.open( tablename );
+		table.touch(key, fData);
+    }
+
+    static void weakTouchProtocol(String tablename, String key, boolean fData) throws Exception
+    {
+    	EndPoint endPoint = null;
+    	try
+    	{
+    		endPoint = StorageService.instance().findSuitableEndPoint(key);
+    	}
+    	catch( Throwable ex)
+    	{
+    		ex.printStackTrace();
+    	}
+    	if(endPoint != null)
+    	{
+    		if(endPoint.equals(StorageService.getLocalStorageEndPoint()))
+    		{
+    	    	touch_local(tablename, key, fData);
+    	    	return;
+    	    }
+            TouchMessage touchMessage = null;
+            touchMessage = new TouchMessage(tablename, key, fData);
+            Message message = TouchMessage.makeTouchMessage(touchMessage);
+            MessagingService.getMessagingInstance().sendOneWay(message, endPoint);
+    	}
+    	return ;
+    }
+    
+    static void strongTouchProtocol(String tablename, String key, boolean fData) throws Exception
+    {
+        Map<EndPoint, EndPoint> endpointMap = StorageService.instance().getNStorageEndPointMap(key);
+        Set<EndPoint> endpoints = endpointMap.keySet();
+        TouchMessage touchMessage = null;
+        touchMessage = new TouchMessage(tablename, key, fData);
+        Message message = TouchMessage.makeTouchMessage(touchMessage);
+        for(EndPoint endpoint : endpoints)
+        {
+            MessagingService.getMessagingInstance().sendOneWay(message, endpoint);
+        }    	
+    }
+    
+    /*
+     * Only touch data on the most suitable end point.
+     */
+    public static void touchProtocol(String tablename, String key, boolean fData, StorageService.ConsistencyLevel consistencyLevel) throws Exception
+    {
+        switch ( consistencyLevel )
+        {
+	        case WEAK:
+	            weakTouchProtocol(tablename, key, fData);
+	            break;
+	            
+	        case STRONG:
+	            strongTouchProtocol(tablename, key, fData);
+	            break;
+	            
+	        default:
+	            weakTouchProtocol(tablename, key, fData);
+	            break;
+        }
+    }  
+
+    /**
+     * Performs the actual reading of a row out of the StorageService, fetching
+     * a specific set of column names from a given column family.
+     */
+    public static Row readProtocol(ReadCommand command, StorageService.ConsistencyLevel consistencyLevel)
+    throws IOException, TimeoutException
+    {
+        long startTime = System.currentTimeMillis();
+        Row row = null;
+        EndPoint[] endpoints = StorageService.instance().getNStorageEndPoint(command.key);
+
+        if (consistencyLevel == StorageService.ConsistencyLevel.WEAK)
+        {
+            boolean foundLocal = Arrays.asList(endpoints).contains(StorageService.getLocalStorageEndPoint());
+            if (foundLocal)
+            {
+                row = weakReadLocal(command);
+            }
+            else
+            {
+                row = weakReadRemote(command);
+            }
+        }
+        else
+        {
+            assert consistencyLevel == StorageService.ConsistencyLevel.STRONG;
+            row = strongRead(command);
+        }
+
+        logger_.debug("Finished reading " + row + " in " + (System.currentTimeMillis() - startTime) + " ms.");
+        return row;
+    }
+
+    public static Map<String, Row> readProtocol(String[] keys, ReadCommand readCommand, StorageService.ConsistencyLevel consistencyLevel) throws Exception
+    {
+        Map<String, Row> rows = new HashMap<String, Row>();        
+        switch ( consistencyLevel )
+        {
+            case WEAK:
+                rows = weakReadProtocol(keys, readCommand);
+                break;
+                
+            case STRONG:
+                rows = strongReadProtocol(keys, readCommand);
+                break;
+                
+            default:
+                rows = weakReadProtocol(keys, readCommand);
+                break;
+        }
+        return rows;
+    }
+
+    /**
+     * This is a multiget version of the above method.
+     * @param tablename
+     * @param keys
+     * @param columnFamily
+     * @param start
+     * @param count
+     * @return
+     * @throws IOException
+     * @throws TimeoutException
+     */
+    public static Map<String, Row> strongReadProtocol(String[] keys, ReadCommand readCommand) throws IOException, TimeoutException
+    {       
+        Map<String, Row> rows = new HashMap<String, Row>();
+        long startTime = System.currentTimeMillis();        
+        // TODO: throw a thrift exception if we do not have N nodes
+        Map<String, ReadCommand[]> readMessages = new HashMap<String, ReadCommand[]>();
+        for (String key : keys )
+        {
+            ReadCommand[] readParameters = new ReadCommand[2];
+            readParameters[0] = readCommand.copy();
+            readParameters[1] = readCommand.copy();
+            readParameters[1].setDigestQuery(true);
+            readMessages.put(key, readParameters);
+        }        
+        rows = doStrongReadProtocol(readMessages);         
+        logger_.debug("readProtocol: " + (System.currentTimeMillis() - startTime) + " ms.");
+        return rows;
+    }
+
+    /*
+     * This function executes the read protocol.
+        // 1. Get the N nodes from storage service where the data needs to be
+        // replicated
+        // 2. Construct a message for read\write
+         * 3. Set one of teh messages to get teh data and teh rest to get teh digest
+        // 4. SendRR ( to all the nodes above )
+        // 5. Wait for a response from atleast X nodes where X <= N and teh data node
+         * 6. If the digest matches return teh data.
+         * 7. else carry out read repair by getting data from all the nodes.
+        // 5. return success
+     */
+    private static Row strongRead(ReadCommand command) throws IOException, TimeoutException
+    {
+        // TODO: throw a thrift exception if we do not have N nodes
+        assert !command.isDigestQuery();
+        ReadCommand readMessageDigestOnly = command.copy();
+        readMessageDigestOnly.setDigestQuery(true);
+
+        Row row = null;
+        Message message = command.makeReadMessage();
+        Message messageDigestOnly = readMessageDigestOnly.makeReadMessage();
+
+        IResponseResolver<Row> readResponseResolver = new ReadResponseResolver();
+        QuorumResponseHandler<Row> quorumResponseHandler = new QuorumResponseHandler<Row>(
+                DatabaseDescriptor.getReplicationFactor(),
+                readResponseResolver);
+        EndPoint dataPoint = StorageService.instance().findSuitableEndPoint(command.key);
+        List<EndPoint> endpointList = new ArrayList<EndPoint>(Arrays.asList(StorageService.instance().getNStorageEndPoint(command.key)));
+        /* Remove the local storage endpoint from the list. */
+        endpointList.remove(dataPoint);
+        EndPoint[] endPoints = new EndPoint[endpointList.size() + 1];
+        Message messages[] = new Message[endpointList.size() + 1];
+
+        /*
+         * First message is sent to the node that will actually get
+         * the data for us. The other two replicas are only sent a
+         * digest query.
+        */
+        endPoints[0] = dataPoint;
+        messages[0] = message;
+        for (int i = 1; i < endPoints.length; i++)
+        {
+            endPoints[i] = endpointList.get(i - 1);
+            messages[i] = messageDigestOnly;
+        }
+        logger_.debug("strongread reading " + command + " from " + StringUtils.join(endPoints, ", "));
+
+        try
+        {
+            MessagingService.getMessagingInstance().sendRR(messages, endPoints, quorumResponseHandler);
+
+            long startTime2 = System.currentTimeMillis();
+            row = quorumResponseHandler.get();
+            logger_.debug("quorumResponseHandler: " + (System.currentTimeMillis() - startTime2) + " ms.");
+        }
+        catch (DigestMismatchException ex)
+        {
+            if ( DatabaseDescriptor.getConsistencyCheck())
+            {
+                IResponseResolver<Row> readResponseResolverRepair = new ReadResponseResolver();
+                QuorumResponseHandler<Row> quorumResponseHandlerRepair = new QuorumResponseHandler<Row>(
+                        DatabaseDescriptor.getReplicationFactor(),
+                        readResponseResolverRepair);
+                logger_.info("DigestMismatchException: " + command.key);
+                Message messageRepair = command.makeReadMessage();
+                MessagingService.getMessagingInstance().sendRR(messageRepair, endPoints,
+                                                               quorumResponseHandlerRepair);
+                try
+                {
+                    row = quorumResponseHandlerRepair.get();
+                }
+                catch (DigestMismatchException e)
+                {
+                    // TODO should this be a thrift exception?
+                    throw new RuntimeException(e);
+                }
+            }
+        }
+
+        return row;
+    }
+
+    private static Map<String, Message[]> constructReplicaMessages(Map<String, ReadCommand[]> readMessages) throws IOException
+    {
+        Map<String, Message[]> messages = new HashMap<String, Message[]>();
+        Set<String> keys = readMessages.keySet();
+        
+        for ( String key : keys )
+        {
+            Message[] msg = new Message[DatabaseDescriptor.getReplicationFactor()];
+            ReadCommand[] readParameters = readMessages.get(key);
+            msg[0] = readParameters[0].makeReadMessage();
+            for ( int i = 1; i < msg.length; ++i )
+            {
+                msg[i] = readParameters[1].makeReadMessage();
+            }
+        }        
+        return messages;
+    }
+    
+    private static MultiQuorumResponseHandler dispatchMessages(Map<String, ReadCommand[]> readMessages, Map<String, Message[]> messages) throws IOException
+    {
+        Set<String> keys = messages.keySet();
+        /* This maps the keys to the original data read messages */
+        Map<String, ReadCommand> readMessage = new HashMap<String, ReadCommand>();
+        /* This maps the keys to their respective endpoints/replicas */
+        Map<String, EndPoint[]> endpoints = new HashMap<String, EndPoint[]>();
+        /* Groups the messages that need to be sent to the individual keys */
+        Message[][] msgList = new Message[messages.size()][DatabaseDescriptor.getReplicationFactor()];
+        /* Respects the above grouping and provides the endpoints for the above messages */
+        EndPoint[][] epList = new EndPoint[messages.size()][DatabaseDescriptor.getReplicationFactor()];
+        
+        int i = 0;
+        for ( String key : keys )
+        {
+            /* This is the primary */
+            EndPoint dataPoint = StorageService.instance().findSuitableEndPoint(key);
+            List<EndPoint> replicas = new ArrayList<EndPoint>( StorageService.instance().getNLiveStorageEndPoint(key) );
+            replicas.remove(dataPoint);
+            /* Get the messages to be sent index 0 is the data messages and index 1 is the digest message */
+            Message[] message = messages.get(key);           
+            msgList[i][0] = message[0];
+            int N = DatabaseDescriptor.getReplicationFactor();
+            for ( int j = 1; j < N; ++j )
+            {
+                msgList[i][j] = message[1];
+            }
+            /* Get the endpoints to which the above messages need to be sent */
+            epList[i][0] = dataPoint;
+            for ( int j = 1; i < N; ++i )
+            {                
+                epList[i][j] = replicas.get(j - 1);
+            } 
+            /* Data ReadMessage associated with this key */
+            readMessage.put( key, readMessages.get(key)[0] );
+            /* EndPoints for this specific key */
+            endpoints.put(key, epList[i]);
+            ++i;
+        }
+                
+        /* Handles the read semantics for this entire set of keys */
+        MultiQuorumResponseHandler quorumResponseHandlers = new MultiQuorumResponseHandler(readMessage, endpoints);
+        MessagingService.getMessagingInstance().sendRR(msgList, epList, quorumResponseHandlers);
+        return quorumResponseHandlers;
+    }
+    
+    /**
+    *  This method performs the read from the replicas for a bunch of keys.
+    *  @param readMessages map of key --> readMessage[] of two entries where 
+    *         the first entry is the readMessage for the data and the second
+    *         is the entry for the digest 
+    *  @return map containing key ---> Row
+    *  @throws IOException, TimeoutException
+   */
+    private static Map<String, Row> doStrongReadProtocol(Map<String, ReadCommand[]> readMessages) throws IOException
+    {        
+        Map<String, Row> rows = new HashMap<String, Row>();
+        /* Construct the messages to be sent to the replicas */
+        Map<String, Message[]> replicaMessages = constructReplicaMessages(readMessages);
+        /* Dispatch the messages to the different replicas */
+        MultiQuorumResponseHandler cb = dispatchMessages(readMessages, replicaMessages);
+        try
+        {
+            Row[] rows2 = cb.get();
+            for ( Row row : rows2 )
+            {
+                rows.put(row.key(), row);
+            }
+        }
+        catch ( TimeoutException ex )
+        {
+            logger_.info("Operation timed out waiting for responses ...");
+            logger_.info(LogUtil.throwableToString(ex));
+        }
+        return rows;
+    }
+
+    /**
+     * This version is used when results for multiple keys needs to be
+     * retrieved.
+     * 
+     * @param tablename name of the table that needs to be queried
+     * @param keys keys whose values we are interested in 
+     * @param columnFamily name of the "column" we are interested in
+     * @param columns the columns we are interested in
+     * @return a mapping of key --> Row
+     * @throws Exception
+     */
+    public static Map<String, Row> weakReadProtocol(String[] keys, ReadCommand readCommand) throws Exception
+    {
+        Row row = null;
+        long startTime = System.currentTimeMillis();
+        Map<String, ReadCommand> readMessages = new HashMap<String, ReadCommand>();
+        for ( String key : keys )
+        {
+            ReadCommand readCmd = readCommand.copy();
+            readMessages.put(key, readCmd);
+        }
+        /* Performs the multiget in parallel */
+        Map<String, Row> rows = doReadProtocol(readMessages);
+        /*
+         * Do the consistency checks for the keys that are being queried
+         * in the background.
+        */
+        for ( String key : keys )
+        {
+            List<EndPoint> endpoints = StorageService.instance().getNLiveStorageEndPoint(key);
+            /* Remove the local storage endpoint from the list. */
+            endpoints.remove( StorageService.getLocalStorageEndPoint() );
+            if ( endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
+                StorageService.instance().doConsistencyCheck(row, endpoints, readMessages.get(key));
+        }
+        return rows;
+    }
+
+    /*
+    * This function executes the read protocol locally and should be used only if consistency is not a concern.
+    * Read the data from the local disk and return if the row is NOT NULL. If the data is NULL do the read from
+    * one of the other replicas (in the same data center if possible) till we get the data. In the event we get
+    * the data we perform consistency checks and figure out if any repairs need to be done to the replicas.
+    */
+    private static Row weakReadLocal(ReadCommand command) throws IOException
+    {
+        logger_.debug("weakreadlocal for " + command);
+        List<EndPoint> endpoints = StorageService.instance().getNLiveStorageEndPoint(command.key);
+        /* Remove the local storage endpoint from the list. */
+        endpoints.remove(StorageService.getLocalStorageEndPoint());
+        // TODO: throw a thrift exception if we do not have N nodes
+
+        Table table = Table.open(DatabaseDescriptor.getTables().get(0));
+        Row row = command.getRow(table);
+
+        /*
+           * Do the consistency checks in the background and return the
+           * non NULL row.
+           */
+        if (endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
+            StorageService.instance().doConsistencyCheck(row, endpoints, command);
+        return row;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
index e69de29b..d6640f48 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
@@ -0,0 +1,1251 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.File;
+import java.io.IOException;
+import java.lang.management.ManagementFactory;
+import java.lang.reflect.InvocationTargetException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.analytics.AnalyticsContext;
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.MultiThreadedStage;
+import org.apache.cassandra.concurrent.SingleThreadedStage;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.BinaryVerbHandler;
+import org.apache.cassandra.db.CalloutDeployVerbHandler;
+import org.apache.cassandra.db.DBManager;
+import org.apache.cassandra.db.DataFileVerbHandler;
+import org.apache.cassandra.db.HintedHandOffManager;
+import org.apache.cassandra.db.LoadVerbHandler;
+import org.apache.cassandra.db.Memtable;
+import org.apache.cassandra.db.ReadRepairVerbHandler;
+import org.apache.cassandra.db.ReadVerbHandler;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.RowMutationVerbHandler;
+import org.apache.cassandra.db.SystemTable;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.db.TouchVerbHandler;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.dht.BootStrapper;
+import org.apache.cassandra.dht.BootstrapInitiateMessage;
+import org.apache.cassandra.dht.BootstrapMetadataVerbHandler;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.OrderPreservingPartitioner;
+import org.apache.cassandra.dht.RandomPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.gms.IEndPointStateChangeSubscriber;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.EndPointState;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.SelectorManager;
+import org.apache.cassandra.net.http.HttpConnection;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.locator.IEndPointSnitch;
+import org.apache.cassandra.locator.TokenMetadata;
+import org.apache.cassandra.locator.IReplicaPlacementStrategy;
+import org.apache.cassandra.locator.EndPointSnitch;
+import org.apache.cassandra.locator.RackUnawareStrategy;
+import org.apache.cassandra.locator.RackAwareStrategy;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.tools.MembershipCleanerVerbHandler;
+
+import org.apache.zookeeper.CreateMode;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.WatchedEvent;
+import org.apache.zookeeper.Watcher;
+import org.apache.zookeeper.ZooDefs.Ids;
+import org.apache.zookeeper.ZooKeeper;
+import org.apache.zookeeper.data.Stat;
+
+/*
+ * This abstraction contains the token/identifier of this node
+ * on the identifier space. This token gets gossiped around.
+ * This class will also maintain histograms of the load information
+ * of other nodes in the cluster.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public final class StorageService implements IEndPointStateChangeSubscriber, StorageServiceMBean
+{
+    private static Logger logger_ = Logger.getLogger(StorageService.class);     
+    private final static String nodeId_ = "NODE-IDENTIFIER";
+    private final static String loadAll_ = "LOAD-ALL";
+    /* Gossip load after every 5 mins. */
+    private static final long threshold_ = 5 * 60 * 1000L;
+    
+    /* All stage identifiers */
+    public final static String mutationStage_ = "ROW-MUTATION-STAGE";
+    public final static String readStage_ = "ROW-READ-STAGE";
+    public final static String mrStage_ = "MAP-REDUCE-STAGE";
+    
+    /* All verb handler identifiers */
+    public final static String mutationVerbHandler_ = "ROW-MUTATION-VERB-HANDLER";
+    public final static String tokenVerbHandler_ = "TOKEN-VERB-HANDLER";
+    public final static String loadVerbHandler_ = "LOAD-VERB-HANDLER";
+    public final static String binaryVerbHandler_ = "BINARY-VERB-HANDLER";
+    public final static String readRepairVerbHandler_ = "READ-REPAIR-VERB-HANDLER";
+    public final static String readVerbHandler_ = "ROW-READ-VERB-HANDLER";
+    public final static String bootStrapInitiateVerbHandler_ = "BOOTSTRAP-INITIATE-VERB-HANDLER";
+    public final static String bootStrapInitiateDoneVerbHandler_ = "BOOTSTRAP-INITIATE-DONE-VERB-HANDLER";
+    public final static String bootStrapTerminateVerbHandler_ = "BOOTSTRAP-TERMINATE-VERB-HANDLER";
+    public final static String dataFileVerbHandler_ = "DATA-FILE-VERB-HANDLER";
+    public final static String mbrshipCleanerVerbHandler_ = "MBRSHIP-CLEANER-VERB-HANDLER";
+    public final static String bsMetadataVerbHandler_ = "BS-METADATA-VERB-HANDLER";
+    public final static String calloutDeployVerbHandler_ = "CALLOUT-DEPLOY-VERB-HANDLER";
+    public final static String touchVerbHandler_ = "TOUCH-VERB-HANDLER";
+    public static String rangeVerbHandler_ = "RANGE-VERB-HANDLER";
+
+    public static enum ConsistencyLevel
+    {
+    	WEAK,
+    	STRONG
+    }
+
+    private static StorageService instance_;
+    /* Used to lock the factory for creation of StorageService instance */
+    private static Lock createLock_ = new ReentrantLock();
+    private static EndPoint tcpAddr_;
+    private static EndPoint udpAddr_;
+    private static IPartitioner partitioner_;
+
+    public static EndPoint getLocalStorageEndPoint()
+    {
+        return tcpAddr_;
+    }
+
+    public static EndPoint getLocalControlEndPoint()
+    {
+        return udpAddr_;
+    }
+
+    public static String getHostUrl()
+    {
+        return "http://" + tcpAddr_.getHost() + ":" + DatabaseDescriptor.getHttpPort();
+    }
+
+    public static IPartitioner getPartitioner() {
+        return partitioner_;
+    }
+    
+    public static enum BootstrapMode
+    {
+        HINT,
+        FULL
+    }
+
+    public static class BootstrapInitiateDoneVerbHandler implements IVerbHandler
+    {
+        private static Logger logger_ = Logger.getLogger( BootstrapInitiateDoneVerbHandler.class );
+
+        public void doVerb(Message message)
+        {
+            logger_.debug("Received a bootstrap initiate done message ...");
+            /* Let the Stream Manager do his thing. */
+            StreamManager.instance(message.getFrom()).start();            
+        }
+    }
+
+    /*
+     * Factory method that gets an instance of the StorageService
+     * class.
+    */
+    public static StorageService instance()
+    {
+        if ( instance_ == null )
+        {
+            StorageService.createLock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                {
+                    try
+                    {
+                        instance_ = new StorageService();
+                    }
+                    catch ( Throwable th )
+                    {
+                        logger_.error(LogUtil.throwableToString(th));
+                        System.exit(1);
+                    }
+                }
+            }
+            finally
+            {
+                createLock_.unlock();
+            }
+        }
+        return instance_;
+    }
+
+    /*
+     * This is the endpoint snitch which depends on the network architecture. We
+     * need to keep this information for each endpoint so that we make decisions
+     * while doing things like replication etc.
+     *
+     */
+    private IEndPointSnitch endPointSnitch_;
+
+    /* This abstraction maintains the token/endpoint metadata information */
+    private TokenMetadata tokenMetadata_ = new TokenMetadata();
+    private DBManager.StorageMetadata storageMetadata_;
+
+    /*
+     * Maintains a list of all components that need to be shutdown
+     * for a clean exit.
+    */
+    private Set<IComponentShutdown> components_ = new HashSet<IComponentShutdown>();
+    /* Timer is used to disseminate load information */
+    private Timer loadTimer_ = new Timer(false);
+
+    /*
+     * This variable indicates if the local storage instance
+     * has been shutdown.
+    */
+    private AtomicBoolean isShutdown_ = new AtomicBoolean(false);
+
+    /* This thread pool is used to do the bootstrap for a new node */
+    private ExecutorService bootStrapper_ = new DebuggableThreadPoolExecutor(1, 1,
+            Integer.MAX_VALUE, TimeUnit.SECONDS,
+            new LinkedBlockingQueue<Runnable>(), new ThreadFactoryImpl(
+                    "BOOT-STRAPPER"));
+    
+    /* This thread pool does consistency checks when the client doesn't care about consistency */
+    private ExecutorService consistencyManager_;
+
+    /* This is the entity that tracks load information of all nodes in the cluster */
+    private StorageLoadBalancer storageLoadBalancer_;
+    /* We use this interface to determine where replicas need to be placed */
+    private IReplicaPlacementStrategy nodePicker_;
+    /* Handle to a ZooKeeper instance */
+    private ZooKeeper zk_;
+    
+    /*
+     * Registers with Management Server
+     */
+    private void init()
+    {
+        // Register this instance with JMX
+        try
+        {
+            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
+            mbs.registerMBean(this, new ObjectName(
+                    "org.apache.cassandra.service:type=StorageService"));
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+        }
+    }
+
+    public StorageService()
+    {
+        init();
+        storageLoadBalancer_ = new StorageLoadBalancer(this);
+        endPointSnitch_ = new EndPointSnitch();
+        
+        /* register the verb handlers */
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.tokenVerbHandler_, new TokenUpdateVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.binaryVerbHandler_, new BinaryVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.loadVerbHandler_, new LoadVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.mutationVerbHandler_, new RowMutationVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.readRepairVerbHandler_, new ReadRepairVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.readVerbHandler_, new ReadVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bootStrapInitiateVerbHandler_, new Table.BootStrapInitiateVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bootStrapInitiateDoneVerbHandler_, new StorageService.BootstrapInitiateDoneVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bootStrapTerminateVerbHandler_, new StreamManager.BootstrapTerminateVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(HttpConnection.httpRequestVerbHandler_, new HttpRequestVerbHandler(this) );
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.dataFileVerbHandler_, new DataFileVerbHandler() );
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.mbrshipCleanerVerbHandler_, new MembershipCleanerVerbHandler() );
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bsMetadataVerbHandler_, new BootstrapMetadataVerbHandler() );        
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.calloutDeployVerbHandler_, new CalloutDeployVerbHandler() );
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.touchVerbHandler_, new TouchVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.rangeVerbHandler_, new RangeVerbHandler());
+        
+        /* register the stage for the mutations */
+        int threadCount = DatabaseDescriptor.getThreadsPerPool();
+        consistencyManager_ = new DebuggableThreadPoolExecutor(threadCount,
+        		threadCount,
+                Integer.MAX_VALUE, TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(), new ThreadFactoryImpl(
+                        "CONSISTENCY-MANAGER"));
+        
+        StageManager.registerStage(StorageService.mutationStage_, new MultiThreadedStage(StorageService.mutationStage_, threadCount));
+        StageManager.registerStage(StorageService.readStage_, new MultiThreadedStage(StorageService.readStage_, 2*threadCount));        
+        StageManager.registerStage(StorageService.mrStage_, new MultiThreadedStage(StorageService.mrStage_, threadCount));
+        /* Stage for handling the HTTP messages. */
+        StageManager.registerStage(HttpConnection.httpStage_, new SingleThreadedStage("HTTP-REQUEST"));
+
+        if ( DatabaseDescriptor.isRackAware() )
+            nodePicker_ = new RackAwareStrategy(tokenMetadata_, partitioner_, DatabaseDescriptor.getReplicationFactor(), DatabaseDescriptor.getStoragePort());
+        else
+            nodePicker_ = new RackUnawareStrategy(tokenMetadata_, partitioner_, DatabaseDescriptor.getReplicationFactor(), DatabaseDescriptor.getStoragePort());
+    }
+    
+    private void reportToZookeeper() throws Throwable
+    {
+        try
+        {
+            zk_ = new ZooKeeper(DatabaseDescriptor.getZkAddress(), DatabaseDescriptor.getZkSessionTimeout(), new Watcher()
+                {
+                    public void process(WatchedEvent we)
+                    {                    
+                        String path = "/Cassandra/" + DatabaseDescriptor.getClusterName() + "/Leader";
+                        String eventPath = we.getPath();
+                        logger_.debug("PROCESS EVENT : " + eventPath);
+                        if (eventPath != null && (eventPath.contains(path)))
+                        {                                                           
+                            logger_.debug("Signalling the leader instance ...");
+                            LeaderElector.instance().signal();                                        
+                        }                                                  
+                    }
+                });
+            
+            Stat stat = zk_.exists("/", false);
+            if ( stat != null )
+            {
+                stat = zk_.exists("/Cassandra", false);
+                if ( stat == null )
+                {
+                    logger_.debug("Creating the Cassandra znode ...");
+                    zk_.create("/Cassandra", new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
+                }
+                
+                String path = "/Cassandra/" + DatabaseDescriptor.getClusterName();
+                stat = zk_.exists(path, false);
+                if ( stat == null )
+                {
+                    logger_.debug("Creating the cluster znode " + path);
+                    zk_.create(path, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
+                }
+                
+                /* Create the Leader, Locks and Misc znode */
+                stat = zk_.exists(path + "/Leader", false);
+                if ( stat == null )
+                {
+                    logger_.debug("Creating the leader znode " + path);
+                    zk_.create(path + "/Leader", new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
+                }
+                
+                stat = zk_.exists(path + "/Locks", false);
+                if ( stat == null )
+                {
+                    logger_.debug("Creating the locks znode " + path);
+                    zk_.create(path + "/Locks", new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
+                }
+                                
+                stat = zk_.exists(path + "/Misc", false);
+                if ( stat == null )
+                {
+                    logger_.debug("Creating the misc znode " + path);
+                    zk_.create(path + "/Misc", new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
+                }
+            }
+        }
+        catch ( KeeperException ke )
+        {
+            LogUtil.throwableToString(ke);
+            /* do the re-initialize again. */
+            reportToZookeeper();
+        }
+    }
+    
+    protected ZooKeeper getZooKeeperHandle()
+    {
+        return zk_;
+    }
+    
+    public boolean isLeader(EndPoint endpoint)
+    {
+        EndPoint leader = getLeader();
+        return leader.equals(endpoint);
+    }
+    
+    public EndPoint getLeader()
+    {
+        return LeaderElector.instance().getLeader();
+    }
+
+    public void registerComponentForShutdown(IComponentShutdown component)
+    {
+    	components_.add(component);
+    }
+
+    static
+    {
+        try
+        {
+            Class cls = Class.forName(DatabaseDescriptor.getPartitionerClass());
+            partitioner_ = (IPartitioner) cls.getConstructor().newInstance();
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+    
+    public void start() throws IOException
+    {
+        storageMetadata_ = DBManager.instance().start();
+        tcpAddr_ = new EndPoint(DatabaseDescriptor.getStoragePort());
+        udpAddr_ = new EndPoint(DatabaseDescriptor.getControlPort());
+        /* Listen for application messages */
+        MessagingService.getMessagingInstance().listen(tcpAddr_, false);
+        /* Listen for control messages */
+        MessagingService.getMessagingInstance().listenUDP(udpAddr_);
+        /* Listen for HTTP messages */
+        MessagingService.getMessagingInstance().listen( new EndPoint(DatabaseDescriptor.getHttpPort() ), true );
+
+        SelectorManager.getSelectorManager().start();
+        SelectorManager.getUdpSelectorManager().start();
+
+        /* start the analytics context package */
+        AnalyticsContext.instance().start();
+        /* starts a load timer thread */
+        loadTimer_.schedule( new LoadDisseminator(), StorageService.threshold_, StorageService.threshold_);
+        
+        /* report our existence to ZooKeeper instance and start the leader election service */
+        
+        //reportToZookeeper(); 
+        /* start the leader election algorithm */
+        //LeaderElector.instance().start();
+        /* start the map reduce framework */
+        //startMapReduceFramework();
+        
+        /* Start the storage load balancer */
+        storageLoadBalancer_.start();
+        /* Register with the Gossiper for EndPointState notifications */
+        Gossiper.instance().register(this);
+        /*
+         * Start the gossiper with the generation # retrieved from the System
+         * table
+         */
+        Gossiper.instance().start(udpAddr_, storageMetadata_.getGeneration());
+        /* Make sure this token gets gossiped around. */
+        tokenMetadata_.update(storageMetadata_.getStorageId(), StorageService.tcpAddr_);
+        Gossiper.instance().addApplicationState(StorageService.nodeId_, new ApplicationState(storageMetadata_.getStorageId().toString()));
+    }
+
+    public void killMe() throws Throwable
+    {
+        isShutdown_.set(true);
+        /* 
+         * Shutdown the Gossiper to stop responding/sending Gossip messages.
+         * This causes other nodes to detect you as dead and starting hinting
+         * data for the local endpoint. 
+        */
+        Gossiper.instance().shutdown();
+        final long nodeDeadDetectionTime = 25000L;
+        Thread.sleep(nodeDeadDetectionTime);
+        /* Now perform a force flush of the table */
+        String table = DatabaseDescriptor.getTables().get(0);
+        Table.open(table).flush(false);
+        /* Now wait for the flush to complete */
+        Thread.sleep(nodeDeadDetectionTime);
+        /* Shutdown all other components */
+        StorageService.instance().shutdown();
+    }
+
+    public boolean isShutdown()
+    {
+    	return isShutdown_.get();
+    }
+
+    public void shutdown()
+    {
+        bootStrapper_.shutdownNow();
+        /* shut down all stages */
+        StageManager.shutdown();
+        /* shut down the messaging service */
+        MessagingService.shutdown();
+        /* shut down all memtables */
+        Memtable.shutdown();
+        /* shut down the load disseminator */
+        loadTimer_.cancel();
+        /* shut down the cleaner thread in FileUtils */
+        FileUtils.shutdown();
+
+        /* shut down all registered components */
+        for ( IComponentShutdown component : components_ )
+        {
+        	component.shutdown();
+        }
+    }
+
+    public TokenMetadata getTokenMetadata()
+    {
+        return tokenMetadata_.cloneMe();
+    }
+
+    /* TODO: remove later */
+    public void updateTokenMetadata(Token token, EndPoint endpoint)
+    {
+        tokenMetadata_.update(token, endpoint);
+    }
+
+    public IEndPointSnitch getEndPointSnitch()
+    {
+    	return endPointSnitch_;
+    }
+    
+    /*
+     * Given an EndPoint this method will report if the
+     * endpoint is in the same data center as the local
+     * storage endpoint.
+    */
+    public boolean isInSameDataCenter(EndPoint endpoint) throws IOException
+    {
+        return endPointSnitch_.isInSameDataCenter(StorageService.tcpAddr_, endpoint);
+    }
+    
+    /*
+     * This method performs the requisite operations to make
+     * sure that the N replicas are in sync. We do this in the
+     * background when we do not care much about consistency.
+     */
+    public void doConsistencyCheck(Row row, List<EndPoint> endpoints, ReadCommand message)
+    {
+        Runnable consistencySentinel = new ConsistencyManager(row.cloneMe(), endpoints, message);
+        consistencyManager_.submit(consistencySentinel);
+    }
+
+    public Map<Range, List<EndPoint>> getRangeToEndPointMap()
+    {
+        /* Get the token to endpoint map. */
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        /* All the ranges for the tokens */
+        Range[] ranges = getAllRanges(tokenToEndPointMap.keySet());
+        return constructRangeToEndPointMap(ranges);
+    }
+
+    /**
+     * Construct the range to endpoint mapping based on the true view 
+     * of the world. 
+     * @param ranges
+     * @return mapping of ranges to the replicas responsible for them.
+    */
+    public Map<Range, List<EndPoint>> constructRangeToEndPointMap(Range[] ranges)
+    {
+        logger_.debug("Constructing range to endpoint map ...");
+        Map<Range, List<EndPoint>> rangeToEndPointMap = new HashMap<Range, List<EndPoint>>();
+        for ( Range range : ranges )
+        {
+            EndPoint[] endpoints = getNStorageEndPoint(range.right());
+            rangeToEndPointMap.put(range, new ArrayList<EndPoint>( Arrays.asList(endpoints) ) );
+        }
+        logger_.debug("Done constructing range to endpoint map ...");
+        return rangeToEndPointMap;
+    }
+    
+    /**
+     * Construct the range to endpoint mapping based on the view as dictated
+     * by the mapping of token to endpoints passed in. 
+     * @param ranges
+     * @param tokenToEndPointMap mapping of token to endpoints.
+     * @return mapping of ranges to the replicas responsible for them.
+    */
+    public Map<Range, List<EndPoint>> constructRangeToEndPointMap(Range[] ranges, Map<Token, EndPoint> tokenToEndPointMap)
+    {
+        logger_.debug("Constructing range to endpoint map ...");
+        Map<Range, List<EndPoint>> rangeToEndPointMap = new HashMap<Range, List<EndPoint>>();
+        for ( Range range : ranges )
+        {
+            EndPoint[] endpoints = getNStorageEndPoint(range.right(), tokenToEndPointMap);
+            rangeToEndPointMap.put(range, new ArrayList<EndPoint>( Arrays.asList(endpoints) ) );
+        }
+        logger_.debug("Done constructing range to endpoint map ...");
+        return rangeToEndPointMap;
+    }
+    
+    /**
+     * Construct a mapping from endpoint to ranges that endpoint is
+     * responsible for.
+     * @return the mapping from endpoint to the ranges it is responsible
+     * for.
+     */
+    public Map<EndPoint, List<Range>> constructEndPointToRangesMap()
+    {
+        Map<EndPoint, List<Range>> endPointToRangesMap = new HashMap<EndPoint, List<Range>>();
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        Collection<EndPoint> mbrs = tokenToEndPointMap.values();
+        for ( EndPoint mbr : mbrs )
+        {
+            endPointToRangesMap.put(mbr, getRangesForEndPoint(mbr));
+        }
+        return endPointToRangesMap;
+    }
+
+    /**
+     *  Called when there is a change in application state. In particular
+     *  we are interested in new tokens as a result of a new node or an
+     *  existing node moving to a new location on the ring.
+    */
+    public void onChange(EndPoint endpoint, EndPointState epState)
+    {
+        EndPoint ep = new EndPoint(endpoint.getHost(), DatabaseDescriptor.getStoragePort());
+        /* node identifier for this endpoint on the identifier space */
+        ApplicationState nodeIdState = epState.getApplicationState(StorageService.nodeId_);
+        if (nodeIdState != null)
+        {
+            Token newToken = getPartitioner().getTokenFactory().fromString(nodeIdState.getState());
+            logger_.debug("CHANGE IN STATE FOR " + endpoint + " - has token " + nodeIdState.getState());
+            Token oldToken = tokenMetadata_.getToken(ep);
+
+            if ( oldToken != null )
+            {
+                /*
+                 * If oldToken equals the newToken then the node had crashed
+                 * and is coming back up again. If oldToken is not equal to
+                 * the newToken this means that the node is being relocated
+                 * to another position in the ring.
+                */
+                if ( !oldToken.equals(newToken) )
+                {
+                    logger_.debug("Relocation for endpoint " + ep);
+                    tokenMetadata_.update(newToken, ep);                    
+                }
+                else
+                {
+                    /*
+                     * This means the node crashed and is coming back up.
+                     * Deliver the hints that we have for this endpoint.
+                    */
+                    logger_.debug("Sending hinted data to " + ep);
+                    doBootstrap(endpoint, BootstrapMode.HINT);
+                }
+            }
+            else
+            {
+                /*
+                 * This is a new node and we just update the token map.
+                */
+                tokenMetadata_.update(newToken, ep);
+            }
+        }
+        else
+        {
+            /*
+             * If we are here and if this node is UP and already has an entry
+             * in the token map. It means that the node was behind a network partition.
+            */
+            if ( epState.isAlive() && tokenMetadata_.isKnownEndPoint(endpoint) )
+            {
+                logger_.debug("EndPoint " + ep + " just recovered from a partition. Sending hinted data.");
+                doBootstrap(ep, BootstrapMode.HINT);
+            }
+        }
+
+        /* Check if a bootstrap is in order */
+        ApplicationState loadAllState = epState.getApplicationState(StorageService.loadAll_);
+        if ( loadAllState != null )
+        {
+            String nodes = loadAllState.getState();
+            if ( nodes != null )
+            {
+                doBootstrap(ep, BootstrapMode.FULL);
+            }
+        }
+    }
+
+    /**
+     * Get the count of primary keys from the sampler.
+    */
+    public String getLoadInfo()
+    {
+        long diskSpace = FileUtils.getUsedDiskSpace();
+    	return FileUtils.stringifyFileSize(diskSpace);
+    }
+
+    /**
+     * Get the primary count info for this endpoint.
+     * This is gossiped around and cached in the
+     * StorageLoadBalancer.
+    */
+    public String getLoadInfo(EndPoint ep)
+    {
+        LoadInfo li = storageLoadBalancer_.getLoad(ep);
+        return ( li == null ) ? "N/A" : li.toString();
+    }
+
+    /*
+     * This method updates the token on disk and modifies the cached
+     * StorageMetadata instance. This is only for the local endpoint.
+    */
+    public void updateToken(Token token) throws IOException
+    {
+        /* update the token on disk */
+        SystemTable.openSystemTable(SystemTable.name_).updateToken(token);
+        /* Update the storageMetadata cache */
+        storageMetadata_.setStorageId(token);
+        /* Update the token maps */
+        /* Get the old token. This needs to be removed. */
+        tokenMetadata_.update(token, StorageService.tcpAddr_);
+        /* Gossip this new token for the local storage instance */
+        Gossiper.instance().addApplicationState(StorageService.nodeId_, new ApplicationState(token.toString()));
+    }
+    
+    /*
+     * This method removes the state associated with this endpoint
+     * from the TokenMetadata instance.
+     * 
+     *  param@ endpoint remove the token state associated with this 
+     *         endpoint.
+     */
+    public void removeTokenState(EndPoint endpoint) 
+    {
+        tokenMetadata_.remove(endpoint);
+        /* Remove the state from the Gossiper */
+        Gossiper.instance().removeFromMembership(endpoint);
+    }
+    
+    /*
+     * This method is invoked by the Loader process to force the
+     * node to move from its current position on the token ring, to
+     * a position to be determined based on the keys. This will help
+     * all nodes to start off perfectly load balanced. The array passed
+     * in is evaluated as follows by the loader process:
+     * If there are 10 keys in the system and a totality of 5 nodes
+     * then each node needs to have 2 keys i.e the array is made up
+     * of every 2nd key in the total list of keys.
+    */
+    public void relocate(String[] keys) throws IOException
+    {
+    	if ( keys.length > 0 )
+    	{
+            Token token = tokenMetadata_.getToken(StorageService.tcpAddr_);
+	        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+	        Token[] tokens = tokenToEndPointMap.keySet().toArray(new Token[tokenToEndPointMap.keySet().size()]);
+	        Arrays.sort(tokens);
+	        int index = Arrays.binarySearch(tokens, token) * (keys.length/tokens.length);
+            Token newToken = partitioner_.getInitialToken(keys[index]);
+	        /* update the token */
+	        updateToken(newToken);
+    	}
+    }
+    
+    /**
+     * This method takes a colon separated string of nodes that need
+     * to be bootstrapped. It is also used to filter some source of 
+     * data. Suppose the nodes to be bootstrapped are A, B and C. Then
+     * <i>allNodes</i> must be specified as A:B:C.
+     * 
+    */
+    private void doBootstrap(String nodes)
+    {
+        String[] allNodesAndFilter = nodes.split("-");
+        String nodesToLoad;
+        String filterSources = null;
+        
+        if ( allNodesAndFilter.length == 2 )
+        {
+            nodesToLoad = allNodesAndFilter[0];
+            filterSources = allNodesAndFilter[1];
+        }
+        else
+        {
+            nodesToLoad = allNodesAndFilter[0];
+        }        
+        String[] allNodes = nodesToLoad.split(":");
+        EndPoint[] endpoints = new EndPoint[allNodes.length];
+        Token[] tokens = new Token[allNodes.length];
+        
+        for ( int i = 0; i < allNodes.length; ++i )
+        {
+            endpoints[i] = new EndPoint( allNodes[i].trim(), DatabaseDescriptor.getStoragePort() );
+            tokens[i] = tokenMetadata_.getToken(endpoints[i]);
+        }
+        
+        /* Start the bootstrap algorithm */
+        if ( filterSources == null )
+        bootStrapper_.submit( new BootStrapper(endpoints, tokens) );
+        else
+        {
+            String[] allFilters = filterSources.split(":");
+            EndPoint[] filters = new EndPoint[allFilters.length];
+            for ( int i = 0; i < allFilters.length; ++i )
+            {
+                filters[i] = new EndPoint( allFilters[i].trim(), DatabaseDescriptor.getStoragePort() );
+            }
+            bootStrapper_.submit( new BootStrapper(endpoints, tokens, filters) );
+        }
+    }
+
+    /**
+     * Starts the bootstrap operations for the specified endpoint.
+     * The name of this method is however a misnomer since it does
+     * handoff of data to the specified node when it has crashed
+     * and come back up, marked as alive after a network partition
+     * and also when it joins the ring either as an old node being
+     * relocated or as a brand new node.
+    */
+    public final void doBootstrap(EndPoint endpoint, BootstrapMode mode)
+    {
+        switch ( mode )
+        {
+            case FULL:
+                Token token = tokenMetadata_.getToken(endpoint);
+                bootStrapper_.submit(new BootStrapper(new EndPoint[]{endpoint}, token));
+                break;
+
+            case HINT:
+                /* Deliver the hinted data to this endpoint. */
+                HintedHandOffManager.instance().deliverHints(endpoint);
+                break;
+
+            default:
+                break;
+        }
+    }
+
+    /* This methods belong to the MBean interface */
+    
+    public String getToken(EndPoint ep)
+    {
+        // render a String representation of the Token corresponding to this endpoint
+        // for a human-facing UI.  If there is no such Token then we use "" since
+        // it is not a valid value either for BigIntegerToken or StringToken.
+        EndPoint ep2 = new EndPoint(ep.getHost(), DatabaseDescriptor.getStoragePort());
+        Token token = tokenMetadata_.getToken(ep2);
+        // if there is no token for an endpoint, return an empty string to denote that
+        return ( token == null ) ? "" : token.toString();
+    }
+
+    public String getToken()
+    {
+        return tokenMetadata_.getToken(StorageService.tcpAddr_).toString();
+    }
+
+    public String getLiveNodes()
+    {
+        return stringify(Gossiper.instance().getLiveMembers());
+    }
+
+    public String getUnreachableNodes()
+    {
+        return stringify(Gossiper.instance().getUnreachableMembers());
+    }
+
+    /* Helper for the MBean interface */
+    private String stringify(Set<EndPoint> eps)
+    {
+        StringBuilder sb = new StringBuilder("");
+        for (EndPoint ep : eps)
+        {
+            sb.append(ep);
+            sb.append(" ");
+        }
+        return sb.toString();
+    }
+
+    public void loadAll(String nodes)
+    {        
+        doBootstrap(nodes);
+    }
+    
+    public void doGC()
+    {
+        List<String> tables = DatabaseDescriptor.getTables();
+        for ( String tName : tables )
+        {
+            Table table = Table.open(tName);
+            table.doGC();
+        }
+    }
+    
+    public void forceHandoff(String directories, String host) throws IOException
+    {       
+        List<File> filesList = new ArrayList<File>();
+        String[] sources = directories.split(":");
+        for (String source : sources)
+        {
+            File directory = new File(source);
+            Collections.addAll(filesList, directory.listFiles());            
+        }
+        
+        File[] files = filesList.toArray(new File[0]);
+        StreamContextManager.StreamContext[] streamContexts = new StreamContextManager.StreamContext[files.length];
+        int i = 0;
+        for ( File file : files )
+        {
+            streamContexts[i] = new StreamContextManager.StreamContext(file.getAbsolutePath(), file.length());
+            logger_.debug("Stream context metadata " + streamContexts[i]);
+            ++i;
+        }
+        
+        if ( files.length > 0 )
+    {
+            EndPoint target = new EndPoint(host, DatabaseDescriptor.getStoragePort());
+            /* Set up the stream manager with the files that need to streamed */
+            StreamManager.instance(target).addFilesToStream(streamContexts);
+            /* Send the bootstrap initiate message */
+            BootstrapInitiateMessage biMessage = new BootstrapInitiateMessage(streamContexts);
+            Message message = BootstrapInitiateMessage.makeBootstrapInitiateMessage(biMessage);
+            logger_.debug("Sending a bootstrap initiate message to " + target + " ...");
+            MessagingService.getMessagingInstance().sendOneWay(message, target);                
+            logger_.debug("Waiting for transfer to " + target + " to complete");
+            StreamManager.instance(target).waitForStreamCompletion();
+            logger_.debug("Done with transfer to " + target);  
+        }
+    }
+
+    /* End of MBean interface methods */
+    
+    /**
+     * This method returns the predecessor of the endpoint ep on the identifier
+     * space.
+     */
+    EndPoint getPredecessor(EndPoint ep)
+    {
+        Token token = tokenMetadata_.getToken(ep);
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        int index = Collections.binarySearch(tokens, token);
+        return (index == 0) ? tokenToEndPointMap.get(tokens
+                .get(tokens.size() - 1)) : tokenToEndPointMap.get(tokens
+                .get(--index));
+    }
+
+    /*
+     * This method returns the successor of the endpoint ep on the identifier
+     * space.
+     */
+    public EndPoint getSuccessor(EndPoint ep)
+    {
+        Token token = tokenMetadata_.getToken(ep);
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        int index = Collections.binarySearch(tokens, token);
+        return (index == (tokens.size() - 1)) ? tokenToEndPointMap
+                .get(tokens.get(0))
+                : tokenToEndPointMap.get(tokens.get(++index));
+    }
+
+    /**
+     * Get the primary range for the specified endpoint.
+     * @param ep endpoint we are interested in.
+     * @return range for the specified endpoint.
+     */
+    public Range getPrimaryRangeForEndPoint(EndPoint ep)
+    {
+        Token right = tokenMetadata_.getToken(ep);
+        EndPoint predecessor = getPredecessor(ep);
+        Token left = tokenMetadata_.getToken(predecessor);
+        return new Range(left, right);
+    }
+    
+    /**
+     * Get all ranges an endpoint is responsible for.
+     * @param ep endpoint we are interested in.
+     * @return ranges for the specified endpoint.
+     */
+    List<Range> getRangesForEndPoint(EndPoint ep)
+    {
+        List<Range> ranges = new ArrayList<Range>();
+        ranges.add( getPrimaryRangeForEndPoint(ep) );
+        
+        EndPoint predecessor = ep;
+        int count = DatabaseDescriptor.getReplicationFactor() - 1;
+        for ( int i = 0; i < count; ++i )
+        {
+            predecessor = getPredecessor(predecessor);
+            ranges.add( getPrimaryRangeForEndPoint(predecessor) );
+        }
+        
+        return ranges;
+    }
+    
+    /**
+     * Get all ranges that span the ring as per
+     * current snapshot of the token distribution.
+     * @return all ranges in sorted order.
+     */
+    public Range[] getAllRanges()
+    {
+        return getAllRanges(tokenMetadata_.cloneTokenEndPointMap().keySet());
+    }
+    
+    /**
+     * Get all ranges that span the ring given a set
+     * of tokens. All ranges are in sorted order of 
+     * ranges.
+     * @return ranges in sorted order
+    */
+    public Range[] getAllRanges(Set<Token> tokens)
+    {
+        List<Range> ranges = new ArrayList<Range>();
+        List<Token> allTokens = new ArrayList<Token>(tokens);
+        Collections.sort(allTokens);
+        int size = allTokens.size();
+        for ( int i = 1; i < size; ++i )
+        {
+            Range range = new Range( allTokens.get(i - 1), allTokens.get(i) );
+            ranges.add(range);
+        }
+        Range range = new Range( allTokens.get(size - 1), allTokens.get(0) );
+        ranges.add(range);
+        return ranges.toArray( new Range[0] );
+    }
+
+    /**
+     * This method returns the endpoint that is responsible for storing the
+     * specified key.
+     *
+     * param @ key - key for which we need to find the endpoint
+     * return value - the endpoint responsible for this key
+     */
+    public EndPoint getPrimary(String key)
+    {
+        EndPoint endpoint = StorageService.tcpAddr_;
+        Token token = partitioner_.getInitialToken(key);
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
+        if (tokens.size() > 0)
+        {
+            Collections.sort(tokens);
+            int index = Collections.binarySearch(tokens, token);
+            if (index >= 0)
+            {
+                /*
+                 * retrieve the endpoint based on the token at this index in the
+                 * tokens list
+                 */
+                endpoint = tokenToEndPointMap.get(tokens.get(index));
+            }
+            else
+            {
+                index = (index + 1) * (-1);
+                if (index < tokens.size())
+                    endpoint = tokenToEndPointMap.get(tokens.get(index));
+                else
+                    endpoint = tokenToEndPointMap.get(tokens.get(0));
+            }
+        }
+        return endpoint;
+    }
+
+    /**
+     * This method determines whether the local endpoint is the
+     * primary for the given key.
+     * @param key
+     * @return true if the local endpoint is the primary replica.
+    */
+    public boolean isPrimary(String key)
+    {
+        EndPoint endpoint = getPrimary(key);
+        return StorageService.tcpAddr_.equals(endpoint);
+    }
+
+    /**
+     * This method returns the N endpoints that are responsible for storing the
+     * specified key i.e for replication.
+     *
+     * param @ key - key for which we need to find the endpoint return value -
+     * the endpoint responsible for this key
+     */
+    public EndPoint[] getNStorageEndPoint(String key)
+    {
+        return nodePicker_.getStorageEndPoints(partitioner_.getInitialToken(key));
+    }
+    
+    private Map<String, EndPoint[]> getNStorageEndPoints(String[] keys)
+    {
+    	return nodePicker_.getStorageEndPoints(keys);
+    }
+    
+    
+    /**
+     * This method attempts to return N endpoints that are responsible for storing the
+     * specified key i.e for replication.
+     *
+     * param @ key - key for which we need to find the endpoint return value -
+     * the endpoint responsible for this key
+     */
+    public List<EndPoint> getNLiveStorageEndPoint(String key)
+    {
+    	List<EndPoint> liveEps = new ArrayList<EndPoint>();
+    	EndPoint[] endpoints = getNStorageEndPoint(key);
+    	
+    	for ( EndPoint endpoint : endpoints )
+    	{
+    		if ( FailureDetector.instance().isAlive(endpoint) )
+    			liveEps.add(endpoint);
+    	}
+    	
+    	return liveEps;
+    }
+
+    /**
+     * This method returns the N endpoints that are responsible for storing the
+     * specified key i.e for replication.
+     *
+     * param @ key - key for which we need to find the endpoint return value -
+     * the endpoint responsible for this key
+     */
+    public Map<EndPoint, EndPoint> getNStorageEndPointMap(String key)
+    {
+        return nodePicker_.getHintedStorageEndPoints(partitioner_.getInitialToken(key));
+    }
+
+    /**
+     * This method returns the N endpoints that are responsible for storing the
+     * specified token i.e for replication.
+     *
+     * param @ token - position on the ring
+     */
+    public EndPoint[] getNStorageEndPoint(Token token)
+    {
+        return nodePicker_.getStorageEndPoints(token);
+    }
+    
+    /**
+     * This method returns the N endpoints that are responsible for storing the
+     * specified token i.e for replication and are based on the token to endpoint 
+     * mapping that is passed in.
+     *
+     * param @ token - position on the ring
+     * param @ tokens - w/o the following tokens in the token list
+     */
+    protected EndPoint[] getNStorageEndPoint(Token token, Map<Token, EndPoint> tokenToEndPointMap)
+    {
+        return nodePicker_.getStorageEndPoints(token, tokenToEndPointMap);
+    }
+
+    /**
+     * This function finds the most suitable endpoint given a key.
+     * It checks for loclity and alive test.
+     */
+	public EndPoint findSuitableEndPoint(String key) throws IOException
+	{
+		EndPoint[] endpoints = getNStorageEndPoint(key);
+		for(EndPoint endPoint: endpoints)
+		{
+			if(endPoint.equals(StorageService.getLocalStorageEndPoint()))
+			{
+				return endPoint;
+			}
+		}
+		int j = 0;
+		for ( ; j < endpoints.length; ++j )
+		{
+			if ( StorageService.instance().isInSameDataCenter(endpoints[j]) && FailureDetector.instance().isAlive(endpoints[j]) )
+			{
+				logger_.debug("EndPoint " + endpoints[j] + " is in the same data center as local storage endpoint.");
+				return endpoints[j];
+			}
+		}
+		// We have tried to be really nice but looks like theer are no servers 
+		// in the local data center that are alive and can service this request so 
+		// just send it to teh first alive guy and see if we get anything.
+		j = 0;
+		for ( ; j < endpoints.length; ++j )
+		{
+			if ( FailureDetector.instance().isAlive(endpoints[j]) )
+			{
+				logger_.debug("EndPoint " + endpoints[j] + " is alive so get data from it.");
+				return endpoints[j];
+			}
+		}
+		return null;
+	}
+	
+	public Map<String, EndPoint> findSuitableEndPoints(String[] keys) throws IOException
+	{
+		Map<String, EndPoint> suitableEndPoints = new HashMap<String, EndPoint>();
+		Map<String, EndPoint[]> results = getNStorageEndPoints(keys);
+		for ( String key : keys )
+		{
+			EndPoint[] endpoints = results.get(key);
+			/* indicates if we have to move on to the next key */
+			boolean moveOn = false;
+			for(EndPoint endPoint: endpoints)
+			{
+				if(endPoint.equals(StorageService.getLocalStorageEndPoint()))
+				{
+					suitableEndPoints.put(key, endPoint);
+					moveOn = true;
+					break;
+				}
+			}
+			
+			if ( moveOn )
+				continue;
+				
+			int j = 0;
+			for ( ; j < endpoints.length; ++j )
+			{
+				if ( StorageService.instance().isInSameDataCenter(endpoints[j]) && FailureDetector.instance().isAlive(endpoints[j]) )
+				{
+					logger_.debug("EndPoint " + endpoints[j] + " is in the same data center as local storage endpoint.");
+					suitableEndPoints.put(key, endpoints[j]);
+					moveOn = true;
+					break;
+				}
+			}
+			
+			if ( moveOn )
+				continue;
+			
+			// We have tried to be really nice but looks like theer are no servers 
+			// in the local data center that are alive and can service this request so 
+			// just send it to the first alive guy and see if we get anything.
+			j = 0;
+			for ( ; j < endpoints.length; ++j )
+			{
+				if ( FailureDetector.instance().isAlive(endpoints[j]) )
+				{
+					logger_.debug("EndPoint " + endpoints[j] + " is alive so get data from it.");
+					suitableEndPoints.put(key, endpoints[j]);
+					break;
+				}
+			}
+		}
+		return suitableEndPoints;
+	}
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageServiceMBean.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageServiceMBean.java
index e69de29b..a42c6dd8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageServiceMBean.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageServiceMBean.java
@@ -0,0 +1,61 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface StorageServiceMBean
+{    
+    public String getLiveNodes();
+    public String getUnreachableNodes();
+    public String getToken();
+    
+    /**
+     * This method will cause the local node initiate
+     * the bootstrap process for all the nodes specified
+     * in the string parameter passed in. This local node
+     * will calculate who gives what ranges to the nodes
+     * and then instructs the nodes to do so.
+     * 
+     * @param nodes colon delimited list of endpoints that need
+     *              to be bootstrapped
+    */
+    public void loadAll(String nodes);
+    
+    /**
+     * 
+     */
+    public void doGC();
+
+    /**
+     * Stream the files in the bootstrap directory over to the
+     * node being bootstrapped. This is used in case of normal
+     * bootstrap failure. Use a tool to re-calculate the cardinality
+     * at a later point at the destination.
+     * @param sources colon separated list of directories from where 
+     *                files need to be picked up.
+     * @param target endpoint receiving data.
+    */
+    public void forceHandoff(String directories, String target) throws IOException;
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StreamManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StreamManager.java
index e69de29b..0dd4081d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StreamManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StreamManager.java
@@ -0,0 +1,159 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.service.StorageService.BootstrapInitiateDoneVerbHandler;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/*
+ * This class manages the streaming of multiple files 
+ * one after the other. 
+*/
+public final class StreamManager
+{   
+    private static Logger logger_ = Logger.getLogger( StreamManager.class );
+    
+    public static class BootstrapTerminateVerbHandler implements IVerbHandler
+    {
+        private static Logger logger_ = Logger.getLogger( BootstrapInitiateDoneVerbHandler.class );
+
+        public void doVerb(Message message)
+        {
+            byte[] body = (byte[])message.getMessageBody()[0];
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+
+            try
+            {
+                StreamContextManager.StreamStatusMessage streamStatusMessage = StreamContextManager.StreamStatusMessage.serializer().deserialize(bufIn);
+                StreamContextManager.StreamStatus streamStatus = streamStatusMessage.getStreamStatus();
+                                               
+                switch( streamStatus.getAction() )
+                {
+                    case DELETE:                              
+                        StreamManager.instance(message.getFrom()).finish(streamStatus.getFile());
+                        break;
+
+                    case STREAM:
+                        logger_.debug("Need to re-stream file " + streamStatus.getFile());
+                        StreamManager.instance(message.getFrom()).repeat();
+                        break;
+
+                    default:
+                        break;
+                }
+            }
+            catch ( IOException ex )
+            {
+                logger_.info(LogUtil.throwableToString(ex));
+            }
+        }
+    }
+    
+    private static Map<EndPoint, StreamManager> streamManagers_ = new HashMap<EndPoint, StreamManager>();
+    
+    public static StreamManager instance(EndPoint to)
+    {
+        StreamManager streamManager = streamManagers_.get(to);
+        if ( streamManager == null )
+        {
+            streamManager = new StreamManager(to);
+            streamManagers_.put(to, streamManager);
+        }
+        return streamManager;
+    }
+    
+    private List<File> filesToStream_ = new ArrayList<File>();
+    private EndPoint to_;
+    private long totalBytesToStream_ = 0L;
+    
+    private StreamManager(EndPoint to)
+    {
+        to_ = to;
+    }
+    
+    public void addFilesToStream(StreamContextManager.StreamContext[] streamContexts)
+    {
+        for ( StreamContextManager.StreamContext streamContext : streamContexts )
+        {
+            logger_.debug("Adding file " + streamContext.getTargetFile() + " to be streamed.");
+            filesToStream_.add( new File( streamContext.getTargetFile() ) );
+            totalBytesToStream_ += streamContext.getExpectedBytes();
+        }
+    }
+    
+    void start()
+    {
+        if ( filesToStream_.size() > 0 )
+        {
+            File file = filesToStream_.get(0);
+            logger_.debug("Streaming file " + file + " ...");
+            MessagingService.getMessagingInstance().stream(file.getAbsolutePath(), 0L, file.length(), StorageService.getLocalStorageEndPoint(), to_);
+        }
+    }
+    
+    void repeat()
+    {
+        if ( filesToStream_.size() > 0 )
+            start();
+    }
+    
+    void finish(String file) throws IOException
+    {
+        File f = new File(file);
+        logger_.debug("Deleting file " + file + " after streaming " + f.length() + "/" + totalBytesToStream_ + " bytes.");
+        FileUtils.delete(file);
+        filesToStream_.remove(0);
+        if ( filesToStream_.size() > 0 )
+            start();
+        else
+        {
+            synchronized(this)
+            {
+                logger_.debug("Signalling that streaming is done for " + to_);
+                notifyAll();
+            }
+        }
+    }
+    
+    public synchronized void waitForStreamCompletion()
+    {
+        try
+        {
+            wait();
+        }
+        catch(InterruptedException ex)
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/TokenUpdateVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/TokenUpdateVerbHandler.java
index e69de29b..82e2ca63 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/TokenUpdateVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/TokenUpdateVerbHandler.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TokenUpdateVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(TokenUpdateVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+    	byte[] body = (byte[])message.getMessageBody()[0];
+        Token token = StorageService.getPartitioner().getTokenFactory().fromByteArray(body);
+        try
+        {
+        	logger_.info("Updating the token to [" + token + "]");
+        	StorageService.instance().updateToken(token);
+        }
+    	catch( IOException ex )
+    	{
+    		logger_.debug(LogUtil.throwableToString(ex));
+    	}
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/WriteResponseResolver.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/WriteResponseResolver.java
index e69de29b..bfe62d8d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/WriteResponseResolver.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/WriteResponseResolver.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.util.List;
+
+import org.apache.cassandra.db.WriteResponse;
+import org.apache.cassandra.net.Message;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class WriteResponseResolver implements IResponseResolver<Boolean> {
+
+	private static Logger logger_ = Logger.getLogger(WriteResponseResolver.class);
+
+	/*
+	 * The resolve function for the Write looks at all the responses if all the
+	 * respones returned are false then we have a problem since that means the
+	 * key wa not written to any of the servers we want to notify the client of
+	 * this so in that case we should return a false saying that the write
+	 * failed.
+	 * 
+	 */
+	public Boolean resolve(List<Message> responses) throws DigestMismatchException 
+	{
+		// TODO: We need to log error responses here for example
+		// if a write fails for a key log that the key could not be replicated
+		boolean returnValue = false;
+		for (Message response : responses) {
+			Object[] body = response.getMessageBody();
+			WriteResponse writeResponse = (WriteResponse) body[0];
+			boolean result = writeResponse.isSuccess();
+			if (!result) {
+				logger_.debug("Write at " + response.getFrom()
+						+ " may have failed for the key " + writeResponse.key());
+			}
+			returnValue |= result;
+		}
+		return returnValue;
+	}
+
+	public boolean isDataPresent(List<Message> responses)
+	{
+		return true;
+	}
+	
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ZookeeperWatcher.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ZookeeperWatcher.java
index e69de29b..b75387d3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ZookeeperWatcher.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/ZookeeperWatcher.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.log4j.Logger;
+import org.apache.zookeeper.WatchedEvent;
+import org.apache.zookeeper.Watcher;
+
+
+public class ZookeeperWatcher implements Watcher
+{
+    private static final Logger logger_ = Logger.getLogger(ZookeeperWatcher.class);
+    private static final String leader_ = "/Cassandra/" + DatabaseDescriptor.getClusterName() + "/Leader";
+    private static final String lock_ = "/Cassandra/" + DatabaseDescriptor.getClusterName() + "/Locks";
+    
+    public void process(WatchedEvent we)
+    {                            
+        String eventPath = we.getPath();
+        logger_.debug("PROCESS EVENT : " + eventPath);
+        if (eventPath != null && (eventPath.contains(leader_)))
+        {                                                           
+            logger_.debug("Signalling the leader instance ...");
+            LeaderElector.instance().signal();                                        
+        }
+        
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/DBTest.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/DBTest.java
index e69de29b..5949b5ed 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/DBTest.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/DBTest.java
@@ -0,0 +1,187 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.test;
+
+import java.io.FileInputStream;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.Scanner;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.IFileWriter;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.SequenceFile;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.LogUtil;
+
+
+public class DBTest
+{
+    private static void doWrites() throws Throwable
+    {         
+        for ( int i = 0; i < 512*1024; ++i )
+        {
+            String key = Integer.toString(i);
+            RowMutation rm = new RowMutation("Mailbox", key);
+            String value = "Data for key " + key;
+            rm.add("Test:" + "Column", value.getBytes(), i);                
+            rm.apply();
+        }
+        System.out.println("Write done");
+    }
+    
+    private static void doReads() throws Throwable
+    {
+        Table table = Table.open("Mailbox");
+        for ( int i = 100; i < 1000; ++i )
+        {        
+            String key = Integer.toString(i);
+            Row row = table.getRow(key, "Test");
+            System.out.println( row.getColumnFamily("Test") );
+            System.out.println("Row read done");            
+            ColumnFamily cf = table.get(key, "Test");                                  
+            if (cf == null)
+                System.out.println("KEY " + key + " is missing");
+            else
+            {
+                Collection<IColumn> superColumns = cf.getAllColumns();                
+                System.out.println("Success ...");
+            }
+        }
+        System.out.println("Read done ...");  
+    }
+    
+    private static void doRead(String key) throws Throwable
+    {
+        Table table = Table.open("Mailbox");
+        Row row = table.getRow(key, "Test");    
+        ColumnFamily cf = table.get(key, "Test");                                  
+        if (cf == null)
+            System.out.println("KEY " + key + " is missing");
+        else
+        {
+            Collection<IColumn> columns = cf.getAllColumns();                
+            for ( IColumn column : columns )
+            {
+                System.out.println(column.name());
+                System.out.println( new String( column.value() ) );
+            }
+        }
+        System.out.println("Read done ...");
+    }
+    
+    private static void doScannerTest() throws Throwable
+    {
+        Scanner scanner = new Scanner("Mailbox");
+        scanner.fetch(Integer.toString(105), "MailboxMailList0");
+        
+        while ( scanner.hasNext() )
+        {
+            System.out.println(scanner.next().name());
+        }             
+    }
+
+    public static void doTest()
+    {
+        String host = "insearch00";
+        String host2 = "insearch0";
+        Set<EndPoint> allNodes = new HashSet<EndPoint>();
+        for ( int i = 1; i <= 3; ++i )
+        {
+            if ( i < 10 )
+                allNodes.add( new EndPoint(host + i + ".sf2p.facebook.com", 7000) );
+            else
+                allNodes.add( new EndPoint(host2 + i + ".sf2p.facebook.com", 7000) );
+        }
+        
+        for ( int i = 1; i <= 2; ++i )
+        {
+            if ( i < 10 )
+                allNodes.add( new EndPoint(host + i + ".ash1.facebook.com", 7000) );
+            else
+                allNodes.add( new EndPoint(host2 + i + ".ash1.facebook.com", 7000) );
+        }
+        
+        TestChoice t = new TestChoice(allNodes);
+        t.assignReplicas();
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {
+        /*
+        SSTable ssTable = new SSTable("C:\\Engagements\\", "Sample-Bf");
+        BloomFilter bf = new BloomFilter(512*1024, 15);
+        for ( int i = 0; i < 512*1024; ++i )
+        {
+            bf.fill( Integer.toString(i) );
+        }        
+        ssTable.close(bf);
+        */
+        /*
+        IFileWriter writer = SequenceFile.bufferedWriter("C:\\Engagements\\Sample-Bf-Data.db", 4*1024*1024);
+        BloomFilter bf = new BloomFilter(512*1024, 15);
+        for ( int i = 0; i < 512*1024; ++i )
+        {
+            bf.fill( Integer.toString(i) );
+        }
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        BloomFilter.serializer().serialize(bf, bufOut);
+        bufOut.close();
+        writer.close(bufOut.getData(), bufOut.getLength());
+        writer.close();        
+        
+        IFileReader reader = SequenceFile.bufferedReader("C:\\Engagements\\Sample-Bf-Data.db", 4*1024*1024);
+        //DataOutputBuffer bufOut = new DataOutputBuffer();
+        bufOut.reset();
+        reader.next(bufOut);
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(bufOut.getData(), bufOut.getLength());
+        bufIn.readUTF();
+        bufIn.readInt();
+        BloomFilter bf2 = BloomFilter.serializer().deserialize(bufIn);
+        int count = 0;
+        for ( int i = 0; i < 512*1024; ++i )
+        {
+            if ( !bf2.isPresent(Integer.toString(i)) )
+                ++count;
+        }
+        System.out.println(count);
+        reader.close();
+        */
+        //LogUtil.init();
+        //StorageService.instance().start(); 
+        //doWrites();
+        //doRead("543");
+        
+        DBTest.doTest();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/DataImporter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/DataImporter.java
index e69de29b..b2673448 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/DataImporter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/DataImporter.java
@@ -0,0 +1,1535 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.test;
+
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.protocol.TBinaryProtocol;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.StringTokenizer;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.ThreadFactory;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.RowReadCommand;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.ReadResponse;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.RowMutationMessage;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IAsyncResult;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.Cassandra;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.service.batch_mutation_super_t;
+import org.apache.cassandra.service.batch_mutation_t;
+import org.apache.cassandra.service.column_t;
+import org.apache.cassandra.service.superColumn_t;
+
+import org.apache.log4j.Logger;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class DataImporter {
+	private static final String delimiter_ = new String(",");
+
+	private static Logger logger_ = Logger.getLogger(DataImporter.class);
+
+	private static final String tablename_ = new String("Mailbox");
+
+	public static EndPoint from_ = new EndPoint("172.21.211.181", 10001);
+
+	public static EndPoint to_ = new EndPoint("hadoop071.sf2p.facebook.com",
+			7000);
+
+	public static EndPoint[] tos_ = new EndPoint[]{ new EndPoint("hadoop038.sf2p.facebook.com",	7000),
+													new EndPoint("hadoop039.sf2p.facebook.com", 7000),
+													new EndPoint("hadoop040.sf2p.facebook.com", 7000),
+													new EndPoint("hadoop041.sf2p.facebook.com",	7000)
+												};
+	private static final String columnFamily_ = new String("MailboxUserList");
+
+	private Cassandra.Client peerstorageClient_ = null;
+
+	public void test(String line) throws IOException {
+		StringTokenizer st = new StringTokenizer(line, delimiter_);
+		StringBuilder sb = new StringBuilder("");
+		int i = 0;
+		String column = null;
+		int ts = 0;
+		String columnValue = null;
+
+		while (st.hasMoreElements()) {
+			switch (i) {
+			case 0:
+				sb.append((String) st.nextElement());
+				sb.append(":");
+				break;
+
+			case 1:
+				sb.append((String) st.nextElement());
+				break;
+
+			case 2:
+				column = (String) st.nextElement();
+				break;
+
+			case 3:
+				ts = Integer.parseInt((String) st.nextElement());
+				break;
+
+			case 4:
+				columnValue = (String) st.nextElement();
+				break;
+
+			default:
+				break;
+			}
+			++i;
+		}
+
+		String rowKey = sb.toString();
+		try {
+			long t = System.currentTimeMillis();
+			peerstorageClient_.insert(tablename_, rowKey, columnFamily_ + ":"
+					+ column, columnValue.getBytes(), ts);
+			logger_.debug("Time taken for thrift..."
+					+ (System.currentTimeMillis() - t));
+		} catch (Exception e) {
+			e.printStackTrace();
+		}
+		/* Added the thrift call to storage. */
+	}
+
+	private int roundRobin_ = 0 ;
+    private Random random_ = new Random();
+    
+	public void apply(batch_mutation_t batchMutation) {
+
+		columnFamilyHack_++;
+		try {
+			Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+			long t = System.currentTimeMillis();
+			peerstorageClient_.batch_insert(batchMutation);
+			logger_.debug("Time taken for thrift..."
+					+ (System.currentTimeMillis() - t));
+		} catch (Exception e) {
+			try {
+				peerstorageClient_ = connect();
+				peerstorageClient_.batch_insert(batchMutation);
+			} catch (Exception e1) {
+				e1.printStackTrace();
+			}
+		}
+	}
+
+	public void apply(batch_mutation_super_t batchMutation) {
+
+		columnFamilyHack_++;
+		try {
+			Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+			long t = System.currentTimeMillis();
+			peerstorageClient_.batch_insert_superColumn(batchMutation);
+			logger_.debug("Time taken for thrift..."
+					+ (System.currentTimeMillis() - t));
+		} catch (Exception e) {
+			try {
+				peerstorageClient_ = connect();
+				peerstorageClient_.batch_insert_superColumn(batchMutation);
+			} catch (Exception e1) {
+				e1.printStackTrace();
+			}
+		}
+	}
+	TTransport transport_ = null;
+	public Cassandra.Client connect() {
+//		String host = "hadoop034.sf2p.facebook.com";
+		String[] hosts = new String[] { "hadoop038.sf2p.facebook.com",
+				"hadoop039.sf2p.facebook.com",
+				"hadoop040.sf2p.facebook.com",
+				"hadoop041.sf2p.facebook.com"
+			  };
+		int port = 9160;
+            
+		//TNonBlockingSocket socket = new TNonBlockingSocket(hosts[roundRobin_], port);
+		TSocket socket = new TSocket("hadoop071.sf2p.facebook.com", port); 
+		roundRobin_ = (roundRobin_+1)%4;
+		if(transport_ != null)
+			transport_.close();
+		transport_ = socket;
+
+		TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport_, false,
+				false);
+		Cassandra.Client peerstorageClient = new Cassandra.Client(
+				binaryProtocol);
+		try
+		{
+			transport_.open();
+		}
+		catch(Exception e)
+		{
+			e.printStackTrace();
+		}
+		return peerstorageClient;
+	}
+    
+    private static boolean isNumeric(String str)
+    {
+        try
+        {
+            Integer.parseInt(str);
+            return true;
+        }
+        catch (NumberFormatException nfe)
+        {
+            return false;
+        }
+    }
+
+    int columnFamilyHack_ = 0 ;
+    public int  divideby_ = 4;
+
+	public void testBatchRunner(String filepath) throws IOException {
+		BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+				new FileInputStream(filepath)), 16 * 1024 * 1024);
+		String line = null;
+		String delimiter_ = new String(",");
+		String firstuser = null;
+		String nextuser = null;
+		batch_mutation_t rmInbox = null;
+		batch_mutation_t rmOutbox = null;
+		while ((line = bufReader.readLine()) != null) {
+			StringTokenizer st = new StringTokenizer(line, delimiter_);
+			int i = 0;
+			String threadId = null;
+			int lastUpdated = 0;
+			int isDeleted = 0;
+			int folder = 0;
+			int uid =0;
+			String user = null;
+			while (st.hasMoreElements()) {
+				switch (i) {
+				case 0:
+					user = (String) st.nextElement();// sb.append((String)st.nextElement());
+                    if ( !isNumeric(user))
+                        continue;
+					
+					break;
+
+				case 1:
+					folder = Integer.parseInt((String) st.nextElement());// sb.append((String)st.nextElement());
+					break;
+
+				case 2:
+					threadId = (String) st.nextElement();
+					break;
+
+				case 3:
+					lastUpdated = Integer.parseInt((String) st.nextElement());
+					break;
+
+				case 4:
+					isDeleted = Integer.parseInt((String) st.nextElement());// (String)st.nextElement();
+					break;
+
+				default:
+					break;
+				}
+				++i;
+			}
+
+			nextuser = user;
+			if (firstuser == null || firstuser.compareTo(nextuser) != 0) {
+				firstuser = nextuser;
+				if (rmInbox != null && !rmInbox.cfmap.isEmpty()) {
+/*					fos_.write(rmInbox.key.getBytes());
+					fos_.write( System.getProperty("line.separator").getBytes());
+	                counter_.incrementAndGet();
+*/					apply(rmInbox);
+				}
+				if (rmOutbox != null && !rmOutbox.cfmap.isEmpty()) {
+/*					fos_.write(rmOutbox.key.getBytes());
+					fos_.write( System.getProperty("line.separator").getBytes());
+	                counter_.incrementAndGet();
+*/					apply(rmOutbox);
+				}
+				rmInbox = new batch_mutation_t();
+				rmInbox.table = "Mailbox";
+				rmInbox.key = firstuser + ":0";
+				rmInbox.cfmap = new HashMap<String, List<column_t>>();
+
+				rmOutbox = new batch_mutation_t();
+				rmOutbox.table = "Mailbox";
+				rmOutbox.key = firstuser + ":1";
+				rmOutbox.cfmap = new HashMap<String, List<column_t>>();
+			}
+			column_t columnData = new column_t();
+			columnData.columnName = threadId;
+			columnData.value = String.valueOf(isDeleted).getBytes();
+			columnData.timestamp = lastUpdated;
+			// List <MboxStruct> list = userthreadmap.get(rs.getString(1));
+			if (folder == 0) {
+				List<column_t> list = rmInbox.cfmap.get("MailboxUserList"+(columnFamilyHack_%divideby_));
+				if (list == null) {
+					list = new ArrayList<column_t>();
+					rmInbox.cfmap.put("MailboxUserList"+(columnFamilyHack_%divideby_), list);
+				}
+                //if(list.size() < 500)
+                    list.add(columnData);
+			} else {
+				List<column_t> list = rmOutbox.cfmap
+						.get("MailboxUserList"+(columnFamilyHack_%divideby_));
+				if (list == null) {
+					list = new ArrayList<column_t>();
+					rmOutbox.cfmap.put("MailboxUserList"+(columnFamilyHack_%divideby_), list);
+				}
+                //if(list.size() < 500)
+                    list.add(columnData);
+			}
+		}
+		if (firstuser != null) {
+			if (rmInbox != null && !rmInbox.cfmap.isEmpty()) {
+/*				fos_.write(rmInbox.key.getBytes());
+				fos_.write( System.getProperty("line.separator").getBytes());
+                counter_.incrementAndGet();
+*/				apply(rmInbox);
+			}
+			if (rmOutbox != null && !rmOutbox.cfmap.isEmpty()) {
+/*				fos_.write(rmOutbox.key.getBytes());
+				fos_.write( System.getProperty("line.separator").getBytes());
+                counter_.incrementAndGet();
+*/				apply(rmOutbox);
+			}
+		}
+		/* Added the thrift call to storage. */
+	}
+
+
+	public void testMailboxBatchRunner(String filepath) throws IOException {
+		BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+				new FileInputStream(filepath)), 16 * 1024 * 1024);
+		String line = null;
+		String delimiter_ = new String(",");
+		String firstuser = null;
+		String nextuser = null;
+		batch_mutation_t rmInbox = null;
+		while ((line = bufReader.readLine()) != null) {
+			StringTokenizer st = new StringTokenizer(line, delimiter_);
+			int i = 0;
+			String threadId = null;
+			int lastUpdated = 0;
+			int isDeleted = 0;
+			int folder = 0;
+			int uid =0;
+			String user = null;
+			while (st.hasMoreElements()) {
+				switch (i) {
+				case 0:
+					user = (String) st.nextElement();// sb.append((String)st.nextElement());
+                    if ( !isNumeric(user))
+                        continue;
+					
+					break;
+
+				case 1:
+					folder = Integer.parseInt((String) st.nextElement());// sb.append((String)st.nextElement());
+					break;
+
+				case 2:
+					threadId = (String) st.nextElement();
+					break;
+
+				case 3:
+					lastUpdated = Integer.parseInt((String) st.nextElement());
+					break;
+
+				case 4:
+					isDeleted = Integer.parseInt((String) st.nextElement());// (String)st.nextElement();
+					break;
+
+				default:
+					break;
+				}
+				++i;
+			}
+
+			nextuser = user;
+			if (firstuser == null || firstuser.compareTo(nextuser) != 0) {
+				firstuser = nextuser;
+				if (rmInbox != null && !rmInbox.cfmap.isEmpty()) {
+					apply(rmInbox);
+				}
+				rmInbox = new batch_mutation_t();
+				rmInbox.table = "Mailbox";
+				rmInbox.key = firstuser;
+				rmInbox.cfmap = new HashMap<String, List<column_t>>();
+			}
+			column_t columnData = new column_t();
+			columnData.columnName = threadId;
+			columnData.value = String.valueOf(isDeleted).getBytes();
+			columnData.timestamp = lastUpdated;
+			List<column_t> list = rmInbox.cfmap.get("MailboxMailList"+(columnFamilyHack_%divideby_));
+			if (list == null) {
+				list = new ArrayList<column_t>();
+				rmInbox.cfmap.put("MailboxMailList"+(columnFamilyHack_%divideby_), list);
+			}
+            list.add(columnData);
+		}
+		if (firstuser != null) {
+			if (rmInbox != null && !rmInbox.cfmap.isEmpty()) {
+				apply(rmInbox);
+			}
+		}
+	}
+
+	public void testSuperBatchRunner(String filepath) throws IOException {
+		BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+				new FileInputStream(filepath)), 16 * 1024 * 1024);
+		String line = null;
+		String delimiter_ = new String(",");
+		String firstuser = null;
+		String nextuser = null;
+		batch_mutation_super_t rmInbox = null;
+		batch_mutation_super_t rmOutbox = null;
+		while ((line = bufReader.readLine()) != null) {
+			StringTokenizer st = new StringTokenizer(line, delimiter_);
+			int i = 0;
+			String threadId = null;
+			int lastUpdated = 0;
+			int isDeleted = 0;
+			int folder = 0;
+			int uid =0;
+			String user = null;
+			String subject = null;
+			String body = null;
+			while (st.hasMoreElements()) {
+				switch (i) {
+				case 0:
+					user = (String) st.nextElement();// sb.append((String)st.nextElement());
+                    if ( !isNumeric(user))
+                        continue;
+					
+					break;
+
+				case 1:
+					folder = Integer.parseInt((String) st.nextElement());// sb.append((String)st.nextElement());
+					break;
+
+				case 2:
+					threadId = (String) st.nextElement();
+					break;
+
+				case 3:
+					lastUpdated = Integer.parseInt((String) st.nextElement());
+					break;
+
+				case 4:
+					isDeleted = Integer.parseInt((String) st.nextElement());// (String)st.nextElement();
+					break;
+
+				case 5:
+					st.nextElement();
+					break;
+
+				case 6:
+					st.nextElement();
+					break;
+
+				case 7:
+					subject = (String) st.nextElement();
+					break;
+
+				case 8:
+					body = (String) st.nextElement();
+					break;
+				default:
+					st.nextElement();
+					break;
+				}
+				++i;
+			}
+
+			nextuser = user;
+			if (firstuser == null || firstuser.compareTo(nextuser) != 0) {
+				firstuser = nextuser;
+				if (rmInbox != null && !rmInbox.cfmap.isEmpty()) {
+					fos_.write(rmInbox.key.getBytes());
+					fos_.write( System.getProperty("line.separator").getBytes());
+	                counter_.incrementAndGet();
+					apply(rmInbox);
+				}
+				if (rmOutbox != null && !rmOutbox.cfmap.isEmpty()) {
+					fos_.write(rmOutbox.key.getBytes());
+					fos_.write( System.getProperty("line.separator").getBytes());
+	                counter_.incrementAndGet();
+					apply(rmOutbox);
+				}
+				rmInbox = new batch_mutation_super_t();
+				rmInbox.table = "Mailbox";
+				rmInbox.key = firstuser ;//+ ":0";
+				rmInbox.cfmap = new HashMap<String, List<superColumn_t>>();
+
+				rmOutbox = new batch_mutation_super_t();
+				rmOutbox.table = "Mailbox";
+				rmOutbox.key = firstuser ;//+ ":1";
+				rmOutbox.cfmap = new HashMap<String, List<superColumn_t>>();
+			}
+			column_t columnData = new column_t();
+			columnData.columnName = threadId;
+			columnData.value = String.valueOf(isDeleted).getBytes();
+			columnData.timestamp = lastUpdated;
+			// List <MboxStruct> list = userthreadmap.get(rs.getString(1));
+			if (folder == 0) {
+				List<superColumn_t> list = rmInbox.cfmap.get("MailboxThreadList"+(columnFamilyHack_%divideby_));
+				if (list == null) {
+					list = new ArrayList<superColumn_t>();
+					rmInbox.cfmap.put("MailboxThreadList"+(columnFamilyHack_%divideby_), list);
+				}
+				if( subject == null)
+					subject = "";
+				if( body == null ) 
+					body = "";
+				List<String> tokenList  = tokenize(subject + " " + body);
+				for(String token : tokenList)
+				{
+					superColumn_t superColumn = new superColumn_t();
+					superColumn.name = token;
+					superColumn.columns = new ArrayList<column_t>();
+					superColumn.columns.add(columnData);
+					list.add(superColumn);
+				}
+			} else {
+				List<superColumn_t> list = rmOutbox.cfmap.get("MailboxThreadList"+(columnFamilyHack_%divideby_));
+				if (list == null) {
+					list = new ArrayList<superColumn_t>();
+					rmOutbox.cfmap.put("MailboxThreadList"+(columnFamilyHack_%divideby_), list);
+				}
+				if( subject == null)
+					subject = "";
+				if( body == null ) 
+					body = "";
+				List<String> tokenList  = tokenize(subject + " " + body);
+				for(String token : tokenList)
+				{
+					superColumn_t superColumn = new superColumn_t();
+					superColumn.name = token;
+					superColumn.columns = new ArrayList<column_t>();
+					superColumn.columns.add(columnData);
+					list.add(superColumn);
+				}
+			}
+		}
+		if (firstuser != null) {
+			if (rmInbox != null && !rmInbox.cfmap.isEmpty()) {
+				fos_.write(rmInbox.key.getBytes());
+				fos_.write( System.getProperty("line.separator").getBytes());
+                counter_.incrementAndGet();
+				apply(rmInbox);
+			}
+			if (rmOutbox != null && !rmOutbox.cfmap.isEmpty()) {
+				fos_.write(rmOutbox.key.getBytes());
+				fos_.write( System.getProperty("line.separator").getBytes());
+                counter_.incrementAndGet();
+				apply(rmOutbox);
+			}
+		}
+		/* Added the thrift call to storage. */
+	}
+	
+    public static String[] getTokens(String str, String delim)
+    {
+        StringTokenizer st = new StringTokenizer(str, delim);
+        String[] values = new String[st.countTokens()];
+        int i = 0;
+        while ( st.hasMoreElements() )
+	    {
+		values[i++] = (String)st.nextElement();
+	    }
+        return values;
+    }
+
+    public static boolean checkUser(String user, String[] list)
+    {
+    	boolean bFound = false;
+    	for(String l:list)
+    	{	
+    		if(user.equals(l))
+    		{
+    			bFound = true;
+    		}
+    	}
+    	return bFound;
+    }
+    
+    public void testSuperUserBatchRunner(String filepath) throws IOException {
+		BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+				new FileInputStream(filepath)), 16 * 1024 * 1024);
+		String line = null;
+		String delimiter_ = new String(",");
+		String firstuser = null;
+		String nextuser = null;
+		batch_mutation_super_t rmInbox = null;
+		batch_mutation_super_t rmOutbox = null;
+		while ((line = bufReader.readLine()) != null) {
+			StringTokenizer st = new StringTokenizer(line, delimiter_);
+			int i = 0;
+			String threadId = null;
+			int lastUpdated = 0;
+			int isDeleted = 0;
+			int folder = 0;
+			int uid =0;
+			String user = null;
+			String subject = null;
+			String body = null;
+			String authors = null;
+			String participants = null;
+			
+			while (st.hasMoreElements()) {
+				switch (i) {
+				case 0:
+					user = (String) st.nextElement();// sb.append((String)st.nextElement());
+                    if ( !isNumeric(user))
+                        continue;
+					
+					break;
+
+				case 1:
+					folder = Integer.parseInt((String) st.nextElement());// sb.append((String)st.nextElement());
+					break;
+
+				case 2:
+					threadId = (String) st.nextElement();
+					break;
+
+				case 3:
+					lastUpdated = Integer.parseInt((String) st.nextElement());
+					break;
+
+				case 4:
+					isDeleted = Integer.parseInt((String) st.nextElement());// (String)st.nextElement();
+					break;
+
+				case 5:
+					authors = (String) st.nextElement();
+					break;
+
+				case 6:
+					participants = (String)st.nextElement();
+					break;
+
+				case 7:
+					subject = (String) st.nextElement();
+					break;
+
+				case 8:
+					body = (String) st.nextElement();
+					break;
+				default:
+					st.nextElement();
+					break;
+				}
+				++i;
+			}
+
+			nextuser = user;
+			if (firstuser == null || firstuser.compareTo(nextuser) != 0) {
+				firstuser = nextuser;
+				if (rmInbox != null && !rmInbox.cfmap.isEmpty()) {
+					fos_.write(rmInbox.key.getBytes());
+					fos_.write( System.getProperty("line.separator").getBytes());
+	                counter_.incrementAndGet();
+					apply(rmInbox);
+				}
+				if (rmOutbox != null && !rmOutbox.cfmap.isEmpty()) {
+					fos_.write(rmOutbox.key.getBytes());
+					fos_.write( System.getProperty("line.separator").getBytes());
+	                counter_.incrementAndGet();
+					apply(rmOutbox);
+				}
+				rmInbox = new batch_mutation_super_t();
+				rmInbox.table = "Mailbox";
+				rmInbox.key = firstuser ;//+ ":0";
+				rmInbox.cfmap = new HashMap<String, List<superColumn_t>>();
+
+				rmOutbox = new batch_mutation_super_t();
+				rmOutbox.table = "Mailbox";
+				rmOutbox.key = firstuser ;//+ ":1";
+				rmOutbox.cfmap = new HashMap<String, List<superColumn_t>>();
+			}
+			column_t columnData = new column_t();
+			columnData.columnName = threadId;
+			columnData.value = String.valueOf(isDeleted).getBytes();
+			columnData.timestamp = lastUpdated;
+			// List <MboxStruct> list = userthreadmap.get(rs.getString(1));
+			if (folder == 0) {
+				List<superColumn_t> list = rmInbox.cfmap.get("MailboxUserList"+(columnFamilyHack_%divideby_));
+				if (list == null) {
+					list = new ArrayList<superColumn_t>();
+					rmInbox.cfmap.put("MailboxUserList"+(columnFamilyHack_%divideby_), list);
+				}
+				if( authors == null)
+					authors = "";
+				if( participants == null ) 
+					participants = "";
+				String[] authorList = getTokens(authors,":");
+				String[] partList = getTokens(participants,":");
+				String[] tokenList = null;
+				if(checkUser(user,authorList))
+				{
+					tokenList = partList;
+				}
+				else
+				{
+					tokenList = authorList;
+				}
+				
+				for(String token : tokenList)
+				{
+					superColumn_t superColumn = new superColumn_t();
+					superColumn.name = token;
+					superColumn.columns = new ArrayList<column_t>();
+					superColumn.columns.add(columnData);
+					list.add(superColumn);
+				}
+			} else {
+				List<superColumn_t> list = rmOutbox.cfmap.get("MailboxUserList"+(columnFamilyHack_%divideby_));
+				if (list == null) {
+					list = new ArrayList<superColumn_t>();
+					rmOutbox.cfmap.put("MailboxUserList"+(columnFamilyHack_%divideby_), list);
+				}
+				if( authors == null)
+					authors = "";
+				if( participants == null ) 
+					participants = "";
+				String[] authorList = getTokens(authors,":");
+				String[] partList = getTokens(participants,":");
+				String[] tokenList = null;
+				if(checkUser(user,authorList))
+				{
+					tokenList = partList;
+				}
+				else
+				{
+					tokenList = authorList;
+				}
+				for(String token : tokenList)
+				{
+					superColumn_t superColumn = new superColumn_t();
+					superColumn.name = token;
+					superColumn.columns = new ArrayList<column_t>();
+					superColumn.columns.add(columnData);
+					list.add(superColumn);
+				}
+			}
+		}
+		if (firstuser != null) {
+			if (rmInbox != null && !rmInbox.cfmap.isEmpty()) {
+				fos_.write(rmInbox.key.getBytes());
+				fos_.write( System.getProperty("line.separator").getBytes());
+                counter_.incrementAndGet();
+				apply(rmInbox);
+			}
+			if (rmOutbox != null && !rmOutbox.cfmap.isEmpty()) {
+				fos_.write(rmOutbox.key.getBytes());
+				fos_.write( System.getProperty("line.separator").getBytes());
+                counter_.incrementAndGet();
+				apply(rmOutbox);
+			}
+		}
+		/* Added the thrift call to storage. */
+	}
+	
+	
+	
+	// Defining these privates here as they make more snese with the functions
+	// below
+	// Sorry
+
+	private int numCreated_ = 0;
+
+	ThreadFactory tf_ = null;
+
+	ScheduledExecutorService pool_ = null;
+
+	private int requestsPerSecond_ = 50;
+    public FileOutputStream fos_ = null;
+    private AtomicInteger counter_ = new AtomicInteger(0);
+    
+	// This is the task that gets scheduled
+	// This could be different for different kind of tasks
+	class Task implements Runnable {       
+		RowMutationMessage rmMsg_ = null;
+		
+		public Task(RowMutationMessage rmMsg) {
+			rmMsg_ = rmMsg;
+		}
+
+		public void run() {
+			try {
+				long t = System.currentTimeMillis();
+                counter_.incrementAndGet();
+				Message message = new Message(DataImporter.from_,
+						StorageService.mutationStage_,
+						StorageService.mutationVerbHandler_,
+						new Object[] { rmMsg_ });
+				MessagingService.getMessagingInstance().sendOneWay(message,
+						DataImporter.to_);
+			} catch (Exception e) {
+				e.printStackTrace();
+			}
+		}
+	}
+
+	public DataImporter() throws Throwable {
+		tf_ = new ThreadFactoryImpl("LOAD-GENERATOR");
+		pool_ = new DebuggableScheduledThreadPoolExecutor(100, tf_);
+		fos_ = new FileOutputStream("keys.dat", true);
+	}
+    
+	public long errorCount_ = 0;
+
+	public long queryCount_ = 0;
+
+	public void testRead(String filepath) throws Throwable {
+		BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+				new FileInputStream(filepath)), 16 * 1024 * 1024);
+		String line = null;
+		String delimiter_ = new String(",");
+		RowMutationMessage rmInbox = null;
+		RowMutationMessage rmOutbox = null;
+		ColumnFamily cfInbox = null;
+		ColumnFamily cfOutbox = null;
+		while ((line = bufReader.readLine()) != null) {
+			StringTokenizer st = new StringTokenizer(line, delimiter_);
+			int i = 0;
+			String threadId = null;
+			int lastUpdated = 0;
+			int isDeleted = 0;
+			int folder = 0;
+			String user = null;
+			while (st.hasMoreElements()) {
+				switch (i) {
+				case 0:
+					user = (String) st.nextElement();// sb.append((String)st.nextElement());
+					break;
+
+				case 1:
+					folder = Integer.parseInt((String) st.nextElement());// sb.append((String)st.nextElement());
+					break;
+
+				case 2:
+					threadId = (String) st.nextElement();
+					break;
+
+				case 3:
+					lastUpdated = Integer.parseInt((String) st.nextElement());
+					break;
+
+				case 4:
+					isDeleted = Integer.parseInt((String) st.nextElement());// (String)st.nextElement();
+					break;
+
+				default:
+					break;
+				}
+				++i;
+			}
+			String key = null;
+			if (folder == 0) {
+				key = user + ":0";
+			} else {
+				key = user + ":1";
+			}
+
+			ReadCommand readCommand = new RowReadCommand(tablename_, key);
+			Message message = new Message(from_, StorageService.readStage_,
+					StorageService.readVerbHandler_,
+					new Object[] {readCommand});
+			IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(
+					message, to_);
+			Object[] result = iar.get();
+			ReadResponse readResponse = (ReadResponse) result[0];
+			Row row = readResponse.row();
+			if (row == null) {
+				logger_.debug("ERROR No row for this key .....: " + line);
+                Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+				errorCount_++;
+			} else {
+				Map<String, ColumnFamily> cfMap = row.getColumnFamilyMap();
+				if (cfMap == null || cfMap.size() == 0) {
+					logger_
+							.debug("ERROR ColumnFamil map is missing.....: "
+									+ threadId + "   key:" + key
+									+ "    record:" + line);
+					System.out
+							.println("ERROR ColumnFamil map is missing.....: "
+									+ threadId + "   key:" + key
+									+ "    record:" + line);
+					errorCount_++;
+					continue;
+				}
+				ColumnFamily cfamily = cfMap.get(columnFamily_);
+				if (cfamily == null) {
+					logger_
+							.debug("ERROR ColumnFamily  is missing.....: "
+									+ threadId + "   key:" + key
+									+ "    record:" + line);
+					System.out
+							.println("ERROR ColumnFamily  is missing.....: "
+									+ threadId + "   key:" + key
+									+ "    record:" + line);
+					errorCount_++;
+					continue;
+				}
+
+				IColumn clmn = cfamily.getColumn(threadId);
+				queryCount_++;
+				if (clmn == null) {
+					logger_.debug("ERROR Column is missing.....: " + threadId
+							+ "    record:" + line);
+					System.out.println("ERROR Column is missing.....: "
+							+ threadId + "    record:" + line);
+                    Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+					errorCount_++;
+				} else {
+					// logger_.debug("SUCCESS .....for column : "+clmn.name()+"
+					// Record:" + line);
+                    Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+				}
+			}
+			
+		}
+
+	}
+
+	private List<String> tokenize(String string)
+	{
+		List<String> stringList = new ArrayList<String>();
+	    Analyzer analyzer = new StandardAnalyzer();
+	    TokenStream ts = analyzer.tokenStream("superColumn", new StringReader(string));
+	    Token token = null;
+	    try
+	    {
+		    token = ts.next();
+		    while(token != null)
+		    {
+		    	stringList.add(token.termText());
+			    token = ts.next();
+		    }
+	    }
+	    catch(IOException ex)
+	    {
+	    	ex.printStackTrace();
+	    }
+		
+		return stringList;
+	}
+	private int numReqs_ = 0;
+	private long totalTime_ = 0 ;
+	
+	public void testReadThrift(String filepath) throws Throwable {
+		BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+				new FileInputStream(filepath)), 16 * 1024 * 1024);
+		String line = null;
+		String delimiter_ = new String(",");
+		RowMutationMessage rmInbox = null;
+		RowMutationMessage rmOutbox = null;
+		ColumnFamily cfInbox = null;
+		ColumnFamily cfOutbox = null;
+		String firstuser = null ;
+		String nextuser = null;
+		while ((line = bufReader.readLine()) != null) {
+			StringTokenizer st = new StringTokenizer(line, delimiter_);
+			int i = 0;
+			String threadId = null;
+			int lastUpdated = 0;
+			int isDeleted = 0;
+			int folder = 0;
+			String user = null;
+			while (st.hasMoreElements()) {
+				switch (i) {
+				case 0:
+					user = (String) st.nextElement();// sb.append((String)st.nextElement());
+                    if ( !isNumeric(user))
+                        continue;
+					break;
+
+				case 1:
+					folder = Integer.parseInt((String) st.nextElement());// sb.append((String)st.nextElement());
+					break;
+
+				case 2:
+					threadId = (String) st.nextElement();
+					break;
+
+				case 3:
+					lastUpdated = Integer.parseInt((String) st.nextElement());
+					break;
+
+				case 4:
+					isDeleted = Integer.parseInt((String) st.nextElement());// (String)st.nextElement();
+					break;
+
+				default:
+					break;
+				}
+				++i;
+			}
+			String key = null;
+			if (folder == 0) {
+				key = user + ":0";
+			} else {
+				key = user + ":1";
+			}
+
+			nextuser = key;
+			if(firstuser == null || firstuser.compareTo(nextuser) != 0)
+			{
+				List<column_t> columns = null;
+				firstuser = key;
+				try {
+                    Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+					long t = System.currentTimeMillis();
+					
+					columns = peerstorageClient_.get_slice(tablename_,key,columnFamily_+(columnFamilyHack_%divideby_),0,10);
+					numReqs_++;
+					totalTime_ = totalTime_ + (System.currentTimeMillis() - t);
+					logger_.debug("Numreqs:" + numReqs_ + " Average: " + totalTime_/numReqs_+  "   Time taken for thrift..."
+							+ (System.currentTimeMillis() - t));
+				} catch (Exception e) {
+						e.printStackTrace();
+					}
+				if (columns == null) {
+					logger_.debug("ERROR No row for this key .....: " + line);
+                    Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+					errorCount_++;
+				} else {
+					if (columns.size() == 0) {
+						logger_
+								.debug("ERROR ColumnFamil map is missing.....: "
+										+ threadId + "   key:" + key
+										+ "    record:" + line);
+						System.out
+								.println("ERROR ColumnFamil map is missing.....: "
+										+ threadId + "   key:" + key
+										+ "    record:" + line);
+						errorCount_++;
+						continue;
+					}
+					else {
+						//logger_.debug("SUCCESS .....for key : "+key);
+						//System.out.println("SUCCESS .....for key : "+key);
+						//for(int j = 0 ; j< columns.size() ; j++ ){
+							//System.out.print("  " + columns.get(j)+",");
+						//}
+						// Record:" + line);
+						//Thread.sleep(5);
+					}
+				}
+				queryCount_++;
+			}
+		}
+
+	}
+	
+
+	public void testSuperReadThrift(String filepath) throws Throwable {
+		BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+				new FileInputStream(filepath)), 16 * 1024 * 1024);
+		String line = null;
+		String delimiter_ = new String(",");
+		RowMutationMessage rmInbox = null;
+		RowMutationMessage rmOutbox = null;
+		ColumnFamily cfInbox = null;
+		ColumnFamily cfOutbox = null;
+		String firstuser = null ;
+		String nextuser = null;
+		while ((line = bufReader.readLine()) != null) {
+			StringTokenizer st = new StringTokenizer(line, delimiter_);
+			int i = 0;
+			String threadId = null;
+			int lastUpdated = 0;
+			int isDeleted = 0;
+			int folder = 0;
+			String user = null;
+			String subject = null;
+			String body = null;
+			while (st.hasMoreElements()) {
+				switch (i) {
+				case 0:
+					user = (String) st.nextElement();// sb.append((String)st.nextElement());
+                    if ( !isNumeric(user))
+                        continue;
+					break;
+
+				case 1:
+					folder = Integer.parseInt((String) st.nextElement());// sb.append((String)st.nextElement());
+					break;
+
+				case 2:
+					threadId = (String) st.nextElement();
+					break;
+
+				case 3:
+					lastUpdated = Integer.parseInt((String) st.nextElement());
+					break;
+
+				case 4:
+					isDeleted = Integer.parseInt((String) st.nextElement());// (String)st.nextElement();
+					break;
+				
+				case 5:
+					st.nextElement();
+					break;
+
+
+				case 6:
+					st.nextElement();
+					break;
+
+				case 7:
+					subject = (String) st.nextElement();
+					break;
+
+				case 8:
+					body = (String) st.nextElement();
+					break;
+				default:
+					st.nextElement();
+					break;
+				}
+				++i;
+			}
+			String key = null;
+			if (folder == 0) {
+				key = user ;//+ ":0";
+			} else {
+				key = user ;//+ ":1";
+			}
+
+			List<column_t> columns = null;
+			firstuser = key;
+			try {
+                Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+				if( subject == null )
+					subject = "";
+				if( body == null )
+					body = "";
+				List<String> tokenList = tokenize(subject + " " + body ) ;
+				
+				for( String token: tokenList )
+				{
+					long t = System.currentTimeMillis();
+					columns = peerstorageClient_.get_slice(tablename_,key,"MailboxThreadList"+(columnFamilyHack_%divideby_)+":"+token,0,10);
+					totalTime_ = totalTime_ + (System.currentTimeMillis() - t);
+					numReqs_++;
+					logger_.debug("Numreqs:" + numReqs_ + " Average: " + totalTime_/numReqs_+  "   Time taken for thrift..."
+							+ (System.currentTimeMillis() - t));
+					if (columns == null) {
+						logger_.debug(" TOKEN: " + token + "  ERROR No row for this key .....: " + line);
+	                    Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+						errorCount_++;
+					} else {
+						if (columns.size() == 0) {
+							logger_
+									.debug("ERROR ColumnFamil map is missing.....: "
+											+ threadId + "   key:" + key
+											+ " TOKEN: " + token
+											+ "    record:" + line);
+							System.out
+									.println("ERROR ColumnFamil map is missing.....: "
+											+ threadId + "   key:" + key
+											+ " TOKEN: " + token
+											+ "    record:" + line);
+							errorCount_++;
+							continue;
+						}
+						else {
+							boolean found = false;
+							for(column_t column : columns)
+							{
+								if(column.columnName.equalsIgnoreCase(threadId))
+								{
+									found = true ;
+									break;
+								}
+							}
+							if(!found)
+							{
+								logger_
+										.debug("ERROR column is missing.....: "
+												+ threadId + "   key:" + key
+												+ " TOKEN: " + token
+												+ "    record:" + line);
+								System.out
+										.println("ERROR column is missing.....: "
+												+ threadId + "   key:" + key
+												+ " TOKEN: " + token
+												+ "    record:" + line);
+								errorCount_++;
+										
+							}
+							//logger_.debug("SUCCESS .....for key : "+key);
+							//System.out.println("SUCCESS .....for key : "+key);
+							//for(int j = 0 ; j< columns.size() ; j++ ){
+								//System.out.print("  " + columns.get(j)+",");
+							//}
+							// Record:" + line);
+							//Thread.sleep(5);
+						}
+					}
+				}
+				queryCount_++;
+			} catch (Exception e) {
+				e.printStackTrace();
+			}
+				
+		}
+
+	}
+	
+	
+	
+	public void testLoadGeneratorBatchRunner(String filepath) throws Throwable
+    {
+        BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+                new FileInputStream(filepath)), 16 * 1024 * 1024);
+        String line = null;
+        String delimiter_ = new String(",");
+        String firstuser = null;
+        String nextuser = null;
+        RowMutation rmInbox = null;
+        RowMutation rmOutbox = null;
+        ColumnFamily cfInbox = null;
+        ColumnFamily cfOutbox = null;
+        while ((line = bufReader.readLine()) != null)
+        {
+            StringTokenizer st = new StringTokenizer(line, delimiter_);
+            int i = 0;
+            String threadId = null;
+            int lastUpdated = 0;
+            int isDeleted = 0;
+            int folder = 0;
+            String user = null;
+            while (st.hasMoreElements())
+            {
+                switch (i)
+                {
+                case 0:
+                    user = (String) st.nextElement();// sb.append((String)st.nextElement());
+                    break;
+
+                case 1:
+                    folder = Integer.parseInt((String) st.nextElement());// sb.append((String)st.nextElement());
+                    break;
+
+                case 2:
+                    threadId = (String) st.nextElement();
+                    break;
+
+                case 3:
+                    lastUpdated = Integer.parseInt((String) st.nextElement());
+                    break;
+
+                case 4:
+                    isDeleted = Integer.parseInt((String) st.nextElement());// (String)st.nextElement();
+                    break;
+
+                default:
+                    break;
+                }
+                ++i;
+            }
+
+         nextuser = user;
+          if (firstuser == null || firstuser.compareTo(nextuser) != 0) {
+                  firstuser = nextuser;
+                  if (rmInbox != null) {
+                          applyLoad(rmInbox);
+                  }
+                  if (rmOutbox != null) {
+                          applyLoad(rmOutbox);
+                  }
+                  rmInbox = new RowMutation(tablename_, firstuser + ":0");
+                  rmOutbox = new RowMutation(tablename_, firstuser + ":1");
+          }
+          // List <MboxStruct> list = userthreadmap.get(rs.getString(1));
+          if (folder == 0) {
+              rmInbox.add(columnFamily_+(columnFamilyHack_%divideby_)+":"+threadId, String.valueOf(isDeleted).getBytes(), lastUpdated);
+          } else {
+              rmOutbox.add(columnFamily_+(columnFamilyHack_%divideby_)+":"+threadId,String.valueOf(isDeleted).getBytes(),lastUpdated);
+          }
+  }
+  if (firstuser != null) {
+          if (rmInbox != null) {
+                  applyLoad(rmInbox);
+          }
+          if (rmOutbox != null) {
+                  applyLoad(rmOutbox);
+          }
+  }
+    
+    
+    
+    
+    }
+    
+    /*
+     * This function will apply the given task . It is based on a requests per
+     * second member variable which can be set to teh required ammount , it will
+     * generate only those many requests and if thos emany requests have already
+     * been entered then it will sleep . This function assumes that there is no
+     * waiting in any other part of the code so the requests are being generated
+     * instantaniously .
+     */
+    public void applyLoad(RowMutation rm) throws IOException {
+        try
+        {
+            long t = System.currentTimeMillis();
+            counter_.incrementAndGet();
+    		columnFamilyHack_++;
+            EndPoint to = new EndPoint(7000);
+            RowMutationMessage rmMsg = new RowMutationMessage(rm);           
+            Message message = new Message(to, 
+                    StorageService.mutationStage_,
+                    StorageService.mutationVerbHandler_, 
+                    new Object[]{ rmMsg }
+            );                                                            
+			MessagingService.getMessagingInstance().sendRR(message, to);
+            Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
+            
+        }
+        catch (Exception e)
+        {
+            e.printStackTrace();
+        }
+        
+    }
+
+	public void run(String[] args) throws Throwable
+    {
+		if (args[0].compareTo("-testWriteMailbox") == 0  ||
+			args[0].compareTo("-testSuperUserWrite") == 0  ||
+			args[0].compareTo("-testPhp") == 0  ||
+			args[0].compareTo("-testWrite") == 0  ||
+			args[0].compareTo("-testRead") == 0 ||
+			args[0].compareTo("-testWriteSuper") == 0  ||
+			args[0].compareTo("-testReadSuper") == 0 ||
+			args[0].compareTo("-testRemove") == 0 
+			) 
+		{
+			int totalNumofServers = 5; // total number of servers we want to
+			if (args.length > 4) {
+				totalNumofServers = Integer.parseInt((String) args[4]);
+			}			// run on
+			if (args.length > 5) {
+				divideby_ = Integer.parseInt((String) args[5]);
+			}			// run on
+			int index = Integer.parseInt(args[1]);
+			int fileCount = 0; // need to get the file count
+			int start = 0;
+			int end = 0;
+			File file = new File(args[2]);
+			fileCount = file.list().length;
+			if (index == -1) {
+				start = 0;
+				end = fileCount;
+			} else {
+				int skip = fileCount / totalNumofServers;
+				if (fileCount != 0 && skip == 0) {
+					skip = 1;
+				}
+				start = skip * index;
+				if (index == totalNumofServers - 1) {
+					end = fileCount;
+				} else {
+					end = start + skip;
+				}
+			}
+			if (args.length > 3) {
+				requestsPerSecond_ = Integer.parseInt((String) args[3]);
+			}
+			long t = System.currentTimeMillis();
+			peerstorageClient_ = connect();
+			for (int i = start; i < end; i++) {
+				String fileName = file.list()[i];
+				if ( args[0].compareTo("-testRead") == 0 )
+				{
+					testReadThrift(args[2] + System.getProperty("file.separator")
+							+ fileName);
+				}
+				if ( args[0].compareTo("-testWrite") == 0 )
+				{
+					testBatchRunner(args[2]
+							+ System.getProperty("file.separator") + fileName);
+				}
+				if ( args[0].compareTo("-testWriteSuper") == 0 )
+				{
+					testSuperBatchRunner(args[2]
+							+ System.getProperty("file.separator") + fileName);
+				}
+				if ( args[0].compareTo("-testReadSuper") == 0 )
+				{
+					testSuperReadThrift(args[2]
+							+ System.getProperty("file.separator") + fileName);
+				}
+				if ( args[0].compareTo("-testPhp") == 0 )
+				{
+					testPhp(args[2]
+							+ System.getProperty("file.separator") + fileName);
+				}
+				if ( args[0].compareTo("-testSuperUserWrite") == 0 )
+				{
+					testSuperUserBatchRunner(args[2]
+							+ System.getProperty("file.separator") + fileName);
+				}
+				if ( args[0].compareTo("-testWriteMailbox") == 0 )
+				{
+					testMailboxBatchRunner(args[2]
+							+ System.getProperty("file.separator") + fileName);
+				}
+				System.out.println(args[2]
+						+ System.getProperty("file.separator") + fileName);
+			}
+			if(transport_ != null)
+				transport_.close();
+			System.out.println("start :" + start + "    end : " + end);
+			System.out.println("Time taken  .."
+					+ (System.currentTimeMillis() - t));
+            System.out.println("Keys sent over: " + counter_.get());
+			fos_.close();
+			return;
+		}
+		else
+		{
+			System.out.println("Invalid option");
+		}
+    }
+
+
+	
+	public void testPhp(String filepath) throws IOException {
+		BufferedReader bufReader = new BufferedReader(new InputStreamReader(
+				new FileInputStream(filepath)), 16 * 1024 * 1024);
+		String line = null;
+		String delimiter_ = new String(",");
+		String firstuser = null;
+		String nextuser = null;
+		batch_mutation_t rmInbox = null;
+		batch_mutation_t rmOutbox = null;
+		while ((line = bufReader.readLine()) != null) {
+			StringTokenizer st = new StringTokenizer(line, delimiter_);
+			int i = 0;
+			String threadId = null;
+			int lastUpdated = 0;
+			int isDeleted = 0;
+			int folder = 0;
+			int uid =0;
+			String user = null;
+			while (st.hasMoreElements()) {
+				switch (i) {
+				case 0:
+					user = (String) st.nextElement();// sb.append((String)st.nextElement());
+                    if ( !isNumeric(user))
+                        continue;
+					
+					break;
+
+				case 1:
+					folder = Integer.parseInt((String) st.nextElement());// sb.append((String)st.nextElement());
+					break;
+
+				case 2:
+					threadId = (String) st.nextElement();
+					break;
+
+				case 3:
+					lastUpdated = Integer.parseInt((String) st.nextElement());
+					break;
+
+				case 4:
+					isDeleted = Integer.parseInt((String) st.nextElement());// (String)st.nextElement();
+					break;
+
+				default:
+					break;
+				}
+				++i;
+			}
+			String cmd = "php /home/pmalik/www/scripts/mbox_index/search_test.php  "
+				+ (new File(filepath)).getName() + "  " + user +"  " +threadId + "  "+ line;
+			Process process = Runtime.getRuntime().exec(cmd);
+		
+		}
+	}
+    class PhpExecute implements Runnable
+    {
+        private String cmdLine_;
+        
+        PhpExecute(String cmdLine)
+        {
+        	cmdLine_ = cmdLine;
+        }
+        
+        public void run()
+        {
+            try
+            {
+    			System.out.println(cmdLine_);
+    			Process process = Runtime.getRuntime().exec(cmdLine_);
+    			try
+    			{
+    				//process.waitFor();
+    			}
+    			catch ( Exception e)
+    			{
+    				e.printStackTrace();
+    			}
+            }
+            catch (Exception ex)
+            {
+            	ex.printStackTrace();
+            }
+        }        
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/StressTest.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/StressTest.java
index e69de29b..c4edb8ca 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/StressTest.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/StressTest.java
@@ -0,0 +1,877 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.test;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Random;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.db.ColumnReadCommand;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.RowMutationMessage;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.Cassandra;
+import org.apache.cassandra.service.IResponseResolver;
+import org.apache.cassandra.service.QuorumResponseHandler;
+import org.apache.cassandra.service.ReadResponseResolver;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.service.batch_mutation_super_t;
+import org.apache.cassandra.service.batch_mutation_t;
+import org.apache.cassandra.service.column_t;
+import org.apache.cassandra.service.superColumn_t;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.transport.TTransport;
+import com.martiansoftware.jsap.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class StressTest
+{
+	private static Logger logger_ = Logger.getLogger(DataImporter.class);
+
+	private static final String tablename_ = new String("Test");
+
+	public static EndPoint from_ = new EndPoint("172.24.24.209", 10001);
+
+	public static EndPoint to_ = new EndPoint("hadoop071.sf2p.facebook.com", 7000);
+	private static  String server_ = new String("hadoop071.sf2p.facebook.com");
+	private static final String columnFamilyColumn_ = new String("ColumnList");
+	private static final String columnFamilySuperColumn_ = new String("SuperColumnList");
+	private static final String keyFix_ = new String("");
+	private static final String columnFix_ = new String("Column-");
+	private static final String superColumnFix_ = new String("SuperColumn-");
+
+	private Cassandra.Client peerstorageClient_ = null;
+	TTransport transport_ = null;
+	private int requestsPerSecond_ = 1000;
+    private ExecutorService runner_ = null;
+
+    
+    class LoadManager implements Runnable
+    {
+    	private RowMutationMessage rmsg_ = null;
+    	private batch_mutation_t bt_ = null;
+    	private batch_mutation_super_t bts_ = null;
+   
+    	LoadManager(RowMutationMessage rmsg)
+        {
+    		rmsg_ = rmsg;
+        }
+    	LoadManager(batch_mutation_t bt)
+        {
+    		bt_ = bt;
+        }
+    	LoadManager(batch_mutation_super_t bts)
+        {
+    		bts_ = bts;
+        }
+        
+        public void run()
+        {
+        	if( rmsg_ != null )
+        	{
+				Message message = new Message(from_ , StorageService.mutationStage_,
+						StorageService.loadVerbHandler_, new Object[] { rmsg_ });
+				MessagingService.getMessagingInstance().sendOneWay(message, to_);
+        	}
+        	
+       	}
+    }
+
+	
+    /*
+     * This function will apply the given task . It is based on a requests per
+     * second member variable which can be set to teh required ammount , it will
+     * generate only those many requests and if thos emany requests have already
+     * been entered then it will sleep . This function assumes that there is no
+     * waiting in any other part of the code so the requests are being generated
+     * instantaniously .
+     */
+    public void applyLoad(RowMutation rm) throws IOException {
+        try
+        {
+            long t = System.currentTimeMillis();
+            RowMutationMessage rmMsg = new RowMutationMessage(rm);           
+            Message message = new Message(from_, 
+                    StorageService.mutationStage_,
+                    StorageService.mutationVerbHandler_, 
+                    new Object[]{ rmMsg }
+            );                                                            
+			MessagingService.getMessagingInstance().sendOneWay(message, to_);
+            Thread.sleep(1, 1000000000/requestsPerSecond_);
+            
+        }
+        catch (Exception e)
+        {
+            e.printStackTrace();
+        }
+        
+    }	
+	
+    public void readLoad(ReadCommand readCommand)
+    {
+		IResponseResolver<Row> readResponseResolver = new ReadResponseResolver();
+		QuorumResponseHandler<Row> quorumResponseHandler = new QuorumResponseHandler<Row>(
+				1,
+				readResponseResolver);
+		Message message = new Message(from_, StorageService.readStage_,
+				StorageService.readVerbHandler_,
+				new Object[] {readCommand});
+		MessagingService.getMessagingInstance().sendOneWay(message, to_);
+		/*IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(message, to_);
+		try
+		{
+			long t = System.currentTimeMillis();
+			iar.get(2000, TimeUnit.MILLISECONDS );
+			logger_.debug("Time taken for read..."
+					+ (System.currentTimeMillis() - t));
+			
+		}
+		catch (Exception ex)
+		{
+            ex.printStackTrace();
+		}*/
+    }
+    
+    
+    
+    
+    
+	public void randomReadColumn  (int keys, int columns, int size, int tps)
+	{
+        Random random = new Random();
+		try
+		{
+			while(true)
+			{
+				int key = random.nextInt(keys) + 1;
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+            	int j = random.nextInt(columns) + 1;
+	            ReadCommand rm = new ColumnReadCommand(tablename_, stringKey, columnFamilyColumn_ + ":" + columnFix_ + j);
+	            readLoad(rm);
+				if ( requestsPerSecond_ > 1000)
+					Thread.sleep(0, 1000000000/requestsPerSecond_);
+				else
+					Thread.sleep(1000/requestsPerSecond_);
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+	}
+
+	public void randomWriteColumn(int keys, int columns, int size, int tps)
+	{
+        Random random = new Random();
+        byte[] bytes = new byte[size];
+		int ts = 1;
+		try
+		{
+			while(true)
+			{
+				int key = random.nextInt(keys) + 1;
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+	            RowMutation rm = new RowMutation(tablename_, stringKey);
+            	int j = random.nextInt(columns) + 1;
+                random.nextBytes(bytes);
+                rm.add( columnFamilyColumn_ + ":" + columnFix_ + j, bytes, ts);
+                if ( ts == Integer.MAX_VALUE)
+                {
+                	ts = 0 ;
+                }
+                ts++;
+				for(int k = 0 ; k < requestsPerSecond_/1000 +1 ; k++ )
+				{
+					runner_.submit(new LoadManager(new RowMutationMessage(rm)));
+				}
+				try
+				{
+					if ( requestsPerSecond_ > 1000)
+						Thread.sleep(1);
+					else
+						Thread.sleep(1000/requestsPerSecond_);
+				}
+				catch ( Exception ex)
+				{
+					
+				}
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+	}
+	
+	public void randomReadSuperColumn(int keys, int superColumns, int columns, int size, int tps)
+	{
+        Random random = new Random();
+		try
+		{
+			while(true)
+			{
+				int key = random.nextInt(keys) + 1;
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+            	int i = random.nextInt(superColumns) + 1;
+            	int j = random.nextInt(columns) + 1;
+	            ReadCommand rm = new ColumnReadCommand(tablename_, stringKey, columnFamilySuperColumn_ + ":" + superColumnFix_ + i + ":" + columnFix_ + j);
+	            readLoad(rm);
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+	}
+
+	
+	public void randomWriteSuperColumn(int keys, int superColumns,int columns, int size, int tps)
+	{
+        Random random = new Random();
+        byte[] bytes = new byte[size];
+		int ts = 1;
+		try
+		{
+			while(true)
+			{
+				int key = random.nextInt(keys) + 1;
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+	            RowMutation rm = new RowMutation(tablename_, stringKey);
+            	int i = random.nextInt(superColumns) + 1;
+            	int j = random.nextInt(columns) + 1;
+                random.nextBytes(bytes);
+                rm.add( columnFamilySuperColumn_ + ":" + superColumnFix_ + i + ":" + columnFix_ + j, bytes, ts);
+                if ( ts == Integer.MAX_VALUE )
+                {
+                	ts = 0 ;
+                }
+                ts++;
+				for(int k = 0 ; k < requestsPerSecond_/1000 +1 ; k++ )
+				{
+					runner_.submit(new LoadManager(new RowMutationMessage(rm)));
+				}
+				try
+				{
+					if ( requestsPerSecond_ > 1000)
+						Thread.sleep(1);
+					else
+						Thread.sleep(1000/requestsPerSecond_);
+				}
+				catch ( Exception ex)
+				{
+					
+				}
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+	}
+
+	public void bulkWriteColumn(int keys, int columns, int size, int tps)
+	{
+        Random random = new Random();
+        byte[] bytes = new byte[size];
+		int ts = 1;
+		long time = System.currentTimeMillis();
+		try
+		{
+			for(int key = 1; key <= keys ; key++)
+			{
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+	            RowMutation rm = new RowMutation(tablename_, stringKey);
+	            for( int j = 1; j <= columns ; j++)
+	            {
+	                random.nextBytes(bytes);
+	                rm.add( columnFamilyColumn_ + ":" + columnFix_ + j, bytes, ts);
+	            }
+				RowMutationMessage rmMsg = new RowMutationMessage(rm);
+				
+				for(int k = 0 ; k < requestsPerSecond_/1000 +1 ; k++ )
+				{
+					runner_.submit(new LoadManager(rmMsg));
+				}
+				try
+				{
+					if ( requestsPerSecond_ > 1000)
+						Thread.sleep(1);
+					else
+						Thread.sleep(1000/requestsPerSecond_);
+				}
+				catch ( Exception ex)
+				{
+					
+				}
+				
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+		System.out.println(System.currentTimeMillis() - time);
+	}
+	
+	public void bulkWriteSuperColumn(int keys, int superColumns, int columns, int size, int tps)
+	{
+        Random random = new Random();
+        byte[] bytes = new byte[size];
+		int ts = 1;
+		try
+		{
+			for(int key = 1; key <= keys ; key++)
+			{
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+	            RowMutation rm = new RowMutation(tablename_, stringKey);
+	            for( int i = 1; i <= superColumns ; i++)
+	            {
+		            for( int j = 1; j <= columns ; j++)
+		            {
+		                random.nextBytes(bytes);
+		                rm.add( columnFamilySuperColumn_ + ":" + superColumnFix_ + i + ":" + columnFix_ + j, bytes, ts);
+		            }
+	            }
+	            RowMutationMessage rmMsg = new RowMutationMessage(rm);
+				for(int k = 0 ; k < requestsPerSecond_/1000 +1 ; k++ )
+				{
+					runner_.submit(new LoadManager(rmMsg));
+				}
+				try
+				{
+					if ( requestsPerSecond_ > 1000)
+						Thread.sleep(1);
+					else
+						Thread.sleep(1000/requestsPerSecond_);
+				}
+				catch ( Exception ex)
+				{
+					
+				}
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+	}
+
+	//  Stress the server using the thrift API 
+	
+	public Cassandra.Client connect() {
+		int port = 9160;
+		TSocket socket = new TSocket(server_, port); 
+		if(transport_ != null)
+			transport_.close();
+		transport_ = socket;
+
+		TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport_, false,
+				false);
+		Cassandra.Client peerstorageClient = new Cassandra.Client(
+				binaryProtocol);
+		try
+		{
+			transport_.open();
+		}
+		catch(Exception e)
+		{
+			e.printStackTrace();
+		}
+		return peerstorageClient;
+	}
+
+	public void applyThrift(String table, String key, String columnFamily, byte[] bytes, long ts ) {
+
+		try {
+			if ( requestsPerSecond_ > 1000)
+				Thread.sleep(0, 1000000000/requestsPerSecond_);
+			else
+				Thread.sleep(1000/requestsPerSecond_);
+			peerstorageClient_.insert(table, key, columnFamily, bytes, ts);
+		} catch (Exception e) {
+			try {
+				peerstorageClient_ = connect();
+				peerstorageClient_.insert(table, key, columnFamily, bytes, ts);
+			} catch (Exception e1) {
+				e1.printStackTrace();
+			}
+		}
+	}
+
+	
+	public void apply(batch_mutation_t batchMutation) {
+
+		try {
+			if ( requestsPerSecond_ > 1000)
+				Thread.sleep(0, 1000000000/requestsPerSecond_);
+			else
+				Thread.sleep(1000/requestsPerSecond_);
+			peerstorageClient_.batch_insert(batchMutation);
+		} catch (Exception e) {
+			try {
+				peerstorageClient_ = connect();
+				peerstorageClient_.batch_insert(batchMutation);
+			} catch (Exception e1) {
+				e1.printStackTrace();
+			}
+		}
+	}
+
+	public void apply(batch_mutation_super_t batchMutation) {
+
+		try {
+			if ( requestsPerSecond_ > 1000)
+				Thread.sleep(0, 1000000000/requestsPerSecond_);
+			else
+				Thread.sleep(1000/requestsPerSecond_);
+			long t = System.currentTimeMillis();
+			peerstorageClient_.batch_insert_superColumn(batchMutation);
+			logger_.debug("Time taken for thrift..."
+					+ (System.currentTimeMillis() - t));
+		} catch (Exception e) {
+			try {
+				peerstorageClient_ = connect();
+				peerstorageClient_.batch_insert_superColumn(batchMutation);
+			} catch (Exception e1) {
+				e1.printStackTrace();
+			}
+		}
+	}
+	
+	public void readLoadColumn(String tableName, String key, String cf)
+	{
+		try
+		{
+			column_t column = peerstorageClient_.get_column(tableName, key, cf);
+		}
+		catch(Exception ex)
+		{
+			peerstorageClient_ = connect();
+			ex.printStackTrace();
+		}
+	}
+	
+	public void randomReadColumnThrift(int keys, int columns, int size, int tps)
+	{
+        Random random = new Random();
+		try
+		{
+			while(true)
+			{
+				int key = random.nextInt(keys) + 1;
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+            	int j = random.nextInt(columns) + 1;
+            	readLoadColumn(tablename_, stringKey, columnFamilyColumn_ + ":" + columnFix_ + j);
+				if ( requestsPerSecond_ > 1000)
+					Thread.sleep(0, 1000000000/requestsPerSecond_);
+				else
+					Thread.sleep(1000/requestsPerSecond_);
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+	}
+
+	public void randomWriteColumnThrift(int keys, int columns, int size, int tps)
+	{
+        Random random = new Random();
+        byte[] bytes = new byte[size];
+		int ts = 1;
+		try
+		{
+			while(true)
+			{
+				int key = random.nextInt(keys) + 1;
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+            	int j = random.nextInt(columns) + 1;
+                random.nextBytes(bytes);
+                if ( ts == Integer.MAX_VALUE)
+                {
+                	ts = 0 ;
+                }
+                ts++;
+	            applyThrift(tablename_, stringKey, columnFamilyColumn_ + ":" + columnFix_ + j, bytes, ts);
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+	}
+	
+	public void randomReadSuperColumnThrift(int keys, int superColumns, int columns, int size, int tps)
+	{
+        Random random = new Random();
+		try
+		{
+			while(true)
+			{
+				int key = random.nextInt(keys) + 1;
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+            	int i = random.nextInt(superColumns) + 1;
+            	int j = random.nextInt(columns) + 1;
+            	readLoadColumn(tablename_, stringKey, columnFamilySuperColumn_ + ":" + superColumnFix_ + i + ":" + columnFix_ + j);
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+	}
+
+	
+	public void randomWriteSuperColumnThrift(int keys, int superColumns,int columns, int size, int tps)
+	{
+        Random random = new Random();
+        byte[] bytes = new byte[size];
+		int ts = 1;
+		try
+		{
+			while(true)
+			{
+				int key = random.nextInt(keys) + 1;
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+            	int i = random.nextInt(superColumns) + 1;
+            	int j = random.nextInt(columns) + 1;
+                random.nextBytes(bytes);
+                if ( ts == Integer.MAX_VALUE)
+                {
+                	ts = 0 ;
+                }
+                ts++;
+	            applyThrift(tablename_, stringKey, columnFamilySuperColumn_ + ":" + superColumnFix_ + i + ":" + columnFix_ + j, bytes, ts);
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+
+	
+	}
+
+	public void bulkWriteColumnThrift(int keys, int columns, int size, int tps)
+	{
+        Random random = new Random();
+        byte[] bytes = new byte[size];
+		int ts = 1;
+		long time = System.currentTimeMillis();
+		try
+		{
+			for(int key = 1; key <= keys ; key++)
+			{
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+	            batch_mutation_t bt = new batch_mutation_t();
+	            bt.key = stringKey;
+	            bt.table = tablename_;
+	            bt.cfmap = new HashMap<String,List<column_t>>();
+	            ArrayList<column_t> column_arr = new ArrayList<column_t>();
+	            for( int j = 1; j <= columns ; j++)
+	            {
+	                random.nextBytes(bytes);
+	                column_arr.add(new column_t(columnFix_ + j, bytes, ts));
+	            }
+	            bt.cfmap.put(columnFamilyColumn_, column_arr);
+	            apply(bt);
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+		System.out.println(System.currentTimeMillis() - time);
+	}
+	
+	public void bulkWriteSuperColumnThrift(int keys, int supercolumns, int columns, int size, int tps)
+	{
+        Random random = new Random();
+        byte[] bytes = new byte[size];
+		int ts = 1;
+		long time = System.currentTimeMillis();
+		try
+		{
+			for(int key = 1; key <= keys ; key++)
+			{
+	            String stringKey = new Integer(key).toString();
+	            stringKey = stringKey + keyFix_ ;
+	            batch_mutation_super_t bt = new batch_mutation_super_t();
+	            bt.key = stringKey;
+	            bt.table = tablename_;
+	            bt.cfmap = new HashMap<String,List<superColumn_t>>();
+	            ArrayList<superColumn_t> superColumn_arr = new ArrayList<superColumn_t>();
+	            
+	            for( int i = 1; i <= supercolumns; i++ )
+	            {
+		            ArrayList<column_t> column_arr = new ArrayList<column_t>();
+		            for( int j = 1; j <= columns ; j++)
+		            {
+		                random.nextBytes(bytes);
+		                column_arr.add(new column_t(columnFix_ + j, bytes, ts));
+		            }
+	            	superColumn_arr.add(new superColumn_t(superColumnFix_ + i, column_arr));	
+	            }
+	            bt.cfmap.put(columnFamilySuperColumn_, superColumn_arr);
+	            apply(bt);
+			}
+		}
+		catch(Exception ex)
+		{
+			ex.printStackTrace();
+		}
+		System.out.println(System.currentTimeMillis() - time);
+	}
+	
+	public void testCommitLog() throws Throwable
+	{
+        Random random = new Random(System.currentTimeMillis());
+    	byte[] bytes = new byte[4096];
+    	random.nextBytes(bytes);
+    	byte[] bytes1 = new byte[64];
+    	random.nextBytes(bytes1);
+    	peerstorageClient_ = connect();
+    	int t = 0 ;
+    	while( true )
+    	{
+	    	int key = random.nextInt();
+	    	int threadId = random.nextInt();
+	    	int word = random.nextInt();
+			peerstorageClient_.insert("Mailbox", Integer.toString(key), "MailboxMailList0:" + Integer.toString(threadId), bytes1, t++);
+			peerstorageClient_.insert("Mailbox", Integer.toString(key), "MailboxThreadList0:" + Integer.toString(word) + ":" + Integer.toString(threadId), bytes, t++);
+			peerstorageClient_.insert("Mailbox", Integer.toString(key), "MailboxUserList0:"+ Integer.toString(word) + ":" + Integer.toString(threadId), bytes, t++);
+    	}
+	}
+
+	JSAPResult ParseArguments(String[] args)
+	{
+        JSAPResult config = null;    
+		try
+		{
+			
+	        SimpleJSAP jsap = new SimpleJSAP( 
+	                "StressTest", 
+	                "Runs stress test for Cassandra",
+	                new Parameter[] {
+	                    new FlaggedOption( "keys", JSAP.INTEGER_PARSER, "10000", JSAP.REQUIRED, 'k', JSAP.NO_LONGFLAG, 
+	                        "The number of keys from 1 to this number" ),
+	                    new FlaggedOption( "columns", JSAP.INTEGER_PARSER, "1000", JSAP.REQUIRED, 'c', JSAP.NO_LONGFLAG, 
+	                        "The number of columns from 1 to this number" ),
+	                    new FlaggedOption( "supercolumns", JSAP.INTEGER_PARSER, "0", JSAP.NOT_REQUIRED, 'u', JSAP.NO_LONGFLAG, 
+	                        "The number of super columns from 1 to this number" ),
+	                    new FlaggedOption( "size", JSAP.INTEGER_PARSER, "1000", JSAP.REQUIRED, 's', JSAP.NO_LONGFLAG, 
+	                        "The Size in bytes of each column" ),
+	                    new FlaggedOption( "tps", JSAP.INTEGER_PARSER, "1000", JSAP.REQUIRED, 't', JSAP.NO_LONGFLAG, 
+	                        "Requests per second" ),
+	                    new FlaggedOption( "thrift", JSAP.INTEGER_PARSER, "0", JSAP.REQUIRED, 'h', JSAP.NO_LONGFLAG, 
+	                        "Use Thrift - 1 , use messaging - 0" ),
+	                    new FlaggedOption( "mailboxstress", JSAP.INTEGER_PARSER, "0", JSAP.REQUIRED, 'M', JSAP.NO_LONGFLAG, 
+	                        "Run mailbox stress  - 1 , hmm default - 0" ),
+	                    new FlaggedOption( "commitLogTest", JSAP.INTEGER_PARSER, "0", JSAP.REQUIRED, 'C', JSAP.NO_LONGFLAG, 
+	                        "Run mailbox stress  - 1 , hmm default - 0" ),
+	                    new QualifiedSwitch( "randomize", JSAP.STRING_PARSER, JSAP.NO_DEFAULT, JSAP.NOT_REQUIRED, 'z', "randomize", 
+	                        "Random reads or writes" ).setList( true ).setListSeparator( ',' ),
+	                    new QualifiedSwitch( "reads", JSAP.STRING_PARSER, JSAP.NO_DEFAULT, JSAP.NOT_REQUIRED, 'r', "reads", 
+	                        "Read data" ).setList( true ).setListSeparator( ',' ),
+	                    new QualifiedSwitch( "writes", JSAP.STRING_PARSER, JSAP.NO_DEFAULT, JSAP.NOT_REQUIRED, 'w', "writes", 
+	                        "Write Data" ).setList( false ).setListSeparator( ',' ),
+	                    new QualifiedSwitch( "bulkwrites", JSAP.STRING_PARSER, JSAP.NO_DEFAULT, JSAP.NOT_REQUIRED, 'b', "bulkwrites", 
+	                        "Bulk Write Data" ).setList( false ).setListSeparator( ',' ),
+                        new UnflaggedOption( "Server", JSAP.STRING_PARSER, JSAP.REQUIRED, "Name of the server the request needs to be sent to." ) }
+	            )	;
+	        
+	            
+	        	config = jsap.parse(args);    
+	    		if( !config.success())
+	    		{
+	                System.err.println();
+	                System.err.println("Usage: java "
+	                                    + StressTest.class.getName());
+	                System.err.println("                "
+	                                    + jsap.getUsage());
+	                System.err.println();
+	                // show full help as well
+	                System.err.println(jsap.getHelp());	                
+	                System.err.println("**********Errors*************");
+	    		}
+	            if ( jsap.messagePrinted() ) return null;		
+	    		String hostName = FBUtilities.getHostAddress();
+	    		from_ = new EndPoint(hostName,10001);
+	    		MessagingService.getMessagingInstance().listen(from_, false);
+		}
+		catch ( Exception ex) 
+		{
+			logger_.debug(LogUtil.throwableToString(ex));
+		}
+        return config;
+	}
+	
+	void run( JSAPResult config ) throws Throwable
+	{
+		requestsPerSecond_ = config.getInt("tps");
+		int numThreads = requestsPerSecond_/1000 + 1;
+		if(config.getString("Server") != null)
+		{
+			server_ = config.getString("Server");
+			to_ = new EndPoint(config.getString("Server"), 7000);
+		}
+		runner_ = new DebuggableThreadPoolExecutor( numThreads,
+				numThreads,
+	            Integer.MAX_VALUE,
+	            TimeUnit.SECONDS,
+	            new LinkedBlockingQueue<Runnable>(),
+	            new ThreadFactoryImpl("MEMTABLE-FLUSHER-POOL")
+	            );  
+		if(config.getInt("mailboxstress") == 1)
+		{
+//			stressMailboxWrites();
+			return;
+		}
+		if(config.getInt("commitLogTest") == 1)
+		{
+			testCommitLog();
+			return;
+		}
+		if(config.getInt("thrift") == 0)
+		{
+			if(config.getInt("supercolumns") == 0)
+			{
+				if(config.getBoolean("reads"))
+				{
+					randomReadColumn(config.getInt("keys"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				if(config.getBoolean("bulkwrites"))
+				{
+					bulkWriteColumn(config.getInt("keys"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				if(config.getBoolean("writes"))
+				{
+					randomWriteColumn(config.getInt("keys"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+			}
+			else
+			{
+				if(config.getBoolean("reads"))
+				{
+					randomReadSuperColumn(config.getInt("keys"), config.getInt("supercolumns"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				if(config.getBoolean("bulkwrites"))
+				{
+					bulkWriteSuperColumn(config.getInt("keys"), config.getInt("supercolumns"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				if(config.getBoolean("writes"))
+				{
+					randomWriteSuperColumn(config.getInt("keys"), config.getInt("supercolumns"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				
+			}
+		}
+		else
+		{
+			peerstorageClient_ = connect();
+			if(config.getInt("supercolumns") == 0)
+			{
+				if(config.getBoolean("reads"))
+				{
+					randomReadColumnThrift(config.getInt("keys"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				if(config.getBoolean("bulkwrites"))
+				{
+					bulkWriteColumnThrift(config.getInt("keys"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				if(config.getBoolean("writes"))
+				{
+					randomWriteColumnThrift(config.getInt("keys"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+			}
+			else
+			{
+				if(config.getBoolean("reads"))
+				{
+					randomReadSuperColumnThrift(config.getInt("keys"), config.getInt("supercolumns"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				if(config.getBoolean("bulkwrites"))
+				{
+					bulkWriteSuperColumnThrift(config.getInt("keys"), config.getInt("supercolumns"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				if(config.getBoolean("writes"))
+				{
+					randomWriteSuperColumnThrift(config.getInt("keys"), config.getInt("supercolumns"), config.getInt("columns"), config.getInt("size"), config.getInt("tps"));
+					return;
+				}
+				
+			}
+			
+		}
+		System.out.println(" StressTest : Done !!!!!!");
+	}
+	
+	/**
+	 * @param args
+	 */
+	public static void main(String[] args) throws Throwable
+	{
+		LogUtil.init();
+  		StressTest stressTest = new StressTest();
+		JSAPResult config = stressTest.ParseArguments( args );
+		if( config == null ) System.exit(-1);
+		stressTest.run(config);
+	}
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/TestChoice.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/TestChoice.java
index e69de29b..ec0402e6 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/TestChoice.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/TestChoice.java
@@ -0,0 +1,110 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.test;
+
+import java.net.UnknownHostException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.locator.EndPointSnitch;
+import org.apache.cassandra.locator.IEndPointSnitch;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+public class TestChoice
+{
+    private static final Logger logger_ = Logger.getLogger(TestChoice.class);
+    private Set<EndPoint> allNodes_;
+    private Map<EndPoint, List<EndPoint>> nodeToReplicaMap_ = new HashMap<EndPoint, List<EndPoint>>();
+    
+    public TestChoice(Set<EndPoint> allNodes)
+    {
+        allNodes_ = new HashSet<EndPoint>(allNodes);
+    }
+    
+    public void assignReplicas()
+    {
+        IEndPointSnitch snitch = new EndPointSnitch();
+        Set<EndPoint> allNodes = new HashSet<EndPoint>(allNodes_);
+        Map<EndPoint, Integer> nOccurences = new HashMap<EndPoint, Integer>();
+        
+        for ( EndPoint node : allNodes_ )
+        {
+            nOccurences.put(node, 1);
+        }
+        
+        for ( EndPoint node : allNodes_ )
+        {
+            allNodes.remove(node);
+            for ( EndPoint choice : allNodes )
+            {
+                List<EndPoint> replicasChosen = nodeToReplicaMap_.get(node);
+                if ( replicasChosen == null || replicasChosen.size() < DatabaseDescriptor.getReplicationFactor() - 1 )
+                {
+                    try
+                    {
+                        if ( !snitch.isInSameDataCenter(node, choice) )
+                        {
+                            if ( replicasChosen == null )
+                            {
+                                replicasChosen = new ArrayList<EndPoint>();
+                                nodeToReplicaMap_.put(node, replicasChosen);
+                            }
+                            int nOccurence = nOccurences.get(choice);
+                            if ( nOccurence < DatabaseDescriptor.getReplicationFactor() )
+                            {
+                                nOccurences.put(choice, ++nOccurence);
+                                replicasChosen.add(choice);
+                            }                            
+                        }
+                    }
+                    catch ( UnknownHostException ex )
+                    {
+                        ex.printStackTrace();
+                    }
+                }
+                else
+                {
+                    allNodes.add(node);
+                    break;
+                }
+            }
+        }
+        
+        
+        Set<EndPoint> nodes = nodeToReplicaMap_.keySet();
+        for ( EndPoint node : nodes )
+        {
+            List<EndPoint> replicas = nodeToReplicaMap_.get(node);
+            StringBuilder sb = new StringBuilder("");
+            for ( EndPoint replica : replicas )
+            {
+                sb.append(replica);
+                sb.append(", ");
+            }
+            System.out.println(node + " ---> " + sb.toString() );
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/TestRunner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/TestRunner.java
index e69de29b..f96a2e7f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/TestRunner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/TestRunner.java
@@ -0,0 +1,270 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.test;
+
+import java.util.Collection;
+import java.util.Random;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.cassandra.continuations.Suspendable;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.io.IndexHelper;
+import org.apache.cassandra.io.BufferedRandomAccessFile;
+import org.apache.cassandra.io.DataInputBuffer;
+
+
+import org.apache.log4j.Logger;
+
+
+public class TestRunner
+{
+    private static EndPoint to_ = new EndPoint("tdsearch001.sf2p.facebook.com", 7000);
+    
+    private static void doWrite() throws Throwable
+    {
+        Table table = Table.open("Mailbox");  
+        Random random = new Random();
+        int totalUsed = 0;
+        int[] used = new int[16*1024];
+        byte[] bytes = new byte[4*1024];
+        for (int i = 0; i < 1; ++i)
+        {
+            String key = Integer.toString(i);
+            RowMutation rm = new RowMutation("Mailbox", key);
+            random.nextBytes(bytes);
+            while ( totalUsed != 16*1024 )
+            {
+                int j = random.nextInt(16*1024);                
+                if ( used[j] == 0 )
+                {
+                    used[j] = 1;
+                    ++totalUsed;
+                }             
+                // rm.add("Test:Column-" + j, bytes, System.currentTimeMillis());
+                
+                for ( int k = 0; k < 1; ++k )
+                {                             
+                    rm.add("MailboxMailData0:SuperColumn-" + j + ":Column-" + k, bytes, k);                    
+                }
+            }
+            rm.apply();            
+        }
+        System.out.println("Write done");
+    }
+    
+    private static void doRead() throws Throwable
+    {
+        Table table = Table.open("Mailbox");
+        String key = "511055962";
+                
+        /*
+        List<String> list = new ArrayList<String>();
+        list.add("SuperColumn-0");
+        Row row = table.getRow(key, "MailboxMailList0", list);
+        System.out.println(row);
+        */
+        
+        List<String> list = new ArrayList<String>();
+        list.add("SuperColumn-0");
+        list.add("SuperColumn-189");
+        list.add("SuperColumn-23");
+        Row row = table.getRow("0", "MailboxMailData0", list);
+        try
+        {
+            ColumnFamily cf = row.getColumnFamily("MailboxMailData0");
+            Collection<IColumn> columns = cf.getAllColumns();            
+            for ( IColumn column : columns )
+            {                
+                System.out.println(column.name());                
+                Collection<IColumn> subColumns = column.getSubColumns();
+                for ( IColumn subColumn : subColumns )
+                {
+                    System.out.println(subColumn);
+                }                            
+            }
+        }
+        catch ( Throwable th )
+        {
+            th.printStackTrace();
+        }
+    }
+    
+    private static void doCheck() throws Throwable
+    {
+        BufferedRandomAccessFile fis = new BufferedRandomAccessFile("C:\\Engagements\\buff.dat", "r", 4*1024*1024);
+        IndexHelper.skipBloomFilterAndIndex(fis);
+        byte[] bytes = new byte[(int)(fis.length() - fis.getFilePointer())];
+        fis.readFully(bytes);
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(bytes, bytes.length);
+        
+        ColumnFamily cf = ColumnFamily.serializer().deserialize(bufIn);        
+        Collection<IColumn> columns = cf.getAllColumns();       
+        System.out.println(columns.size());
+        for ( IColumn column : columns )
+        {
+            System.out.println(column.name());
+        }
+        fis.close();
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {  
+        // System.out.println( lastIndexOf("ababcbc", "abc") );
+        /*
+        String name = "/var/cassandra/test.dat";
+        FileInputStream f = new FileInputStream(name);
+        File file = new File("/var/cassandra");
+        Path path = file.toPath();
+        WatchService watcher = FileSystems.getDefault().newWatchService(); 
+        Thread thread = new Thread( new WatchKeyMonitor(watcher) );
+        thread.start();
+        
+        WatchKey wKey = path.register( watcher, StandardWatchEventKind.ENTRY_DELETE );          
+        file = new File(name);
+        file.delete();
+        
+        Thread.sleep(3000);
+        System.out.println("Closing the stream ...");
+        f.close();
+        */
+           
+        //LogUtil.init();
+        //StorageService s = StorageService.instance();
+        //s.start();   
+        // doRead();
+        // doWrite();
+        // doCheck();
+        // doBuffered();
+        
+        /*
+        FileOutputStream fos = new FileOutputStream("C:\\Engagements\\Test.dat", true);
+        SequentialScanner scanner = new SequentialScanner("Mailbox");            
+        int count = 0;
+        while ( scanner.hasNext() )
+        {
+            Row row = scanner.next();  
+            String value = row.key() + System.getProperty("line.separator");
+            fos.write( value.getBytes() );
+           
+            Map<String, ColumnFamily> cfs = row.getColumnFamilyMap();
+            Set<String> keys = cfs.keySet();
+            
+            for ( String key : keys )
+            {
+                System.out.println(row.getColumnFamily(key));
+            }           
+        }          
+        fos.close();
+        System.out.println("Done ...");
+        */
+        /*
+        ExecutorService es = new DebuggableThreadPoolExecutor(1, 1, Integer.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(), new ThreadFactoryImpl("TEST"));
+        es.execute(new TestImpl());
+        */
+        /*
+        LogUtil.init();
+        StorageService s = StorageService.instance();
+        s.start();
+        */
+        /*
+        ReadMessage readMessage = new ReadMessage("Mailbox", args[1], "Test");
+        Message message = ReadMessage.makeReadMessage(readMessage);
+        Runnable task = new MessageDeliveryTask(message);
+               
+        ExecutorService es = new ContinuationsExecutor(1, 1, Integer.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(), new ThreadFactoryImpl("TEST"));
+        int end = Integer.parseInt(args[0]);
+        for ( int i = 0; i < end; ++i )
+        {
+            es.execute(task);
+        }
+        */
+        
+        /*
+        if ( args[0].equals("S") )
+        {  
+            ExecutorService es = new ContinuationsExecutor(1, 1, Integer.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>() );               
+            es.execute( new Scanner() );
+        }
+        */
+        /*
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        String value = "Avinash Lakshman";
+        for ( int i = 0; i < 100; ++i )
+        {
+            bufOut.writeUTF(Integer.toString(i));
+            bufOut.writeInt(value.length());
+            bufOut.write(value.getBytes());                        
+        }
+        
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(bufOut.getData(), bufOut.getLength());
+        DataOutputBuffer buffer = new DataOutputBuffer();        
+        IFileWriter writer = SequenceFile.aioWriter("C:\\Engagements\\test.dat", 64*1024);
+        SortedMap<String, Integer> offsets = getOffsets(bufIn);
+        Set<String> keys = offsets.keySet();                                                      
+        for ( String key : keys )
+        {
+            bufIn.setPosition(offsets.get(key));
+            buffer.reset();
+            buffer.write(bufIn, bufIn.readInt());
+            writer.append(key, buffer);            
+        }
+        writer.close();
+        */
+    }
+}
+
+@Suspendable
+class Scanner implements Runnable
+{   
+    private static final Logger logger_ = Logger.getLogger(Scanner.class);
+    
+    public void run()
+    {        
+/*        try
+        {            
+            SequentialScanner scanner = new SequentialScanner("Mailbox");            
+            
+            while ( scanner.hasNext() )
+            {
+                Row row = scanner.next();    
+                logger_.debug(row.key());
+            }            
+        }
+        catch ( IOException ex )
+        {
+            ex.printStackTrace();
+        }        
+        */
+    }
+}
+
+class Test 
+{
+    public static void goo()
+    {
+        System.out.println("I am goo()");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/UtilsTest.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/UtilsTest.java
index e69de29b..bb06032f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/UtilsTest.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/test/UtilsTest.java
@@ -0,0 +1,49 @@
+package org.apache.cassandra.test;
+
+import java.math.BigInteger;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.FastHashMap;
+import org.apache.cassandra.utils.GuidGenerator;
+
+public class UtilsTest 
+{
+	private static void doHashPerf() throws Throwable
+	{
+		List<BigInteger> list = new ArrayList<BigInteger>();
+		for ( int i = 0; i < 100; ++i )
+		{
+			String guid = GuidGenerator.guid();
+			list.add( FBUtilities.hash(guid) );
+		}
+		Collections.sort(list);
+		
+		int startValue = 1000000;
+		
+		while ( true )
+		{
+			long start = System.currentTimeMillis();
+			for ( int i = 0; i < 1024; ++i )
+			{
+				String key = Integer.toString(startValue + i);
+				BigInteger hash = FBUtilities.hash(key);
+				Collections.binarySearch(list, hash);
+			}
+			System.out.println("TIME TAKEN: " + (System.currentTimeMillis() - start));
+			Thread.sleep(100);
+		}
+	}
+	
+	public static void main(String[] args) throws Throwable
+	{
+	}
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/AdminTool.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/AdminTool.java
index e69de29b..26bddd78 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/AdminTool.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/AdminTool.java
@@ -0,0 +1,213 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.RowMutationMessage;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.BasicUtilities;
+import org.apache.cassandra.utils.LogUtil;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class AdminTool
+{
+
+	String server_ = null;
+	String tableName_ = "Mailbox";
+	String key_ = "Random";
+	String cf1_ = "MailboxThreadList0";
+	String cf2_ = "MailboxUserList0";
+	String cf3_ = "MailboxMailList0";
+	String cf4_ = "MailboxMailData0";
+//	String cf5_ = "MailboxUserList";
+	public static EndPoint from_ = new EndPoint("hadoop071.sf2p.facebook.com", 10001);
+	private static final String[] servers_ =
+	{
+		"insearch001.sf2p.facebook.com",
+		"insearch002.sf2p.facebook.com",
+		"insearch003.sf2p.facebook.com",
+		"insearch004.sf2p.facebook.com",
+		"insearch005.sf2p.facebook.com",
+		"insearch016.sf2p.facebook.com",
+		"insearch007.sf2p.facebook.com",
+		"insearch008.sf2p.facebook.com",
+		"insearch009.sf2p.facebook.com",
+		"insearch010.sf2p.facebook.com",
+		"insearch011.sf2p.facebook.com",
+		"insearch012.sf2p.facebook.com",
+		"insearch013.sf2p.facebook.com",
+		"insearch014.sf2p.facebook.com",
+		"insearch015.sf2p.facebook.com",
+		"insearch016.sf2p.facebook.com",
+		"insearch017.sf2p.facebook.com",
+		"insearch018.sf2p.facebook.com",
+		"insearch019.sf2p.facebook.com",
+		"insearch020.sf2p.facebook.com",
+		"insearch021.sf2p.facebook.com",
+		"insearch022.sf2p.facebook.com",
+		"insearch023.sf2p.facebook.com",
+		"insearch024.sf2p.facebook.com",
+		"insearch025.sf2p.facebook.com",
+		"insearch026.sf2p.facebook.com",
+		"insearch027.sf2p.facebook.com",
+		"insearch028.sf2p.facebook.com",
+		"insearch029.sf2p.facebook.com",
+		"insearch030.sf2p.facebook.com",
+		"insearch031.sf2p.facebook.com",
+		"insearch032.sf2p.facebook.com",
+		"insearch033.sf2p.facebook.com",
+		"insearch034.sf2p.facebook.com",
+		"insearch035.sf2p.facebook.com",
+		"insearch036.sf2p.facebook.com",
+		"insearch037.sf2p.facebook.com",
+		"insearch038.sf2p.facebook.com",
+		"insearch039.sf2p.facebook.com",
+		"insearch040.sf2p.facebook.com",
+
+		"insearch001.ash1.facebook.com",
+		"insearch002.ash1.facebook.com",
+		"insearch003.ash1.facebook.com",
+		"insearch004.ash1.facebook.com",
+		"insearch005.ash1.facebook.com",
+		"insearch016.ash1.facebook.com",
+		"insearch007.ash1.facebook.com",
+		"insearch008.ash1.facebook.com",
+		"insearch009.ash1.facebook.com",
+		"insearch010.ash1.facebook.com",
+		"insearch011.ash1.facebook.com",
+		"insearch012.ash1.facebook.com",
+		"insearch013.ash1.facebook.com",
+		"insearch014.ash1.facebook.com",
+		"insearch015.ash1.facebook.com",
+		"insearch016.ash1.facebook.com",
+		"insearch017.ash1.facebook.com",
+		"insearch018.ash1.facebook.com",
+		"insearch019.ash1.facebook.com",
+		"insearch020.ash1.facebook.com",
+		"insearch021.ash1.facebook.com",
+		"insearch022.ash1.facebook.com",
+		"insearch023.ash1.facebook.com",
+		"insearch024.ash1.facebook.com",
+		"insearch025.ash1.facebook.com",
+		"insearch026.ash1.facebook.com",
+		"insearch027.ash1.facebook.com",
+		"insearch028.ash1.facebook.com",
+		"insearch029.ash1.facebook.com",
+		"insearch030.ash1.facebook.com",
+		"insearch031.ash1.facebook.com",
+		"insearch032.ash1.facebook.com",
+		"insearch033.ash1.facebook.com",
+		"insearch034.ash1.facebook.com",
+		"insearch035.ash1.facebook.com",
+		"insearch036.ash1.facebook.com",
+		"insearch037.ash1.facebook.com",
+		"insearch038.ash1.facebook.com",
+		"insearch039.ash1.facebook.com",
+		"insearch040.ash1.facebook.com",
+	};
+
+	AdminTool()
+	{
+		server_ = null;
+	}
+
+	AdminTool(String server)
+	{
+		server_ = server;
+	}
+
+	public void run(int operation, String columnFamilyName, long skip) throws Throwable
+	{
+        byte[] bytes =  BasicUtilities.longToByteArray( skip );
+        RowMutation rm = new RowMutation(tableName_, key_);
+        if( columnFamilyName == null )
+        {
+			rm.add(Table.recycleBin_ + ":" + cf1_, bytes, operation);
+			rm.add(Table.recycleBin_ + ":" + cf2_, bytes, operation);
+			rm.add(Table.recycleBin_ + ":" + cf3_, bytes, operation);
+			rm.add(Table.recycleBin_ + ":" + cf4_, bytes, operation);
+//			rm.add(Table.recycleBin_ + ":" + cf5_, bytes, operation);
+        }
+        else
+        {
+			rm.add(Table.recycleBin_ + ":" + columnFamilyName, bytes, operation);
+        }
+		RowMutationMessage rmMsg = new RowMutationMessage(rm);
+        if( server_ != null)
+        {
+            Message message = RowMutationMessage.makeRowMutationMessage(rmMsg, StorageService.binaryVerbHandler_);
+	        EndPoint to = new EndPoint(server_, 7000);
+			MessagingService.getMessagingInstance().sendOneWay(message, to);
+        }
+        else
+        {
+        	for( String server : servers_ )
+        	{
+                Message message = RowMutationMessage.makeRowMutationMessage(rmMsg, StorageService.binaryVerbHandler_);
+		        EndPoint to = new EndPoint(server, 7000);
+				MessagingService.getMessagingInstance().sendOneWay(message, to);
+        	}
+        }
+	}
+
+
+	/**
+	 * @param args
+	 */
+	public static void main(String[] args) throws Throwable
+	{
+		LogUtil.init();
+		AdminTool postLoad = null;
+		int operation = 1;
+		String columnFamilyName = null;
+		long skip = 0L;
+		if(args.length < 1 )
+		{
+			System.out.println("Usage: PostLoad <serverName>  < operation 1- flushBinary 2 - compactions 3- flush> <ColumnFamilyName> <skip factor for compactions> or  PostLoad <-all> <operation> <ColumnFamilyName> <skip factor for compactions>");
+		}
+		if(args[0].equals("-all"))
+		{
+			 postLoad = new AdminTool();
+		}
+		else
+		{
+			 postLoad = new AdminTool(args[0]);
+		}
+		if(args.length > 1 )
+			operation = Integer.parseInt(args[1]);
+		if(args.length > 2 )
+			columnFamilyName = args[2];
+		if(args.length > 3 )
+			skip = Long.parseLong(args[3]);
+		postLoad.run(operation, columnFamilyName, skip);
+
+		Thread.sleep(10000);
+		System.out.println("Exiting app...");
+		System.exit(0);
+	}
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/IndexBuilder.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/IndexBuilder.java
index e69de29b..226506c3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/IndexBuilder.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/IndexBuilder.java
@@ -0,0 +1,167 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.IOException;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.IFileWriter;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.SequenceFile;
+import org.apache.cassandra.utils.BasicUtilities;
+import org.apache.cassandra.utils.BloomFilter;
+
+public class IndexBuilder
+{
+    private static final int bufferSize_ = 64*1024;
+
+    public static void main(String[] args)
+    {
+        if ( args.length != 1 )
+        {
+            System.out.println("Usage : java com.facebook.infrastructure.tools.IndexBuilder <full path to the data file>");
+            System.exit(1);
+        }
+
+        try
+        {
+	        int blockCount = getBlockCount(args[0]);
+	        System.out.println("Number of keys in the data file : " + (blockCount + 1)*SSTable.indexInterval());
+	        buildIndex(args[0], blockCount);
+        }
+        catch(Throwable t)
+        {
+        	System.err.println("Exception: " + t.getMessage());
+        	t.printStackTrace(System.err);
+        }
+    }
+
+    private static int getBlockCount(String dataFile) throws IOException
+    {
+        IFileReader dataReader = SequenceFile.bufferedReader(dataFile, bufferSize_);
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        DataInputBuffer bufIn = new DataInputBuffer();
+        int blockCount = 0;
+
+        try
+        {
+            while ( !dataReader.isEOF() )
+            {
+                bufOut.reset();
+                dataReader.next(bufOut);
+                bufIn.reset(bufOut.getData(), bufOut.getLength());
+                /* Key just read */
+                String key = bufIn.readUTF();
+                if ( key.equals(SSTable.blockIndexKey_) )
+                {
+                    ++blockCount;
+                }
+            }
+        }
+        finally
+        {
+            dataReader.close();
+        }
+        return blockCount;
+    }
+
+    private static void buildIndex(String dataFile, int blockCount) throws IOException
+    {
+        String indexFile = dataFile.replace("-Data.", "-Index.");
+        final int bufferSize = 64*1024;
+
+        IFileWriter indexWriter = SequenceFile.bufferedWriter(indexFile, bufferSize);
+        IFileReader dataReader = SequenceFile.bufferedReader(dataFile, bufferSize);
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        DataInputBuffer bufIn = new DataInputBuffer();
+        /* BloomFilter of all data in the data file */
+        BloomFilter bf = new BloomFilter((SSTable.indexInterval() + 1)*blockCount, 8);
+
+        try
+        {
+            while ( !dataReader.isEOF() )
+            {
+                bufOut.reset();
+                /* Record the position of the key. */
+                long blockIndexOffset = dataReader.getCurrentPosition();
+                dataReader.next(bufOut);
+                bufIn.reset(bufOut.getData(), bufOut.getLength());
+                /* Key just read */
+                String key = bufIn.readUTF();
+                if ( key.equals(SSTable.blockIndexKey_) )
+                {
+                    /* Ignore the size of the data associated with the block index */
+                    bufIn.readInt();
+                    /* Number of keys in the block. */
+                    int blockSize = bufIn.readInt();
+                    /* Largest key in the block */
+                    String largestKey = null;
+
+                    /*
+                     * Read the keys in this block and find the largest key in
+                     * this block. This is the key that gets written into the
+                     * index file.
+                    */
+                    for ( int i = 0; i < blockSize; ++i )
+                    {
+                        String currentKey = bufIn.readUTF();
+                        bf.add(currentKey);
+                        if ( largestKey == null )
+                        {
+                            largestKey = currentKey;
+                        }
+                        else
+                        {
+                            if ( currentKey.compareTo(largestKey) > 0 )
+                            {
+                                /* record this key */
+                                largestKey = currentKey;
+                            }
+                        }
+                        /* read the position of the key and the size of key data and throws it away. */
+                        bufIn.readLong();
+                        bufIn.readLong();
+                    }
+
+                    /*
+                     * Write into the index file the largest key in the block
+                     * and the offset of the block index in the data file.
+                    */
+                    indexWriter.append(largestKey, BasicUtilities.longToByteArray(blockIndexOffset));
+                }
+            }
+        }
+        finally
+        {
+            dataReader.close();
+            /* Cache the bloom filter */
+            SSTable.storeBloomFilter(dataFile, bf);
+            /* Write the bloom filter into the index file */
+            bufOut.reset();
+            BloomFilter.serializer().serialize(bf, bufOut);
+            byte[] bytes = new byte[bufOut.getLength()];
+            System.arraycopy(bufOut.getData(), 0, bytes, 0, bytes.length);
+            indexWriter.close(bytes, bytes.length);
+            bufOut.close();
+        }
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/KeyChecker.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/KeyChecker.java
index e69de29b..76a1bd71 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/KeyChecker.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/KeyChecker.java
@@ -0,0 +1,99 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.BufferedReader;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+import java.io.RandomAccessFile;
+
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.LogUtil;
+
+
+public class KeyChecker
+{
+    private static final int bufSize_ = 128*1024*1024;
+    /*
+     * This function checks if the local storage endpoint 
+     * is reponsible for storing this key .
+     */
+    private static boolean checkIfProcessKey(String key)
+    {
+        EndPoint[] endPoints = StorageService.instance().getNStorageEndPoint(key);
+        EndPoint localEndPoint = StorageService.getLocalStorageEndPoint();
+        for(EndPoint endPoint : endPoints)
+        {
+            if(endPoint.equals(localEndPoint))
+                return true;
+        }
+        return false;
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {
+        if ( args.length != 1 )
+        {
+            System.out.println("Usage : java com.facebook.infrastructure.tools.KeyChecker <file containing all keys>");
+            System.exit(1);
+        }
+        
+        LogUtil.init();
+        StorageService s = StorageService.instance();
+        s.start();
+        
+        /* Sleep for proper discovery */
+        Thread.sleep(240000);
+        /* Create the file for the missing keys */
+        RandomAccessFile raf = new RandomAccessFile( "Missing-" + FBUtilities.getHostAddress() + ".dat", "rw");
+        
+        /* Start reading the file that contains the keys */
+        BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(args[0]) ), KeyChecker.bufSize_ );
+        String key = null;
+        boolean bStarted = false;
+        
+        while ( ( key = bufReader.readLine() ) != null )
+        {            
+            if ( !bStarted )
+            {
+                bStarted = true;
+                System.out.println("Started the processing of the file ...");
+            }
+            
+            key = key.trim();
+            if ( StorageService.instance().isPrimary(key) )
+            {
+                System.out.println("Processing key " + key);
+                Row row = Table.open("Mailbox").getRow(key, "MailboxMailList0");
+                if ( row.isEmpty() )
+                {
+                    System.out.println("MISSING KEY : " + key);
+                    raf.write(key.getBytes());
+                    raf.write(System.getProperty("line.separator").getBytes());
+                }
+            }
+        }
+        System.out.println("DONE checking keys ...");
+        raf.close();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/KeyExtracter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/KeyExtracter.java
index e69de29b..cc7e59af 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/KeyExtracter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/KeyExtracter.java
@@ -0,0 +1,111 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.DataOutputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.RandomAccessFile;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.SequenceFile;
+import org.apache.cassandra.io.SSTable.KeyPositionInfo;
+import org.apache.cassandra.utils.BasicUtilities;
+
+
+public class KeyExtracter
+{
+    private static final int bufferSize_ = 64*1024;
+
+    public static void main(String[] args) throws Throwable
+    {
+        if ( args.length != 3 )
+        {
+            System.out.println("Usage : java com.facebook.infrastructure.tools.IndexBuilder <key to extract> <data file> <output file>");
+            System.exit(1);
+        }
+		String keyToExtract = args[0];
+		String dataFile = args[1];
+		String outputFile = args[2];
+
+        extractKeyIntoFile(keyToExtract, dataFile, outputFile);
+    }
+
+    public static boolean extractKeyIntoFile(String keyToExtract, String dataFile, String outputFile) throws IOException
+    {
+		IFileReader dataReader = SequenceFile.bufferedReader(dataFile, bufferSize_);
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        DataInputBuffer bufIn = new DataInputBuffer();
+
+    	try
+    	{
+            while ( !dataReader.isEOF() )
+            {
+                bufOut.reset();
+                dataReader.next(bufOut);
+                bufIn.reset(bufOut.getData(), bufOut.getLength());
+                /* Key just read */
+                String key = bufIn.readUTF();
+                /* check if we want this key */
+                if ( key.equals(keyToExtract) )
+                {
+                	int keySize = bufIn.readInt();
+                	byte[] keyData = new byte[keySize];
+                	bufIn.read(keyData, 0, keySize);
+
+                	/* write the key data into a file */
+                    RandomAccessFile raf = new RandomAccessFile(outputFile, "rw");                	
+                	raf.writeUTF(key);
+                	raf.writeInt(keySize);
+                	raf.write(keyData);
+                    dumpBlockIndex(keyToExtract, 0L, keySize, raf);
+                    raf.close();
+                    return true;
+                }
+            }
+        }
+        finally
+        {
+            dataReader.close();
+        }
+
+        return false;
+    }
+    
+    private static void dumpBlockIndex(String key, long position, long size, RandomAccessFile raf) throws IOException
+    {
+        DataOutputBuffer bufOut = new DataOutputBuffer();                       
+        /* Number of keys in this block */
+        bufOut.writeInt(1);
+        bufOut.writeUTF(key);
+        bufOut.writeLong(position);
+        bufOut.writeLong(size);
+        
+        /* Write out the block index. */
+        raf.writeUTF(SSTable.blockIndexKey_);
+        raf.writeInt(bufOut.getLength());
+        raf.write(bufOut.getData(), 0, bufOut.getLength());
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/MembershipCleaner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/MembershipCleaner.java
index e69de29b..cfe62d45 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/MembershipCleaner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/MembershipCleaner.java
@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.BufferedReader;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.Serializable;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class MembershipCleaner
+{
+    private static final int port_ = 7000;
+    private static final long waitTime_ = 10000;
+    
+    public static void main(String[] args) throws Throwable
+    {
+        if ( args.length != 3 )
+        {
+            System.out.println("Usage : java com.facebook.infrastructure.tools.MembershipCleaner " +
+                    "<ip:port to send the message> " +
+                    "<node which needs to be removed> " +
+                    "<file containing all nodes in the cluster>");
+            System.exit(1);
+        }
+        
+        String ipPort = args[0];
+        String node = args[1];
+        String file = args[2];
+        
+        String[] ipPortPair = ipPort.split(":");
+        EndPoint target = new EndPoint(ipPortPair[0], Integer.valueOf(ipPortPair[1]));
+        MembershipCleanerMessage mcMessage = new MembershipCleanerMessage(node);
+        
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        MembershipCleanerMessage.serializer().serialize(mcMessage, dos);
+        /* Construct the token update message to be sent */
+        Message mbrshipCleanerMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.mbrshipCleanerVerbHandler_, new Object[]{bos.toByteArray()} );
+        
+        BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ) );
+        String line = null;
+       
+        while ( ( line = bufReader.readLine() ) != null )
+        {            
+            mbrshipCleanerMessage.addHeader(line, line.getBytes());
+        }
+        
+        System.out.println("Sending a membership clean message to " + target);
+        MessagingService.getMessagingInstance().sendOneWay(mbrshipCleanerMessage, target);
+        Thread.sleep(MembershipCleaner.waitTime_);
+        System.out.println("Done sending the update message");
+    }
+    
+    public static class MembershipCleanerMessage implements Serializable
+    {
+        private static ICompactSerializer<MembershipCleanerMessage> serializer_;
+        private static AtomicInteger idGen_ = new AtomicInteger(0);
+        
+        static
+        {
+            serializer_ = new MembershipCleanerMessageSerializer();            
+        }
+        
+        static ICompactSerializer<MembershipCleanerMessage> serializer()
+        {
+            return serializer_;
+        }
+
+        private String target_;
+        
+        MembershipCleanerMessage(String target)
+        {
+            target_ = target;        
+        }
+        
+        String getTarget()
+        {
+            return target_;
+        }
+    }
+    
+    public static class MembershipCleanerMessageSerializer implements ICompactSerializer<MembershipCleanerMessage>
+    {
+        public void serialize(MembershipCleanerMessage mcMessage, DataOutputStream dos) throws IOException
+        {            
+            dos.writeUTF(mcMessage.getTarget() );                      
+        }
+        
+        public MembershipCleanerMessage deserialize(DataInputStream dis) throws IOException
+        {            
+            return new MembershipCleanerMessage(dis.readUTF());
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java
index e69de29b..5b20ddcd 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java
@@ -0,0 +1,85 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MembershipCleanerVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(MembershipCleanerVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        byte[] body = (byte[])message.getMessageBody()[0];
+        
+        try
+        {
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+            /* Deserialize to get the token for this endpoint. */
+            MembershipCleaner.MembershipCleanerMessage mcMessage = MembershipCleaner.MembershipCleanerMessage.serializer().deserialize(bufIn);
+            
+            String target = mcMessage.getTarget();
+            logger_.info("Removing the node [" + target + "] from membership");
+            EndPoint targetEndPoint = new EndPoint(target, DatabaseDescriptor.getControlPort());
+            /* Remove the token related information for this endpoint */
+            StorageService.instance().removeTokenState(targetEndPoint);
+            
+            /* Get the headers for this message */
+            Map<String, byte[]> headers = message.getHeaders();
+            headers.remove( StorageService.getLocalStorageEndPoint().getHost() );
+            logger_.debug("Number of nodes in the header " + headers.size());
+            Set<String> nodes = headers.keySet();
+            
+            for ( String node : nodes )
+            {            
+                logger_.debug("Processing node " + node);
+                byte[] bytes = headers.remove(node);
+                /* Send a message to this node to alter its membership state. */
+                EndPoint targetNode = new EndPoint(node, DatabaseDescriptor.getStoragePort());                
+                
+                logger_.debug("Sending a membership clean message to " + targetNode);
+                MessagingService.getMessagingInstance().sendOneWay(message, targetNode);
+                break;
+            }                        
+        }
+        catch( IOException ex )
+        {
+            logger_.debug(LogUtil.throwableToString(ex));
+        }
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/ThreadListBuilder.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/ThreadListBuilder.java
index e69de29b..5f81b55e 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/ThreadListBuilder.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/ThreadListBuilder.java
@@ -0,0 +1,94 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+import java.io.RandomAccessFile;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.utils.BloomFilter;
+
+
+public class ThreadListBuilder
+{
+    private final static int bufSize_ = 64*1024*1024;
+    private final static int count_ = 128*1024*1024;
+    
+    public static void main(String[] args) throws Throwable
+    {
+        if ( args.length != 2 )
+        {
+            System.out.println("Usage : java com.facebook.infrastructure.tools.ThreadListBuilder <directory containing files to be processed> <directory to dump the bloom filter in.>");
+            System.exit(1);
+        }
+        
+        File directory = new File(args[0]);
+        File[] files = directory.listFiles();
+        List<DataOutputBuffer> buffers = new ArrayList<DataOutputBuffer>();    
+        BloomFilter bf = new BloomFilter(count_, 8);        
+        int keyCount = 0;
+        
+        /* Process the list of files. */
+        for ( File file : files )
+        {
+            System.out.println("Processing file " + file);
+            BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ), ThreadListBuilder.bufSize_ );
+            String line = null;
+            
+            while ( (line = bufReader.readLine()) != null )
+            {
+                /* After accumulating count_ keys reset the bloom filter. */
+                if ( keyCount > 0 && keyCount % count_ == 0 )
+                {                       
+                    DataOutputBuffer bufOut = new DataOutputBuffer();
+                    BloomFilter.serializer().serialize(bf, bufOut);
+                    System.out.println("Finished serializing the bloom filter");
+                    buffers.add(bufOut);
+                    bf = new BloomFilter(count_, 8);
+                }
+                line = line.trim();                
+                bf.add(line);
+                ++keyCount;
+            }
+        }
+        
+        /* Add the bloom filter assuming the last one was left out */
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        BloomFilter.serializer().serialize(bf, bufOut);
+        buffers.add(bufOut);
+        
+        
+        int size = buffers.size();
+        for ( int i = 0; i < size; ++i )
+        {
+            DataOutputBuffer buffer = buffers.get(i);
+            String file = args[1] + System.getProperty("file.separator") + "Bloom-Filter-" + i + ".dat";
+            RandomAccessFile raf = new RandomAccessFile(file, "rw");
+            raf.write(buffer.getData(), 0, buffer.getLength());
+            raf.close();
+            buffer.close();
+        }
+        System.out.println("Done writing the bloom filter to disk");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/TokenUpdateVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/TokenUpdateVerbHandler.java
index e69de29b..37e41057 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/TokenUpdateVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/TokenUpdateVerbHandler.java
@@ -0,0 +1,94 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TokenUpdateVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(TokenUpdateVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+    	byte[] body = (byte[])message.getMessageBody()[0];
+        
+        try
+        {
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+            /* Deserialize to get the token for this endpoint. */
+            Token token = Token.serializer().deserialize(bufIn);
+
+            logger_.info("Updating the token to [" + token + "]");
+            StorageService.instance().updateToken(token);
+            
+            /* Get the headers for this message */
+            Map<String, byte[]> headers = message.getHeaders();
+            headers.remove( StorageService.getLocalStorageEndPoint().getHost() );
+            logger_.debug("Number of nodes in the header " + headers.size());
+            Set<String> nodes = headers.keySet();
+            
+            IPartitioner p = StorageService.getPartitioner();
+            for ( String node : nodes )
+            {            
+                logger_.debug("Processing node " + node);
+                byte[] bytes = headers.remove(node);
+                /* Send a message to this node to update its token to the one retreived. */
+                EndPoint target = new EndPoint(node, DatabaseDescriptor.getStoragePort());
+                token = p.getTokenFactory().fromByteArray(bytes);
+                
+                /* Reset the new Message */
+                ByteArrayOutputStream bos = new ByteArrayOutputStream();
+                DataOutputStream dos = new DataOutputStream(bos);
+                Token.serializer().serialize(token, dos);
+                message.setMessageBody(new Object[]{bos.toByteArray()});
+                
+                logger_.debug("Sending a token update message to " + target + " to update it to " + token);
+                MessagingService.getMessagingInstance().sendOneWay(message, target);
+                break;
+            }                        
+        }
+    	catch( IOException ex )
+    	{
+    		logger_.debug(LogUtil.throwableToString(ex));
+    	}
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/TokenUpdater.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/TokenUpdater.java
index e69de29b..a6089fc9 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/TokenUpdater.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/TokenUpdater.java
@@ -0,0 +1,80 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.BufferedReader;
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class TokenUpdater
+{
+    private static final int port_ = 7000;
+    private static final long waitTime_ = 10000;
+    
+    public static void main(String[] args) throws Throwable
+    {
+        if ( args.length != 3 )
+        {
+            System.out.println("Usage : java com.facebook.infrastructure.tools.TokenUpdater <ip:port> <token> <file containing node token info>");
+            System.exit(1);
+        }
+        
+        String ipPort = args[0];
+        IPartitioner p = StorageService.getPartitioner();
+        Token token = p.getTokenFactory().fromString(args[1]);
+        String file = args[2];
+        
+        String[] ipPortPair = ipPort.split(":");
+        EndPoint target = new EndPoint(ipPortPair[0], Integer.valueOf(ipPortPair[1]));
+
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        Token.serializer().serialize(token, dos);
+
+        /* Construct the token update message to be sent */
+        Message tokenUpdateMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.tokenVerbHandler_, new Object[]{bos.toByteArray()} );
+        
+        BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ) );
+        String line = null;
+       
+        while ( ( line = bufReader.readLine() ) != null )
+        {
+            String[] nodeTokenPair = line.split(" ");
+            /* Add the node and the token pair into the header of this message. */
+            Token nodeToken = p.getTokenFactory().fromString(nodeTokenPair[1]);
+            tokenUpdateMessage.addHeader(nodeTokenPair[0], p.getTokenFactory().toByteArray(nodeToken));
+        }
+        
+        System.out.println("Sending a token update message to " + target);
+        MessagingService.getMessagingInstance().sendOneWay(tokenUpdateMessage, target);
+        Thread.sleep(TokenUpdater.waitTime_);
+        System.out.println("Done sending the update message");
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BasicUtilities.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BasicUtilities.java
index e69de29b..604ff263 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BasicUtilities.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BasicUtilities.java
@@ -0,0 +1,71 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.nio.ByteBuffer;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+
+public class BasicUtilities
+{        
+	public static byte[] longToByteArray(long arg)
+	{      
+        byte[] retVal = new byte[8];
+        ByteBuffer bb= ByteBuffer.wrap(retVal);
+        bb.putLong(arg);
+        return retVal; 
+	 }
+	
+	public static long byteArrayToLong(byte[] arg)
+	{
+		ByteBuffer bb= ByteBuffer.wrap(arg);
+		return bb.getLong();
+	}
+	
+	public static byte[] intToByteArray(int arg)
+	{      
+        byte[] retVal = new byte[4];
+        ByteBuffer bb= ByteBuffer.wrap(retVal);
+        bb.putInt(arg);
+        return retVal; 
+	 }
+	
+	public static int byteArrayToInt(byte[] arg)
+	{
+		ByteBuffer bb= ByteBuffer.wrap(arg);
+		return bb.getInt();
+	}
+	
+	public static byte[] shortToByteArray(short arg)
+	{      
+        byte[] retVal = new byte[2];
+        ByteBuffer bb= ByteBuffer.wrap(retVal);
+        bb.putShort(arg);
+        return retVal; 
+	 }
+	
+	public static short byteArrayToShort(byte[] arg)
+	{
+		ByteBuffer bb= ByteBuffer.wrap(arg);
+		return bb.getShort();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BitSetSerializer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BitSetSerializer.java
index e69de29b..5449df38 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BitSetSerializer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BitSetSerializer.java
@@ -0,0 +1,33 @@
+package org.apache.cassandra.utils;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.DataInputStream;
+import java.io.ObjectOutputStream;
+import java.io.ObjectInputStream;
+import java.util.BitSet;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+class BitSetSerializer
+{
+    public static void serialize(BitSet bs, DataOutputStream dos) throws IOException
+    {
+        ObjectOutputStream oos = new ObjectOutputStream(dos);
+        oos.writeObject(bs);
+        oos.flush();
+    }
+
+    public static BitSet deserialize(DataInputStream dis) throws IOException
+    {
+        ObjectInputStream ois = new ObjectInputStream(dis);
+        try
+        {
+            return (BitSet) ois.readObject();
+        }
+        catch (ClassNotFoundException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BloomCalculations.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BloomCalculations.java
index e69de29b..22e91e66 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BloomCalculations.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BloomCalculations.java
@@ -0,0 +1,132 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+/**
+ * The following calculations are taken from:
+ * http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html
+ * "Bloom Filters - the math"
+ *
+ * This class's static methods are meant to facilitate the use of the Bloom
+ * Filter class by helping to choose correct values of 'bits per element' and
+ * 'number of hash functions, k'.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class BloomCalculations {
+
+    private static final int maxBuckets = 15;
+    private static final int minBuckets = 2;
+    private static final int minK = 1;
+    private static final int maxK = 8;
+    private static final int[] optKPerBuckets =
+            new int[]{1, // dummy K for 0 buckets per element
+                      1, // dummy K for 1 buckets per element
+                      1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 8, 8, 8, 8};
+
+    /**
+     * In the following table, the row 'i' shows false positive rates if i buckets
+     * per element are used.  Column 'j' shows false positive rates if j hash
+     * functions are used.  The first row is 'i=0', the first column is 'j=0'.
+     * Each cell (i,j) the false positive rate determined by using i buckets per
+     * element and j hash functions.
+     */
+    static final double[][] probs = new double[][]{
+        {1.0}, // dummy row representing 0 buckets per element
+        {1.0, 1.0}, // dummy row representing 1 buckets per element
+        {1.0, 0.393,  0.400},
+        {1.0, 0.283,  0.237,  0.253},
+        {1.0, 0.221,  0.155,  0.147,   0.160},
+        {1.0, 0.181,  0.109,  0.092,   0.092,   0.101}, // 5
+        {1.0, 0.154,  0.0804, 0.0609,  0.0561,  0.0578,  0.0638},
+        {1.0, 0.133,  0.0618, 0.0423,  0.0359,  0.0347,  0.0364},
+        {1.0, 0.118,  0.0489, 0.0306,  0.024,   0.0217,  0.0216,  0.0229},
+        {1.0, 0.105,  0.0397, 0.0228,  0.0166,  0.0141,  0.0133,  0.0135,  0.0145}, // 9
+        {1.0, 0.0952, 0.0329, 0.0174,  0.0118,  0.00943, 0.00844, 0.00819, 0.00846},
+        {1.0, 0.0869, 0.0276, 0.0136,  0.00864, 0.0065,  0.00552, 0.00513, 0.00509},
+        {1.0, 0.08,   0.0236, 0.0108,  0.00646, 0.00459, 0.00371, 0.00329, 0.00314},
+        {1.0, 0.074,  0.0203, 0.00875, 0.00492, 0.00332, 0.00255, 0.00217, 0.00199},
+        {1.0, 0.0689, 0.0177, 0.00718, 0.00381, 0.00244, 0.00179, 0.00146, 0.00129},
+        {1.0, 0.0645, 0.0156, 0.00596, 0.003,   0.00183, 0.00128, 0.001,   0.000852} // 15
+    };  // the first column is a dummy column representing K=0.
+
+    /**
+     * Given the number of buckets that can be used per element, return the optimal
+     * number of hash functions in order to minimize the false positive rate.
+     *
+     * @param bucketsPerElement
+     * @return The number of hash functions that minimize the false positive rate.
+     */
+    public static int computeBestK(int bucketsPerElement){
+        assert bucketsPerElement >= 0;
+        if(bucketsPerElement >= optKPerBuckets.length)
+            return optKPerBuckets[optKPerBuckets.length-1];
+        return optKPerBuckets[bucketsPerElement];
+    }
+
+    /**
+     * A wrapper class that holds two key parameters for a Bloom Filter: the
+     * number of hash functions used, and the number of buckets per element used.
+     */
+    public static final class BloomSpecification {
+        final int K; // number of hash functions.
+        final int bucketsPerElement;
+
+        public BloomSpecification(int k, int bucketsPerElement) {
+            K = k;
+            this.bucketsPerElement = bucketsPerElement;
+        }
+    }
+
+    /**
+     * Given a maximum tolerable false positive probability, compute a Bloom
+     * specification which will give less than the specified false positive rate,
+     * but minimize the number of buckets per element and the number of hash
+     * functions used.  Because bandwidth (and therefore total bitvector size)
+     * is considered more expensive than computing power, preference is given
+     * to minimizing buckets per element rather than number of hash funtions.
+     *
+     * @param maxFalsePosProb The maximum tolerable false positive rate.
+     * @return A Bloom Specification which would result in a false positive rate
+     * less than specified by the function call.
+     */
+    public static BloomSpecification computeBucketsAndK(double maxFalsePosProb){
+        // Handle the trivial cases
+        if(maxFalsePosProb >= probs[minBuckets][minK]) {
+            return new BloomSpecification(2, optKPerBuckets[2]);
+        }
+        if(maxFalsePosProb < probs[maxBuckets][maxK]) {
+            return new BloomSpecification(maxK, maxBuckets);
+        }
+
+        // First find the minimal required number of buckets:
+        int bucketsPerElement = 2;
+        int K = optKPerBuckets[2];
+        while(probs[bucketsPerElement][K] > maxFalsePosProb){
+            bucketsPerElement++;
+            K = optKPerBuckets[bucketsPerElement];
+        }
+        // Now that the number of buckets is sufficient, see if we can relax K
+        // without losing too much precision.
+        while(probs[bucketsPerElement][K - 1] <= maxFalsePosProb){
+            K--;
+        }
+
+        return new BloomSpecification(K, bucketsPerElement);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BloomFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BloomFilter.java
index e69de29b..b786b560 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BloomFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/BloomFilter.java
@@ -0,0 +1,140 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.BitSet;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+public class BloomFilter extends Filter
+{
+    static ICompactSerializer<BloomFilter> serializer_ = new BloomFilterSerializer();
+
+    public static ICompactSerializer<BloomFilter> serializer()
+    {
+        return serializer_;
+    }
+
+    private BitSet filter_;
+
+    public BloomFilter(int numElements, int bucketsPerElement)
+    {
+        this(BloomCalculations.computeBestK(bucketsPerElement), new BitSet(numElements * bucketsPerElement + 20));
+    }
+
+    public BloomFilter(int numElements, double maxFalsePosProbability)
+    {
+        BloomCalculations.BloomSpecification spec = BloomCalculations
+                .computeBucketsAndK(maxFalsePosProbability);
+        filter_ = new BitSet(numElements * spec.bucketsPerElement + 20);
+        hashCount = spec.K;
+    }
+
+    /*
+     * This version is only used by the deserializer.
+     */
+    BloomFilter(int hashes, BitSet filter)
+    {
+        hashCount = hashes;
+        filter_ = filter;
+    }
+
+    public void clear()
+    {
+        filter_.clear();
+    }
+
+    int buckets()
+    {
+        return filter_.size();
+    }
+
+    BitSet filter()
+    {
+        return filter_;
+    }
+
+    public boolean isPresent(String key)
+    {
+        for (int bucketIndex : getHashBuckets(key))
+        {
+            if (!filter_.get(bucketIndex))
+            {
+                return false;
+            }
+        }
+        return true;
+    }
+
+    /*
+     param@ key -- value whose hash is used to fill
+     the filter_.
+     This is a general purpose API.
+     */
+    public void add(String key)
+    {
+        for (int bucketIndex : getHashBuckets(key))
+        {
+            filter_.set(bucketIndex);
+        }
+    }
+
+    public String toString()
+    {
+        return filter_.toString();
+    }
+
+    ICompactSerializer tserializer()
+    {
+        return serializer_;
+    }
+
+    int emptyBuckets()
+    {
+        int n = 0;
+        for (int i = 0; i < buckets(); i++)
+        {
+            if (!filter_.get(i))
+            {
+                n++;
+            }
+        }
+        return n;
+    }
+}
+
+class BloomFilterSerializer implements ICompactSerializer<BloomFilter>
+{
+    public void serialize(BloomFilter bf, DataOutputStream dos)
+            throws IOException
+    {
+        dos.writeInt(bf.getHashCount());
+        BitSetSerializer.serialize(bf.filter(), dos);
+    }
+
+    public BloomFilter deserialize(DataInputStream dis) throws IOException
+    {
+        int hashes = dis.readInt();
+        BitSet bs = BitSetSerializer.deserialize(dis);
+        return new BloomFilter(hashes, bs);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Cachetable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Cachetable.java
index e69de29b..fce1e8e3 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Cachetable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Cachetable.java
@@ -0,0 +1,218 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.util.*;
+
+import org.apache.log4j.Logger;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Cachetable<K,V> implements ICachetable<K,V>
+{
+    private class CacheableObject<V>
+    {
+        private V value_;
+        private long age_;
+        
+        CacheableObject(V o)
+        {
+            value_ = o;
+            age_ = System.currentTimeMillis();
+        }
+
+        public boolean equals(Object o)
+        {
+            return value_.equals(o);
+        }
+
+        public int hashCode()
+        {
+            return value_.hashCode();
+        }
+
+        V getValue()
+        {
+            return value_;
+        }           
+        
+        boolean isReadyToDie(long expiration)
+        {
+            return ( (System.currentTimeMillis() - age_) > expiration );
+        }
+    }
+    
+    private class CacheMonitor extends TimerTask
+    {
+        private long expiration_;
+        
+        CacheMonitor(long expiration)
+        {
+            expiration_ = expiration;
+        }
+        
+        public void run()
+        {  
+            Map<K,V> expungedValues = new HashMap<K,V>();            
+            synchronized(cache_)
+            {
+                Enumeration<K> e = cache_.keys();
+                while( e.hasMoreElements() )
+                {        
+                    K key = e.nextElement();
+                    CacheableObject<V> co = cache_.get(key);
+                    if ( co != null && co.isReadyToDie(expiration_) )
+                    {
+                        V v = co.getValue();
+                        if(null != v)
+                            expungedValues.put(key, v);
+                        cache_.remove(key);                                       
+                    }
+                }
+            }
+            
+            /* Calling the hooks on the keys that have been expunged */
+            Set<K> keys = expungedValues.keySet();                                               
+            for ( K key : keys )
+            {                                
+                V value = expungedValues.get(key);
+                ICacheExpungeHook<K,V> hook = hooks_.remove(key);
+                try 
+                {
+                    if ( hook != null )
+                    {
+                        hook.callMe(key, value);                    
+                    }
+                    else if ( globalHook_ != null )
+                    {
+                        globalHook_.callMe(key, value);
+                    }
+                }
+                catch(Throwable e)
+                {
+                    logger_.info(LogUtil.throwableToString(e));
+                }
+            }
+            expungedValues.clear();
+        }
+    }   
+
+    private ICacheExpungeHook<K,V> globalHook_;
+    private Hashtable<K, CacheableObject<V>> cache_;
+    private Map<K, ICacheExpungeHook<K,V>> hooks_;
+    private Timer timer_;
+    private static int counter_ = 0;
+    private static Logger logger_ = Logger.getLogger(Cachetable.class);
+
+    private void init(long expiration)
+    {
+        if ( expiration <= 0 )
+            throw new IllegalArgumentException("Argument specified must be a positive number");
+
+        cache_ = new Hashtable<K, CacheableObject<V>>();
+        hooks_ = new Hashtable<K, ICacheExpungeHook<K,V>>();
+        timer_ = new Timer("CACHETABLE-TIMER-" + (++counter_), true);        
+        timer_.schedule(new CacheMonitor(expiration), expiration, expiration);
+    }
+    
+    /*
+     * Specify the TTL for objects in the cache
+     * in milliseconds.
+     */
+    public Cachetable(long expiration)
+    {
+        init(expiration);   
+    }    
+    
+    /*
+     * Specify the TTL for objects in the cache
+     * in milliseconds and a global expunge hook. If
+     * a key has a key-specific hook installed invoke that
+     * instead.
+     */
+    public Cachetable(long expiration, ICacheExpungeHook<K,V> global)
+    {
+        init(expiration);
+        globalHook_ = global;        
+    }
+    
+    public void shutdown()
+    {
+        timer_.cancel();
+    }
+    
+    public void put(K key, V value)
+    {        
+        cache_.put(key, new CacheableObject<V>(value));
+    }
+
+    public void put(K key, V value, ICacheExpungeHook<K,V> hook)
+    {
+        put(key, value);
+        hooks_.put(key, hook);
+    }
+
+    public V get(K key)
+    {
+        V result = null;
+        CacheableObject<V> co = cache_.get(key);
+        if ( co != null )
+        {
+            result = co.getValue();
+        }
+        return result;
+    }
+
+    public V remove(K key)
+    {
+        CacheableObject<V> co = cache_.remove(key);
+        V result = null;
+        if ( co != null )
+        {
+            result = co.getValue();
+        }
+        return result;
+    }
+
+    public int size()
+    {
+        return cache_.size();
+    }
+
+    public boolean containsKey(K key)
+    {
+        return cache_.containsKey(key);
+    }
+
+    public boolean containsValue(V value)
+    {
+        return cache_.containsValue( new CacheableObject<V>(value) );
+    }
+
+    public boolean isEmpty()
+    {
+        return cache_.isEmpty();
+    }
+
+    public Set<K> keySet()
+    {
+        return cache_.keySet();
+    }    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/DestructivePQIterator.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/DestructivePQIterator.java
index e69de29b..0ed96f83 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/DestructivePQIterator.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/DestructivePQIterator.java
@@ -0,0 +1,25 @@
+package org.apache.cassandra.utils;
+
+import java.util.Iterator;
+import java.util.PriorityQueue;
+
+public class DestructivePQIterator<T> implements Iterator<T> {
+    private PriorityQueue<T> pq;
+
+    public DestructivePQIterator(PriorityQueue<T> pq) {
+        this.pq = pq;
+    }
+
+    public boolean hasNext() {
+        return pq.size() > 0;
+    }
+
+    public T next() {
+        return pq.poll();
+    }
+
+    public void remove() {
+        throw new UnsupportedOperationException();
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
index e69de29b..ce67bd16 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
@@ -0,0 +1,376 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.io.PrintWriter;
+import java.io.StringWriter;
+import java.io.UnsupportedEncodingException;
+import java.math.BigInteger;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.security.MessageDigest;
+import java.text.DateFormat;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.StringTokenizer;
+import java.util.zip.DataFormatException;
+import java.util.zip.Deflater;
+import java.util.zip.Inflater;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class FBUtilities
+{
+
+    private static InetAddress localInetAddress_;
+
+    public static String getTimestamp()
+    {
+        Date date = new Date();
+        DateFormat df  = new SimpleDateFormat("MM/dd/yyyy HH:mm:ss");
+        return df.format(date);
+    }
+    
+    public static String getTimestamp(long value)
+    {
+        Date date = new Date(value);
+        DateFormat df  = new SimpleDateFormat("MM/dd/yyyy HH:mm:ss");
+        return df.format(date);
+    }
+
+    public static int getBits(int x, int p, int n)
+    {
+         return ( x >>> (p + 1 - n)) & ~(~0 << n);
+    }
+
+    public static String getCurrentThreadStackTrace()
+    {
+        Throwable throwable = new Throwable();
+        StackTraceElement[] ste = throwable.getStackTrace();
+        StringBuffer sbuf = new StringBuffer();
+
+        for ( int i = ste.length - 1; i > 0; --i )
+        {
+            sbuf.append(ste[i].getClassName() + "." + ste[i].getMethodName());
+            sbuf.append("/");
+        }
+        sbuf.deleteCharAt(sbuf.length() - 1);
+        return sbuf.toString();
+    }
+
+    public static String[] strip(String string, String token)
+    {
+        StringTokenizer st = new StringTokenizer(string, token);
+        List<String> result = new ArrayList<String>();
+        while ( st.hasMoreTokens() )
+        {
+            result.add( (String)st.nextElement() );
+        }
+        return result.toArray( new String[0] );
+    }
+
+    public static byte[] serializeToStream(Object o)
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        byte[] bytes = new byte[0];
+        try
+        {
+    		ObjectOutputStream oos = new ObjectOutputStream(bos);
+            oos.writeObject(o);
+            oos.flush();
+    		bytes = bos.toByteArray();
+    		oos.close();
+    		bos.close();
+        }
+        catch ( IOException e )
+        {
+            LogUtil.getLogger(FBUtilities.class.getName()).info( LogUtil.throwableToString(e) );
+        }
+		return bytes;
+    }
+
+    public static Object deserializeFromStream(byte[] bytes)
+    {
+        Object o = null;
+		ByteArrayInputStream bis = new ByteArrayInputStream(bytes);
+        try
+        {
+    		ObjectInputStream ois = new ObjectInputStream(bis);
+            try
+            {
+    		    o = ois.readObject();
+            }
+            catch ( ClassNotFoundException e )
+            {
+            }
+    		ois.close();
+    		bis.close();
+        }
+        catch ( IOException e )
+        {
+            LogUtil.getLogger(FBUtilities.class.getName()).info( LogUtil.throwableToString(e) );
+        }
+		return o;
+    }
+
+    public static InetAddress getLocalAddress() throws UnknownHostException
+    {
+	if ( localInetAddress_ == null )
+		localInetAddress_ = InetAddress.getLocalHost();
+        return localInetAddress_;
+    }
+
+    public static String getHostAddress() throws UnknownHostException
+    {
+        InetAddress inetAddr = getLocalAddress();
+        if (DatabaseDescriptor.getListenAddress() != null)
+        {
+            inetAddr = InetAddress.getByName(DatabaseDescriptor.getListenAddress());
+        }
+        return inetAddr.getHostAddress();
+    }
+
+    public static boolean isHostLocalHost(InetAddress host)
+    {
+        try {
+            return getLocalAddress().equals(host);
+        }
+        catch ( UnknownHostException e )
+        {
+            return false;
+        }
+    }
+
+    public static byte[] toByteArray(int i)
+    {
+        byte[] bytes = new byte[4];
+        bytes[0] = (byte)( ( i >>> 24 ) & 0xFF);
+        bytes[1] = (byte)( ( i >>> 16 ) & 0xFF);
+        bytes[2] = (byte)( ( i >>> 8 ) & 0xFF);
+        bytes[3] = (byte)( i & 0xFF );
+        return bytes;
+    }
+
+    public static int byteArrayToInt(byte[] bytes)
+    {
+    	return byteArrayToInt(bytes, 0);
+    }
+
+    public static int byteArrayToInt(byte[] bytes, int offset)
+    {
+        if ( bytes.length - offset < 4 )
+        {
+            throw new IllegalArgumentException("An integer must be 4 bytes in size.");
+        }
+        int n = 0;
+        for ( int i = 0; i < 4; ++i )
+        {
+            n <<= 8;
+            n |= bytes[offset + i] & 0xFF;
+        }
+        return n;
+    }
+
+    public static boolean isEqualBits(byte[] bytes1, byte[] bytes2)
+    {
+        return 0 == compareByteArrays(bytes1, bytes2);
+    }
+
+    public static int compareByteArrays(byte[] bytes1, byte[] bytes2){
+        if(null == bytes1){
+            if(null == bytes2) return 0;
+            else return -1;
+        }
+        if(null == bytes2) return 1;
+
+        for(int i = 0; i < bytes1.length && i < bytes2.length; i++){
+            int cmp = compareBytes(bytes1[i], bytes2[i]);
+            if(0 != cmp) return cmp;
+        }
+        if(bytes1.length == bytes2.length) return 0;
+        else return (bytes1.length < bytes2.length)? -1 : 1;
+    }
+
+    public static int compareBytes(byte b1, byte b2){
+        return compareBytes((int)b1, (int)b2);
+    }
+
+    public static int compareBytes(int b1, int b2){
+        int i1 = b1 & 0xFF;
+        int i2 = b2 & 0xFF;
+        if(i1 < i2) return -1;
+        else if(i1 == i2) return 0;
+        else return 1;
+    }
+
+    public static String stackTrace(Throwable e)
+    {
+        StringWriter sw = new StringWriter();
+        PrintWriter pw = new PrintWriter( sw );
+        e.printStackTrace(pw);
+        return sw.toString();
+    }
+
+    public static BigInteger hash(String data)
+    {
+        byte[] result = hash(HashingSchemes.MD5, data.getBytes());
+        BigInteger hash = new BigInteger(result);
+        return hash.abs();        
+    }
+
+    public static byte[] hash(String type, byte[] data)
+    {
+    	byte[] result = null;
+    	try
+        {
+    		MessageDigest messageDigest = MessageDigest.getInstance(type);
+    		result = messageDigest.digest(data);
+    	}
+    	catch (Exception e)
+        {
+    		LogUtil.getLogger(FBUtilities.class.getName()).debug(LogUtil.throwableToString(e));
+    	}
+    	return result;
+	}
+
+    public static boolean isEqual(byte[] digestA, byte[] digestB)
+    {
+        return MessageDigest.isEqual(digestA, digestB);
+    }
+
+    // The given bytearray is compressed onto the specifed stream.
+    // The method does not close the stream. The caller will have to do it.
+    public static void compressToStream(byte[] input, ByteArrayOutputStream bos) throws IOException
+    {
+    	 // Create the compressor with highest level of compression
+        Deflater compressor = new Deflater();
+        compressor.setLevel(Deflater.BEST_COMPRESSION);
+
+        // Give the compressor the data to compress
+        compressor.setInput(input);
+        compressor.finish();
+
+        // Write the compressed data to the stream
+        byte[] buf = new byte[1024];
+        while (!compressor.finished())
+        {
+            int count = compressor.deflate(buf);
+            bos.write(buf, 0, count);
+        }
+    }
+
+
+    public static byte[] compress(byte[] input) throws IOException
+    {
+        // Create an expandable byte array to hold the compressed data.
+        // You cannot use an array that's the same size as the orginal because
+        // there is no guarantee that the compressed data will be smaller than
+        // the uncompressed data.
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(input.length);
+        compressToStream(input,bos);
+        bos.close();
+
+        // Get the compressed data
+        return bos.toByteArray();
+    }
+
+
+    public static byte[] decompress(byte[] compressedData, int off, int len) throws IOException, DataFormatException
+    {
+    	 // Create the decompressor and give it the data to compress
+        Inflater decompressor = new Inflater();
+        decompressor.setInput(compressedData, off, len);
+
+        // Create an expandable byte array to hold the decompressed data
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(compressedData.length);
+
+        // Decompress the data
+        byte[] buf = new byte[1024];
+        while (!decompressor.finished())
+        {
+            int count = decompressor.inflate(buf);
+            bos.write(buf, 0, count);
+        }
+        bos.close();
+
+        // Get the decompressed data
+        return bos.toByteArray();
+    }
+
+    public static byte[] decompress(byte[] compressedData) throws IOException, DataFormatException
+    {
+    	return decompress(compressedData, 0, compressedData.length);
+    }
+
+     public static byte[] xor(byte[] b1, byte[] b2)
+     {
+    	 byte[] bLess = null;
+    	 byte[] bMore = null;
+
+    	 if(b1.length > b2.length)
+    	 {
+    		 bLess = b2;
+    		 bMore = b1;
+    	 }
+    	 else
+    	 {
+    		 bLess = b1;
+    		 bMore = b2;
+    	 }
+
+    	 for(int i = 0 ; i< bLess.length; i++ )
+    	 {
+    		 bMore[i] = (byte)(bMore[i] ^ bLess[i]);
+    	 }
+
+    	 return bMore;
+     }
+
+     public static int getUTF8Length(String string)
+     {
+     	/*
+     	 * We store the string as UTF-8 encoded, so when we calculate the length, it
+     	 * should be converted to UTF-8.
+     	 */
+     	String utfName  = string;
+     	int length = utfName.length();
+     	try
+     	{
+     		//utfName  = new String(string.getBytes("UTF-8"));
+     		length = string.getBytes("UTF-8").length;
+     	}
+     	catch (UnsupportedEncodingException e)
+     	{
+     		LogUtil.getLogger(FBUtilities.class.getName()).info(LogUtil.throwableToString(e));
+     	}
+
+     	return length;
+     }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastHash.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastHash.java
index e69de29b..11b0b60d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastHash.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastHash.java
@@ -0,0 +1,384 @@
+package org.apache.cassandra.utils;
+
+import java.util.Random;
+
+
+/**
+ * Base class for hashtables that use open addressing to resolve collisions.
+ * 
+ * @author Avinash Lakshman
+ * 
+ */
+
+abstract public class FastHash implements Cloneable
+{
+    /** the current number of occupied slots in the hash. */
+    protected transient int size_;
+    
+    /** the current number of free slots in the hash. */
+    protected transient int free_;
+    
+    /** the load above which rehashing occurs. */
+    protected static final float DEFAULT_LOAD_FACTOR = 0.5f;
+    
+    /**
+     * the default initial capacity for the hash table. This is one less than a
+     * prime value because one is added to it when searching for a prime
+     * capacity to account for the free slot required by open addressing. Thus,
+     * the real default capacity is 11.
+     */
+    protected static final int DEFAULT_INITIAL_CAPACITY = 10;
+    
+    /**
+     * Determines how full the internal table can become before rehashing is
+     * required. This must be a value in the range: 0.0 < loadFactor < 1.0. The
+     * default value is 0.5, which is about as large as you can get in open
+     * addressing without hurting performance. Cf. Knuth, Volume 3., Chapter 6.
+     */
+    protected float loadFactor_;
+    
+    /**
+     * The maximum number of elements allowed without allocating more space.
+     */
+    protected int maxSize_;
+    
+    /**
+     * The number of removes that should be performed before an auto-compaction
+     * occurs.
+     */
+    protected int autoCompactRemovesRemaining_;
+    
+    /**
+     * The auto-compaction factor for the table.
+     * 
+     * @see #setAutoCompactionFactor
+     */
+    protected float autoCompactionFactor_;
+    
+    /**
+     * @see
+     */
+    private boolean autoCompactTemporaryDisable_ = false;
+    
+    /**
+     * Creates a new <code>THash</code> instance with the default capacity and
+     * load factor.
+     */
+    public FastHash()
+    {
+        this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);
+    }
+    
+    /**
+     * Creates a new <code>THash</code> instance with a prime capacity at or
+     * near the specified capacity and with the default load factor.
+     * 
+     * @param initialCapacity
+     *            an <code>int</code> value
+     */
+    public FastHash(int initialCapacity)
+    {
+        this(initialCapacity, DEFAULT_LOAD_FACTOR);
+    }
+    
+    /**
+     * Creates a new <code>THash</code> instance with a prime capacity at or
+     * near the minimum needed to hold <tt>initialCapacity</tt> elements with
+     * load factor <tt>loadFactor</tt> without triggering a rehash.
+     * 
+     * @param initialCapacity
+     *            an <code>int</code> value
+     * @param loadFactor
+     *            a <code>float</code> value
+     */
+    public FastHash(int initialCapacity, float loadFactor)
+    {
+        super();
+        loadFactor_ = loadFactor;
+        
+        // Through testing, the load factor (especially the default load factor)
+        // has been
+        // found to be a pretty good starting auto-compaction factor.
+        autoCompactionFactor_ = loadFactor;
+        
+        setUp((int) Math.ceil(initialCapacity / loadFactor));
+    }
+    
+    public Object clone()
+    {
+        try
+        {
+            return super.clone();
+        }
+        catch (CloneNotSupportedException cnse)
+        {
+            return null; // it's supported
+        }
+    }
+    
+    /**
+     * Tells whether this set is currently holding any elements.
+     * 
+     * @return a <code>boolean</code> value
+     */
+    public boolean isEmpty()
+    {
+        return 0 == size_;
+    }
+    
+    /**
+     * Returns the number of distinct elements in this collection.
+     * 
+     * @return an <code>int</code> value
+     */
+    public int size()
+    {
+        return size_;
+    }
+    
+    /**
+     * @return the current physical capacity of the hash table.
+     */
+    abstract protected int capacity();
+    
+    /**
+     * Ensure that this hashtable has sufficient capacity to hold
+     * <tt>desiredCapacity<tt> <b>additional</b> elements without
+     * requiring a rehash.  This is a tuning method you can call
+     * before doing a large insert.
+     *
+     * @param desiredCapacity an <code>int</code> value
+     */
+    public void ensureCapacity(int desiredCapacity)
+    {
+        if (desiredCapacity > (maxSize_ - size()))
+        {
+            rehash(PrimeFinder.nextPrime((int) Math.ceil(desiredCapacity
+                    + size() / loadFactor_) + 1));
+            computeMaxSize(capacity());
+        }
+    }
+    
+    /**
+     * Compresses the hashtable to the minimum prime size (as defined by
+     * PrimeFinder) that will hold all of the elements currently in the table.
+     * If you have done a lot of <tt>remove</tt> operations and plan to do a
+     * lot of queries or insertions or iteration, it is a good idea to invoke
+     * this method. Doing so will accomplish two things:
+     * 
+     * <ol>
+     * <li> You'll free memory allocated to the table but no longer needed
+     * because of the remove()s.</li>
+     * 
+     * <li> You'll get better query/insert/iterator performance because there
+     * won't be any <tt>REMOVED</tt> slots to skip over when probing for
+     * indices in the table.</li>
+     * </ol>
+     */
+    public void compact()
+    {
+        // need at least one free spot for open addressing
+        rehash(PrimeFinder.nextPrime((int) Math.ceil(size() / loadFactor_) + 1));
+        computeMaxSize(capacity());
+        
+        // If auto-compaction is enabled, re-determine the compaction interval
+        if (autoCompactionFactor_ != 0)
+        {
+            computeNextAutoCompactionAmount(size());
+        }
+    }
+    
+    /**
+     * The auto-compaction factor controls whether and when a table performs a
+     * {@link #compact} automatically after a certain number of remove
+     * operations. If the value is non-zero, the number of removes that need to
+     * occur for auto-compaction is the size of table at the time of the
+     * previous compaction (or the initial capacity) multiplied by this factor.
+     * <p>
+     * Setting this value to zero will disable auto-compaction.
+     */
+    public void setAutoCompactionFactor(float factor)
+    {
+        if (factor < 0)
+        {
+            throw new IllegalArgumentException("Factor must be >= 0: " + factor);
+        }
+        
+        autoCompactionFactor_ = factor;
+    }
+    
+    /**
+     * @see #setAutoCompactionFactor
+     */
+    public float getAutoCompactionFactor()
+    {
+        return autoCompactionFactor_;
+    }
+    
+    /**
+     * This simply calls {@link #compact compact}. It is included for symmetry
+     * with other collection classes. Note that the name of this method is
+     * somewhat misleading (which is why we prefer <tt>compact</tt>) as the
+     * load factor may require capacity above and beyond the size of this
+     * collection.
+     * 
+     * @see #compact
+     */
+    public final void trimToSize()
+    {
+        compact();
+    }
+    
+    /**
+     * Delete the record at <tt>index</tt>. Reduces the size of the
+     * collection by one.
+     * 
+     * @param index
+     *            an <code>int</code> value
+     */
+    protected void removeAt(int index)
+    {
+        size_--;
+        
+        // If auto-compaction is enabled, see if we need to compact
+        if (autoCompactionFactor_ != 0)
+        {
+            autoCompactRemovesRemaining_--;
+            
+            if (!autoCompactTemporaryDisable_
+                    && autoCompactRemovesRemaining_ <= 0)
+            {
+                // Do the compact
+                // NOTE: this will cause the next compaction interval to be
+                // calculated
+                compact();
+            }
+        }
+    }
+    
+    /**
+     * Empties the collection.
+     */
+    public void clear()
+    {
+        size_ = 0;
+        free_ = capacity();
+    }
+    
+    /**
+     * initializes the hashtable to a prime capacity which is at least
+     * <tt>initialCapacity + 1</tt>.
+     * 
+     * @param initialCapacity
+     *            an <code>int</code> value
+     * @return the actual capacity chosen
+     */
+    protected int setUp(int initialCapacity)
+    {
+        int capacity;
+        
+        capacity = PrimeFinder.nextPrime(initialCapacity);
+        computeMaxSize(capacity);
+        computeNextAutoCompactionAmount(initialCapacity);
+        
+        return capacity;
+    }
+    
+    /**
+     * Rehashes the set.
+     * 
+     * @param newCapacity
+     *            an <code>int</code> value
+     */
+    protected abstract void rehash(int newCapacity);
+    
+    /**
+     * Temporarily disables auto-compaction. MUST be followed by calling
+     * {@link #reenableAutoCompaction}.
+     */
+    protected void tempDisableAutoCompaction()
+    {
+        autoCompactTemporaryDisable_ = true;
+    }
+    
+    /**
+     * Re-enable auto-compaction after it was disabled via
+     * {@link #tempDisableAutoCompaction()}.
+     * 
+     * @param check_for_compaction
+     *            True if compaction should be performed if needed before
+     *            returning. If false, no compaction will be performed.
+     */
+    protected void reenableAutoCompaction(boolean check_for_compaction)
+    {
+        autoCompactTemporaryDisable_ = false;
+        
+        if (check_for_compaction && autoCompactRemovesRemaining_ <= 0
+                && autoCompactionFactor_ != 0)
+        {
+            
+            // Do the compact
+            // NOTE: this will cause the next compaction interval to be
+            // calculated
+            compact();
+        }
+    }
+    
+    /**
+     * Computes the values of maxSize. There will always be at least one free
+     * slot required.
+     * 
+     * @param capacity
+     *            an <code>int</code> value
+     */
+    private final void computeMaxSize(int capacity)
+    {
+        // need at least one free slot for open addressing
+        maxSize_ = Math.min(capacity - 1, (int) Math.floor(capacity
+                * loadFactor_));
+        free_ = capacity - size_; // reset the free element count
+    }
+    
+    /**
+     * Computes the number of removes that need to happen before the next
+     * auto-compaction will occur.
+     */
+    private void computeNextAutoCompactionAmount(int size)
+    {
+        if (autoCompactionFactor_ != 0)
+        {
+            autoCompactRemovesRemaining_ = Math.round(size
+                    * autoCompactionFactor_);
+        }
+    }
+    
+    /**
+     * After an insert, this hook is called to adjust the size/free values of
+     * the set and to perform rehashing if necessary.
+     */
+    protected final void postInsertHook(boolean usedFreeSlot)
+    {
+        if (usedFreeSlot)
+        {
+            free_--;
+        }
+        
+        // rehash whenever we exhaust the available space in the table
+        if (++size_ > maxSize_ || free_ == 0)
+        {
+            // choose a new capacity suited to the new state of the table
+            // if we've grown beyond our maximum size, double capacity;
+            // if we've exhausted the free spots, rehash to the same capacity,
+            // which will free up any stale removed slots for reuse.
+            int newCapacity = size_ > maxSize_ ? PrimeFinder
+                    .nextPrime(capacity() << 1) : capacity();
+                    rehash(newCapacity);
+                    computeMaxSize(capacity());
+        }
+    }
+    
+    protected int calculateGrownCapacity()
+    {
+        return capacity() << 1;
+    }
+}// THash
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastHashMap.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastHashMap.java
index e69de29b..8351a225 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastHashMap.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastHashMap.java
@@ -0,0 +1,583 @@
+package org.apache.cassandra.utils;
+
+import java.io.*;
+import java.util.*;
+
+/**
+ * An implementation of the Map interface which uses an open addressed hash
+ * table to store its contents
+ * @author Avinash Lakshman
+ */
+public class FastHashMap<K, V> extends FastObjectHash<K> implements Map<K, V>, Serializable
+{
+    static final long serialVersionUID = 1L;
+
+    /** the values of the map */
+    protected transient V[] values_;
+
+    /**
+     * Creates a new <code>FastHashMap</code> instance with the default capacity
+     * and load factor.
+     */
+    public FastHashMap()
+    {
+        super();
+    }
+
+    /**
+     * Creates a new <code>FastHashMap</code> instance with a prime capacity
+     * equal to or greater than <tt>initialCapacity</tt> and with the default
+     * load factor.
+     * 
+     * @param initialCapacity
+     *            an <code>int</code> value
+     */
+    public FastHashMap(int initialCapacity)
+    {
+        super(initialCapacity);
+    }
+
+    /**
+     * Creates a new <code>FastHashMap</code> instance with a prime capacity
+     * equal to or greater than <tt>initialCapacity</tt> and with the
+     * specified load factor.
+     * 
+     * @param initialCapacity
+     *            an <code>int</code> value
+     * @param loadFactor
+     *            a <code>float</code> value
+     */
+    public FastHashMap(int initialCapacity, float loadFactor)
+    {
+        super(initialCapacity, loadFactor);
+    }
+
+    /**
+     * Creates a new <code>FastHashMap</code> instance which contains the
+     * key/value pairs in <tt>map</tt>.
+     * 
+     * @param map
+     *            a <code>Map</code> value
+     */
+    public FastHashMap(Map<K, V> map)
+    {
+        this(map.size());
+        putAll(map);
+    }
+
+    /**
+     * @return a shallow clone of this collection
+     */
+    public FastHashMap<K, V> clone()
+    {
+        FastHashMap<K, V> m = (FastHashMap<K, V>) super.clone();
+        m.values_ = this.values_.clone();
+        return m;
+    }
+
+    /**
+     * initialize the value array of the map.
+     * 
+     * @param initialCapacity
+     *            an <code>int</code> value
+     * @return an <code>int</code> value
+     */
+    protected int setUp(int initialCapacity)
+    {
+        int capacity;
+
+        capacity = super.setUp(initialCapacity);
+        values_ = (V[]) new Object[capacity];
+        return capacity;
+    }
+    
+    void addEntry(Object key, int index)
+    {    	
+    }
+    
+    void removeEntry(Object key, int index)
+    {
+    }
+
+    /**
+     * Inserts a key/value pair into the map.
+     * 
+     * @param key
+     *            an <code>Object</code> value
+     * @param value
+     *            an <code>Object</code> value
+     * @return the previous value associated with <tt>key</tt>, or null if
+     *         none was found.
+     */
+    public V put(K key, V value)
+    {
+        V previous = null;
+        Object oldKey;
+        int index = insertionIndex(key);
+        boolean isNewMapping = true;
+        if (index < 0)
+        {
+            index = -index - 1;
+            previous = values_[index];
+            isNewMapping = false;
+        }
+        oldKey = set_[index];
+        
+        if ( oldKey == FREE )
+        {
+        	/* This is used as a hook to process new put() operations */
+        	addEntry(key, index);
+        }
+    
+        set_[index] = key;
+        values_[index] = value;
+        if (isNewMapping)
+        {
+            postInsertHook(oldKey == FREE);
+        }
+
+        return previous;
+    }
+
+    /**
+     * rehashes the map to the new capacity.
+     * 
+     * @param newCapacity
+     *            an <code>int</code> value
+     */
+    protected void rehash(int newCapacity)
+    {
+        int oldCapacity = set_.length;
+        Object oldKeys[] = set_;
+        V oldVals[] = values_;
+
+        set_ = new Object[newCapacity];
+        Arrays.fill(set_, FREE);
+        values_ = (V[]) new Object[newCapacity];
+
+        for (int i = oldCapacity; i-- > 0;)
+        {
+            if (oldKeys[i] != FREE && oldKeys[i] != REMOVED)
+            {
+                Object o = oldKeys[i];
+                int index = insertionIndex((K) o);
+                if (index < 0)
+                {
+                    throwObjectContractViolation(set_[(-index - 1)], o);
+                }
+                set_[index] = o;
+                values_[index] = oldVals[i];
+            }
+        }
+    }
+
+    /**
+     * retrieves the value for <tt>key</tt>
+     * 
+     * @param key
+     *            an <code>Object</code> value
+     * @return the value of <tt>key</tt> or null if no such mapping exists.
+     */
+    public V get(Object key)
+    {
+        int index = index((K) key);
+        return index < 0 ? null : values_[index];
+    }
+
+    /**
+     * Empties the map.
+     * 
+     */
+    public void clear()
+    {
+        if (size() == 0)
+            return; // optimization
+
+        super.clear();
+        Object[] keys = set_;
+        V[] vals = values_;
+
+        for (int i = keys.length; i-- > 0;)
+        {
+            keys[i] = FREE;
+            vals[i] = null;
+        }
+    }
+
+    /**
+     * Deletes a key/value pair from the map.
+     * 
+     * @param key an <code>Object</code> value
+     * @return an <code>Object</code> value
+     */
+    public V remove(Object key)
+    {
+        V prev = null;
+        int index = index((K)key);
+        if (index >= 0)
+        {
+            prev = values_[index];
+            /* clear key,state; adjust size */
+            removeAt(index);
+            /* This is used as hook to process deleted items */
+            removeEntry(key, index);
+        }
+        return prev;
+    }
+
+    /**
+     * removes the mapping at <tt>index</tt> from the map.
+     * 
+     * @param index an <code>int</code> value
+     */
+    protected void removeAt(int index)
+    {
+        values_[index] = null;
+        /* clear key, state; adjust size */
+        super.removeAt(index); 
+    }
+
+    /**
+     * Returns a view on the values of the map.
+     * 
+     * @return a <code>Collection</code> value
+     */
+    public Collection<V> values()
+    {
+        return Arrays.asList(values_);
+    }
+
+    /**
+     * returns a Set view on the keys of the map.
+     * 
+     * @return a <code>Set</code> value
+     */
+    public Set<K> keySet()
+    {
+        return new KeyView();
+    }
+
+    /**
+     * Returns a Set view on the entries of the map.
+     * 
+     * @return a <code>Set</code> value
+     */
+    public Set<Map.Entry<K, V>> entrySet()
+    {
+        throw new UnsupportedOperationException(
+                "This operation is currently not supported.");
+    }
+
+    /**
+     * checks for the presence of <tt>val</tt> in the values of the map.
+     * 
+     * @param val
+     *            an <code>Object</code> value
+     * @return a <code>boolean</code> value
+     */
+    public boolean containsValue(Object val)
+    {
+        Object[] set = set_;
+        V[] vals = values_;
+
+        // special case null values so that we don't have to
+        // perform null checks before every call to equals()
+        if (null == val)
+        {
+            for (int i = vals.length; i-- > 0;)
+            {
+                if ((set[i] != FREE && set[i] != REMOVED) && val == vals[i])
+                {
+                    return true;
+                }
+            }
+        }
+        else
+        {
+            for (int i = vals.length; i-- > 0;)
+            {
+                if ((set[i] != FREE && set[i] != REMOVED)
+                        && (val == vals[i] || val.equals(vals[i])))
+                {
+                    return true;
+                }
+            }
+        } // end of else
+        return false;
+    }
+
+    /**
+     * checks for the present of <tt>key</tt> in the keys of the map.
+     * 
+     * @param key
+     *            an <code>Object</code> value
+     * @return a <code>boolean</code> value
+     */
+    public boolean containsKey(Object key)
+    {
+        return contains(key);
+    }
+
+    /**
+     * copies the key/value mappings in <tt>map</tt> into this map.
+     * 
+     * @param map
+     *            a <code>Map</code> value
+     */
+    public void putAll(Map<? extends K, ? extends V> map)
+    {
+        ensureCapacity(map.size());
+        // could optimize this for cases when map instanceof FastHashMap
+        for (Iterator<? extends Map.Entry<? extends K, ? extends V>> i = map
+                .entrySet().iterator(); i.hasNext();)
+        {
+            Map.Entry<? extends K, ? extends V> e = i.next();
+            put(e.getKey(), e.getValue());
+        }
+    }
+
+    private abstract class MapBackedView<E> extends AbstractSet<E> implements Set<E>, Iterable<E>
+    {
+        public abstract Iterator<E> iterator();
+        public abstract boolean removeElement(E key);
+        public abstract boolean containsElement(E key);
+
+        public boolean contains(Object key)
+        {
+            return containsElement((E) key);
+        }
+
+        public boolean remove(Object o)
+        {
+            return removeElement((E) o);
+        }
+
+        public boolean containsAll(Collection<?> collection)
+        {
+            for (Iterator i = collection.iterator(); i.hasNext();)
+            {
+                if (!contains(i.next()))
+                {
+                    return false;
+                }
+            }
+            return true;
+        }
+
+        public void clear()
+        {
+            FastHashMap.this.clear();
+        }
+
+        public boolean add(E obj)
+        {
+            throw new UnsupportedOperationException();
+        }
+
+        public int size()
+        {
+            return FastHashMap.this.size();
+        }
+
+        public Object[] toArray()
+        {
+            Object[] result = new Object[size()];
+            Iterator e = iterator();
+            for (int i = 0; e.hasNext(); i++)
+                result[i] = e.next();
+            return result;
+        }
+
+        public <T> T[] toArray(T[] a)
+        {
+            int size = size();
+            if (a.length < size)
+                a = (T[]) java.lang.reflect.Array.newInstance(a.getClass().getComponentType(), size);
+
+            Iterator<E> it = iterator();
+            Object[] result = a;
+            for (int i = 0; i < size; i++)
+            {
+                result[i] = it.next();
+            }
+
+            if (a.length > size)
+            {
+                a[size] = null;
+            }
+
+            return a;
+        }
+
+        public boolean isEmpty()
+        {
+            return FastHashMap.this.isEmpty();
+        }
+
+        public boolean addAll(Collection<? extends E> collection)
+        {
+            throw new UnsupportedOperationException();
+        }
+
+        public boolean retainAll(Collection<?> collection)
+        {
+            boolean changed = false;
+            Iterator i = iterator();
+            while (i.hasNext())
+            {
+                if (!collection.contains(i.next()))
+                {
+                    i.remove();
+                    changed = true;
+                }
+            }
+            return changed;
+        }
+    }
+    
+    protected class FastHashMapIterator<T> implements Iterator<T>
+    {
+        private int nextIndex_;
+        private int expectedSize_;
+        private FastObjectHash<T> tMap_;
+        
+        FastHashMapIterator(FastObjectHash<T> tMap)
+        {
+            nextIndex_ = -1;
+            expectedSize_ = tMap.size();
+            tMap_ = tMap;
+        }
+        
+        public boolean hasNext()
+        {
+            return (expectedSize_ > 0);
+        }
+        
+        public T next()
+        {
+            moveToNextIndex();
+            int index = nextIndex_;
+            /* 
+             * Decrement so that we can track how many 
+             * elements we have already looked at.
+            */
+            --expectedSize_;
+            return (T)tMap_.set_[index];
+        }
+        
+        private void moveToNextIndex()
+        {
+            int i = nextIndex_ + 1;
+            for ( ; i < tMap_.set_.length; ++i )
+            {
+                if ( tMap_.set_[i].equals(FREE) || tMap_.set_[i].equals(REMOVED) )
+                {
+                    continue;
+                }
+                else
+                {                    
+                    break;
+                }
+            }
+            nextIndex_ = i;
+        }
+        
+        public void remove()
+        {
+            tMap_.removeAt(nextIndex_);
+            --expectedSize_;
+        }
+    }
+
+    /**
+     * a view onto the keys of the map.
+     */
+    protected class KeyView extends MapBackedView<K>
+    {
+        public Iterator<K> iterator()
+        {
+            return new FastHashMapIterator(FastHashMap.this);
+        }
+        
+        public boolean removeElement(K key)
+        {
+            return null != FastHashMap.this.remove(key);
+        }
+
+        public boolean containsElement(K key)
+        {
+            return FastHashMap.this.contains(key);
+        }
+    }
+
+    final class Entry implements Map.Entry<K, V>
+    {
+        private K key;
+        private V val;
+        private final int index;
+
+        Entry(final K key, V value, final int index)
+        {
+            this.key = key;
+            this.val = value;
+            this.index = index;
+        }
+
+        void setKey(K aKey)
+        {
+            this.key = aKey;
+        }
+
+        void setValue0(V aValue)
+        {
+            this.val = aValue;
+        }
+
+        public K getKey()
+        {
+            return key;
+        }
+
+        public V getValue()
+        {
+            return val;
+        }
+
+        public V setValue(V o)
+        {
+            if (values_[index] != val)
+            {
+                throw new ConcurrentModificationException();
+            }
+            values_[index] = o;
+            o = val; // need to return previous value
+            val = o; // update this entry's value, in case
+            // setValue is called again
+            return o;
+        }
+
+        public boolean equals(Object o)
+        {
+            if (o instanceof Map.Entry)
+            {
+                Map.Entry e1 = this;
+                Map.Entry e2 = (Map.Entry) o;
+                return (e1.getKey() == null ? e2.getKey() == null : e1.getKey().equals(e2.getKey()))
+                        && (e1.getValue() == null ? e2.getValue() == null : e1.getValue().equals(e2.getValue()));
+            }
+            return false;
+        }
+
+        public int hashCode()
+        {
+            return (getKey() == null ? 0 : getKey().hashCode()) ^ (getValue() == null ? 0 : getValue().hashCode());
+        }
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {
+        Map<String, String> map = new FastHashMap<String, String>();
+        map.put("Avinash", "Avinash");
+        map.put("Avinash", "Srinivas");
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastLinkedHashMap.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastLinkedHashMap.java
index e69de29b..1b11081f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastLinkedHashMap.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastLinkedHashMap.java
@@ -0,0 +1,24 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+public class FastLinkedHashMap<K,V> extends FastHashMap<K,V>
+{
+    
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastObjectHash.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastObjectHash.java
index e69de29b..b434c6a4 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastObjectHash.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FastObjectHash.java
@@ -0,0 +1,306 @@
+package org.apache.cassandra.utils;
+
+import java.util.Arrays;
+
+/**
+ * An open addressed hashing implementation for Object types.
+ * 
+ * @author Avinash Lakshman
+ */
+abstract public class FastObjectHash<T> extends FastHash
+{
+    static final long serialVersionUID = -3461112548087185871L;
+
+    /** the set of Objects */
+    protected transient Object[] set_;
+    protected static final Object REMOVED = new Object(), FREE = new Object();
+
+    /**
+     * Creates a new <code>TObjectHash</code> instance with the default
+     * capacity and load factor.
+     */
+    public FastObjectHash()
+    {
+        super();        
+    }
+
+    /**
+     * Creates a new <code>TObjectHash</code> instance whose capacity is the
+     * next highest prime above <tt>initialCapacity + 1</tt> unless that value
+     * is already prime.
+     * 
+     * @param initialCapacity
+     *            an <code>int</code> value
+     */
+    public FastObjectHash(int initialCapacity)
+    {
+        super(initialCapacity);        
+    }
+
+    /**
+     * Creates a new <code>TObjectHash</code> instance with a prime value at
+     * or near the specified capacity and load factor.
+     * 
+     * @param initialCapacity
+     *            used to find a prime capacity for the table.
+     * @param loadFactor
+     *            used to calculate the threshold over which rehashing takes
+     *            place.
+     */
+    public FastObjectHash(int initialCapacity, float loadFactor)
+    {
+        super(initialCapacity, loadFactor);        
+    }
+
+    /**
+     * @return a shallow clone of this collection
+     */
+    public FastObjectHash<T> clone()
+    {
+        FastObjectHash<T> h = (FastObjectHash<T>) super.clone();
+        h.set_ = (Object[]) this.set_.clone();
+        return h;
+    }
+    
+    /**
+     * This method is invoked every time a key is
+     * added into the Map.
+     * @param key key that is inserted
+     * @param index index position of the key being 
+     *              inserted
+     */
+    abstract void addEntry(Object key, int index);
+    
+    /**
+     * This method is invoked every time a key is
+     * deleted from the Map.
+     * @param key key being deleted
+     * @param index index position of the key being
+     *              deleted
+     */
+    abstract void removeEntry(Object key, int index);
+
+    protected int capacity()
+    {
+        return set_.length;
+    }
+
+    protected void removeAt(int index)
+    {
+        set_[index] = REMOVED;
+        super.removeAt(index);
+    }
+
+    /**
+     * initializes the Object set of this hash table.
+     * 
+     * @param initialCapacity
+     *            an <code>int</code> value
+     * @return an <code>int</code> value
+     */
+    protected int setUp(int initialCapacity)
+    {
+        int capacity;
+
+        capacity = super.setUp(initialCapacity);
+        set_ = new Object[capacity];
+        Arrays.fill(set_, FREE);
+        return capacity;
+    }
+
+    /**
+     * Searches the set for <tt>obj</tt>
+     * 
+     * @param obj
+     *            an <code>Object</code> value
+     * @return a <code>boolean</code> value
+     */
+    public boolean contains(Object obj)
+    {
+        return index((T) obj) >= 0;
+    }
+
+    /**
+     * Locates the index of <tt>obj</tt>.
+     * 
+     * @param obj
+     *            an <code>Object</code> value
+     * @return the index of <tt>obj</tt> or -1 if it isn't in the set.
+     */
+    protected int index(Object obj)
+    {        
+        final Object[] set = set_;
+        final int length = set.length;
+        final int hash = obj.hashCode() & 0x7fffffff;
+        int index = hash % length;
+        Object cur = set[index];
+
+        if (cur == FREE)
+            return -1;
+
+        // NOTE: here it has to be REMOVED or FULL (some user-given value)
+        if (cur == REMOVED || cur.equals(obj))
+        {
+            // see Knuth, p. 529
+            final int probe = 1 + (hash % (length - 2));
+
+            while (cur != FREE&& (cur == REMOVED || !cur.equals(obj)))
+            {
+                index -= probe;
+                if (index < 0)
+                {
+                    index += length;
+                }
+                cur = set[index];
+            }            
+        }
+
+        return cur == FREE ? -1 : index;
+    }
+
+    /**
+     * Locates the index at which <tt>obj</tt> can be inserted. if there is
+     * already a value equal()ing <tt>obj</tt> in the set, returns that
+     * value's index as <tt>-index - 1</tt>.
+     * 
+     * @param obj
+     *            an <code>Object</code> value
+     * @return the index of a FREE slot at which obj can be inserted or, if obj
+     *         is already stored in the hash, the negative value of that index,
+     *         minus 1: -index -1.
+     */
+    protected int insertionIndex(T obj)
+    {        
+        final Object[] set = set_;
+        final int length = set.length;
+        final int hash = obj.hashCode() & 0x7fffffff;
+        int index = hash % length;
+        Object cur = set[index];
+
+        if (cur == FREE)
+        {
+            return index; // empty, all done
+        }
+        else if (cur != REMOVED && cur.equals(obj))
+        {
+            return -index - 1; // already stored
+        }
+        else
+        { // already FULL or REMOVED, must probe
+            // compute the double token
+            final int probe = 1 + (hash % (length - 2));
+
+            // if the slot we landed on is FULL (but not removed), probe
+            // until we find an empty slot, a REMOVED slot, or an element
+            // equal to the one we are trying to insert.
+            // finding an empty slot means that the value is not present
+            // and that we should use that slot as the insertion point;
+            // finding a REMOVED slot means that we need to keep searching,
+            // however we want to remember the offset of that REMOVED slot
+            // so we can reuse it in case a "new" insertion (i.e. not an update)
+            // is possible.
+            // finding a matching value means that we've found that our desired
+            // key is already in the table
+            if (cur != REMOVED)
+            {
+                // starting at the natural offset, probe until we find an
+                // offset that isn't full.
+                do
+                {
+                    index -= probe;
+                    if (index < 0)
+                    {
+                        index += length;
+                    }
+                    cur = set[index];
+                }
+                while (cur != FREE && cur != REMOVED
+                        && !cur.equals(obj));
+            }
+
+            // if the index we found was removed: continue probing until we
+            // locate a free location or an element which equal()s the
+            // one we have.
+            if (cur == REMOVED)
+            {
+                int firstRemoved = index;
+                while (cur != FREE
+                        && (cur == REMOVED || !cur.equals(obj)))
+                {
+                    index -= probe;
+                    if (index < 0)
+                    {
+                        index += length;
+                    }
+                    cur = set[index];
+                }
+                // NOTE: cur cannot == REMOVED in this block
+                return (cur != FREE) ? -index - 1 : firstRemoved;
+            }
+            // if it's full, the key is already stored
+            // NOTE: cur cannot equal REMOVE here (would have retuned already
+            // (see above)
+            return (cur != FREE) ? -index - 1 : index;
+        }
+    }
+
+    /**
+     * This is the default implementation of TObjectHashingStrategy: it
+     * delegates hashing to the Object's hashCode method.
+     * 
+     * @param o
+     *            for which the hashcode is to be computed
+     * @return the hashCode
+     * @see Object#hashCode()
+     */
+    public final int computeHashCode(T o)
+    {
+        return o == null ? 0 : o.hashCode();
+    }
+
+    /**
+     * This is the default implementation of TObjectHashingStrategy: it
+     * delegates equality comparisons to the first parameter's equals() method.
+     * 
+     * @param o1
+     *            an <code>Object</code> value
+     * @param o2
+     *            an <code>Object</code> value
+     * @return true if the objects are equal
+     * @see Object#equals(Object)
+     */
+    public final boolean equals(T o1, T o2)
+    {
+        return o1 == null ? o2 == null : o1.equals(o2);
+    }
+
+    /**
+     * Convenience methods for subclasses to use in throwing exceptions about
+     * badly behaved user objects employed as keys. We have to throw an
+     * IllegalArgumentException with a rather verbose message telling the user
+     * that they need to fix their object implementation to conform to the
+     * general contract for java.lang.Object.
+     * 
+     * @param o1
+     *            the first of the equal elements with unequal hash codes.
+     * @param o2
+     *            the second of the equal elements with unequal hash codes.
+     * @exception IllegalArgumentException
+     *                the whole point of this method.
+     */
+    protected final void throwObjectContractViolation(Object o1, Object o2)
+            throws IllegalArgumentException
+    {
+        throw new IllegalArgumentException(
+                "Equal objects must have equal hashcodes. "
+                        + "During rehashing, Trove discovered that "
+                        + "the following two objects claim to be "
+                        + "equal (as in java.lang.Object.equals()) "
+                        + "but their hashCodes (or those calculated by "
+                        + "your TObjectHashingStrategy) are not equal."
+                        + "This violates the general contract of "
+                        + "java.lang.Object.hashCode().  See bullet point two "
+                        + "in that method's documentation. " + "object #1 ="
+                        + o1 + "; object #2 =" + o2);
+    }
+} // TObjectHash
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FileUtils.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FileUtils.java
index e69de29b..7745c463 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FileUtils.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/FileUtils.java
@@ -0,0 +1,262 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.*;
+import java.text.DecimalFormat;
+import java.util.*;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.log4j.Logger;
+
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class FileUtils
+{
+    private static Logger logger_ = Logger.getLogger(FileUtils.class);
+    private static final DecimalFormat df_ = new DecimalFormat("#.##");
+    private static final double kb_ = 1024d;
+    private static final double mb_ = 1024*1024d;
+    private static final double gb_ = 1024*1024*1024d;
+    private static final double tb_ = 1024*1024*1024*1024d;
+
+    private static ExecutorService deleter_ = new DebuggableThreadPoolExecutor( 1,
+            1,
+            Integer.MAX_VALUE,
+            TimeUnit.SECONDS,
+            new LinkedBlockingQueue<Runnable>(),
+            new ThreadFactoryImpl("FILEUTILS-DELETE-POOL")
+            );
+
+    public static void shutdown()
+    {
+    	deleter_.shutdownNow();
+    }
+
+    public static class Deleter implements Runnable
+    {
+    	File file_ = null;
+
+    	public Deleter(File f)
+        {
+    		file_ = f;
+        }
+
+        public void run()
+        {
+        	if(file_ == null)
+        		return;
+        	logger_.info("*** Deleting " + file_.getName() + " ***");
+        	if(!file_.delete())
+        	{
+            	logger_.warn("Warning : Unable to delete file " + file_.getAbsolutePath());
+        	}
+        }
+    }
+
+    public static class FileComparator implements Comparator<File>
+    {
+        public int compare(File f, File f2)
+        {
+            return (int)(f.lastModified() - f2.lastModified());
+        }
+
+        public boolean equals(Object o)
+        {
+            if ( !(o instanceof FileComparator) )
+                return false;
+            return true;
+        }
+    }
+
+    public static void createDirectory(String directory) throws IOException
+    {
+        File file = new File(directory);
+        if (!file.exists())
+        {
+            if (!file.mkdirs())
+            {
+                throw new IOException("unable to mkdirs " + directory);
+            }
+        }
+    }
+
+    public static void createFile(String directory) throws IOException
+    {
+        File file = new File(directory);
+        if ( !file.exists() )
+            file.createNewFile();
+    }
+
+    public static boolean isExists(String filename) throws IOException
+    {
+        File file = new File(filename);
+        return file.exists();
+    }
+
+    public static boolean delete(String file)
+    {
+        File f = new File(file);
+        return f.delete();
+    }
+
+    public static void deleteAsync(String file) throws IOException
+    {
+        File f = new File(file);
+    	Runnable deleter = new Deleter(f);
+        deleter_.submit(deleter);
+    }
+
+    public static boolean delete(List<String> files) throws IOException
+    {
+        boolean bVal = true;
+        for ( int i = 0; i < files.size(); ++i )
+        {
+            String file = files.get(i);
+            bVal = delete(file);
+            if (bVal)
+            {
+            	logger_.debug("Deleted file " + file);
+                files.remove(i);
+            }
+        }
+        return bVal;
+    }
+
+    public static void delete(File[] files) throws IOException
+    {
+        for ( File file : files )
+        {
+            file.delete();
+        }
+    }
+
+    public static String stringifyFileSize(double value)
+    {
+        double d = 0d;
+        if ( value >= tb_ )
+        {
+            d = value / tb_;
+            String val = df_.format(d);
+            return val + " TB";
+        }
+        else if ( value >= gb_ )
+        {
+            d = value / gb_;
+            String val = df_.format(d);
+            return val + " GB";
+        }
+        else if ( value >= mb_ )
+        {
+            d = value / mb_;
+            String val = df_.format(d);
+            return val + " MB";
+        }
+        else if ( value >= kb_ )
+        {
+            d = value / kb_;
+            String val = df_.format(d);
+            return val + " KB";
+        }
+        else
+        {       
+            String val = df_.format(value);
+            return val + " bytes.";
+        }        
+    }
+    
+    public static double stringToFileSize(String value)
+    {        
+        String[] peices = value.split(" ");
+        double d = Double.valueOf(peices[0]);
+        if ( peices[1].equals("TB") )
+        {
+            d *= tb_;
+        }
+        else if ( peices[1].equals("GB") )
+        {
+            d *= gb_;
+        }
+        else if ( peices[1].equals("MB") )
+        {
+            d *= mb_;
+        }
+        else if ( peices[1].equals("KB") )
+        {
+            d *= kb_;
+        }
+        else
+        {
+            d *= 1;
+        }
+        return d;
+    }
+    
+    public static long getUsedDiskSpace()
+    {
+        long diskSpace = 0L;
+        String[] directories = DatabaseDescriptor.getAllDataFileLocations();        
+        for ( String directory : directories )
+        {
+            File f = new File(directory);
+            File[] files = f.listFiles();
+            for ( File file : files )
+            {
+                diskSpace += file.length();
+            }
+        }
+
+        String value = df_.format(diskSpace);
+        return Long.parseLong(value);
+    }    
+    
+    
+	
+    /**
+     * Deletes all files and subdirectories under "dir".
+     * @param dir Directory to be deleted
+     * @return boolean Returns "true" if all deletions were successful.
+     *                 If a deletion fails, the method stops attempting to
+     *                 delete and returns "false".
+     */
+    public static boolean deleteDir(File dir) {
+
+        if (dir.isDirectory()) {
+            String[] children = dir.list();
+            for (int i=0; i<children.length; i++) {
+                boolean success = deleteDir(new File(dir, children[i]));
+                if (!success) {
+                    return false;
+                }
+            }
+        }
+
+        // The directory is now empty so now it can be smoked
+        return dir.delete();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Filter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Filter.java
index e69de29b..c4e0ccb6 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Filter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Filter.java
@@ -0,0 +1,72 @@
+package org.apache.cassandra.utils;
+
+import java.io.UnsupportedEncodingException;
+import java.lang.reflect.Method;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+public abstract class Filter
+{
+    int hashCount;
+
+    private static MurmurHash hasher = new MurmurHash();
+
+    int getHashCount()
+    {
+        return hashCount;
+    }
+
+    public int[] getHashBuckets(String key)
+    {
+        return Filter.getHashBuckets(key, hashCount, buckets());
+    }
+
+    abstract int buckets();
+
+    public abstract void add(String key);
+
+    public abstract boolean isPresent(String key);
+
+    // for testing
+    abstract int emptyBuckets();
+
+    ICompactSerializer<Filter> getSerializer()
+    {
+        Method method = null;
+        try
+        {
+            method = getClass().getMethod("serializer");
+            return (ICompactSerializer<Filter>) method.invoke(null);
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    // murmur is faster than a sha-based approach and provides as-good collision
+    // resistance.  the combinatorial generation approach described in
+    // http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
+    // does prove to work in actual tests, and is obviously faster
+    // than performing further iterations of murmur.
+    static int[] getHashBuckets(String key, int hashCount, int max)
+    {
+        byte[] b;
+        try
+        {
+            b = key.getBytes("UTF-16");
+        }
+        catch (UnsupportedEncodingException e)
+        {
+            throw new RuntimeException(e);
+        }
+        int[] result = new int[hashCount];
+        int hash1 = hasher.hash(b, b.length, 0);
+        int hash2 = hasher.hash(b, b.length, hash1);
+        for (int i = 0; i < hashCount; i++)
+        {
+            result[i] = Math.abs((hash1 + i * hash2) % max);
+        }
+        return result;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/GuidGenerator.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/GuidGenerator.java
index e69de29b..e4dde719 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/GuidGenerator.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/GuidGenerator.java
@@ -0,0 +1,124 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.util.*;
+import java.net.*;
+import java.security.*;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class GuidGenerator {
+    private static Random myRand;
+    private static SecureRandom mySecureRand;
+    private static String s_id;
+    private static SafeMessageDigest md5 = null;
+
+    static {
+        if (System.getProperty("java.security.egd") == null) {
+            System.setProperty("java.security.egd", "file:/dev/urandom");
+        }
+        mySecureRand = new SecureRandom();
+        long secureInitializer = mySecureRand.nextLong();
+        myRand = new Random(secureInitializer);
+        try {
+            s_id = InetAddress.getLocalHost().toString();
+        }
+        catch (UnknownHostException e) {
+            LogUtil.getLogger(GuidGenerator.class.getName()).debug(LogUtil.throwableToString(e));
+        }
+
+        try {
+            MessageDigest myMd5 = MessageDigest.getInstance("MD5");
+            md5 = new SafeMessageDigest(myMd5);
+        }
+        catch (NoSuchAlgorithmException e) {
+            LogUtil.getLogger(GuidGenerator.class.getName()).debug(LogUtil.throwableToString(e));
+        }
+    }
+
+
+    public static String guid() {
+        byte[] array = guidAsBytes();
+        
+        StringBuffer sb = new StringBuffer();
+        for (int j = 0; j < array.length; ++j) {
+            int b = array[j] & 0xFF;
+            if (b < 0x10) sb.append('0');
+            sb.append(Integer.toHexString(b));
+        }
+
+        return convertToStandardFormat( sb.toString() );
+    }
+    
+    public static String guidToString(byte[] bytes)
+    {
+        StringBuffer sb = new StringBuffer();
+        for (int j = 0; j < bytes.length; ++j) {
+            int b = bytes[j] & 0xFF;
+            if (b < 0x10) sb.append('0');
+            sb.append(Integer.toHexString(b));
+        }
+
+        return convertToStandardFormat( sb.toString() );
+    }
+    
+    public static byte[] guidAsBytes()
+    {
+        StringBuffer sbValueBeforeMD5 = new StringBuffer();
+        long time = System.currentTimeMillis();
+        long rand = 0;
+        rand = myRand.nextLong();
+        sbValueBeforeMD5.append(s_id);
+        sbValueBeforeMD5.append(":");
+        sbValueBeforeMD5.append(Long.toString(time));
+        sbValueBeforeMD5.append(":");
+        sbValueBeforeMD5.append(Long.toString(rand));
+
+        String valueBeforeMD5 = sbValueBeforeMD5.toString();
+        return md5.digest(valueBeforeMD5.getBytes());
+    }
+
+    /*
+        * Convert to the standard format for GUID
+        * Example: C2FEEEAC-CFCD-11D1-8B05-00600806D9B6
+    */
+
+    private static String convertToStandardFormat(String valueAfterMD5) {
+        String raw = valueAfterMD5.toUpperCase();
+        StringBuffer sb = new StringBuffer();
+        sb.append(raw.substring(0, 8));
+        sb.append("-");
+        sb.append(raw.substring(8, 12));
+        sb.append("-");
+        sb.append(raw.substring(12, 16));
+        sb.append("-");
+        sb.append(raw.substring(16, 20));
+        sb.append("-");
+        sb.append(raw.substring(20));
+        return sb.toString();
+    }
+}
+
+
+
+
+
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/HashingSchemes.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/HashingSchemes.java
index e69de29b..0b8370cc 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/HashingSchemes.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/HashingSchemes.java
@@ -0,0 +1,34 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+/**
+ * Created by IntelliJ IDEA.
+ * User: lakshman
+ * Date: Aug 17, 2005
+ * Time: 3:32:42 PM
+ * To change this template use File | Settings | File Templates.
+ */
+
+public final class HashingSchemes
+{
+    public static final String SHA_1 = "SHA-1";
+    public static final String SHA1 = "SHA1";
+    public static final String MD5 = "MD5";
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/ICacheExpungeHook.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/ICacheExpungeHook.java
index e69de29b..12483553 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/ICacheExpungeHook.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/ICacheExpungeHook.java
@@ -0,0 +1,31 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+/**
+ * Created by IntelliJ IDEA.
+ * User: lakshman
+ * Date: Aug 16, 2005
+ * Time: 1:08:58 PM
+ * To change this template use File | Settings | File Templates.
+ */
+public interface ICacheExpungeHook<K,V>
+{
+    public void callMe(K key , V value);
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/ICachetable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/ICachetable.java
index e69de29b..f3a6ed11 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/ICachetable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/ICachetable.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.util.Enumeration;
+import java.util.Set;
+
+public interface ICachetable<K,V>
+{
+    public void put(K key, V value);
+    public void put(K key, V value, ICacheExpungeHook<K,V> hook);
+	public V get(K key);
+    public V remove(K key);
+    public int size();
+    public boolean containsKey(K key);
+    public boolean containsValue(V value);
+    public boolean isEmpty();    
+    public Set<K> keySet();
+    public void shutdown();
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Log4jLogger.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Log4jLogger.java
index e69de29b..f4415e7b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Log4jLogger.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/Log4jLogger.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+/**
+ * Log4j configurations may change while the application is running, 
+ * potentially invalidating a logger's appender(s).  This is a convinience
+ * class to wrap logger calls so that a logger is always explicitly 
+ * invoked.
+ */
+
+
+public class Log4jLogger {
+    
+    private String name_ = null;
+    
+    public Log4jLogger(String name){
+        name_ = name;
+    }
+    
+    public void debug(Object arg){ 
+        LogUtil.getLogger(name_).debug(LogUtil.getTimestamp() + " - " + arg);
+    }    
+    public void info(Object arg){
+        LogUtil.getLogger(name_).info(LogUtil.getTimestamp() + " - " + arg);
+    }
+    public void warn(Object arg){
+        LogUtil.getLogger(name_).warn(LogUtil.getTimestamp() + " - " + arg);
+    }
+    public void error(Object arg){
+        LogUtil.getLogger(name_).error(LogUtil.getTimestamp() + " - " + arg);
+    }
+    public void fatal(Object arg){
+        LogUtil.getLogger(name_).fatal(LogUtil.getTimestamp() + " - " + arg);
+    } 
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/LogUtil.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/LogUtil.java
index e69de29b..7759d945 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/LogUtil.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/LogUtil.java
@@ -0,0 +1,111 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.*;
+import java.text.*;
+import java.util.*;
+import org.apache.log4j.*;
+import org.apache.log4j.spi.LoggerFactory;
+import org.apache.log4j.xml.DOMConfigurator;
+
+public class LogUtil
+{
+
+    public LogUtil()
+    {
+    }
+
+    public static void init()
+    {
+        //BasicConfigurator.configure();
+        String file = System.getProperty("storage-config");
+        file += System.getProperty("file.separator") + "log4j.properties";
+        PropertyConfigurator.configure(file);
+    }
+
+    public static Logger getLogger(String name)
+    {
+        return Logger.getLogger(name);
+    }
+    
+    public static String stackTrace(Throwable e)
+    {
+        StringWriter sw = new StringWriter();
+        PrintWriter pw = new PrintWriter(sw);
+        e.printStackTrace(pw);
+        return sw.toString();
+    }
+
+    public static String getTimestamp()
+    {
+        Date date = new Date();
+        DateFormat df = new SimpleDateFormat("MM/dd/yyyy HH:mm:ss");
+        return df.format(date);
+    }
+    
+    public static String throwableToString(Throwable e)
+    {
+        StringBuffer sbuf = new StringBuffer("");
+        String trace = stackTrace(e);
+        sbuf.append((new StringBuilder()).append("Exception was generated at : ").append(getTimestamp()).append(" on thread ").append(Thread.currentThread().getName()).toString());
+        sbuf.append(System.getProperty("line.separator"));
+        String message = e.getMessage();
+        if(message != null)
+            sbuf.append(message);
+        sbuf.append(System.getProperty("line.separator"));
+        sbuf.append(trace);
+        return sbuf.toString();
+    }
+
+    public static String getLogMessage(String message)
+    {
+        StringBuffer sbuf = new StringBuffer((new StringBuilder()).append("Log started at : ").append(getTimestamp()).toString());
+        sbuf.append(System.getProperty("line.separator"));
+        sbuf.append(message);
+        return sbuf.toString();
+    }
+
+    public static void setLogLevel(String logger, String level)
+    {        
+        Logger loggerObj = LogManager.getLogger(logger);
+        if(null == loggerObj)
+            return;
+        level = level.toUpperCase();
+        if(level.equals("DEBUG"))
+            loggerObj.setLevel(Level.DEBUG);
+        else
+        if(level.equals("ERROR"))
+            loggerObj.setLevel(Level.ERROR);
+        else
+        if(level.equals("FATAL"))
+            loggerObj.setLevel(Level.FATAL);
+        else
+        if(level.equals("INFO"))
+            loggerObj.setLevel(Level.INFO);
+        else
+        if(level.equals("OFF"))
+            loggerObj.setLevel(Level.OFF);
+        else
+        if(level.equals("WARN"))
+            loggerObj.setLevel(Level.WARN);
+        else
+            loggerObj.setLevel(Level.ALL);
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/MurmurHash.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/MurmurHash.java
index e69de29b..5c8f08de 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/MurmurHash.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/MurmurHash.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+/**
+ * This is a very fast, non-cryptographic hash suitable for general hash-based
+ * lookup.  See http://murmurhash.googlepages.com/ for more details.
+ * 
+ * <p>The C version of MurmurHash 2.0 found at that site was ported
+ * to Java by Andrzej Bialecki (ab at getopt org).</p>
+ */
+public class MurmurHash {  
+  public int hash(byte[] data, int length, int seed) {
+    int m = 0x5bd1e995;
+    int r = 24;
+
+    int h = seed ^ length;
+
+    int len_4 = length >> 2;
+
+    for (int i = 0; i < len_4; i++) {
+      int i_4 = i << 2;
+      int k = data[i_4 + 3];
+      k = k << 8;
+      k = k | (data[i_4 + 2] & 0xff);
+      k = k << 8;
+      k = k | (data[i_4 + 1] & 0xff);
+      k = k << 8;
+      k = k | (data[i_4 + 0] & 0xff);
+      k *= m;
+      k ^= k >>> r;
+      k *= m;
+      h *= m;
+      h ^= k;
+    }
+
+    // avoid calculating modulo
+    int len_m = len_4 << 2;
+    int left = length - len_m;
+
+    if (left != 0) {
+      if (left >= 3) {
+        h ^= (int) data[length - 3] << 16;
+      }
+      if (left >= 2) {
+        h ^= (int) data[length - 2] << 8;
+      }
+      if (left >= 1) {
+        h ^= (int) data[length - 1];
+      }
+
+      h *= m;
+    }
+
+    h ^= h >>> 13;
+    h *= m;
+    h ^= h >>> 15;
+
+    return h;
+  }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/PrimeFinder.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/PrimeFinder.java
index e69de29b..f24f0e47 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/PrimeFinder.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/PrimeFinder.java
@@ -0,0 +1,144 @@
+package org.apache.cassandra.utils;
+
+import java.util.Arrays;
+
+/**
+ * Used to keep hash table capacities prime numbers. Not of interest for users;
+ * only for implementors of hashtables.
+ * 
+ * <p>
+ * Choosing prime numbers as hash table capacities is a good idea to keep them
+ * working fast, particularly under hash table expansions.
+ * 
+ */
+public final class PrimeFinder
+{
+    /**
+     * The largest prime this class can generate; currently equal to
+     * <tt>Integer.MAX_VALUE</tt>.
+     */
+    public static final int largestPrime = Integer.MAX_VALUE; // yes, it is
+                                                                // prime.
+
+    /**
+     * The prime number list consists of 11 chunks.
+     * 
+     * Each chunk contains prime numbers.
+     * 
+     * A chunk starts with a prime P1. The next element is a prime P2. P2 is the
+     * smallest prime for which holds: P2 >= 2*P1.
+     * 
+     * The next element is P3, for which the same holds with respect to P2, and
+     * so on.
+     * 
+     * Chunks are chosen such that for any desired capacity >= 1000 the list
+     * includes a prime number <= desired capacity * 1.11.
+     * 
+     * Therefore, primes can be retrieved which are quite close to any desired
+     * capacity, which in turn avoids wasting memory.
+     * 
+     * For example, the list includes
+     * 1039,1117,1201,1277,1361,1439,1523,1597,1759,1907,2081.
+     * 
+     * So if you need a prime >= 1040, you will find a prime <= 1040*1.11=1154.
+     * 
+     * Chunks are chosen such that they are optimized for a hashtable
+     * growthfactor of 2.0;
+     * 
+     * If your hashtable has such a growthfactor then, after initially "rounding
+     * to a prime" upon hashtable construction, it will later expand to prime
+     * capacities such that there exist no better primes.
+     * 
+     * In total these are about 32*10=320 numbers -> 1 KB of static memory
+     * needed.
+     * 
+     * If you are stingy, then delete every second or fourth chunk.
+     */
+
+    private static final int[] primeCapacities = {
+    // chunk #0
+            largestPrime,
+
+            // chunk #1
+            5, 11, 23, 47, 97, 197, 397, 797, 1597, 3203, 6421, 12853, 25717,
+            51437, 102877, 205759, 411527, 823117, 1646237, 3292489, 6584983,
+            13169977, 26339969, 52679969, 105359939, 210719881, 421439783,
+            842879579, 1685759167,
+
+            // chunk #2
+            433, 877, 1759, 3527, 7057, 14143, 28289, 56591, 113189, 226379,
+            452759, 905551, 1811107, 3622219, 7244441, 14488931, 28977863,
+            57955739, 115911563, 231823147, 463646329, 927292699, 1854585413,
+
+            // chunk #3
+            953, 1907, 3821, 7643, 15287, 30577, 61169, 122347, 244703, 489407,
+            978821, 1957651, 3915341, 7830701, 15661423, 31322867, 62645741,
+            125291483, 250582987, 501165979, 1002331963, 2004663929,
+
+            // chunk #4
+            1039, 2081, 4177, 8363, 16729, 33461, 66923, 133853, 267713,
+            535481, 1070981, 2141977, 4283963, 8567929, 17135863, 34271747,
+            68543509, 137087021, 274174111, 548348231, 1096696463,
+
+            // chunk #5
+            31, 67, 137, 277, 557, 1117, 2237, 4481, 8963, 17929, 35863, 71741,
+            143483, 286973, 573953, 1147921, 2295859, 4591721, 9183457,
+            18366923, 36733847, 73467739, 146935499, 293871013, 587742049,
+            1175484103,
+
+            // chunk #6
+            599, 1201, 2411, 4831, 9677, 19373, 38747, 77509, 155027, 310081,
+            620171, 1240361, 2480729, 4961459, 9922933, 19845871, 39691759,
+            79383533, 158767069, 317534141, 635068283, 1270136683,
+
+            // chunk #7
+            311, 631, 1277, 2557, 5119, 10243, 20507, 41017, 82037, 164089,
+            328213, 656429, 1312867, 2625761, 5251529, 10503061, 21006137,
+            42012281, 84024581, 168049163, 336098327, 672196673, 1344393353,
+
+            // chunk #8
+            3, 7, 17, 37, 79, 163, 331, 673, 1361, 2729, 5471, 10949, 21911,
+            43853, 87719, 175447, 350899, 701819, 1403641, 2807303, 5614657,
+            11229331, 22458671, 44917381, 89834777, 179669557, 359339171,
+            718678369, 1437356741,
+
+            // chunk #9
+            43, 89, 179, 359, 719, 1439, 2879, 5779, 11579, 23159, 46327,
+            92657, 185323, 370661, 741337, 1482707, 2965421, 5930887, 11861791,
+            23723597, 47447201, 94894427, 189788857, 379577741, 759155483,
+            1518310967,
+
+            // chunk #10
+            379, 761, 1523, 3049, 6101, 12203, 24407, 48817, 97649, 195311,
+            390647, 781301, 1562611, 3125257, 6250537, 12501169, 25002389,
+            50004791, 100009607, 200019221, 400038451, 800076929, 1600153859 };
+
+    static
+    { // initializer
+        // The above prime numbers are formatted for human readability.
+        // To find numbers fast, we sort them once and for all.
+
+        Arrays.sort(primeCapacities);
+    }
+
+    /**
+     * Returns a prime number which is <code>&gt;= desiredCapacity</code> and
+     * very close to <code>desiredCapacity</code> (within 11% if
+     * <code>desiredCapacity &gt;= 1000</code>).
+     * 
+     * @param desiredCapacity
+     *            the capacity desired by the user.
+     * @return the capacity which should be used for a hashtable.
+     */
+    public static final int nextPrime(int desiredCapacity)
+    {
+        int i = Arrays.binarySearch(primeCapacities, desiredCapacity);
+        if (i < 0)
+        {
+            // desired capacity not found, choose next prime greater
+            // than desired capacity
+            i = -i - 1; // remember the semantics of binarySearch...
+        }
+        return primeCapacities[i];
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/SafeMessageDigest.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/SafeMessageDigest.java
index e69de29b..1d1656fe 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/SafeMessageDigest.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/SafeMessageDigest.java
@@ -0,0 +1,85 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SafeMessageDigest
+{
+    private MessageDigest md_ = null;
+
+    public static SafeMessageDigest digest_;
+    static
+    {
+        try
+        {
+            digest_ = new SafeMessageDigest(MessageDigest.getInstance("SHA-1"));
+        }
+        catch (NoSuchAlgorithmException e)
+        {
+            assert (false);
+        }
+    }
+
+    public SafeMessageDigest(MessageDigest md)
+    {
+        md_ = md;
+    }
+
+    public synchronized void update(byte[] theBytes)
+    {
+        md_.update(theBytes);
+    }
+
+    //NOTE: This should be used instead of seperate update() and then digest()
+    public synchronized byte[] digest(byte[] theBytes)
+    {
+        //this does an implicit update()
+        return md_.digest(theBytes);
+    }
+
+    public synchronized byte[] digest()
+    {
+        return md_.digest();
+    }
+
+    public byte[] unprotectedDigest()
+    {
+        return md_.digest();
+    }
+
+    public void unprotectedUpdate(byte[] theBytes)
+    {
+        md_.update(theBytes);
+    }
+
+    public byte[] unprotectedDigest(byte[] theBytes)
+    {
+        return md_.digest(theBytes);
+    }
+
+    public int getDigestLength()
+    {
+        return md_.getDigestLength();
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/XMLUtils.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/XMLUtils.java
index e69de29b..4907337f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/XMLUtils.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/utils/XMLUtils.java
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.util.*;
+import javax.xml.parsers.*;
+import javax.xml.transform.*;
+import javax.xml.xpath.XPath;
+import javax.xml.xpath.XPathConstants;
+import javax.xml.xpath.XPathExpression;
+import javax.xml.xpath.XPathExpressionException;
+import javax.xml.xpath.XPathFactory;
+import java.io.*;
+import org.w3c.dom.*;
+import org.xml.sax.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class XMLUtils
+{
+	private Document document_;
+    private XPath xpath_;
+
+    public XMLUtils(String xmlSrc) throws FileNotFoundException, ParserConfigurationException, SAXException, IOException
+    {        
+        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
+        DocumentBuilder db = dbf.newDocumentBuilder();
+        document_ = db.parse(xmlSrc);
+        
+        XPathFactory xpathFactory = XPathFactory.newInstance();
+        xpath_ = xpathFactory.newXPath();
+    }
+
+	public String getNodeValue(String xql) throws XPathExpressionException
+	{        
+        XPathExpression expr = xpath_.compile(xql);
+        String value = expr.evaluate(document_);
+        if ( value != null && value.equals("") )
+            value = null;
+        return value;	
+    }
+        
+	public String[] getNodeValues(String xql) throws XPathExpressionException
+	{
+        XPathExpression expr = xpath_.compile(xql);        
+        NodeList nl = (NodeList)expr.evaluate(document_, XPathConstants.NODESET);
+        int size = nl.getLength();
+        String[] values = new String[size];
+        
+        for ( int i = 0; i < size; ++i )
+        {
+            Node node = nl.item(i);
+            node = node.getFirstChild();
+            values[i] = node.getNodeValue();
+        }
+        return values;       		
+	}
+
+	public NodeList getRequestedNodeList(String xql) throws XPathExpressionException
+	{
+        XPathExpression expr = xpath_.compile(xql);
+        NodeList nodeList = (NodeList)expr.evaluate(document_, XPathConstants.NODESET);		
+		return nodeList;
+	}
+
+	public static String getAttributeValue(Node node, String attrName) throws TransformerException
+	{        
+		String value = null;
+		node = node.getAttributes().getNamedItem(attrName);
+		if ( node != null )
+		{
+		    value = node.getNodeValue();
+		}
+		return value;
+	}
+
+    public static void main(String[] args) throws Throwable
+    {
+        XMLUtils xmlUtils = new XMLUtils("C:\\Engagements\\Cassandra-Golden\\storage-conf.xml");
+        String[] value = xmlUtils.getNodeValues("/Storage/Seeds/Seed");
+        System.out.println(value);
+    }
+}
