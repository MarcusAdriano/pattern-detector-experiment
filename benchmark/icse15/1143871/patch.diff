diff --git a/lucene/dev/branches/flexscoring/lucene/src/java/org/apache/lucene/index/CompoundFileReader.java b/lucene/dev/branches/flexscoring/lucene/src/java/org/apache/lucene/index/CompoundFileReader.java
index e38fc7ba..e69de29b 100644
--- a/lucene/dev/branches/flexscoring/lucene/src/java/org/apache/lucene/index/CompoundFileReader.java
+++ b/lucene/dev/branches/flexscoring/lucene/src/java/org/apache/lucene/index/CompoundFileReader.java
@@ -1,307 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.BufferedIndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.Lock;
-
-import java.util.Collection;
-import java.util.HashMap;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-
-/**
- * Class for accessing a compound stream.
- * This class implements a directory, but is limited to only read operations.
- * Directory methods that would normally modify data throw an exception.
- * @lucene.experimental
- */
-public class CompoundFileReader extends Directory {
-  
-  private int readBufferSize;
-  
-  private static final class FileEntry {
-    long offset;
-    long length;
-  }
-  
-  // Base info
-  private Directory directory;
-  private String fileName;
-  
-  private IndexInput stream;
-  private HashMap<String,FileEntry> entries = new HashMap<String,FileEntry>();
-  
-  public CompoundFileReader(Directory dir, String name) throws IOException {
-    this(dir, name, BufferedIndexInput.BUFFER_SIZE);
-  }
-  
-  public CompoundFileReader(Directory dir, String name, int readBufferSize) throws IOException {
-    assert !(dir instanceof CompoundFileReader) : "compound file inside of compound file: " + name;
-    directory = dir;
-    fileName = name;
-    this.readBufferSize = readBufferSize;
-    
-    boolean success = false;
-    
-    try {
-      stream = dir.openInput(name, readBufferSize);
-      
-      // read the first VInt. If it is negative, it's the version number
-      // otherwise it's the count (pre-3.1 indexes)
-      int firstInt = stream.readVInt();
-      
-      final int count;
-      final boolean stripSegmentName;
-      if (firstInt < CompoundFileWriter.FORMAT_PRE_VERSION) {
-        if (firstInt < CompoundFileWriter.FORMAT_CURRENT) {
-          throw new CorruptIndexException("Incompatible format version: "
-              + firstInt + " expected " + CompoundFileWriter.FORMAT_CURRENT);
-        }
-        // It's a post-3.1 index, read the count.
-        count = stream.readVInt();
-        stripSegmentName = false;
-      } else {
-        count = firstInt;
-        stripSegmentName = true;
-      }
-      
-      // read the directory and init files
-      FileEntry entry = null;
-      for (int i=0; i<count; i++) {
-        long offset = stream.readLong();
-        String id = stream.readString();
-        
-        if (stripSegmentName) {
-          // Fix the id to not include the segment names. This is relevant for
-          // pre-3.1 indexes.
-          id = IndexFileNames.stripSegmentName(id);
-        }
-        
-        if (entry != null) {
-          // set length of the previous entry
-          entry.length = offset - entry.offset;
-        }
-        
-        entry = new FileEntry();
-        entry.offset = offset;
-        entries.put(id, entry);
-      }
-      
-      // set the length of the final entry
-      if (entry != null) {
-        entry.length = stream.length() - entry.offset;
-      }
-      
-      success = true;
-      
-    } finally {
-      if (!success && (stream != null)) {
-        try {
-          stream.close();
-        } catch (IOException e) { }
-      }
-    }
-  }
-  
-  public Directory getDirectory() {
-    return directory;
-  }
-  
-  public String getName() {
-    return fileName;
-  }
-  
-  @Override
-  public synchronized void close() throws IOException {
-    if (stream == null)
-      throw new IOException("Already closed");
-    
-    entries.clear();
-    stream.close();
-    stream = null;
-  }
-  
-  @Override
-  public synchronized IndexInput openInput(String id) throws IOException {
-    // Default to readBufferSize passed in when we were opened
-    return openInput(id, readBufferSize);
-  }
-  
-  @Override
-  public synchronized IndexInput openInput(String id, int readBufferSize) throws IOException {
-    if (stream == null)
-      throw new IOException("Stream closed");
-    
-    id = IndexFileNames.stripSegmentName(id);
-    final FileEntry entry = entries.get(id);
-    if (entry == null)
-      throw new IOException("No sub-file with id " + id + " found (files: " + entries.keySet() + ")");
-    
-    return new CSIndexInput(stream, entry.offset, entry.length, readBufferSize);
-  }
-  
-  /** Returns an array of strings, one for each file in the directory. */
-  @Override
-  public String[] listAll() {
-    String[] res = entries.keySet().toArray(new String[entries.size()]);
-    // Add the segment name
-    String seg = fileName.substring(0, fileName.indexOf('.'));
-    for (int i = 0; i < res.length; i++) {
-      res[i] = seg + res[i];
-    }
-    return res;
-  }
-  
-  /** Returns true iff a file with the given name exists. */
-  @Override
-  public boolean fileExists(String name) {
-    return entries.containsKey(IndexFileNames.stripSegmentName(name));
-  }
-  
-  /** Returns the time the compound file was last modified. */
-  @Override
-  public long fileModified(String name) throws IOException {
-    return directory.fileModified(fileName);
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException */
-  @Override
-  public void deleteFile(String name) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException */
-  public void renameFile(String from, String to) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Returns the length of a file in the directory.
-   * @throws IOException if the file does not exist */
-  @Override
-  public long fileLength(String name) throws IOException {
-    FileEntry e = entries.get(IndexFileNames.stripSegmentName(name));
-    if (e == null)
-      throw new FileNotFoundException(name);
-    return e.length;
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException */
-  @Override
-  public IndexOutput createOutput(String name) {
-    throw new UnsupportedOperationException();
-  }
-  
-  @Override
-  public void sync(Collection<String> names) throws IOException {
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException */
-  @Override
-  public Lock makeLock(String name) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Implementation of an IndexInput that reads from a portion of the
-   *  compound file. The visibility is left as "package" *only* because
-   *  this helps with testing since JUnit test cases in a different class
-   *  can then access package fields of this class.
-   */
-  static final class CSIndexInput extends BufferedIndexInput {
-    IndexInput base;
-    long fileOffset;
-    long length;
-    
-    CSIndexInput(final IndexInput base, final long fileOffset, final long length) {
-      this(base, fileOffset, length, BufferedIndexInput.BUFFER_SIZE);
-    }
-    
-    CSIndexInput(final IndexInput base, final long fileOffset, final long length, int readBufferSize) {
-      super(readBufferSize);
-      this.base = (IndexInput)base.clone();
-      this.fileOffset = fileOffset;
-      this.length = length;
-    }
-    
-    @Override
-    public Object clone() {
-      CSIndexInput clone = (CSIndexInput)super.clone();
-      clone.base = (IndexInput)base.clone();
-      clone.fileOffset = fileOffset;
-      clone.length = length;
-      return clone;
-    }
-    
-    /** Expert: implements buffer refill.  Reads bytes from the current
-     *  position in the input.
-     * @param b the array to read bytes into
-     * @param offset the offset in the array to start storing bytes
-     * @param len the number of bytes to read
-     */
-    @Override
-    protected void readInternal(byte[] b, int offset, int len) throws IOException {
-      long start = getFilePointer();
-      if(start + len > length)
-        throw new IOException("read past EOF");
-      base.seek(fileOffset + start);
-      base.readBytes(b, offset, len, false);
-    }
-    
-    /** Expert: implements seek.  Sets current position in this file, where
-     *  the next {@link #readInternal(byte[],int,int)} will occur.
-     * @see #readInternal(byte[],int,int)
-     */
-    @Override
-    protected void seekInternal(long pos) {}
-    
-    /** Closes the stream to further operations. */
-    @Override
-    public void close() throws IOException {
-      base.close();
-    }
-    
-    @Override
-    public long length() {
-      return length;
-    }
-    
-    @Override
-    public void copyBytes(IndexOutput out, long numBytes) throws IOException {
-      // Copy first whatever is in the buffer
-      numBytes -= flushBuffer(out, numBytes);
-      
-      // If there are more bytes left to copy, delegate the copy task to the
-      // base IndexInput, in case it can do an optimized copy.
-      if (numBytes > 0) {
-        long start = getFilePointer();
-        if (start + numBytes > length) {
-          throw new IOException("read past EOF");
-        }
-        base.seek(fileOffset + start);
-        base.copyBytes(out, numBytes);
-      }
-    }
-  }
-}
diff --git a/lucene/dev/branches/flexscoring/lucene/src/java/org/apache/lucene/index/CompoundFileWriter.java b/lucene/dev/branches/flexscoring/lucene/src/java/org/apache/lucene/index/CompoundFileWriter.java
index e47fed7b..e69de29b 100644
--- a/lucene/dev/branches/flexscoring/lucene/src/java/org/apache/lucene/index/CompoundFileWriter.java
+++ b/lucene/dev/branches/flexscoring/lucene/src/java/org/apache/lucene/index/CompoundFileWriter.java
@@ -1,252 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.LinkedList;
-
-import org.apache.lucene.index.codecs.MergeState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Combines multiple files into a single compound file.
- * The file format:<br>
- * <ul>
- *     <li>VInt fileCount</li>
- *     <li>{Directory}
- *         fileCount entries with the following structure:</li>
- *         <ul>
- *             <li>long dataOffset</li>
- *             <li>String fileName</li>
- *         </ul>
- *     <li>{File Data}
- *         fileCount entries with the raw data of the corresponding file</li>
- * </ul>
- *
- * The fileCount integer indicates how many files are contained in this compound
- * file. The {directory} that follows has that many entries. Each directory entry
- * contains a long pointer to the start of this file's data section, and a String
- * with that file's name.
- * 
- * @lucene.internal
- */
-public final class CompoundFileWriter {
-
-    private static final class FileEntry {
-        /** source file */
-        String file;
-
-        /** temporary holder for the start of directory entry for this file */
-        long directoryOffset;
-
-        /** temporary holder for the start of this file's data section */
-        long dataOffset;
-        
-        /** the directory which contains the file. */
-        Directory dir;
-    }
-
-    // Before versioning started.
-    static final int FORMAT_PRE_VERSION = 0;
-    
-    // Segment name is not written in the file names.
-    static final int FORMAT_NO_SEGMENT_PREFIX = -1;
-
-    // NOTE: if you introduce a new format, make it 1 lower
-    // than the current one, and always change this if you
-    // switch to a new format!
-    static final int FORMAT_CURRENT = FORMAT_NO_SEGMENT_PREFIX;
-
-    private Directory directory;
-    private String fileName;
-    private HashSet<String> ids;
-    private LinkedList<FileEntry> entries;
-    private boolean merged = false;
-    private MergeState.CheckAbort checkAbort;
-
-    /** Create the compound stream in the specified file. The file name is the
-     *  entire name (no extensions are added).
-     *  @throws NullPointerException if <code>dir</code> or <code>name</code> is null
-     */
-    public CompoundFileWriter(Directory dir, String name) {
-      this(dir, name, null);
-    }
-
-    CompoundFileWriter(Directory dir, String name, MergeState.CheckAbort checkAbort) {
-        if (dir == null)
-            throw new NullPointerException("directory cannot be null");
-        if (name == null)
-            throw new NullPointerException("name cannot be null");
-        this.checkAbort = checkAbort;
-        directory = dir;
-        fileName = name;
-        ids = new HashSet<String>();
-        entries = new LinkedList<FileEntry>();
-    }
-
-    /** Returns the directory of the compound file. */
-    public Directory getDirectory() {
-        return directory;
-    }
-
-    /** Returns the name of the compound file. */
-    public String getName() {
-        return fileName;
-    }
-
-    /** Add a source stream. <code>file</code> is the string by which the 
-     *  sub-stream will be known in the compound stream.
-     * 
-     *  @throws IllegalStateException if this writer is closed
-     *  @throws NullPointerException if <code>file</code> is null
-     *  @throws IllegalArgumentException if a file with the same name
-     *   has been added already
-     */
-    public void addFile(String file) {
-      addFile(file, directory);
-    }
-
-    /**
-     * Same as {@link #addFile(String)}, only for files that are found in an
-     * external {@link Directory}.
-     */
-    public void addFile(String file, Directory dir) {
-        if (merged)
-            throw new IllegalStateException(
-                "Can't add extensions after merge has been called");
-
-        if (file == null)
-            throw new NullPointerException(
-                "file cannot be null");
-
-        if (! ids.add(file))
-            throw new IllegalArgumentException(
-                "File " + file + " already added");
-
-        FileEntry entry = new FileEntry();
-        entry.file = file;
-        entry.dir = dir;
-        entries.add(entry);
-    }
-
-    /** Merge files with the extensions added up to now.
-     *  All files with these extensions are combined sequentially into the
-     *  compound stream.
-     *  @throws IllegalStateException if close() had been called before or
-     *   if no file has been added to this object
-     */
-    public void close() throws IOException {
-        if (merged)
-            throw new IllegalStateException("Merge already performed");
-
-        if (entries.isEmpty())
-            throw new IllegalStateException("No entries to merge have been defined");
-
-        merged = true;
-
-        // open the compound stream
-        IndexOutput os = directory.createOutput(fileName);
-        IOException priorException = null;
-        try {
-            // Write the Version info - must be a VInt because CFR reads a VInt
-            // in older versions!
-            os.writeVInt(FORMAT_CURRENT);
-            
-            // Write the number of entries
-            os.writeVInt(entries.size());
-
-            // Write the directory with all offsets at 0.
-            // Remember the positions of directory entries so that we can
-            // adjust the offsets later
-            long totalSize = 0;
-            for (FileEntry fe : entries) {
-                fe.directoryOffset = os.getFilePointer();
-                os.writeLong(0);    // for now
-                os.writeString(IndexFileNames.stripSegmentName(fe.file));
-                totalSize += fe.dir.fileLength(fe.file);
-            }
-
-            // Pre-allocate size of file as optimization --
-            // this can potentially help IO performance as
-            // we write the file and also later during
-            // searching.  It also uncovers a disk-full
-            // situation earlier and hopefully without
-            // actually filling disk to 100%:
-            final long finalLength = totalSize+os.getFilePointer();
-            os.setLength(finalLength);
-
-            // Open the files and copy their data into the stream.
-            // Remember the locations of each file's data section.
-            for (FileEntry fe : entries) {
-                fe.dataOffset = os.getFilePointer();
-                copyFile(fe, os);
-            }
-
-            // Write the data offsets into the directory of the compound stream
-            for (FileEntry fe : entries) {
-                os.seek(fe.directoryOffset);
-                os.writeLong(fe.dataOffset);
-            }
-
-            assert finalLength == os.length();
-
-            // Close the output stream. Set the os to null before trying to
-            // close so that if an exception occurs during the close, the
-            // finally clause below will not attempt to close the stream
-            // the second time.
-            IndexOutput tmp = os;
-            os = null;
-            tmp.close();
-        } catch (IOException e) {
-          priorException = e;
-        } finally {
-          IOUtils.closeSafely(priorException, os);
-        }
-    }
-
-  /**
-   * Copy the contents of the file with specified extension into the provided
-   * output stream.
-   */
-  private void copyFile(FileEntry source, IndexOutput os) throws IOException {
-    IndexInput is = source.dir.openInput(source.file);
-    try {
-      long startPtr = os.getFilePointer();
-      long length = is.length();
-      os.copyBytes(is, length);
-
-      if (checkAbort != null) {
-        checkAbort.work(length);
-      }
-
-      // Verify that the output length diff is equal to original file
-      long endPtr = os.getFilePointer();
-      long diff = endPtr - startPtr;
-      if (diff != length)
-        throw new IOException("Difference in the output file offsets " + diff
-            + " does not match the original file length " + length);
-
-    } finally {
-      is.close();
-    }
-  }
-}
diff --git a/lucene/dev/branches/flexscoring/solr/src/java/org/apache/solr/schema/TrieFieldHelper.java b/lucene/dev/branches/flexscoring/solr/src/java/org/apache/solr/schema/TrieFieldHelper.java
index c40ecd87..e69de29b 100644
--- a/lucene/dev/branches/flexscoring/solr/src/java/org/apache/solr/schema/TrieFieldHelper.java
+++ b/lucene/dev/branches/flexscoring/solr/src/java/org/apache/solr/schema/TrieFieldHelper.java
@@ -1,166 +0,0 @@
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.schema;
-
-import java.util.Date;
-
-import org.apache.lucene.analysis.NumericTokenStream;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Fieldable;
-
-/**
- * Helper class to make TrieFields compatible with ones written in solr
- * 
- * TODO -- Something like this should be in in lucene
- *  see: LUCENE-3001
- */
-public class TrieFieldHelper {
-  
-  private TrieFieldHelper() {}
-  
-  public static class FieldInfo {
-    public int precisionStep = 8; // same as solr default
-    public boolean store = true;
-    public boolean index = true;
-    public boolean omitNorms = true;
-    public boolean omitTF = true;
-  }
-
-  //----------------------------------------------
-  // Create Field
-  //----------------------------------------------
-
-  private static Fieldable createField(String name, byte[] arr, TokenStream ts, FieldInfo info, float boost) {
-
-    Field f;
-    if (info.store) {
-      f = new Field(name, arr);
-      if (info.index) f.setTokenStream(ts);
-    } else {
-      f = new Field(name, ts);
-    }
-
-    // term vectors aren't supported
-    f.setOmitNorms(info.omitNorms);
-    f.setOmitTermFreqAndPositions(info.omitTF);
-    f.setBoost(boost);
-    return f;
-  }
-
-  public static Fieldable createIntField(String name, int value, FieldInfo info, float boost) {
-
-    byte[] arr=null;
-    TokenStream ts=null;
-
-    if (info.store) arr = TrieFieldHelper.toArr(value);
-    if (info.index) ts = new NumericTokenStream(info.precisionStep).setIntValue(value);
-    
-    return createField(name, arr, ts, info, boost);
-  }
-
-  public static Fieldable createFloatField(String name, float value, FieldInfo info, float boost) {
-
-    byte[] arr=null;
-    TokenStream ts=null;
-
-    if (info.store) arr = TrieFieldHelper.toArr(value);
-    if (info.index) ts = new NumericTokenStream(info.precisionStep).setFloatValue(value);
-    
-    return createField(name, arr, ts, info, boost);
-  }
-
-  public static Fieldable createLongField(String name, long value, FieldInfo info, float boost) {
-
-    byte[] arr=null;
-    TokenStream ts=null;
-
-    if (info.store) arr = TrieFieldHelper.toArr(value);
-    if (info.index) ts = new NumericTokenStream(info.precisionStep).setLongValue(value);
-    
-    return createField(name, arr, ts, info, boost);
-  }
-
-  public static Fieldable createDoubleField(String name, double value, FieldInfo info, float boost) {
-
-    byte[] arr=null;
-    TokenStream ts=null;
-
-    if (info.store) arr = TrieFieldHelper.toArr(value);
-    if (info.index) ts = new NumericTokenStream(info.precisionStep).setDoubleValue(value);
-    
-    return createField(name, arr, ts, info, boost);
-  }
-
-  public static Fieldable createDateField(String name, Date value, FieldInfo info, float boost) {
-    // TODO, make sure the date is within long range!
-    return createLongField(name, value.getTime(), info, boost);
-  }
-  
-  
-  //----------------------------------------------
-  // number <=> byte[]
-  //----------------------------------------------
-
-  public static int toInt(byte[] arr) {
-    return (arr[0]<<24) | ((arr[1]&0xff)<<16) | ((arr[2]&0xff)<<8) | (arr[3]&0xff);
-  }
-  
-  public static long toLong(byte[] arr) {
-    int high = (arr[0]<<24) | ((arr[1]&0xff)<<16) | ((arr[2]&0xff)<<8) | (arr[3]&0xff);
-    int low = (arr[4]<<24) | ((arr[5]&0xff)<<16) | ((arr[6]&0xff)<<8) | (arr[7]&0xff);
-    return (((long)high)<<32) | (low&0x0ffffffffL);
-  }
-
-  public static float toFloat(byte[] arr) {
-    return Float.intBitsToFloat(toInt(arr));
-  }
-
-  public static double toDouble(byte[] arr) {
-    return Double.longBitsToDouble(toLong(arr));
-  }
-
-  public static byte[] toArr(int val) {
-    byte[] arr = new byte[4];
-    arr[0] = (byte)(val>>>24);
-    arr[1] = (byte)(val>>>16);
-    arr[2] = (byte)(val>>>8);
-    arr[3] = (byte)(val);
-    return arr;
-  }
-
-  public static byte[] toArr(long val) {
-    byte[] arr = new byte[8];
-    arr[0] = (byte)(val>>>56);
-    arr[1] = (byte)(val>>>48);
-    arr[2] = (byte)(val>>>40);
-    arr[3] = (byte)(val>>>32);
-    arr[4] = (byte)(val>>>24);
-    arr[5] = (byte)(val>>>16);
-    arr[6] = (byte)(val>>>8);
-    arr[7] = (byte)(val);
-    return arr;
-  }
-
-  public static byte[] toArr(float val) {
-    return toArr(Float.floatToRawIntBits(val));
-  }
-
-  public static byte[] toArr(double val) {
-    return toArr(Double.doubleToRawLongBits(val));
-  }
-}
diff --git a/lucene/dev/branches/flexscoring/solr/src/java/org/apache/solr/search/MutableValueBool.java b/lucene/dev/branches/flexscoring/solr/src/java/org/apache/solr/search/MutableValueBool.java
index 2e7f8c64..e69de29b 100644
--- a/lucene/dev/branches/flexscoring/solr/src/java/org/apache/solr/search/MutableValueBool.java
+++ b/lucene/dev/branches/flexscoring/solr/src/java/org/apache/solr/search/MutableValueBool.java
@@ -1,62 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.solr.search;
-
-import org.apache.lucene.util.mutable.MutableValue;
-
-public class MutableValueBool extends MutableValue {
-  public boolean value;
-
-  @Override
-  public Object toObject() {
-    return exists ? value : null;
-  }
-
-  @Override
-  public void copy(MutableValue source) {
-    MutableValueBool s = (MutableValueBool) source;
-    value = s.value;
-    exists = s.exists;
-  }
-
-  @Override
-  public MutableValue duplicate() {
-    MutableValueBool v = new MutableValueBool();
-    v.value = this.value;
-    v.exists = this.exists;
-    return v;
-  }
-
-  @Override
-  public boolean equalsSameType(Object other) {
-    MutableValueBool b = (MutableValueBool)other;
-    return value == b.value && exists == b.exists;
-  }
-
-  @Override
-  public int compareSameType(Object other) {
-    MutableValueBool b = (MutableValueBool)other;
-    if (value != b.value) return value ? 1 : 0;
-    if (exists == b.exists) return 0;
-    return exists ? 1 : -1;
-  }
-
-  @Override
-  public int hashCode() {
-    return value ? 2 : (exists ? 1 : 0);
-  }
-}
