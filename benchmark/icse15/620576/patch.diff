diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/java/trunk/src/java/org/apache/lucene/index/CheckIndex.java
index 79504251..b11f799a 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/CheckIndex.java
@@ -113,7 +113,10 @@ else if (format == SegmentInfos.FORMAT_SINGLE_NORM_FILE)
       sFormat = "FORMAT_SINGLE_NORM_FILE [Lucene 2.2]";
     else if (format == SegmentInfos.FORMAT_SHARED_DOC_STORE)
       sFormat = "FORMAT_SHARED_DOC_STORE [Lucene 2.3]";
-    else if (format < SegmentInfos.FORMAT_SHARED_DOC_STORE) {
+    else if (format < SegmentInfos.FORMAT_CHECKSUM) {
+      sFormat = "FORMAT_CHECKSUM [Lucene 2.4]";
+      skip = true;
+    } else if (format < SegmentInfos.FORMAT_CHECKSUM) {
       sFormat = "int=" + format + " [newer version of Lucene than this tool]";
       skip = true;
     } else {
@@ -320,7 +323,7 @@ else if (format < SegmentInfos.FORMAT_SHARED_DOC_STORE) {
       }
       out.print("Writing...");
       try {
-        newSIS.write(dir);
+        newSIS.commit(dir);
       } catch (Throwable t) {
         out.println("FAILED; exiting");
         t.printStackTrace(out);
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java b/lucene/java/trunk/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java
index b954f965..9ea7903b 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java
@@ -46,6 +46,7 @@
 
   private boolean closed;
   protected IndexWriter writer;
+  protected int mergeThreadCount;
 
   public ConcurrentMergeScheduler() {
     if (allInstances != null) {
@@ -211,10 +212,11 @@ protected void doMerge(MergePolicy.OneMerge merge)
   }
 
   /** Create and return a new MergeThread */
-  protected MergeThread getMergeThread(IndexWriter writer, MergePolicy.OneMerge merge) throws IOException {
+  protected synchronized MergeThread getMergeThread(IndexWriter writer, MergePolicy.OneMerge merge) throws IOException {
     final MergeThread thread = new MergeThread(writer, merge);
     thread.setThreadPriority(mergeThreadPriority);
     thread.setDaemon(true);
+    thread.setName("Lucene Merge Thread #" + mergeThreadCount++);
     return thread;
   }
 
@@ -297,9 +299,9 @@ public void run() {
         }
       } finally {
         synchronized(ConcurrentMergeScheduler.this) {
+          ConcurrentMergeScheduler.this.notifyAll();
           boolean removed = mergeThreads.remove(this);
           assert removed;
-          ConcurrentMergeScheduler.this.notifyAll();
         }
       }
     }
@@ -334,6 +336,12 @@ public static boolean anyUnhandledExceptions() {
     }
   }
 
+  public static void clearUnhandledExceptions() {
+    synchronized(allInstances) {
+      anyExceptions = false;
+    }
+  }
+
   /** Used for testing */
   private void addMyself() {
     synchronized(allInstances) {
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/DirectoryIndexReader.java b/lucene/java/trunk/src/java/org/apache/lucene/index/DirectoryIndexReader.java
index 658656e3..a1726e8a 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/DirectoryIndexReader.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/DirectoryIndexReader.java
@@ -19,6 +19,9 @@
 
 import java.io.IOException;
 
+import java.util.HashSet;
+import java.util.List;
+
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockObtainFailedException;
@@ -37,6 +40,7 @@
   private SegmentInfos segmentInfos;
   private Lock writeLock;
   private boolean stale;
+  private HashSet synced = new HashSet();
 
   /** Used by commit() to record pre-commit state in case
    * rollback is necessary */
@@ -44,16 +48,28 @@
   private SegmentInfos rollbackSegmentInfos;
 
   
-  void init(Directory directory, SegmentInfos segmentInfos, boolean closeDirectory) {
+  void init(Directory directory, SegmentInfos segmentInfos, boolean closeDirectory)
+    throws IOException {
     this.directory = directory;
     this.segmentInfos = segmentInfos;
     this.closeDirectory = closeDirectory;
+
+    if (segmentInfos != null) {
+      // We assume that this segments_N was previously
+      // properly sync'd:
+      for(int i=0;i<segmentInfos.size();i++) {
+        final SegmentInfo info = segmentInfos.info(i);
+        List files = info.files();
+        for(int j=0;j<files.size();j++)
+          synced.add(files.get(j));
+      }
+    }
   }
   
   protected DirectoryIndexReader() {}
   
   DirectoryIndexReader(Directory directory, SegmentInfos segmentInfos,
-      boolean closeDirectory) {
+      boolean closeDirectory) throws IOException {
     super();
     init(directory, segmentInfos, closeDirectory);
   }
@@ -190,7 +206,22 @@ protected void doCommit() throws IOException {
         boolean success = false;
         try {
           commitChanges();
-          segmentInfos.write(directory);
+
+          // Sync all files we just wrote
+          for(int i=0;i<segmentInfos.size();i++) {
+            final SegmentInfo info = segmentInfos.info(i);
+            final List files = info.files();
+            for(int j=0;j<files.size();j++) {
+              final String fileName = (String) files.get(j);
+              if (!synced.contains(fileName)) {
+                assert directory.fileExists(fileName);
+                directory.sync(fileName);
+                synced.add(fileName);
+              }
+            }
+          }
+
+          segmentInfos.commit(directory);
           success = true;
         } finally {
 
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java
index 011bd0a0..e135ec6b 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -453,6 +453,7 @@ else if (t instanceof Error)
           assert false: "unknown exception: " + t;
       }
     } finally {
+      if (ae != null)
       abortCount--;
       notifyAll();
     }
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/IndexFileDeleter.java b/lucene/java/trunk/src/java/org/apache/lucene/index/IndexFileDeleter.java
index ec66ecd1..9560483d 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/IndexFileDeleter.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/IndexFileDeleter.java
@@ -34,11 +34,18 @@
  * This class keeps track of each SegmentInfos instance that
  * is still "live", either because it corresponds to a 
  * segments_N file in the Directory (a "commit", i.e. a 
- * committed SegmentInfos) or because it's the in-memory SegmentInfos 
- * that a writer is actively updating but has not yet committed 
- * (currently this only applies when autoCommit=false in IndexWriter).
- * This class uses simple reference counting to map the live
- * SegmentInfos instances to individual files in the Directory. 
+ * committed SegmentInfos) or because it's an in-memory
+ * SegmentInfos that a writer is actively updating but has
+ * not yet committed.  This class uses simple reference
+ * counting to map the live SegmentInfos instances to
+ * individual files in the Directory.
+ *
+ * When autoCommit=true, IndexWriter currently commits only
+ * on completion of a merge (though this may change with
+ * time: it is not a guarantee).  When autoCommit=false,
+ * IndexWriter only commits when it is closed.  Regardless
+ * of autoCommit, the user may call IndexWriter.commit() to
+ * force a blocking commit.
  * 
  * The same directory file may be referenced by more than
  * one IndexCommitPoints, i.e. more than one SegmentInfos.
@@ -260,7 +267,7 @@ private void deleteCommits() throws IOException {
       for(int i=0;i<size;i++) {
         CommitPoint commit = (CommitPoint) commitsToDelete.get(i);
         if (infoStream != null) {
-          message("deleteCommits: now remove commit \"" + commit.getSegmentsFileName() + "\"");
+          message("deleteCommits: now decRef commit \"" + commit.getSegmentsFileName() + "\"");
         }
         int size2 = commit.files.size();
         for(int j=0;j<size2;j++) {
@@ -382,13 +389,6 @@ public void checkpoint(SegmentInfos segmentInfos, boolean isCommit) throws IOExc
 
     // Incref the files:
     incRef(segmentInfos, isCommit);
-    final List docWriterFiles;
-    if (docWriter != null) {
-      docWriterFiles = docWriter.files();
-      if (docWriterFiles != null)
-        incRef(docWriterFiles);
-    } else
-      docWriterFiles = null;
 
     if (isCommit) {
       // Append to our commits list:
@@ -399,7 +399,18 @@ public void checkpoint(SegmentInfos segmentInfos, boolean isCommit) throws IOExc
 
       // Decref files for commits that were deleted by the policy:
       deleteCommits();
-    }
+    } else {
+
+      final List docWriterFiles;
+      if (docWriter != null) {
+        docWriterFiles = docWriter.files();
+        if (docWriterFiles != null)
+          // We must incRef thes files before decRef'ing
+          // last files to make sure we don't accidentally
+          // delete them:
+          incRef(docWriterFiles);
+      } else
+        docWriterFiles = null;
 
     // DecRef old files from the last checkpoint, if any:
     int size = lastFiles.size();
@@ -409,7 +420,6 @@ public void checkpoint(SegmentInfos segmentInfos, boolean isCommit) throws IOExc
       lastFiles.clear();
     }
 
-    if (!isCommit) {
       // Save files so we can decr on next checkpoint/commit:
       size = segmentInfos.size();
       for(int i=0;i<size;i++) {
@@ -418,10 +428,10 @@ public void checkpoint(SegmentInfos segmentInfos, boolean isCommit) throws IOExc
           lastFiles.add(segmentInfo.files());
         }
       }
-    }
     if (docWriterFiles != null)
       lastFiles.add(docWriterFiles);
   }
+  }
 
   void incRef(SegmentInfos segmentInfos, boolean isCommit) throws IOException {
     int size = segmentInfos.size();
@@ -458,7 +468,7 @@ void decRef(List files) throws IOException {
     }
   }
 
-  private void decRef(String fileName) throws IOException {
+  void decRef(String fileName) throws IOException {
     RefCount rc = getRefCount(fileName);
     if (infoStream != null && VERBOSE_REF_COUNTS) {
       message("  DecRef \"" + fileName + "\": pre-decr count is " + rc.count);
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/java/trunk/src/java/org/apache/lucene/index/IndexWriter.java
index fb811285..b848c320 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/IndexWriter.java
@@ -27,11 +27,13 @@
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.Parameter;
+import org.apache.lucene.util.Constants;
 
 import java.io.File;
 import java.io.IOException;
 import java.io.PrintStream;
 import java.util.List;
+import java.util.Collection;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Set;
@@ -83,33 +85,45 @@ addDocument calls (see <a href="#mergePolicy">below</a>
   for changing the {@link MergeScheduler}).</p>
 
   <a name="autoCommit"></a>
-  <p>The optional <code>autoCommit</code> argument to the
-  <a href="#IndexWriter(org.apache.lucene.store.Directory, boolean, org.apache.lucene.analysis.Analyzer)"><b>constructors</b></a>
-  controls visibility of the changes to {@link IndexReader} instances reading the same index.
-  When this is <code>false</code>, changes are not
-  visible until {@link #close()} is called.
-  Note that changes will still be flushed to the
-  {@link org.apache.lucene.store.Directory} as new files,
-  but are not committed (no new <code>segments_N</code> file
-  is written referencing the new files) until {@link #close} is
-  called.  If something goes terribly wrong (for example the
-  JVM crashes) before {@link #close()}, then
-  the index will reflect none of the changes made (it will
-  remain in its starting state).
-  You can also call {@link #abort()}, which closes the writer without committing any
-  changes, and removes any index
+  <p>[<b>Deprecated</b>: Note that in 3.0, IndexWriter will
+  no longer accept autoCommit=true (it will be hardwired to
+  false).  You can always call {@link IndexWriter#commit()} yourself
+  when needed].  The optional <code>autoCommit</code> argument to the <a
+  href="#IndexWriter(org.apache.lucene.store.Directory,
+  boolean,
+  org.apache.lucene.analysis.Analyzer)"><b>constructors</b></a>
+  controls visibility of the changes to {@link IndexReader}
+  instances reading the same index.  When this is
+  <code>false</code>, changes are not visible until {@link
+  #close()} is called.  Note that changes will still be
+  flushed to the {@link org.apache.lucene.store.Directory}
+  as new files, but are not committed (no new
+  <code>segments_N</code> file is written referencing the
+  new files, nor are the files sync'd to stable storage)
+  until {@link #commit} or {@link #close} is called.  If something
+  goes terribly wrong (for example the JVM crashes), then
+  the index will reflect none of the changes made since the
+  last commit, or the starting state if commit was not called.
+  You can also call {@link #abort}, which closes the writer
+  without committing any changes, and removes any index
   files that had been flushed but are now unreferenced.
   This mode is useful for preventing readers from refreshing
   at a bad time (for example after you've done all your
-  deletes but before you've done your adds).
-  It can also be used to implement simple single-writer
-  transactional semantics ("all or none").</p>
+  deletes but before you've done your adds).  It can also be
+  used to implement simple single-writer transactional
+  semantics ("all or none").</p>
 
   <p>When <code>autoCommit</code> is <code>true</code> then
-  every flush is also a commit ({@link IndexReader}
-  instances will see each flush as changes to the index).
-  This is the default, to match the behavior before 2.2.
-  When running in this mode, be careful not to refresh your
+  the writer will periodically commit on its own.  This is
+  the default, to match the behavior before 2.2.  However,
+  in 3.0, autoCommit will be hardwired to false.  There is
+  no guarantee when exactly an auto commit will occur (it
+  used to be after every flush, but it is now after every
+  completed merge, as of 2.4).  If you want to force a
+  commit, call {@link #commit}, or, close the writer.  Once
+  a commit has finished, ({@link IndexReader} instances will
+  see the changes to the index as of that commit.  When
+  running in this mode, be careful not to refresh your
   readers while optimize or segment merges are taking place
   as this can tie up substantial disk space.</p>
   
@@ -251,6 +265,19 @@ also selects merges to do for optimize().  (The default is
    */
   public final static int MAX_TERM_LENGTH = DocumentsWriter.MAX_TERM_LENGTH;
   
+  /**
+   * Default for {@link #getMaxSyncPauseSeconds}.  On
+   * Windows this defaults to 10.0 seconds; elsewhere it's
+   * 0.
+   */
+  public final static double DEFAULT_MAX_SYNC_PAUSE_SECONDS;
+  static {
+    if (Constants.WINDOWS)
+      DEFAULT_MAX_SYNC_PAUSE_SECONDS = 10.0;
+    else
+      DEFAULT_MAX_SYNC_PAUSE_SECONDS = 0.0;
+  }
+
   // The normal read buffer size defaults to 1024, but
   // increasing this during merging seems to yield
   // performance gains.  However we don't want to increase
@@ -269,14 +296,18 @@ also selects merges to do for optimize().  (The default is
 
   private Similarity similarity = Similarity.getDefault(); // how to normalize
 
-  private boolean commitPending; // true if segmentInfos has changes not yet committed
+  private volatile boolean commitPending; // true if segmentInfos has changes not yet committed
   private SegmentInfos rollbackSegmentInfos;      // segmentInfos we will fallback to if the commit fails
+  private HashMap rollbackSegments;
 
   private SegmentInfos localRollbackSegmentInfos;      // segmentInfos we will fallback to if the commit fails
   private boolean localAutoCommit;                // saved autoCommit during local transaction
   private boolean autoCommit = true;              // false if we should commit only on close
 
   private SegmentInfos segmentInfos = new SegmentInfos();       // the segments
+  private int syncCount;
+  private int syncCountSaved = -1;
+
   private DocumentsWriter docWriter;
   private IndexFileDeleter deleter;
 
@@ -302,6 +333,12 @@ also selects merges to do for optimize().  (The default is
   private long mergeGen;
   private boolean stopMerges;
 
+  private int flushCount;
+  private double maxSyncPauseSeconds = DEFAULT_MAX_SYNC_PAUSE_SECONDS;
+
+  // Last (right most) SegmentInfo created by a merge
+  private SegmentInfo lastMergeInfo;
+
   /**
    * Used internally to throw an {@link
    * AlreadyClosedException} if this IndexWriter has been
@@ -432,7 +469,9 @@ public int getTermIndexInterval() {
    * Constructs an IndexWriter for the index in <code>path</code>.
    * Text will be analyzed with <code>a</code>.  If <code>create</code>
    * is true, then a new, empty index will be created in
-   * <code>path</code>, replacing the index already there, if any.
+   * <code>path</code>, replacing the index already there,
+   * if any.  Note that autoCommit defaults to true, but
+   * starting in 3.0 it will be hardwired to false.
    *
    * @param path the path to the index directory
    * @param a the analyzer to use
@@ -487,6 +526,8 @@ public IndexWriter(String path, Analyzer a, boolean create)
    * Text will be analyzed with <code>a</code>.  If <code>create</code>
    * is true, then a new, empty index will be created in
    * <code>path</code>, replacing the index already there, if any.
+   * Note that autoCommit defaults to true, but starting in 3.0
+   * it will be hardwired to false.
    *
    * @param path the path to the index directory
    * @param a the analyzer to use
@@ -541,6 +582,8 @@ public IndexWriter(File path, Analyzer a, boolean create)
    * Text will be analyzed with <code>a</code>.  If <code>create</code>
    * is true, then a new, empty index will be created in
    * <code>d</code>, replacing the index already there, if any.
+   * Note that autoCommit defaults to true, but starting in 3.0
+   * it will be hardwired to false.
    *
    * @param d the index directory
    * @param a the analyzer to use
@@ -595,6 +638,8 @@ public IndexWriter(Directory d, Analyzer a, boolean create)
    * <code>path</code>, first creating it if it does not
    * already exist.  Text will be analyzed with
    * <code>a</code>.
+   * Note that autoCommit defaults to true, but starting in 3.0
+   * it will be hardwired to false.
    *
    * @param path the path to the index directory
    * @param a the analyzer to use
@@ -641,6 +686,8 @@ public IndexWriter(String path, Analyzer a)
    * <code>path</code>, first creating it if it does not
    * already exist.  Text will be analyzed with
    * <code>a</code>.
+   * Note that autoCommit defaults to true, but starting in 3.0
+   * it will be hardwired to false.
    *
    * @param path the path to the index directory
    * @param a the analyzer to use
@@ -687,6 +734,8 @@ public IndexWriter(File path, Analyzer a)
    * <code>d</code>, first creating it if it does not
    * already exist.  Text will be analyzed with
    * <code>a</code>.
+   * Note that autoCommit defaults to true, but starting in 3.0
+   * it will be hardwired to false.
    *
    * @param d the index directory
    * @param a the analyzer to use
@@ -746,6 +795,10 @@ public IndexWriter(Directory d, Analyzer a)
    * @throws IOException if the directory cannot be
    *  read/written to or if there is any other low-level
    *  IO error
+   * @deprecated This will be removed in 3.0, when
+   * autoCommit will be hardwired to false.  Use {@link
+   * #IndexWriter(Directory,Analyzer,MaxFieldLength)}
+   * instead, and call {@link #commit} when needed.
    */
   public IndexWriter(Directory d, boolean autoCommit, Analyzer a, MaxFieldLength mfl)
     throws CorruptIndexException, LockObtainFailedException, IOException {
@@ -798,6 +851,10 @@ public IndexWriter(Directory d, boolean autoCommit, Analyzer a)
    *  if it does not exist and <code>create</code> is
    *  <code>false</code> or if there is any other low-level
    *  IO error
+   * @deprecated This will be removed in 3.0, when
+   * autoCommit will be hardwired to false.  Use {@link
+   * #IndexWriter(Directory,Analyzer,boolean,MaxFieldLength)}
+   * instead, and call {@link #commit} when needed.
    */
   public IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, MaxFieldLength mfl)
        throws CorruptIndexException, LockObtainFailedException, IOException {
@@ -832,6 +889,31 @@ public IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create)
     init(d, a, create, false, null, autoCommit, DEFAULT_MAX_FIELD_LENGTH);
   }
 
+  /**
+   * Expert: constructs an IndexWriter with a custom {@link
+   * IndexDeletionPolicy}, for the index in <code>d</code>,
+   * first creating it if it does not already exist.  Text
+   * will be analyzed with <code>a</code>.
+   * Note that autoCommit defaults to true, but starting in 3.0
+   * it will be hardwired to false.
+   *
+   * @param d the index directory
+   * @param a the analyzer to use
+   * @param deletionPolicy see <a href="#deletionPolicy">above</a>
+   * @param mfl whether or not to limit field lengths
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws LockObtainFailedException if another writer
+   *  has this index open (<code>write.lock</code> could not
+   *  be obtained)
+   * @throws IOException if the directory cannot be
+   *  read/written to or if there is any other low-level
+   *  IO error
+   */
+  public IndexWriter(Directory d, Analyzer a, IndexDeletionPolicy deletionPolicy, MaxFieldLength mfl)
+    throws CorruptIndexException, LockObtainFailedException, IOException {
+    init(d, a, false, deletionPolicy, true, mfl.getLimit());
+  }
+
   /**
    * Expert: constructs an IndexWriter with a custom {@link
    * IndexDeletionPolicy}, for the index in <code>d</code>,
@@ -851,6 +933,10 @@ public IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create)
    * @throws IOException if the directory cannot be
    *  read/written to or if there is any other low-level
    *  IO error
+   * @deprecated This will be removed in 3.0, when
+   * autoCommit will be hardwired to false.  Use {@link
+   * #IndexWriter(Directory,Analyzer,IndexDeletionPolicy,MaxFieldLength)}
+   * instead, and call {@link #commit} when needed.
    */
   public IndexWriter(Directory d, boolean autoCommit, Analyzer a, IndexDeletionPolicy deletionPolicy, MaxFieldLength mfl)
     throws CorruptIndexException, LockObtainFailedException, IOException {
@@ -882,6 +968,37 @@ public IndexWriter(Directory d, boolean autoCommit, Analyzer a, IndexDeletionPol
     init(d, a, false, deletionPolicy, autoCommit, DEFAULT_MAX_FIELD_LENGTH);
   }
   
+  /**
+   * Expert: constructs an IndexWriter with a custom {@link
+   * IndexDeletionPolicy}, for the index in <code>d</code>.
+   * Text will be analyzed with <code>a</code>.  If
+   * <code>create</code> is true, then a new, empty index
+   * will be created in <code>d</code>, replacing the index
+   * already there, if any.
+   * Note that autoCommit defaults to true, but starting in 3.0
+   * it will be hardwired to false.
+   *
+   * @param d the index directory
+   * @param a the analyzer to use
+   * @param create <code>true</code> to create the index or overwrite
+   *  the existing one; <code>false</code> to append to the existing
+   *  index
+   * @param deletionPolicy see <a href="#deletionPolicy">above</a>
+   * @param mfl whether or not to limit field lengths
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws LockObtainFailedException if another writer
+   *  has this index open (<code>write.lock</code> could not
+   *  be obtained)
+   * @throws IOException if the directory cannot be read/written to, or
+   *  if it does not exist and <code>create</code> is
+   *  <code>false</code> or if there is any other low-level
+   *  IO error
+   */
+  public IndexWriter(Directory d, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy, MaxFieldLength mfl)
+       throws CorruptIndexException, LockObtainFailedException, IOException {
+    init(d, a, create, false, deletionPolicy, true, mfl.getLimit());
+  }
+
   /**
    * Expert: constructs an IndexWriter with a custom {@link
    * IndexDeletionPolicy}, for the index in <code>d</code>.
@@ -907,6 +1024,10 @@ public IndexWriter(Directory d, boolean autoCommit, Analyzer a, IndexDeletionPol
    *  if it does not exist and <code>create</code> is
    *  <code>false</code> or if there is any other low-level
    *  IO error
+   * @deprecated This will be removed in 3.0, when
+   * autoCommit will be hardwired to false.  Use {@link
+   * #IndexWriter(Directory,Analyzer,boolean,IndexDeletionPolicy,MaxFieldLength)}
+   * instead, and call {@link #commit} when needed.
    */
   public IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy, MaxFieldLength mfl)
        throws CorruptIndexException, LockObtainFailedException, IOException {
@@ -984,15 +1105,22 @@ private void init(Directory d, Analyzer a, final boolean create, boolean closeDi
         } catch (IOException e) {
           // Likely this means it's a fresh directory
         }
-        segmentInfos.write(directory);
+        segmentInfos.commit(directory);
       } else {
         segmentInfos.read(directory);
+
+        // We assume that this segments_N was previously
+        // properly sync'd:
+        for(int i=0;i<segmentInfos.size();i++) {
+          final SegmentInfo info = segmentInfos.info(i);
+          List files = info.files();
+          for(int j=0;j<files.size();j++)
+            synced.add(files.get(j));
+        }
       }
 
       this.autoCommit = autoCommit;
-      if (!autoCommit) {
-        rollbackSegmentInfos = (SegmentInfos) segmentInfos.clone();
-      }
+      setRollbackSegmentInfos();
 
       docWriter = new DocumentsWriter(directory, this);
       docWriter.setInfoStream(infoStream);
@@ -1017,6 +1145,14 @@ private void init(Directory d, Analyzer a, final boolean create, boolean closeDi
     }
   }
 
+  private void setRollbackSegmentInfos() {
+    rollbackSegmentInfos = (SegmentInfos) segmentInfos.clone();
+    rollbackSegments = new HashMap();
+    final int size = rollbackSegmentInfos.size();
+    for(int i=0;i<size;i++)
+      rollbackSegments.put(rollbackSegmentInfos.info(i), new Integer(i));
+  }
+
   /**
    * Expert: set the merge policy used by this writer.
    */
@@ -1309,6 +1445,31 @@ public int getMergeFactor() {
     return getLogMergePolicy().getMergeFactor();
   }
 
+  /**
+   * Expert: returns max delay inserted before syncing a
+   * commit point.  On Windows, at least, pausing before
+   * syncing can increase net indexing throughput.  The
+   * delay is variable based on size of the segment's files,
+   * and is only inserted when using
+   * ConcurrentMergeScheduler for merges.
+   * @deprecated This will be removed in 3.0, when
+   * autoCommit=true is removed from IndexWriter.
+   */
+  public double getMaxSyncPauseSeconds() {
+    return maxSyncPauseSeconds;
+  }
+
+  /**
+   * Expert: sets the max delay before syncing a commit
+   * point.
+   * @see #getMaxSyncPauseSeconds
+   * @deprecated This will be removed in 3.0, when
+   * autoCommit=true is removed from IndexWriter.
+   */
+  public void setMaxSyncPauseSeconds(double seconds) {
+    maxSyncPauseSeconds = seconds;
+  }
+
   /** If non-null, this will be the default infoStream used
    * by a newly instantiated IndexWriter.
    * @see #setInfoStream
@@ -1397,8 +1558,11 @@ public static long getDefaultWriteLockTimeout() {
   }
 
   /**
-   * Flushes all changes to an index and closes all
-   * associated files.
+   * Commits all changes to an index and closes all
+   * associated files.  Note that this may be a costly
+   * operation, so, try to re-use a single writer instead of
+   * closing and opening a new one.  See {@link #commit} for
+   * caveats about write caching done by some IO devices.
    *
    * <p> If an Exception is hit during close, eg due to disk
    * full or some other reason, then both the on-disk index
@@ -1490,33 +1654,16 @@ private void closeInternal(boolean waitForMerges) throws CorruptIndexException,
 
       mergeScheduler.close();
 
-      synchronized(this) {
-        if (commitPending) {
-          boolean success = false;
-          try {
-            segmentInfos.write(directory);         // now commit changes
-            success = true;
-          } finally {
-            if (!success) {
               if (infoStream != null)
-                message("hit exception committing segments file during close");
-              deletePartialSegmentsFile();
-            }
-          }
-          if (infoStream != null)
-            message("close: wrote segments file \"" + segmentInfos.getCurrentSegmentFileName() + "\"");
+        message("now call final sync()");
 
-          deleter.checkpoint(segmentInfos, true);
-
-          commitPending = false;
-          rollbackSegmentInfos = null;
-        }
+      sync(true, 0);
 
         if (infoStream != null)
           message("at close: " + segString());
 
+      synchronized(this) {
         docWriter = null;
-
         deleter.close();
       }
       
@@ -1527,7 +1674,9 @@ private void closeInternal(boolean waitForMerges) throws CorruptIndexException,
         writeLock.release();                          // release write lock
         writeLock = null;
       }
+      synchronized(this) {
       closed = true;
+      }
 
     } finally {
       synchronized(this) {
@@ -1581,34 +1730,24 @@ private synchronized boolean flushDocStores() throws IOException {
       
           // Perform the merge
           cfsWriter.close();
-
-          for(int i=0;i<numSegments;i++) {
-            SegmentInfo si = segmentInfos.info(i);
-            if (si.getDocStoreOffset() != -1 &&
-                si.getDocStoreSegment().equals(docStoreSegment))
-              si.setDocStoreIsCompoundFile(true);
-          }
-          checkpoint();
           success = true;
+
         } finally {
           if (!success) {
-
             if (infoStream != null)
               message("hit exception building compound file doc store for segment " + docStoreSegment);
+            deleter.deleteFile(compoundFileName);
+          }
+        }
             
-            // Rollback to no compound file
             for(int i=0;i<numSegments;i++) {
               SegmentInfo si = segmentInfos.info(i);
               if (si.getDocStoreOffset() != -1 &&
                   si.getDocStoreSegment().equals(docStoreSegment))
-                si.setDocStoreIsCompoundFile(false);
-            }
-            deleter.deleteFile(compoundFileName);
-            deletePartialSegmentsFile();
-          }
+            si.setDocStoreIsCompoundFile(true);
         }
 
-        deleter.checkpoint(segmentInfos, false);
+        checkpoint();
       }
     }
 
@@ -1851,6 +1990,11 @@ final synchronized int getDocCount(int i) {
     }
   }
 
+  // for test purpose
+  final synchronized int getFlushCount() {
+    return flushCount;
+  }
+
   final String newSegmentName() {
     // Cannot synchronize on IndexWriter because that causes
     // deadlock
@@ -1985,7 +2129,7 @@ public void optimize(int maxNumSegments, boolean doWait) throws CorruptIndexExce
     if (infoStream != null)
       message("optimize: index now " + segString());
 
-    flush();
+    flush(true, false);
 
     synchronized(this) {
       resetMergeExceptions();
@@ -2029,7 +2173,9 @@ public void optimize(int maxNumSegments, boolean doWait) throws CorruptIndexExce
               final MergePolicy.OneMerge merge = (MergePolicy.OneMerge) mergeExceptions.get(0);
               if (merge.optimize) {
                 IOException err = new IOException("background merge hit exception: " + merge.segString(directory));
-                err.initCause(merge.getException());
+                final Throwable t = merge.getException();
+                if (t != null)
+                  err.initCause(t);
                 throw err;
               }
             }
@@ -2157,7 +2303,8 @@ private void startTransaction() throws IOException {
       if (infoStream != null)
         message("flush at startTransaction");
 
-      flush();
+      flush(true, false);
+
       // Turn off auto-commit during our local transaction:
       autoCommit = false;
     } else
@@ -2196,6 +2343,7 @@ private void rollbackTransaction() throws IOException {
 
     deleter.refresh();
     finishMerges(false);
+    lastMergeInfo = null;
     stopMerges = false;
   }
 
@@ -2212,27 +2360,26 @@ private void commitTransaction() throws IOException {
     // First restore autoCommit in case we hit an exception below:
     autoCommit = localAutoCommit;
 
+    // Give deleter a chance to remove files now:
+    checkpoint();
+
+    if (autoCommit) {
     boolean success = false;
     try {
-      checkpoint();
+        sync(true, 0);
       success = true;
     } finally {
       if (!success) {
         if (infoStream != null)
           message("hit exception committing transaction");
-
         rollbackTransaction();
       }
     }
-
-    if (!autoCommit)
+    } else
       // Remove the incRef we did in startTransaction.
       deleter.decRef(localRollbackSegmentInfos);
 
     localRollbackSegmentInfos = null;
-
-    // Give deleter a chance to remove files now:
-    deleter.checkpoint(segmentInfos, autoCommit);
   }
 
   /**
@@ -2353,19 +2500,11 @@ private synchronized void finishMerges(boolean waitForMerges) throws IOException
   /*
    * Called whenever the SegmentInfos has been updated and
    * the index files referenced exist (correctly) in the
-   * index directory.  If we are in autoCommit mode, we
-   * commit the change immediately.  Else, we mark
-   * commitPending.
+   * index directory.
    */
   private synchronized void checkpoint() throws IOException {
-    if (autoCommit) {
-      segmentInfos.write(directory);
-      commitPending = false;
-      if (infoStream != null)
-        message("checkpoint: wrote segments file \"" + segmentInfos.getCurrentSegmentFileName() + "\"");
-    } else {
       commitPending = true;
-    }
+    deleter.checkpoint(segmentInfos, false);
   }
 
   /** Merges all segments from an array of indexes into this index.
@@ -2426,7 +2565,7 @@ public synchronized void addIndexes(Directory[] dirs)
     ensureOpen();
     if (infoStream != null)
       message("flush at addIndexes");
-    flush();
+    flush(true, false);
 
     boolean success = false;
 
@@ -2488,7 +2627,7 @@ public synchronized void addIndexesNoOptimize(Directory[] dirs)
     ensureOpen();
     if (infoStream != null)
       message("flush at addIndexesNoOptimize");
-    flush();
+    flush(true, false);
 
     boolean success = false;
 
@@ -2657,15 +2796,47 @@ void doAfterFlush()
   /**
    * Flush all in-memory buffered updates (adds and deletes)
    * to the Directory. 
-   * <p>Note: if <code>autoCommit=false</code>, flushed data would still 
-   * not be visible to readers, until {@link #close} is called.
+   * <p>Note: while this will force buffered docs to be
+   * pushed into the index, it will not make these docs
+   * visible to a reader.  Use {@link #commit} instead
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
+   * @deprecated please call {@link #commit}) instead
    */
   public final void flush() throws CorruptIndexException, IOException {  
     flush(true, false);
   }
 
+  /**
+   * <p>Commits all pending updates (added & deleted documents)
+   * to the index, and syncs all referenced index files,
+   * such that a reader will see the changes.  Note that
+   * this does not wait for any running background merges to
+   * finish.  This may be a costly operation, so you should
+   * test the cost in your application and do it only when
+   * really necessary.</p>
+   *
+   * <p> Note that this operation calls Directory.sync on
+   * the index files.  That call should not return until the
+   * file contents & metadata are on stable storage.  For
+   * FSDirectory, this calls the OS's fsync.  But, beware:
+   * some hardware devices may in fact cache writes even
+   * during fsync, and return before the bits are actually
+   * on stable storage, to give the appearance of faster
+   * performance.  If you have such a device, and it does
+   * not have a battery backup (for example) then on power
+   * loss it may still lose data.  Lucene cannot guarantee
+   * consistency on such devices.  </p>
+   */
+  public final void commit() throws CorruptIndexException, IOException {
+    commit(true);
+  }
+
+  private final void commit(boolean triggerMerges) throws CorruptIndexException, IOException {
+    flush(triggerMerges, true);
+    sync(true, 0);
+  }
+
   /**
    * Flush all in-memory buffered udpates (adds and deletes)
    * to the Directory.
@@ -2681,10 +2852,15 @@ protected final void flush(boolean triggerMerge, boolean flushDocStores) throws
       maybeMerge();
   }
 
+  // TODO: this method should not have to be entirely
+  // synchronized, ie, merges should be allowed to commit
+  // even while a flush is happening
   private synchronized final boolean doFlush(boolean flushDocStores) throws CorruptIndexException, IOException {
 
     // Make sure no threads are actively adding a document
 
+    flushCount++;
+
     // Returns true if docWriter is currently aborting, in
     // which case we skip flushing this segment
     if (docWriter.pauseAllThreads()) {
@@ -2717,10 +2893,18 @@ private synchronized final boolean doFlush(boolean flushDocStores) throws Corrup
       // apply to more than just the last flushed segment
       boolean flushDeletes = docWriter.hasDeletes();
 
+      int docStoreOffset = docWriter.getDocStoreOffset();
+
+      // docStoreOffset should only be non-zero when
+      // autoCommit == false
+      assert !autoCommit || 0 == docStoreOffset;
+
+      boolean docStoreIsCompoundFile = false;
+
       if (infoStream != null) {
         message("  flush: segment=" + docWriter.getSegment() +
                 " docStoreSegment=" + docWriter.getDocStoreSegment() +
-                " docStoreOffset=" + docWriter.getDocStoreOffset() +
+                " docStoreOffset=" + docStoreOffset +
                 " flushDocs=" + flushDocs +
                 " flushDeletes=" + flushDeletes +
                 " flushDocStores=" + flushDocStores +
@@ -2729,14 +2913,6 @@ private synchronized final boolean doFlush(boolean flushDocStores) throws Corrup
         message("  index before flush " + segString());
       }
 
-      int docStoreOffset = docWriter.getDocStoreOffset();
-
-      // docStoreOffset should only be non-zero when
-      // autoCommit == false
-      assert !autoCommit || 0 == docStoreOffset;
-
-      boolean docStoreIsCompoundFile = false;
-
       // Check if the doc stores must be separately flushed
       // because other segments, besides the one we are about
       // to flush, reference it
@@ -2754,17 +2930,22 @@ private synchronized final boolean doFlush(boolean flushDocStores) throws Corrup
       // If we are flushing docs, segment must not be null:
       assert segment != null || !flushDocs;
 
-      if (flushDocs || flushDeletes) {
-
-        SegmentInfos rollback = null;
-
-        if (flushDeletes)
-          rollback = (SegmentInfos) segmentInfos.clone();
+      if (flushDocs) {
 
         boolean success = false;
+        final int flushedDocCount;
 
         try {
-          if (flushDocs) {
+          flushedDocCount = docWriter.flush(flushDocStores);
+          success = true;
+        } finally {
+          if (!success) {
+            if (infoStream != null)
+              message("hit exception flushing segment " + segment);
+            docWriter.abort(null);
+            deleter.refresh(segment);
+          }
+        }
 
             if (0 == docStoreOffset && flushDocStores) {
               // This means we are flushing private doc stores
@@ -2777,37 +2958,35 @@ private synchronized final boolean doFlush(boolean flushDocStores) throws Corrup
               docStoreSegment = null;
             }
 
-            int flushedDocCount = docWriter.flush(flushDocStores);
-          
+        // Create new SegmentInfo, but do not add to our
+        // segmentInfos until deletes are flushed
+        // successfully.
             newSegment = new SegmentInfo(segment,
                                          flushedDocCount,
                                          directory, false, true,
                                          docStoreOffset, docStoreSegment,
                                          docStoreIsCompoundFile);
-            segmentInfos.addElement(newSegment);
           }
 
           if (flushDeletes) {
+        try {
+          SegmentInfos rollback = (SegmentInfos) segmentInfos.clone();
+
+          boolean success = false;
+          try {
             // we should be able to change this so we can
             // buffer deletes longer and then flush them to
-            // multiple flushed segments, when
-            // autoCommit=false
-            applyDeletes(flushDocs);
-            doAfterFlush();
-          }
-
-          checkpoint();
+            // multiple flushed segments only when a commit()
+            // finally happens
+            applyDeletes(newSegment);
           success = true;
         } finally {
           if (!success) {
-
             if (infoStream != null)
-              message("hit exception flushing segment " + segment);
+                message("hit exception flushing deletes");
                 
-            if (flushDeletes) {
-
-              // Carefully check if any partial .del files
-              // should be removed:
+              // Carefully remove any partially written .del
+              // files
               final int size = rollback.size();
               for(int i=0;i<size;i++) {
                 final String newDelFileName = segmentInfos.info(i).getDelFileName();
@@ -2816,56 +2995,50 @@ private synchronized final boolean doFlush(boolean flushDocStores) throws Corrup
                   deleter.deleteFile(newDelFileName);
               }
 
+              // Remove just flushed segment
+              deleter.refresh(segment);
+
               // Fully replace the segmentInfos since flushed
               // deletes could have changed any of the
               // SegmentInfo instances:
               segmentInfos.clear();
               segmentInfos.addAll(rollback);
-              
-            } else {
-              // Remove segment we added, if any:
-              if (newSegment != null && 
-                  segmentInfos.size() > 0 && 
-                  segmentInfos.info(segmentInfos.size()-1) == newSegment)
-                segmentInfos.remove(segmentInfos.size()-1);
             }
-            if (flushDocs)
-              docWriter.abort(null);
-            deletePartialSegmentsFile();
-            deleter.checkpoint(segmentInfos, false);
-
-            if (segment != null)
-              deleter.refresh(segment);
+          }
+        } finally {
+          // Regardless of success of failure in flushing
+          // deletes, we must clear them from our buffer:
+          docWriter.clearBufferedDeletes();
           }
         }
 
-        deleter.checkpoint(segmentInfos, autoCommit);
+      if (flushDocs)
+        segmentInfos.addElement(newSegment);
+
+      if (flushDocs || flushDeletes)
+        checkpoint();
+
+      doAfterFlush();
 
-        if (flushDocs && mergePolicy.useCompoundFile(segmentInfos,
-                                                     newSegment)) {
-          success = false;
+      if (flushDocs && mergePolicy.useCompoundFile(segmentInfos, newSegment)) {
+        // Now build compound file
+        boolean success = false;
           try {
             docWriter.createCompoundFile(segment);
-            newSegment.setUseCompoundFile(true);
-            checkpoint();
             success = true;
           } finally {
             if (!success) {
               if (infoStream != null)
                 message("hit exception creating compound file for newly flushed segment " + segment);
-              newSegment.setUseCompoundFile(false);
               deleter.deleteFile(segment + "." + IndexFileNames.COMPOUND_FILE_EXTENSION);
-              deletePartialSegmentsFile();
             }
           }
 
-          deleter.checkpoint(segmentInfos, autoCommit);
+        newSegment.setUseCompoundFile(true);
+        checkpoint();
         }
       
-        return true;
-      } else {
-        return false;
-      }
+      return flushDocs || flushDeletes;
 
     } finally {
       docWriter.clearFlushPending();
@@ -2913,36 +3086,19 @@ private int ensureContiguousMerge(MergePolicy.OneMerge merge) {
     return first;
   }
 
-  /* FIXME if we want to support non-contiguous segment merges */
-  synchronized private boolean commitMerge(MergePolicy.OneMerge merge) throws IOException {
-
-    assert merge.registerDone;
-
-    // If merge was explicitly aborted, or, if abort() or
-    // rollbackTransaction() had been called since our merge
-    // started (which results in an unqualified
-    // deleter.refresh() call that will remove any index
-    // file that current segments does not reference), we
-    // abort this merge
-    if (merge.isAborted()) {
-      if (infoStream != null)
-        message("commitMerge: skipping merge " + merge.segString(directory) + ": it was aborted");
-
-      assert merge.increfDone;
-      decrefMergeSegments(merge);
-      deleter.refresh(merge.info.name);
-      return false;
-    }
-
-    boolean success = false;
-
-    int start;
-
-    try {
-      SegmentInfos sourceSegmentsClone = merge.segmentsClone;
-      SegmentInfos sourceSegments = merge.segments;
+  /** Carefully merges deletes for the segments we just
+   *  merged.  This is tricky because, although merging will
+   *  clear all deletes (compacts the documents), new
+   *  deletes may have been flushed to the segments since
+   *  the merge was started.  This method "carries over"
+   *  such new deletes onto the newly merged segment, and
+   *  saves the results deletes file (incrementing the
+   *  delete generation for merge.info).  If no deletes were
+   *  flushed, no new deletes file is saved. */
+  synchronized private void commitMergedDeletes(MergePolicy.OneMerge merge) throws IOException {
+    final SegmentInfos sourceSegmentsClone = merge.segmentsClone;
+    final SegmentInfos sourceSegments = merge.segments;
 
-      start = ensureContiguousMerge(merge);
       if (infoStream != null)
         message("commitMerge " + merge.segString(directory));
 
@@ -3011,22 +3167,39 @@ synchronized private boolean commitMerge(MergePolicy.OneMerge merge) throws IOEx
         } else
           // No deletes before or after
           docUpto += currentInfo.docCount;
-
-        merge.checkAborted(directory);
       }
 
       if (deletes != null) {
         merge.info.advanceDelGen();
         deletes.write(directory, merge.info.getDelFileName());
       }
-      success = true;
-    } finally {
-      if (!success) {
+  }
+
+  /* FIXME if we want to support non-contiguous segment merges */
+  synchronized private boolean commitMerge(MergePolicy.OneMerge merge) throws IOException {
+
         if (infoStream != null)
-          message("hit exception creating merged deletes file");
+      message("commitMerge: " + merge.segString(directory));
+
+    assert merge.registerDone;
+
+    // If merge was explicitly aborted, or, if abort() or
+    // rollbackTransaction() had been called since our merge
+    // started (which results in an unqualified
+    // deleter.refresh() call that will remove any index
+    // file that current segments does not reference), we
+    // abort this merge
+    if (merge.isAborted()) {
+      if (infoStream != null)
+        message("commitMerge: skipping merge " + merge.segString(directory) + ": it was aborted");
+
         deleter.refresh(merge.info.name);
+      return false;
       }
-    }
+
+    final int start = ensureContiguousMerge(merge);
+
+    commitMergedDeletes(merge);
 
     // Simple optimization: if the doc store we are using
     // has been closed and is in now compound format (but
@@ -3047,24 +3220,10 @@ synchronized private boolean commitMerge(MergePolicy.OneMerge merge) throws IOEx
       }
     }
 
-    success = false;
-    SegmentInfos rollback = null;
-    try {
-      rollback = (SegmentInfos) segmentInfos.clone();
       segmentInfos.subList(start, start + merge.segments.size()).clear();
       segmentInfos.add(start, merge.info);
-      checkpoint();
-      success = true;
-    } finally {
-      if (!success && rollback != null) {
-        if (infoStream != null)
-          message("hit exception when checkpointing after merge");
-        segmentInfos.clear();
-        segmentInfos.addAll(rollback);
-        deletePartialSegmentsFile();
-        deleter.refresh(merge.info.name);
-      }
-    }
+    if (lastMergeInfo == null || segmentInfos.indexOf(lastMergeInfo) < start)
+      lastMergeInfo = merge.info;
 
     if (merge.optimize)
       segmentsToOptimize.add(merge.info);
@@ -3072,7 +3231,7 @@ synchronized private boolean commitMerge(MergePolicy.OneMerge merge) throws IOEx
     // Must checkpoint before decrefing so any newly
     // referenced files in the new merge.info are incref'd
     // first:
-    deleter.checkpoint(segmentInfos, autoCommit);
+    checkpoint();
 
     decrefMergeSegments(merge);
 
@@ -3101,15 +3260,11 @@ private void decrefMergeSegments(MergePolicy.OneMerge merge) throws IOException
   final void merge(MergePolicy.OneMerge merge)
     throws CorruptIndexException, IOException {
 
-    assert merge.registerDone;
-    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;
-
     boolean success = false;
 
     try {
 
       try {
-        if (merge.info == null)
           mergeInit(merge);
 
         if (infoStream != null)
@@ -3131,11 +3286,17 @@ final void merge(MergePolicy.OneMerge merge)
     } finally {
       synchronized(this) {
         try {
-          if (!success && infoStream != null)
-            message("hit exception during merge");
 
           mergeFinish(merge);
 
+          if (!success) {
+            if (infoStream != null)
+              message("hit exception during merge");
+            addMergeException(merge);
+            if (merge.info != null && !segmentInfos.contains(merge.info))
+              deleter.refresh(merge.info.name);
+          }
+
           // This merge (and, generally, any change to the
           // segments) may now enable new merges, so we call
           // merge policy & update pending merges.
@@ -3200,6 +3361,11 @@ final synchronized boolean registerMerge(MergePolicy.OneMerge merge) {
   final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {
 
     assert merge.registerDone;
+    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;
+
+    if (merge.info != null)
+      // mergeInit already done
+      return;
 
     if (merge.isAborted())
       return;
@@ -3323,6 +3489,50 @@ else if (next != si.getDocStoreOffset())
                                  docStoreOffset,
                                  docStoreSegment,
                                  docStoreIsCompoundFile);
+
+    // Also enroll the merged segment into mergingSegments;
+    // this prevents it from getting selected for a merge
+    // after our merge is done but while we are building the
+    // CFS:
+    mergingSegments.add(merge.info);
+  }
+
+  /** This is called after merging a segment and before
+   *  building its CFS.  Return true if the files should be
+   *  sync'd.  If you return false, then the source segment
+   *  files that were merged cannot be deleted until the CFS
+   *  file is built & sync'd.  So, returning false consumes
+   *  more transient disk space, but saves performance of
+   *  not having to sync files which will shortly be deleted
+   *  anyway.
+   * @deprecated -- this will be removed in 3.0 when
+   * autoCommit is hardwired to false */
+  private synchronized boolean doCommitBeforeMergeCFS(MergePolicy.OneMerge merge) throws IOException {
+    long freeableBytes = 0;
+    final int size = merge.segments.size();
+    for(int i=0;i<size;i++) {
+      final SegmentInfo info = merge.segments.info(i);
+      // It's only important to sync if the most recent
+      // commit actually references this segment, because if
+      // it doesn't, even without syncing we will free up
+      // the disk space:
+      Integer loc = (Integer) rollbackSegments.get(info);
+      if (loc != null) {
+        final SegmentInfo oldInfo = rollbackSegmentInfos.info(loc.intValue());
+        if (oldInfo.getUseCompoundFile() != info.getUseCompoundFile())
+          freeableBytes += info.sizeInBytes();
+      }
+    }
+    // If we would free up more than 1/3rd of the index by
+    // committing now, then do so:
+    long totalBytes = 0;
+    final int numSegments = segmentInfos.size();
+    for(int i=0;i<numSegments;i++)
+      totalBytes += segmentInfos.info(i).sizeInBytes();
+    if (3*freeableBytes > totalBytes)
+      return true;
+    else
+      return false;
   }
 
   /** Does fininishing for a merge, which is fast but holds
@@ -3338,6 +3548,7 @@ final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOExcepti
     final int end = sourceSegments.size();
     for(int i=0;i<end;i++)
       mergingSegments.remove(sourceSegments.info(i));
+    mergingSegments.remove(merge.info);
     merge.registerDone = false;
   }
 
@@ -3364,11 +3575,10 @@ final private int mergeMiddle(MergePolicy.OneMerge merge)
 
     merger = new SegmentMerger(this, mergedName, merge);
     
-    // This is try/finally to make sure merger's readers are
-    // closed:
-
     boolean success = false;
 
+    // This is try/finally to make sure merger's readers are
+    // closed:
     try {
       int totDocCount = 0;
 
@@ -3384,6 +3594,7 @@ final private int mergeMiddle(MergePolicy.OneMerge merge)
 
       merge.checkAborted(directory);
 
+      // This is where all the work happens:
       mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);
 
       assert mergedDocCount == totDocCount;
@@ -3396,14 +3607,6 @@ final private int mergeMiddle(MergePolicy.OneMerge merge)
       if (merger != null) {
         merger.closeReaders();
       }
-      if (!success) {
-        if (infoStream != null)
-          message("hit exception during merge; now refresh deleter on segment " + mergedName);
-        synchronized(this) {
-          addMergeException(merge);
-          deleter.refresh(mergedName);
-        }
-      }
     }
 
     if (!commitMerge(merge))
@@ -3412,81 +3615,59 @@ final private int mergeMiddle(MergePolicy.OneMerge merge)
 
     if (merge.useCompoundFile) {
       
+      // Maybe force a sync here to allow reclaiming of the
+      // disk space used by the segments we just merged:
+      if (autoCommit && doCommitBeforeMergeCFS(merge))
+        sync(false, merge.info.sizeInBytes());
+      
       success = false;
-      boolean skip = false;
       final String compoundFileName = mergedName + "." + IndexFileNames.COMPOUND_FILE_EXTENSION;
 
       try {
-        try {
           merger.createCompoundFile(compoundFileName);
           success = true;
-        } catch (IOException ioe) {
-          synchronized(this) {
-            if (segmentInfos.indexOf(merge.info) == -1) {
-              // If another merge kicked in and merged our
-              // new segment away while we were trying to
-              // build the compound file, we can hit a
-              // FileNotFoundException and possibly
-              // IOException over NFS.  We can tell this has
-              // happened because our SegmentInfo is no
-              // longer in the segments; if this has
-              // happened it is safe to ignore the exception
-              // & skip finishing/committing our compound
-              // file creating.
-              if (infoStream != null)
-                message("hit exception creating compound file; ignoring it because our info (segment " + merge.info.name + ") has been merged away");
-              skip = true;
-            } else
-              throw ioe;
-          }
-        }
       } finally {
         if (!success) {
           if (infoStream != null)
-            message("hit exception creating compound file during merge: skip=" + skip);
-
+            message("hit exception creating compound file during merge");
           synchronized(this) {
-            if (!skip)
               addMergeException(merge);
             deleter.deleteFile(compoundFileName);
           }
         }
       }
 
-      if (!skip) {
+      if (merge.isAborted()) {
+        if (infoStream != null)
+          message("abort merge after building CFS");
+        deleter.deleteFile(compoundFileName);
+        return 0;
+      }
 
         synchronized(this) {
-          if (skip || segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {
+        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {
             // Our segment (committed in non-compound
             // format) got merged away while we were
             // building the compound format.
             deleter.deleteFile(compoundFileName);
           } else {
-            success = false;
-            try {
               merge.info.setUseCompoundFile(true);
               checkpoint();
-              success = true;
-            } finally {
-              if (!success) {  
-                if (infoStream != null)
-                  message("hit exception checkpointing compound file during merge");
-
-                // Must rollback:
-                addMergeException(merge);
-                merge.info.setUseCompoundFile(false);
-                deletePartialSegmentsFile();
-                deleter.deleteFile(compoundFileName);
-              }
-            }
-      
-            // Give deleter a chance to remove files now.
-            deleter.checkpoint(segmentInfos, autoCommit);
-          }
         }
       }
     }
 
+    // Force a sync after commiting the merge.  Once this
+    // sync completes then all index files referenced by the
+    // current segmentInfos are on stable storage so if the
+    // OS/machine crashes, or power cord is yanked, the
+    // index will be intact.  Note that this is just one
+    // (somewhat arbitrary) policy; we could try other
+    // policies like only sync if it's been > X minutes or
+    // more than Y bytes have been written, etc.
+    if (autoCommit)
+      sync(false, merge.info.sizeInBytes());
+
     return mergedDocCount;
   }
 
@@ -3495,23 +3676,11 @@ synchronized void addMergeException(MergePolicy.OneMerge merge) {
       mergeExceptions.add(merge);
   }
 
-  private void deletePartialSegmentsFile() throws IOException  {
-    if (segmentInfos.getLastGeneration() != segmentInfos.getGeneration()) {
-      String segmentFileName = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
-                                                                     "",
-                                                                     segmentInfos.getGeneration());
-      if (infoStream != null)
-        message("now delete partial segments file \"" + segmentFileName + "\"");
-
-      deleter.deleteFile(segmentFileName);
-    }
-  }
-
   // Called during flush to apply any buffered deletes.  If
   // flushedNewSegment is true then a new segment was just
   // created and flushed from the ram segments, so we will
   // selectively apply the deletes to that new segment.
-  private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {
+  private final void applyDeletes(SegmentInfo newSegment) throws CorruptIndexException, IOException {
 
     final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();
     final List bufferedDeleteDocIDs = docWriter.getBufferedDeleteDocIDs();
@@ -3521,13 +3690,13 @@ private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexEx
               bufferedDeleteDocIDs.size() + " deleted docIDs on "
               + segmentInfos.size() + " segments.");
 
-    if (flushedNewSegment) {
+    if (newSegment != null) {
       IndexReader reader = null;
       try {
         // Open readers w/o opening the stored fields /
         // vectors because these files may still be held
         // open for writing by docWriter
-        reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);
+        reader = SegmentReader.get(newSegment, false);
 
         // Apply delete terms to the segment just flushed from ram
         // apply appropriately so that a delete term is only applied to
@@ -3544,10 +3713,7 @@ private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexEx
       }
     }
 
-    int infosEnd = segmentInfos.size();
-    if (flushedNewSegment) {
-      infosEnd--;
-    }
+    final int infosEnd = segmentInfos.size();
 
     for (int i = 0; i < infosEnd; i++) {
       IndexReader reader = null;
@@ -3567,9 +3733,6 @@ private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexEx
         }
       }
     }
-
-    // Clean up bufferedDeleteTerms.
-    docWriter.clearBufferedDeletes();
   }
 
   // For test purposes.
@@ -3644,6 +3807,236 @@ public synchronized String segString() {
     return buffer.toString();
   }
 
+  // Files that have been sync'd already
+  private HashSet synced = new HashSet();
+
+  // Files that are now being sync'd
+  private HashSet syncing = new HashSet();
+
+  private boolean startSync(String fileName, Collection pending) {
+    synchronized(synced) {
+      if (!synced.contains(fileName)) {
+        if (!syncing.contains(fileName)) {
+          syncing.add(fileName);
+          return true;
+        } else {
+          pending.add(fileName);
+          return false;
+        }
+      } else
+        return false;
+    }
+  }
+
+  private void finishSync(String fileName, boolean success) {
+    synchronized(synced) {
+      assert syncing.contains(fileName);
+      syncing.remove(fileName);
+      if (success)
+        synced.add(fileName);
+      synced.notifyAll();
+    }
+  }
+
+  /** Blocks until all files in syncing are sync'd */
+  private boolean waitForAllSynced(Collection syncing) throws IOException {
+    synchronized(synced) {
+      Iterator it = syncing.iterator();
+      while(it.hasNext()) {
+        final String fileName = (String) it.next();
+        while(!synced.contains(fileName)) {
+          if (!syncing.contains(fileName))
+            // There was an error because a file that was
+            // previously syncing failed to appear in synced
+            return false;
+          else
+            try {
+              synced.wait();
+            } catch (InterruptedException ie) {
+              continue;
+            }
+        }
+      }
+      return true;
+    }
+  }
+
+  /** Pauses before syncing.  On Windows, at least, it's
+   *  best (performance-wise) to pause in order to let OS
+   *  flush writes to disk on its own, before forcing a
+   *  sync.
+   * @deprecated -- this will be removed in 3.0 when
+   * autoCommit is hardwired to false */
+  private void syncPause(long sizeInBytes) {
+    if (mergeScheduler instanceof ConcurrentMergeScheduler && maxSyncPauseSeconds > 0) {
+      // Rough heuristic: for every 10 MB, we pause for 1
+      // second, up until the max
+      long pauseTime = (long) (1000*sizeInBytes/10/1024/1024);
+      final long maxPauseTime = (long) (maxSyncPauseSeconds*1000);
+      if (pauseTime > maxPauseTime)
+        pauseTime = maxPauseTime;
+      final int sleepCount = (int) (pauseTime / 100);
+      for(int i=0;i<sleepCount;i++) {
+        synchronized(this) {
+          if (stopMerges || closing)
+            break;
+        }
+        try {
+          Thread.sleep(100);
+        } catch (InterruptedException ie) {
+          Thread.currentThread().interrupt();
+        }
+      }
+    }
+  }
+
+  /** Walk through all files referenced by the current
+   *  segmentInfos, minus flushes, and ask the Directory to
+   *  sync each file, if it wasn't already.  If that
+   *  succeeds, then we write a new segments_N file & sync
+   *  that. */
+  private void sync(boolean includeFlushes, long sizeInBytes) throws IOException {
+
+    message("start sync() includeFlushes=" + includeFlushes);
+
+    if (!includeFlushes)
+      syncPause(sizeInBytes);
+
+    // First, we clone & incref the segmentInfos we intend
+    // to sync, then, without locking, we sync() each file
+    // referenced by toSync, in the background.  Multiple
+    // threads can be doing this at once, if say a large
+    // merge and a small merge finish at the same time:
+
+    SegmentInfos toSync = null;
+    final int mySyncCount;
+    synchronized(this) {
+
+      if (!commitPending) {
+        message("  skip sync(): no commit pending");
+        return;
+      }
+
+      // Create the segmentInfos we want to sync, by copying
+      // the current one and possibly removing flushed
+      // segments:
+      toSync = (SegmentInfos) segmentInfos.clone();
+      final int numSegmentsToSync = toSync.size();
+
+      boolean newCommitPending = false;
+
+      if (!includeFlushes) {
+        // Do not sync flushes:
+        assert lastMergeInfo != null;
+        assert toSync.contains(lastMergeInfo);
+        int downTo = numSegmentsToSync-1;
+        while(!toSync.info(downTo).equals(lastMergeInfo)) {
+          message("  skip segment " + toSync.info(downTo).name);
+          toSync.remove(downTo);
+          downTo--;
+          newCommitPending = true;
+        }
+
+      } else if (numSegmentsToSync > 0)
+        // Force all subsequent syncs to include up through
+        // the final info in the current segments.  This
+        // ensure that a call to commit() will force another
+        // sync (due to merge finishing) to sync all flushed
+        // segments as well:
+        lastMergeInfo = toSync.info(numSegmentsToSync-1);
+
+      mySyncCount = syncCount++;
+      deleter.incRef(toSync, false);
+
+      commitPending = newCommitPending;
+    }
+
+    boolean success0 = false;
+
+    try {
+
+      // Loop until all files toSync references are sync'd:
+      while(true) {
+
+        final Collection pending = new ArrayList();
+
+        for(int i=0;i<toSync.size();i++) {
+          final SegmentInfo info = toSync.info(i);
+          final List files = info.files();
+          for(int j=0;j<files.size();j++) {
+            final String fileName = (String) files.get(j);
+            if (startSync(fileName, pending)) {
+              boolean success = false;
+              try {
+                // Because we incRef'd this commit point, above,
+                // the file had better exist:
+                assert directory.fileExists(fileName);
+                message("now sync " + fileName);
+                directory.sync(fileName);
+                success = true;
+              } finally {
+                finishSync(fileName, success);
+              }
+            }
+          }
+        }
+
+        // All files that I require are either synced or being
+        // synced by other threads.  If they are being synced,
+        // we must at this point block until they are done.
+        // If this returns false, that means an error in
+        // another thread resulted in failing to actually
+        // sync one of our files, so we repeat:
+        if (waitForAllSynced(pending))
+          break;
+      }
+
+      synchronized(this) {
+        // If someone saved a newer version of segments file
+        // since I first started syncing my version, I can
+        // safely skip saving myself since I've been
+        // superseded:
+        if (mySyncCount > syncCountSaved) {
+          
+          if (segmentInfos.getGeneration() > toSync.getGeneration())
+            toSync.updateGeneration(segmentInfos);
+
+          boolean success = false;
+          try {
+            toSync.commit(directory);
+            success = true;
+          } finally {
+            // Have our master segmentInfos record the
+            // generations we just sync'd
+            segmentInfos.updateGeneration(toSync);
+            if (!success) {
+              commitPending = true;
+              message("hit exception committing segments file");
+            }
+          }
+          message("commit complete");
+
+          syncCountSaved = mySyncCount;
+
+          deleter.checkpoint(toSync, true);
+          setRollbackSegmentInfos();
+        } else
+          message("sync superseded by newer infos");
+      }
+
+      message("done all syncs");
+
+      success0 = true;
+
+    } finally {
+      synchronized(this) {
+        deleter.decRef(toSync);
+        if (!success0)
+          commitPending = true;
+      }
+    }
+  }
+
   /**
    * Specifies maximum field length in {@link IndexWriter} constructors.
    * {@link IndexWriter#setMaxFieldLength(int)} overrides the value set by
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/SegmentInfos.java b/lucene/java/trunk/src/java/org/apache/lucene/index/SegmentInfos.java
index c5fade9d..d6477d97 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/SegmentInfos.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/SegmentInfos.java
@@ -20,6 +20,8 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.ChecksumIndexOutput;
+import org.apache.lucene.store.ChecksumIndexInput;
 
 import java.io.File;
 import java.io.FileNotFoundException;
@@ -55,8 +57,12 @@
    * vectors and stored fields file. */
   public static final int FORMAT_SHARED_DOC_STORE = -4;
 
+  /** This format adds a checksum at the end of the file to
+   *  ensure all bytes were successfully written. */
+  public static final int FORMAT_CHECKSUM = -5;
+
   /* This must always point to the most recent file format. */
-  private static final int CURRENT_FORMAT = FORMAT_SHARED_DOC_STORE;
+  private static final int CURRENT_FORMAT = FORMAT_CHECKSUM;
   
   public int counter = 0;    // used to name new segments
   /**
@@ -197,7 +203,7 @@ public final void read(Directory directory, String segmentFileName) throws Corru
     // Clear any previous segments:
     clear();
 
-    IndexInput input = directory.openInput(segmentFileName);
+    ChecksumIndexInput input = new ChecksumIndexInput(directory.openInput(segmentFileName));
 
     generation = generationFromSegmentsFileName(segmentFileName);
 
@@ -226,6 +232,13 @@ public final void read(Directory directory, String segmentFileName) throws Corru
         else
           version = input.readLong(); // read version
       }
+
+      if (format <= FORMAT_CHECKSUM) {
+        final long checksumNow = input.getChecksum();
+        final long checksumThen = input.readLong();
+        if (checksumNow != checksumThen)
+          throw new CorruptIndexException("checksum mismatch in segments file");
+      }
       success = true;
     }
     finally {
@@ -257,7 +270,7 @@ protected Object doBody(String segmentFileName) throws CorruptIndexException, IO
     }.run();
   }
 
-  public final void write(Directory directory) throws IOException {
+  private final void write(Directory directory) throws IOException {
 
     String segmentFileName = getNextSegmentFileName();
 
@@ -268,7 +281,7 @@ public final void write(Directory directory) throws IOException {
       generation++;
     }
 
-    IndexOutput output = directory.createOutput(segmentFileName);
+    ChecksumIndexOutput output = new ChecksumIndexOutput(directory.createOutput(segmentFileName));
 
     boolean success = false;
 
@@ -281,28 +294,30 @@ public final void write(Directory directory) throws IOException {
       for (int i = 0; i < size(); i++) {
         info(i).write(output);
       }         
-    }
-    finally {
+      final long checksum = output.getChecksum();
+      output.writeLong(checksum);
+      success = true;
+    } finally {
+      boolean success2 = false;
       try {
         output.close();
-        success = true;
+        success2 = true;
       } finally {
-        if (!success) {
+        if (!success || !success2)
           // Try not to leave a truncated segments_N file in
           // the index:
           directory.deleteFile(segmentFileName);
         }
       }
-    }
 
     try {
-      output = directory.createOutput(IndexFileNames.SEGMENTS_GEN);
+      IndexOutput genOutput = directory.createOutput(IndexFileNames.SEGMENTS_GEN);
       try {
-        output.writeInt(FORMAT_LOCKLESS);
-        output.writeLong(generation);
-        output.writeLong(generation);
+        genOutput.writeInt(FORMAT_LOCKLESS);
+        genOutput.writeLong(generation);
+        genOutput.writeLong(generation);
       } finally {
-        output.close();
+        genOutput.close();
       }
     } catch (IOException e) {
       // It's OK if we fail to write this file since it's
@@ -620,7 +635,7 @@ public Object run() throws CorruptIndexException, IOException {
             retry = true;
           }
 
-        } else {
+        } else if (0 == method) {
           // Segment file has advanced since our last loop, so
           // reset retry:
           retry = false;
@@ -701,4 +716,50 @@ public SegmentInfos range(int first, int last) {
     infos.addAll(super.subList(first, last));
     return infos;
   }
+
+  // Carry over generation numbers from another SegmentInfos
+  void updateGeneration(SegmentInfos other) {
+    assert other.generation > generation;
+    lastGeneration = other.lastGeneration;
+    generation = other.generation;
+  }
+
+  /** Writes & syncs to the Directory dir, taking care to
+   *  remove the segments file on exception */
+  public final void commit(Directory dir) throws IOException {
+    boolean success = false;
+    try {
+      write(dir);
+      success = true;
+    } finally {
+      if (!success) {
+        // Must carefully compute fileName from "generation"
+        // since lastGeneration isn't incremented:
+        final String segmentFileName = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS,
+                                                                             "",
+                                                                             generation);
+        dir.deleteFile(segmentFileName);
+      }
+    }
+
+    // NOTE: if we crash here, we have left a segments_N
+    // file in the directory in a possibly corrupt state (if
+    // some bytes made it to stable storage and others
+    // didn't).  But, the segments_N file now includes
+    // checksum at the end, which should catch this case.
+    // So when a reader tries to read it, it will throw a
+    // CorruptIndexException, which should cause the retry
+    // logic in SegmentInfos to kick in and load the last
+    // good (previous) segments_N-1 file.
+
+    final String fileName = getCurrentSegmentFileName();
+    success = false;
+    try {
+      dir.sync(fileName);
+      success = true;
+    } finally {
+      if (!success)
+        dir.deleteFile(fileName);
+    }
+  }
 }
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/store/ChecksumIndexInput.java b/lucene/java/trunk/src/java/org/apache/lucene/store/ChecksumIndexInput.java
index e69de29b..e90f6a6d 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/store/ChecksumIndexInput.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/store/ChecksumIndexInput.java
@@ -0,0 +1,67 @@
+package org.apache.lucene.store;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+/** Writes bytes through to a primary IndexOutput, computing
+ *  checksum as it goes. Note that you cannot use seek(). */
+public class ChecksumIndexInput extends IndexInput {
+  IndexInput main;
+  Checksum digest;
+
+  public ChecksumIndexInput(IndexInput main) {
+    this.main = main;
+    digest = new CRC32();
+  }
+
+  public byte readByte() throws IOException {
+    final byte b = main.readByte();
+    digest.update(b);
+    return b;
+  }
+
+  public void readBytes(byte[] b, int offset, int len)
+    throws IOException {
+    main.readBytes(b, offset, len);
+    digest.update(b, offset, len);
+  }
+
+  
+  public long getChecksum() {
+    return digest.getValue();
+  }
+
+  public void close() throws IOException {
+    main.close();
+  }
+
+  public long getFilePointer() {
+    return main.getFilePointer();
+  }
+
+  public void seek(long pos) {
+    throw new RuntimeException("not allowed");
+  }
+
+  public long length() {
+    return main.length();
+  }
+}
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/store/ChecksumIndexOutput.java b/lucene/java/trunk/src/java/org/apache/lucene/store/ChecksumIndexOutput.java
index 3f24d583..82f4123c 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/store/ChecksumIndexOutput.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/store/ChecksumIndexOutput.java
@@ -1 +1,69 @@
   + native
+package org.apache.lucene.store;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+/** Writes bytes through to a primary IndexOutput, computing
+ *  checksum.  Note that you cannot use seek().*/
+public class ChecksumIndexOutput extends IndexOutput {
+  IndexOutput main;
+  Checksum digest;
+
+  public ChecksumIndexOutput(IndexOutput main) {
+    this.main = main;
+    digest = new CRC32();
+  }
+
+  public void writeByte(byte b) throws IOException {
+    digest.update(b);
+    main.writeByte(b);
+  }
+
+  public void writeBytes(byte[] b, int offset, int length) throws IOException {
+    digest.update(b, offset, length);
+    main.writeBytes(b, offset, length);
+  }
+
+  public long getChecksum() {
+    return digest.getValue();
+  }
+
+  public void flush() throws IOException {
+    main.flush();
+  }
+
+  public void close() throws IOException {
+    main.close();
+  }
+
+  public long getFilePointer() {
+    return main.getFilePointer();
+  }
+
+  public void seek(long pos) {
+    throw new RuntimeException("not allowed");    
+  }
+
+  public long length() throws IOException {
+    return main.length();
+  }
+}
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/store/Directory.java b/lucene/java/trunk/src/java/org/apache/lucene/store/Directory.java
index fd715e5a..d28151bb 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/store/Directory.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/store/Directory.java
@@ -83,6 +83,11 @@ public abstract long fileLength(String name)
       Returns a stream writing this file. */
   public abstract IndexOutput createOutput(String name) throws IOException;
 
+  /** Ensure that any writes to this file are moved to
+   *  stable storage.  Lucene uses this to properly commit
+   *  changes to the index, to prevent a machine/OS crash
+   *  from corrupting the index. */
+  public void sync(String name) throws IOException {}
 
   /** Returns a stream reading an existing file. */
   public abstract IndexInput openInput(String name)
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/store/FSDirectory.java b/lucene/java/trunk/src/java/org/apache/lucene/store/FSDirectory.java
index c11233ec..49d6c2db 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/store/FSDirectory.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/store/FSDirectory.java
@@ -436,6 +436,39 @@ public IndexOutput createOutput(String name) throws IOException {
     return new FSIndexOutput(file);
   }
 
+  public void sync(String name) throws IOException {
+    File fullFile = new File(directory, name);
+    boolean success = false;
+    int retryCount = 0;
+    IOException exc = null;
+    while(!success && retryCount < 5) {
+      retryCount++;
+      RandomAccessFile file = null;
+      try {
+        try {
+          file = new RandomAccessFile(fullFile, "rw");
+          file.getFD().sync();
+          success = true;
+        } finally {
+          if (file != null)
+            file.close();
+        }
+      } catch (IOException ioe) {
+        if (exc == null)
+          exc = ioe;
+        try {
+          // Pause 5 msec
+          Thread.sleep(5);
+        } catch (InterruptedException ie) {
+          Thread.currentThread().interrupt();
+        }
+      }
+    }
+    if (!success)
+      // Throw original exception
+      throw exc;
+  }
+
   // Inherit javadoc
   public IndexInput openInput(String name) throws IOException {
     return openInput(name, BufferedIndexInput.BUFFER_SIZE);
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestAtomicUpdate.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestAtomicUpdate.java
index 27db24cc..1693eb62 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestAtomicUpdate.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestAtomicUpdate.java
@@ -20,12 +20,8 @@
 import org.apache.lucene.store.*;
 import org.apache.lucene.document.*;
 import org.apache.lucene.analysis.*;
-import org.apache.lucene.index.*;
 import org.apache.lucene.search.*;
 import org.apache.lucene.queryParser.*;
-import org.apache.lucene.util._TestUtil;
-
-import org.apache.lucene.util.LuceneTestCase;
 
 import java.util.Random;
 import java.io.File;
@@ -83,7 +79,6 @@ public void doWork() throws Exception {
       // Update all 100 docs...
       for(int i=0; i<100; i++) {
         Document d = new Document();
-        int n = RANDOM.nextInt();
         d.add(new Field("id", Integer.toString(i), Field.Store.YES, Field.Index.UN_TOKENIZED));
         d.add(new Field("contents", English.intToEnglish(i+10*count), Field.Store.NO, Field.Index.TOKENIZED));
         writer.updateDocument(new Term("id", Integer.toString(i)), d);
@@ -127,7 +122,7 @@ public void runTest(Directory directory) throws Exception {
       d.add(new Field("contents", English.intToEnglish(i), Field.Store.NO, Field.Index.TOKENIZED));
       writer.addDocument(d);
     }
-    writer.flush();
+    writer.commit();
 
     IndexerThread indexerThread = new IndexerThread(writer, threads);
     threads[0] = indexerThread;
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index f64ad651..ad4309f3 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -349,7 +349,6 @@ public void testExactFileNames() throws IOException {
  
         IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
         writer.setRAMBufferSizeMB(16.0);
-        //IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
         for(int i=0;i<35;i++) {
           addDoc(writer, i);
         }
@@ -390,12 +389,9 @@ public void testExactFileNames() throws IOException {
         expected = new String[] {"_0.cfs",
                     "_0_1.del",
                     "_0_1.s" + contentFieldIndex,
-                    "segments_4",
+                    "segments_3",
                     "segments.gen"};
 
-        if (!autoCommit)
-          expected[3] = "segments_3";
-
         String[] actual = dir.list();
         Arrays.sort(expected);
         Arrays.sort(actual);
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestCrash.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestCrash.java
index e69de29b..1dd41953 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestCrash.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestCrash.java
@@ -0,0 +1,181 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.store.MockRAMDirectory;
+import org.apache.lucene.store.NoLockFactory;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+public class TestCrash extends LuceneTestCase {
+
+  private IndexWriter initIndex() throws IOException {
+    return initIndex(new MockRAMDirectory());
+  }
+
+  private IndexWriter initIndex(MockRAMDirectory dir) throws IOException {
+    dir.setLockFactory(NoLockFactory.getNoLockFactory());
+
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer());
+    //writer.setMaxBufferedDocs(2);
+    writer.setMaxBufferedDocs(10);
+    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();
+
+    Document doc = new Document();
+    doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.TOKENIZED));
+    doc.add(new Field("id", "0", Field.Store.YES, Field.Index.TOKENIZED));
+    for(int i=0;i<157;i++)
+      writer.addDocument(doc);
+
+    return writer;
+  }
+
+  private void crash(final IndexWriter writer) throws IOException {
+    final MockRAMDirectory dir = (MockRAMDirectory) writer.getDirectory();
+    ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) writer.getMergeScheduler();
+    dir.crash();
+    cms.sync();
+    dir.clearCrash();
+  }
+
+  public void testCrashWhileIndexing() throws IOException {
+    IndexWriter writer = initIndex();
+    MockRAMDirectory dir = (MockRAMDirectory) writer.getDirectory();
+    crash(writer);
+    IndexReader reader = IndexReader.open(dir);
+    assertTrue(reader.numDocs() < 157);
+  }
+
+  public void testWriterAfterCrash() throws IOException {
+    IndexWriter writer = initIndex();
+    MockRAMDirectory dir = (MockRAMDirectory) writer.getDirectory();
+    dir.setPreventDoubleWrite(false);
+    crash(writer);
+    writer = initIndex(dir);
+    writer.close();
+
+    IndexReader reader = IndexReader.open(dir);
+    assertTrue(reader.numDocs() < 314);
+  }
+
+  public void testCrashAfterReopen() throws IOException {
+    IndexWriter writer = initIndex();
+    MockRAMDirectory dir = (MockRAMDirectory) writer.getDirectory();
+    writer.close();
+    writer = initIndex(dir);
+    assertEquals(314, writer.docCount());
+    crash(writer);
+
+    /*
+    System.out.println("\n\nTEST: open reader");
+    String[] l = dir.list();
+    Arrays.sort(l);
+    for(int i=0;i<l.length;i++)
+      System.out.println("file " + i + " = " + l[i] + " " +
+    dir.fileLength(l[i]) + " bytes");
+    */
+
+    IndexReader reader = IndexReader.open(dir);
+    assertTrue(reader.numDocs() >= 157);
+  }
+
+  public void testCrashAfterClose() throws IOException {
+    
+    IndexWriter writer = initIndex();
+    MockRAMDirectory dir = (MockRAMDirectory) writer.getDirectory();
+
+    writer.close();
+    dir.crash();
+
+    /*
+    String[] l = dir.list();
+    Arrays.sort(l);
+    for(int i=0;i<l.length;i++)
+      System.out.println("file " + i + " = " + l[i] + " " + dir.fileLength(l[i]) + " bytes");
+    */
+
+    IndexReader reader = IndexReader.open(dir);
+    assertEquals(157, reader.numDocs());
+  }
+
+  public void testCrashAfterCloseNoWait() throws IOException {
+    
+    IndexWriter writer = initIndex();
+    MockRAMDirectory dir = (MockRAMDirectory) writer.getDirectory();
+
+    writer.close(false);
+
+    dir.crash();
+
+    /*
+    String[] l = dir.list();
+    Arrays.sort(l);
+    for(int i=0;i<l.length;i++)
+      System.out.println("file " + i + " = " + l[i] + " " + dir.fileLength(l[i]) + " bytes");
+    */
+    IndexReader reader = IndexReader.open(dir);
+    assertEquals(157, reader.numDocs());
+  }
+
+  public void testCrashReaderDeletes() throws IOException {
+    
+    IndexWriter writer = initIndex();
+    MockRAMDirectory dir = (MockRAMDirectory) writer.getDirectory();
+
+    writer.close(false);
+    IndexReader reader = IndexReader.open(dir);
+    reader.deleteDocument(3);
+
+    dir.crash();
+
+    /*
+    String[] l = dir.list();
+    Arrays.sort(l);
+    for(int i=0;i<l.length;i++)
+      System.out.println("file " + i + " = " + l[i] + " " + dir.fileLength(l[i]) + " bytes");
+    */
+    reader = IndexReader.open(dir);
+    assertEquals(157, reader.numDocs());
+  }
+
+  public void testCrashReaderDeletesAfterClose() throws IOException {
+    
+    IndexWriter writer = initIndex();
+    MockRAMDirectory dir = (MockRAMDirectory) writer.getDirectory();
+
+    writer.close(false);
+    IndexReader reader = IndexReader.open(dir);
+    reader.deleteDocument(3);
+    reader.close();
+
+    dir.crash();
+
+    /*
+    String[] l = dir.list();
+    Arrays.sort(l);
+    for(int i=0;i<l.length;i++)
+      System.out.println("file " + i + " = " + l[i] + " " + dir.fileLength(l[i]) + " bytes");
+    */
+    reader = IndexReader.open(dir);
+    assertEquals(156, reader.numDocs());
+  }
+}
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestDeletionPolicy.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestDeletionPolicy.java
index 3fba9275..d50a0e45 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestDeletionPolicy.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestDeletionPolicy.java
@@ -270,13 +270,10 @@ public void testKeepAllDeletionPolicy() throws IOException {
       writer.close();
 
       assertEquals(2, policy.numOnInit);
-      if (autoCommit) {
-        assertTrue(policy.numOnCommit > 2);
-      } else {
+      if (!autoCommit)
         // If we are not auto committing then there should
         // be exactly 2 commits (one per close above):
         assertEquals(2, policy.numOnCommit);
-      }
 
       // Simplistic check: just verify all segments_N's still
       // exist, and, I can open a reader on each:
@@ -334,13 +331,10 @@ public void testKeepNoneOnInitDeletionPolicy() throws IOException {
       writer.close();
 
       assertEquals(2, policy.numOnInit);
-      if (autoCommit) {
-        assertTrue(policy.numOnCommit > 2);
-      } else {
+      if (!autoCommit)
         // If we are not auto committing then there should
         // be exactly 2 commits (one per close above):
         assertEquals(2, policy.numOnCommit);
-      }
 
       // Simplistic check: just verify the index is in fact
       // readable:
@@ -459,11 +453,8 @@ public void testKeepLastNDeletionPolicyWithReader() throws IOException {
       writer.close();
 
       assertEquals(2*(N+2), policy.numOnInit);
-      if (autoCommit) {
-        assertTrue(policy.numOnCommit > 2*(N+2)-1);
-      } else {
+      if (!autoCommit)
         assertEquals(2*(N+2)-1, policy.numOnCommit);
-      }
 
       IndexSearcher searcher = new IndexSearcher(dir);
       Hits hits = searcher.search(query);
@@ -565,11 +556,8 @@ public void testKeepLastNDeletionPolicyWithCreates() throws IOException {
       }
 
       assertEquals(1+3*(N+1), policy.numOnInit);
-      if (autoCommit) {
-        assertTrue(policy.numOnCommit > 3*(N+1)-1);
-      } else {
+      if (!autoCommit)
         assertEquals(2*(N+1), policy.numOnCommit);
-      }
 
       IndexSearcher searcher = new IndexSearcher(dir);
       Hits hits = searcher.search(query);
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexFileDeleter.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
index 7ff3fc44..b7beb19e 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
@@ -18,17 +18,8 @@
  */
 
 import org.apache.lucene.util.LuceneTestCase;
-import java.util.Vector;
-import java.util.Arrays;
-import java.io.ByteArrayOutputStream;
-import java.io.ObjectOutputStream;
-import java.io.IOException;
-import java.io.File;
 
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
@@ -77,8 +68,8 @@ public void testDeleteLeftoverFiles() throws IOException {
     String[] files = dir.list();
 
     /*
-    for(int i=0;i<files.length;i++) {
-      System.out.println(i + ": " + files[i]);
+    for(int j=0;j<files.length;j++) {
+      System.out.println(j + ": " + files[j]);
     }
     */
 
@@ -145,8 +136,8 @@ public void testDeleteLeftoverFiles() throws IOException {
     copyFile(dir, "_0.cfs", "deletable");
 
     // Create some old segments file:
-    copyFile(dir, "segments_a", "segments");
-    copyFile(dir, "segments_a", "segments_2");
+    copyFile(dir, "segments_3", "segments");
+    copyFile(dir, "segments_3", "segments_2");
 
     // Create a bogus cfs file shadowing a non-cfs segment:
     copyFile(dir, "_2.cfs", "_3.cfs");
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexModifier.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexModifier.java
index b882ae1c..6fd3a549 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexModifier.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexModifier.java
@@ -202,7 +202,7 @@ public int docFreq(Term term) throws IOException {
 
 class IndexThread extends Thread {
 
-  private final static int ITERATIONS = 500;       // iterations of thread test
+  private final static int TEST_SECONDS = 3;       // how many seconds to run each test 
 
   static int id = 0;
   static Stack idStack = new Stack();
@@ -224,8 +224,10 @@ public int docFreq(Term term) throws IOException {
   }
   
   public void run() {
+
+    final long endTime = System.currentTimeMillis() + 1000*TEST_SECONDS;
     try {
-      for(int i = 0; i < ITERATIONS; i++) {
+      while(System.currentTimeMillis() < endTime) {
         int rand = random.nextInt(101);
         if (rand < 5) {
           index.optimize();
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexReader.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexReader.java
index 3d28639a..a5c70dbd 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexReader.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexReader.java
@@ -463,7 +463,7 @@ public void testOpenEmptyDirectory() throws IOException{
         fileDirName.mkdir();
       }
       try {
-        IndexReader reader = IndexReader.open(fileDirName);
+        IndexReader.open(fileDirName);
         fail("opening IndexReader on empty directory failed to produce FileNotFoundException");
       } catch (FileNotFoundException e) {
         // GOOD
@@ -779,6 +779,11 @@ public void testDiskFull() throws IOException {
       // Iterate w/ ever increasing free disk space:
       while(!done) {
         MockRAMDirectory dir = new MockRAMDirectory(startDir);
+
+        // If IndexReader hits disk full, it can write to
+        // the same files again.
+        dir.setPreventDoubleWrite(false);
+
         IndexReader reader = IndexReader.open(dir);
 
         // For each disk size, first try to commit against
@@ -838,6 +843,7 @@ public void testDiskFull() throws IOException {
           } catch (IOException e) {
             if (debug) {
               System.out.println("  hit IOException: " + e);
+              e.printStackTrace(System.out);
             }
             err = e;
             if (1 == x) {
@@ -855,7 +861,7 @@ public void testDiskFull() throws IOException {
           String[] startFiles = dir.list();
           SegmentInfos infos = new SegmentInfos();
           infos.read(dir);
-          IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
+          new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
           String[] endFiles = dir.list();
 
           Arrays.sort(startFiles);
@@ -1030,7 +1036,7 @@ public void testOpenReaderAfterDelete() throws IOException {
                           "deletetest");
       Directory dir = FSDirectory.getDirectory(dirFile);
       try {
-        IndexReader reader = IndexReader.open(dir);
+        IndexReader.open(dir);
         fail("expected FileNotFoundException");
       } catch (FileNotFoundException e) {
         // expected
@@ -1040,7 +1046,7 @@ public void testOpenReaderAfterDelete() throws IOException {
 
       // Make sure we still get a CorruptIndexException (not NPE):
       try {
-        IndexReader reader = IndexReader.open(dir);
+        IndexReader.open(dir);
         fail("expected FileNotFoundException");
       } catch (FileNotFoundException e) {
         // expected
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java
index e0eca91c..5a1770b9 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -651,19 +651,19 @@ public void testOptimizeMaxNumSegments2() throws IOException {
       writer.setMaxBufferedDocs(2);
 
       for(int iter=0;iter<10;iter++) {
-
         for(int i=0;i<19;i++)
           writer.addDocument(doc);
 
-        writer.flush();
+        ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();
+        writer.commit();
 
         SegmentInfos sis = new SegmentInfos();
-        ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();
         sis.read(dir);
 
         final int segCount = sis.size();
 
         writer.optimize(7);
+        writer.commit();
 
         sis = new SegmentInfos();
         ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();
@@ -1045,7 +1045,7 @@ public void testCommitOnClose() throws IOException {
      * and add docs to it.
      */
     public void testCommitOnCloseAbort() throws IOException {
-      Directory dir = new RAMDirectory();      
+      MockRAMDirectory dir = new MockRAMDirectory();      
       IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       for (int i = 0; i < 14; i++) {
@@ -1086,6 +1086,11 @@ public void testCommitOnCloseAbort() throws IOException {
       // and all is good:
       writer = new IndexWriter(dir, false, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
+
+      // On abort, writer in fact may write to the same
+      // segments_N file:
+      dir.setPreventDoubleWrite(false);
+
       for(int i=0;i<12;i++) {
         for(int j=0;j<17;j++) {
           addDoc(writer);
@@ -1273,48 +1278,48 @@ public void testChangingRAMBuffer() throws IOException {
       writer.setMaxBufferedDocs(10);
       writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
 
-      long lastGen = -1;
+      int lastFlushCount = -1;
       for(int j=1;j<52;j++) {
         Document doc = new Document();
         doc.add(new Field("field", "aaa" + j, Field.Store.YES, Field.Index.TOKENIZED));
         writer.addDocument(doc);
         _TestUtil.syncConcurrentMerges(writer);
-        long gen = SegmentInfos.generationFromSegmentsFileName(SegmentInfos.getCurrentSegmentFileName(dir.list()));
+        int flushCount = writer.getFlushCount();
         if (j == 1)
-          lastGen = gen;
+          lastFlushCount = flushCount;
         else if (j < 10)
           // No new files should be created
-          assertEquals(gen, lastGen);
+          assertEquals(flushCount, lastFlushCount);
         else if (10 == j) {
-          assertTrue(gen > lastGen);
-          lastGen = gen;
+          assertTrue(flushCount > lastFlushCount);
+          lastFlushCount = flushCount;
           writer.setRAMBufferSizeMB(0.000001);
           writer.setMaxBufferedDocs(IndexWriter.DISABLE_AUTO_FLUSH);
         } else if (j < 20) {
-          assertTrue(gen > lastGen);
-          lastGen = gen;
+          assertTrue(flushCount > lastFlushCount);
+          lastFlushCount = flushCount;
         } else if (20 == j) {
           writer.setRAMBufferSizeMB(16);
           writer.setMaxBufferedDocs(IndexWriter.DISABLE_AUTO_FLUSH);
-          lastGen = gen;
+          lastFlushCount = flushCount;
         } else if (j < 30) {
-          assertEquals(gen, lastGen);
+          assertEquals(flushCount, lastFlushCount);
         } else if (30 == j) {
           writer.setRAMBufferSizeMB(0.000001);
           writer.setMaxBufferedDocs(IndexWriter.DISABLE_AUTO_FLUSH);
         } else if (j < 40) {
-          assertTrue(gen> lastGen);
-          lastGen = gen;
+          assertTrue(flushCount> lastFlushCount);
+          lastFlushCount = flushCount;
         } else if (40 == j) {
           writer.setMaxBufferedDocs(10);
           writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
-          lastGen = gen;
+          lastFlushCount = flushCount;
         } else if (j < 50) {
-          assertEquals(gen, lastGen);
+          assertEquals(flushCount, lastFlushCount);
           writer.setMaxBufferedDocs(10);
           writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
         } else if (50 == j) {
-          assertTrue(gen > lastGen);
+          assertTrue(flushCount > lastFlushCount);
         }
       }
       writer.close();
@@ -1334,46 +1339,46 @@ public void testChangingRAMBuffer2() throws IOException {
         writer.addDocument(doc);
       }
       
-      long lastGen = -1;
+      int lastFlushCount = -1;
       for(int j=1;j<52;j++) {
         writer.deleteDocuments(new Term("field", "aaa" + j));
         _TestUtil.syncConcurrentMerges(writer);
-        long gen = SegmentInfos.generationFromSegmentsFileName(SegmentInfos.getCurrentSegmentFileName(dir.list()));
+        int flushCount = writer.getFlushCount();
         if (j == 1)
-          lastGen = gen;
+          lastFlushCount = flushCount;
         else if (j < 10) {
           // No new files should be created
-          assertEquals(gen, lastGen);
+          assertEquals(flushCount, lastFlushCount);
         } else if (10 == j) {
-          assertTrue(gen > lastGen);
-          lastGen = gen;
+          assertTrue(flushCount > lastFlushCount);
+          lastFlushCount = flushCount;
           writer.setRAMBufferSizeMB(0.000001);
           writer.setMaxBufferedDeleteTerms(IndexWriter.DISABLE_AUTO_FLUSH);
         } else if (j < 20) {
-          assertTrue(gen > lastGen);
-          lastGen = gen;
+          assertTrue(flushCount > lastFlushCount);
+          lastFlushCount = flushCount;
         } else if (20 == j) {
           writer.setRAMBufferSizeMB(16);
           writer.setMaxBufferedDeleteTerms(IndexWriter.DISABLE_AUTO_FLUSH);
-          lastGen = gen;
+          lastFlushCount = flushCount;
         } else if (j < 30) {
-          assertEquals(gen, lastGen);
+          assertEquals(flushCount, lastFlushCount);
         } else if (30 == j) {
           writer.setRAMBufferSizeMB(0.000001);
           writer.setMaxBufferedDeleteTerms(IndexWriter.DISABLE_AUTO_FLUSH);
         } else if (j < 40) {
-          assertTrue(gen> lastGen);
-          lastGen = gen;
+          assertTrue(flushCount> lastFlushCount);
+          lastFlushCount = flushCount;
         } else if (40 == j) {
           writer.setMaxBufferedDeleteTerms(10);
           writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
-          lastGen = gen;
+          lastFlushCount = flushCount;
         } else if (j < 50) {
-          assertEquals(gen, lastGen);
+          assertEquals(flushCount, lastFlushCount);
           writer.setMaxBufferedDeleteTerms(10);
           writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
         } else if (50 == j) {
-          assertTrue(gen > lastGen);
+          assertTrue(flushCount > lastFlushCount);
         }
       }
       writer.close();
@@ -1831,15 +1836,22 @@ public void clearDoFail() {
     public void eval(MockRAMDirectory dir)  throws IOException {
       if (doFail) {
         StackTraceElement[] trace = new Exception().getStackTrace();
+        boolean sawAppend = false;
+        boolean sawFlush = false;
         for (int i = 0; i < trace.length; i++) {
-          if ("org.apache.lucene.index.DocumentsWriter".equals(trace[i].getClassName()) && "appendPostings".equals(trace[i].getMethodName()) && count++ == 30) {
+          if ("org.apache.lucene.index.DocumentsWriter".equals(trace[i].getClassName()) && "appendPostings".equals(trace[i].getMethodName()))
+            sawAppend = true;
+          if ("doFlush".equals(trace[i].getMethodName()))
+            sawFlush = true;
+        }
+
+        if (sawAppend && sawFlush && count++ >= 30) {
             doFail = false;
             throw new IOException("now failing during flush");
           }
         }
       }
     }
-  }
 
   // LUCENE-1072: make sure an errant exception on flushing
   // one segment only takes out those docs in that one flush
@@ -2263,6 +2275,7 @@ public void run() {
         try {
           writer.updateDocument(new Term("id", ""+(idUpto++)), doc);
         } catch (IOException ioe) {
+          //ioe.printStackTrace(System.out);
           if (ioe.getMessage().startsWith("fake disk full at") ||
               ioe.getMessage().equals("now failing on purpose")) {
             diskFull = true;
@@ -2282,6 +2295,7 @@ public void run() {
             break;
           }
         } catch (Throwable t) {
+          //t.printStackTrace(System.out);
           if (noErrors) {
             System.out.println(Thread.currentThread().getName() + ": ERROR: unexpected Throwable:");
             t.printStackTrace(System.out);
@@ -2300,7 +2314,7 @@ public void run() {
   public void testCloseWithThreads() throws IOException {
     int NUM_THREADS = 3;
 
-    for(int iter=0;iter<50;iter++) {
+    for(int iter=0;iter<20;iter++) {
       MockRAMDirectory dir = new MockRAMDirectory();
       IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
       ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
@@ -2310,7 +2324,6 @@ public void testCloseWithThreads() throws IOException {
       writer.setMergeFactor(4);
 
       IndexerThread[] threads = new IndexerThread[NUM_THREADS];
-      boolean diskFull = false;
 
       for(int i=0;i<NUM_THREADS;i++)
         threads[i] = new IndexerThread(writer, false);
@@ -2319,7 +2332,7 @@ public void testCloseWithThreads() throws IOException {
         threads[i].start();
 
       try {
-        Thread.sleep(50);
+        Thread.sleep(100);
       } catch (InterruptedException ie) {
         Thread.currentThread().interrupt();
       }
@@ -2403,7 +2416,6 @@ public void testImmediateDiskFullWithThreads() throws IOException {
       dir.setMaxSizeInBytes(4*1024+20*iter);
 
       IndexerThread[] threads = new IndexerThread[NUM_THREADS];
-      boolean diskFull = false;
 
       for(int i=0;i<NUM_THREADS;i++)
         threads[i] = new IndexerThread(writer, true);
@@ -2441,7 +2453,7 @@ public void testImmediateDiskFullWithThreads() throws IOException {
   private static class FailOnlyOnAbortOrFlush extends MockRAMDirectory.Failure {
     private boolean onlyOnce;
     public FailOnlyOnAbortOrFlush(boolean onlyOnce) {
-      this.onlyOnce = true;
+      this.onlyOnce = onlyOnce;
     }
     public void eval(MockRAMDirectory dir)  throws IOException {
       if (doFail) {
@@ -2501,7 +2513,6 @@ public void _testMultipleThreadsFailure(MockRAMDirectory.Failure failure) throws
       writer.setMergeFactor(4);
 
       IndexerThread[] threads = new IndexerThread[NUM_THREADS];
-      boolean diskFull = false;
 
       for(int i=0;i<NUM_THREADS;i++)
         threads[i] = new IndexerThread(writer, true);
@@ -2538,6 +2549,8 @@ public void _testMultipleThreadsFailure(MockRAMDirectory.Failure failure) throws
         writer.close(false);
         success = true;
       } catch (IOException ioe) {
+        failure.clearDoFail();
+        writer.close(false);
       }
 
       if (success) {
@@ -2583,7 +2596,7 @@ public void testIOExceptionDuringAbortWithThreadsOnlyOnce() throws IOException {
   private static class FailOnlyInCloseDocStore extends MockRAMDirectory.Failure {
     private boolean onlyOnce;
     public FailOnlyInCloseDocStore(boolean onlyOnce) {
-      this.onlyOnce = true;
+      this.onlyOnce = onlyOnce;
     }
     public void eval(MockRAMDirectory dir)  throws IOException {
       if (doFail) {
@@ -2623,7 +2636,7 @@ public void testIOExceptionDuringCloseDocStoreWithThreadsOnlyOnce() throws IOExc
   private static class FailOnlyInWriteSegment extends MockRAMDirectory.Failure {
     private boolean onlyOnce;
     public FailOnlyInWriteSegment(boolean onlyOnce) {
-      this.onlyOnce = true;
+      this.onlyOnce = onlyOnce;
     }
     public void eval(MockRAMDirectory dir)  throws IOException {
       if (doFail) {
@@ -2682,6 +2695,125 @@ public void testUnlimitedMaxFieldLength() throws IOException {
     dir.close();
   }
 
+  // LUCENE-1044: Simulate checksum error in segments_N
+  public void testSegmentsChecksumError() throws IOException {
+    Directory dir = new MockRAMDirectory();
+
+    IndexWriter writer = null;
+
+    writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+
+    // add 100 documents
+    for (int i = 0; i < 100; i++) {
+      addDoc(writer);
+    }
+
+    // close
+    writer.close();
+
+    long gen = SegmentInfos.getCurrentSegmentGeneration(dir);
+    assertTrue("segment generation should be > 1 but got " + gen, gen > 1);
+
+    final String segmentsFileName = SegmentInfos.getCurrentSegmentFileName(dir);
+    IndexInput in = dir.openInput(segmentsFileName);
+    IndexOutput out = dir.createOutput(IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS, "", 1+gen));
+    out.copyBytes(in, in.length()-1);
+    byte b = in.readByte();
+    out.writeByte((byte) (1+b));
+    out.close();
+    in.close();
+
+    IndexReader reader = null;
+    try {
+      reader = IndexReader.open(dir);
+    } catch (IOException e) {
+      e.printStackTrace(System.out);
+      fail("segmentInfos failed to retry fallback to correct segments_N file");
+    }
+    reader.close();
+  }
+
+  // LUCENE-1044: test writer.commit() when ac=false
+  public void testForceCommit() throws IOException {
+    Directory dir = new MockRAMDirectory();
+
+    IndexWriter writer  = new IndexWriter(dir, false, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    writer.setMaxBufferedDocs(2);
+    writer.setMergeFactor(5);
+
+    for (int i = 0; i < 23; i++)
+      addDoc(writer);
+
+    IndexReader reader = IndexReader.open(dir);
+    assertEquals(0, reader.numDocs());
+    writer.commit();
+    IndexReader reader2 = reader.reopen();
+    assertEquals(0, reader.numDocs());
+    assertEquals(23, reader2.numDocs());
+    reader.close();
+
+    for (int i = 0; i < 17; i++)
+      addDoc(writer);
+    assertEquals(23, reader2.numDocs());
+    reader2.close();
+    reader = IndexReader.open(dir);
+    assertEquals(23, reader.numDocs());
+    reader.close();
+    writer.commit();
+
+    reader = IndexReader.open(dir);
+    assertEquals(40, reader.numDocs());
+    reader.close();
+    writer.close();
+    dir.close();
+  }
+
+  // Throws IOException during MockRAMDirectory.sync
+  private static class FailOnlyInSync extends MockRAMDirectory.Failure {
+    boolean didFail;
+    public void eval(MockRAMDirectory dir)  throws IOException {
+      if (doFail) {
+        StackTraceElement[] trace = new Exception().getStackTrace();
+        for (int i = 0; i < trace.length; i++) {
+          if (doFail && "org.apache.lucene.store.MockRAMDirectory".equals(trace[i].getClassName()) && "sync".equals(trace[i].getMethodName())) {
+            didFail = true;
+            throw new IOException("now failing on purpose during sync");
+          }
+        }
+      }
+    }
+  }
+
+  // LUCENE-1044: test exception during sync
+  public void testExceptionDuringSync() throws IOException {
+    MockRAMDirectory dir = new MockRAMDirectory();
+    FailOnlyInSync failure = new FailOnlyInSync();
+    dir.failOn(failure);
+
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    failure.setDoFail();
+
+    ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
+    // We expect sync exceptions in the merge threads
+    cms.setSuppressExceptions();
+    writer.setMergeScheduler(cms);
+    writer.setMaxBufferedDocs(2);
+    writer.setMergeFactor(5);
+
+    for (int i = 0; i < 23; i++)
+      addDoc(writer);
+
+    cms.sync();
+    assertTrue(failure.didFail);
+    failure.clearDoFail();
+    writer.close();
+
+    IndexReader reader = IndexReader.open(dir);
+    assertEquals(23, reader.numDocs());
+    reader.close();
+    dir.close();
+  }
+
   // LUCENE-1168
   public void testTermVectorCorruption() throws IOException {
 
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriterDelete.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
index 18d190f4..8aadafd0 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
@@ -30,7 +30,6 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockRAMDirectory;
-import org.apache.lucene.store.RAMDirectory;
 
 public class TestIndexWriterDelete extends LuceneTestCase {
 
@@ -45,7 +44,7 @@ public void testSimpleCase() throws IOException {
     for(int pass=0;pass<2;pass++) {
       boolean autoCommit = (0==pass);
 
-      Directory dir = new RAMDirectory();
+      Directory dir = new MockRAMDirectory();
       IndexWriter modifier = new IndexWriter(dir, autoCommit,
                                              new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
       modifier.setUseCompoundFile(true);
@@ -65,28 +64,17 @@ public void testSimpleCase() throws IOException {
         modifier.addDocument(doc);
       }
       modifier.optimize();
-
-      if (!autoCommit) {
-        modifier.close();
-      }
+      modifier.commit();
 
       Term term = new Term("city", "Amsterdam");
       int hitCount = getHitCount(dir, term);
       assertEquals(1, hitCount);
-      if (!autoCommit) {
-        modifier = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
-        modifier.setUseCompoundFile(true);
-      }
       modifier.deleteDocuments(term);
-      if (!autoCommit) {
-        modifier.close();
-      }
+      modifier.commit();
       hitCount = getHitCount(dir, term);
       assertEquals(0, hitCount);
 
-      if (autoCommit) {
         modifier.close();
-      }
       dir.close();
     }
   }
@@ -96,7 +84,7 @@ public void testNonRAMDelete() throws IOException {
     for(int pass=0;pass<2;pass++) {
       boolean autoCommit = (0==pass);
 
-      Directory dir = new RAMDirectory();
+      Directory dir = new MockRAMDirectory();
       IndexWriter modifier = new IndexWriter(dir, autoCommit,
                                              new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
       modifier.setMaxBufferedDocs(2);
@@ -108,38 +96,26 @@ public void testNonRAMDelete() throws IOException {
       for (int i = 0; i < 7; i++) {
         addDoc(modifier, ++id, value);
       }
-      modifier.flush();
+      modifier.commit();
 
       assertEquals(0, modifier.getNumBufferedDocuments());
       assertTrue(0 < modifier.getSegmentCount());
 
-      if (!autoCommit) {
-        modifier.close();
-      }
+      modifier.commit();
 
       IndexReader reader = IndexReader.open(dir);
       assertEquals(7, reader.numDocs());
       reader.close();
 
-      if (!autoCommit) {
-        modifier = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
-        modifier.setMaxBufferedDocs(2);
-        modifier.setMaxBufferedDeleteTerms(2);
-      }
-
       modifier.deleteDocuments(new Term("value", String.valueOf(value)));
       modifier.deleteDocuments(new Term("value", String.valueOf(value)));
 
-      if (!autoCommit) {
-        modifier.close();
-      }
+      modifier.commit();
 
       reader = IndexReader.open(dir);
       assertEquals(0, reader.numDocs());
       reader.close();
-      if (autoCommit) {
         modifier.close();
-      }
       dir.close();
     }
   }
@@ -148,7 +124,7 @@ public void testNonRAMDelete() throws IOException {
   public void testRAMDeletes() throws IOException {
     for(int pass=0;pass<2;pass++) {
       boolean autoCommit = (0==pass);
-      Directory dir = new RAMDirectory();
+      Directory dir = new MockRAMDirectory();
       IndexWriter modifier = new IndexWriter(dir, autoCommit,
                                              new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
       modifier.setMaxBufferedDocs(4);
@@ -169,9 +145,7 @@ public void testRAMDeletes() throws IOException {
       assertEquals(0, modifier.getSegmentCount());
       modifier.flush();
 
-      if (!autoCommit) {
-        modifier.close();
-      }
+      modifier.commit();
 
       IndexReader reader = IndexReader.open(dir);
       assertEquals(1, reader.numDocs());
@@ -179,9 +153,7 @@ public void testRAMDeletes() throws IOException {
       int hitCount = getHitCount(dir, new Term("id", String.valueOf(id)));
       assertEquals(1, hitCount);
       reader.close();
-      if (autoCommit) {
         modifier.close();
-      }
       dir.close();
     }
   }
@@ -191,7 +163,7 @@ public void testBothDeletes() throws IOException {
     for(int pass=0;pass<2;pass++) {
       boolean autoCommit = (0==pass);
 
-      Directory dir = new RAMDirectory();
+      Directory dir = new MockRAMDirectory();
       IndexWriter modifier = new IndexWriter(dir, autoCommit,
                                              new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
       modifier.setMaxBufferedDocs(100);
@@ -208,31 +180,26 @@ public void testBothDeletes() throws IOException {
       for (int i = 0; i < 5; i++) {
         addDoc(modifier, ++id, value);
       }
-      modifier.flush();
+      modifier.commit();
 
       for (int i = 0; i < 5; i++) {
         addDoc(modifier, ++id, value);
       }
       modifier.deleteDocuments(new Term("value", String.valueOf(value)));
 
-      modifier.flush();
-      if (!autoCommit) {
-        modifier.close();
-      }
+      modifier.commit();
 
       IndexReader reader = IndexReader.open(dir);
       assertEquals(5, reader.numDocs());
-      if (autoCommit) {
         modifier.close();
       }
     }
-  }
 
   // test that batched delete terms are flushed together
   public void testBatchDeletes() throws IOException {
     for(int pass=0;pass<2;pass++) {
       boolean autoCommit = (0==pass);
-      Directory dir = new RAMDirectory();
+      Directory dir = new MockRAMDirectory();
       IndexWriter modifier = new IndexWriter(dir, autoCommit,
                                              new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
       modifier.setMaxBufferedDocs(2);
@@ -244,29 +211,17 @@ public void testBatchDeletes() throws IOException {
       for (int i = 0; i < 7; i++) {
         addDoc(modifier, ++id, value);
       }
-      modifier.flush();
-      if (!autoCommit) {
-        modifier.close();
-      }
+      modifier.commit();
 
       IndexReader reader = IndexReader.open(dir);
       assertEquals(7, reader.numDocs());
       reader.close();
       
-      if (!autoCommit) {
-        modifier = new IndexWriter(dir, autoCommit,
-                                   new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
-        modifier.setMaxBufferedDocs(2);
-        modifier.setMaxBufferedDeleteTerms(2);
-      }
-
       id = 0;
       modifier.deleteDocuments(new Term("id", String.valueOf(++id)));
       modifier.deleteDocuments(new Term("id", String.valueOf(++id)));
 
-      if (!autoCommit) {
-        modifier.close();
-      }
+      modifier.commit();
 
       reader = IndexReader.open(dir);
       assertEquals(5, reader.numDocs());
@@ -276,23 +231,13 @@ public void testBatchDeletes() throws IOException {
       for (int i = 0; i < terms.length; i++) {
         terms[i] = new Term("id", String.valueOf(++id));
       }
-      if (!autoCommit) {
-        modifier = new IndexWriter(dir, autoCommit,
-                                   new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
-        modifier.setMaxBufferedDocs(2);
-        modifier.setMaxBufferedDeleteTerms(2);
-      }
       modifier.deleteDocuments(terms);
-      if (!autoCommit) {
-        modifier.close();
-      }
+      modifier.commit();
       reader = IndexReader.open(dir);
       assertEquals(2, reader.numDocs());
       reader.close();
 
-      if (autoCommit) {
         modifier.close();
-      }
       dir.close();
     }
   }
@@ -338,7 +283,7 @@ private void testOperationsOnDiskFull(boolean updates) throws IOException {
       boolean autoCommit = (0==pass);
 
       // First build up a starting index:
-      RAMDirectory startDir = new RAMDirectory();
+      MockRAMDirectory startDir = new MockRAMDirectory();
       IndexWriter writer = new IndexWriter(startDir, autoCommit,
                                            new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
       for (int i = 0; i < 157; i++) {
@@ -444,38 +389,10 @@ private void testOperationsOnDiskFull(boolean updates) throws IOException {
             }
           }
 
-          // Whether we succeeded or failed, check that all
-          // un-referenced files were in fact deleted (ie,
-          // we did not create garbage). Just create a
-          // new IndexFileDeleter, have it delete
-          // unreferenced files, then verify that in fact
-          // no files were deleted:
-          String[] startFiles = dir.list();
-          SegmentInfos infos = new SegmentInfos();
-          infos.read(dir);
-          new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
-          String[] endFiles = dir.list();
-
-          Arrays.sort(startFiles);
-          Arrays.sort(endFiles);
-
-          // for(int i=0;i<startFiles.length;i++) {
-          // System.out.println(" startFiles: " + i + ": " + startFiles[i]);
-          // }
-
-          if (!Arrays.equals(startFiles, endFiles)) {
-            String successStr;
-            if (success) {
-              successStr = "success";
-            } else {
-              successStr = "IOException";
-              err.printStackTrace();
-            }
-            fail("reader.close() failed to delete unreferenced files after "
-                 + successStr + " (" + diskFree + " bytes): before delete:\n    "
-                 + arrayToString(startFiles) + "\n  after delete:\n    "
-                 + arrayToString(endFiles));
-          }
+          // If the close() succeeded, make sure there are
+          // no unreferenced files.
+          if (success)
+            TestIndexWriter.assertNoUnreferencedFiles(dir, "after writer.close");
 
           // Finally, verify index is not corrupt, and, if
           // we succeeded, we see all docs changed, and if
@@ -618,12 +535,8 @@ public void eval(MockRAMDirectory dir)  throws IOException {
       // flush (and commit if ac)
 
       modifier.optimize();
+      modifier.commit();
 
-      // commit if !ac
-
-      if (!autoCommit) {
-        modifier.close();
-      }
       // one of the two files hits
 
       Term term = new Term("city", "Amsterdam");
@@ -632,11 +545,6 @@ public void eval(MockRAMDirectory dir)  throws IOException {
 
       // open the writer again (closed above)
 
-      if (!autoCommit) {
-        modifier = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
-        modifier.setUseCompoundFile(true);
-      }
-
       // delete the doc
       // max buf del terms is two, so this is buffered
 
@@ -648,7 +556,7 @@ public void eval(MockRAMDirectory dir)  throws IOException {
       Document doc = new Document();
       modifier.addDocument(doc);
 
-      // flush the changes, the buffered deletes, and the new doc
+      // commit the changes, the buffered deletes, and the new doc
 
       // The failure object will fail on the first write after the del
       // file gets created when processing the buffered delete
@@ -659,38 +567,28 @@ public void eval(MockRAMDirectory dir)  throws IOException {
       // in the !ac case, a new segments file won't be created but in
       // this case, creation of the cfs file happens next so we need
       // the doc (to test that it's okay that we don't lose deletes if
-      // failing while creating the cfs file
+      // failing while creating the cfs file)
 
       boolean failed = false;
       try {
-        modifier.flush();
+        modifier.commit();
       } catch (IOException ioe) {
         failed = true;
       }
 
       assertTrue(failed);
 
-      // The flush above failed, so we need to retry it (which will
+      // The commit above failed, so we need to retry it (which will
       // succeed, because the failure is a one-shot)
 
-      if (!autoCommit) {
-        modifier.close();
-      } else {
-        modifier.flush();
-      }
+      modifier.commit();
 
       hitCount = getHitCount(dir, term);
 
-      // If the delete was not cleared then hit count will
-      // be 0.  With autoCommit=false, we hit the exception
-      // on creating the compound file, so the delete was
-      // flushed successfully.
-      assertEquals(autoCommit ? 1:0, hitCount);
+      // Make sure the delete was successfully flushed:
+      assertEquals(0, hitCount);
 
-      if (autoCommit) {
         modifier.close();
-      }
-
       dir.close();
     }
   }
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestMultiSegmentReader.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestMultiSegmentReader.java
index d35518ba..adf394da 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestMultiSegmentReader.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestMultiSegmentReader.java
@@ -46,8 +46,8 @@ protected void setUp() throws Exception {
     doc2 = new Document();
     DocHelper.setupDoc(doc1);
     DocHelper.setupDoc(doc2);
-    SegmentInfo info1 = DocHelper.writeDoc(dir, doc1);
-    SegmentInfo info2 = DocHelper.writeDoc(dir, doc2);
+    DocHelper.writeDoc(dir, doc1);
+    DocHelper.writeDoc(dir, doc2);
     sis = new SegmentInfos();
     sis.read(dir);
   }
@@ -102,7 +102,7 @@ public void doTestUndeleteAll() throws IOException {
     if (reader instanceof MultiReader)
       // MultiReader does not "own" the directory so it does
       // not write the changes to sis on commit:
-      sis.write(dir);
+      sis.commit(dir);
 
     sis.read(dir);
     reader = openReader();
@@ -115,7 +115,7 @@ public void doTestUndeleteAll() throws IOException {
     if (reader instanceof MultiReader)
       // MultiReader does not "own" the directory so it does
       // not write the changes to sis on commit:
-      sis.write(dir);
+      sis.commit(dir);
     sis.read(dir);
     reader = openReader();
     assertEquals( 1, reader.numDocs() );
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestStressIndexing.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestStressIndexing.java
index 0aa4c1f5..14f1bd83 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestStressIndexing.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestStressIndexing.java
@@ -21,12 +21,9 @@
 import org.apache.lucene.store.*;
 import org.apache.lucene.document.*;
 import org.apache.lucene.analysis.*;
-import org.apache.lucene.index.*;
 import org.apache.lucene.search.*;
 import org.apache.lucene.queryParser.*;
 
-import org.apache.lucene.util.LuceneTestCase;
-
 import java.util.Random;
 import java.io.File;
 
@@ -124,6 +121,7 @@ public void runStressTest(Directory directory, boolean autoCommit, MergeSchedule
     modifier.setMaxBufferedDocs(10);
 
     TimedThread[] threads = new TimedThread[4];
+    int numThread = 0;
 
     if (mergeScheduler != null)
       modifier.setMergeScheduler(mergeScheduler);
@@ -131,34 +129,30 @@ public void runStressTest(Directory directory, boolean autoCommit, MergeSchedule
     // One modifier that writes 10 docs then removes 5, over
     // and over:
     IndexerThread indexerThread = new IndexerThread(modifier, threads);
-    threads[0] = indexerThread;
+    threads[numThread++] = indexerThread;
     indexerThread.start();
       
     IndexerThread indexerThread2 = new IndexerThread(modifier, threads);
-    threads[2] = indexerThread2;
+    threads[numThread++] = indexerThread2;
     indexerThread2.start();
       
     // Two searchers that constantly just re-instantiate the
     // searcher:
     SearcherThread searcherThread1 = new SearcherThread(directory, threads);
-    threads[3] = searcherThread1;
+    threads[numThread++] = searcherThread1;
     searcherThread1.start();
 
     SearcherThread searcherThread2 = new SearcherThread(directory, threads);
-    threads[3] = searcherThread2;
+    threads[numThread++] = searcherThread2;
     searcherThread2.start();
 
-    indexerThread.join();
-    indexerThread2.join();
-    searcherThread1.join();
-    searcherThread2.join();
+    for(int i=0;i<numThread;i++)
+      threads[i].join();
 
     modifier.close();
 
-    assertTrue("hit unexpected exception in indexer", !indexerThread.failed);
-    assertTrue("hit unexpected exception in indexer2", !indexerThread2.failed);
-    assertTrue("hit unexpected exception in search1", !searcherThread1.failed);
-    assertTrue("hit unexpected exception in search2", !searcherThread2.failed);
+    for(int i=0;i<numThread;i++)
+      assertTrue(!((TimedThread) threads[i]).failed);
 
     //System.out.println("    Writer: " + indexerThread.count + " iterations");
     //System.out.println("Searcher 1: " + searcherThread1.count + " searchers created");
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestThreadedOptimize.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestThreadedOptimize.java
index 089c1644..a03666bf 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestThreadedOptimize.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestThreadedOptimize.java
@@ -39,10 +39,10 @@
   private final static int NUM_THREADS = 3;
   //private final static int NUM_THREADS = 5;
 
-  private final static int NUM_ITER = 2;
+  private final static int NUM_ITER = 1;
   //private final static int NUM_ITER = 10;
 
-  private final static int NUM_ITER2 = 2;
+  private final static int NUM_ITER2 = 1;
   //private final static int NUM_ITER2 = 5;
 
   private boolean failed;
@@ -138,8 +138,8 @@ public void run() {
   */
   public void testThreadedOptimize() throws Exception {
     Directory directory = new MockRAMDirectory();
-    runTest(directory, false, null);
-    runTest(directory, true, null);
+    runTest(directory, false, new SerialMergeScheduler());
+    runTest(directory, true, new SerialMergeScheduler());
     runTest(directory, false, new ConcurrentMergeScheduler());
     runTest(directory, true, new ConcurrentMergeScheduler());
     directory.close();
@@ -150,8 +150,8 @@ public void testThreadedOptimize() throws Exception {
 
     String dirName = tempDir + "/luceneTestThreadedOptimize";
     directory = FSDirectory.getDirectory(dirName);
-    runTest(directory, false, null);
-    runTest(directory, true, null);
+    runTest(directory, false, new SerialMergeScheduler());
+    runTest(directory, true, new SerialMergeScheduler());
     runTest(directory, false, new ConcurrentMergeScheduler());
     runTest(directory, true, new ConcurrentMergeScheduler());
     directory.close();
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMDirectory.java b/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMDirectory.java
index 3510b877..c775c945 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMDirectory.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMDirectory.java
@@ -24,7 +24,10 @@
 import java.util.Random;
 import java.util.Map;
 import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Set;
 import java.util.ArrayList;
+import java.util.Arrays;
 
 /**
  * This is a subclass of RAMDirectory that adds methods
@@ -40,6 +43,10 @@
   double randomIOExceptionRate;
   Random randomState;
   boolean noDeleteOpenFile = true;
+  boolean preventDoubleWrite = true;
+  private Set unSyncedFiles;
+  private Set createdFiles;
+  volatile boolean crashed;
 
   // NOTE: we cannot initialize the Map here due to the
   // order in which our constructor actually does this
@@ -47,29 +54,78 @@
   // like super is called, then our members are initialized:
   Map openFiles;
 
-  public MockRAMDirectory() {
-    super();
-    if (openFiles == null) {
+  private void init() {
+    if (openFiles == null)
       openFiles = new HashMap();
+    if (createdFiles == null)
+      createdFiles = new HashSet();
+    if (unSyncedFiles == null)
+      unSyncedFiles = new HashSet();
     }
+
+  public MockRAMDirectory() {
+    super();
+    init();
   }
   public MockRAMDirectory(String dir) throws IOException {
     super(dir);
-    if (openFiles == null) {
-      openFiles = new HashMap();
-    }
+    init();
   }
   public MockRAMDirectory(Directory dir) throws IOException {
     super(dir);
-    if (openFiles == null) {
-      openFiles = new HashMap();
-    }
+    init();
   }
   public MockRAMDirectory(File dir) throws IOException {
     super(dir);
-    if (openFiles == null) {
+    init();
+  }
+
+  /** If set to true, we throw an IOException if the same
+   *  file is opened by createOutput, ever. */
+  public void setPreventDoubleWrite(boolean value) {
+    preventDoubleWrite = value;
+  }
+
+  public synchronized void sync(String name) throws IOException {
+    maybeThrowDeterministicException();
+    if (crashed)
+      throw new IOException("cannot sync after crash");
+    if (unSyncedFiles.contains(name))
+      unSyncedFiles.remove(name);
+  }
+
+  /** Simulates a crash of OS or machine by overwriting
+   *  unsycned files. */
+  public void crash() throws IOException {
+    synchronized(this) {
+      crashed = true;
       openFiles = new HashMap();
     }
+    Iterator it = unSyncedFiles.iterator();
+    unSyncedFiles = new HashSet();
+    int count = 0;
+    while(it.hasNext()) {
+      String name = (String) it.next();
+      RAMFile file = (RAMFile) fileMap.get(name);
+      if (count % 3 == 0) {
+        deleteFile(name, true);
+      } else if (count % 3 == 1) {
+        // Zero out file entirely
+        final int numBuffers = file.numBuffers();
+        for(int i=0;i<numBuffers;i++) {
+          byte[] buffer = file.getBuffer(i);
+          Arrays.fill(buffer, (byte) 0);
+        }
+      } else if (count % 3 == 2) {
+        // Truncate the file:
+        file.setLength(file.getLength()/2);
+      }
+      count++;
+    }
+  }
+
+  public synchronized void clearCrash() throws IOException {
+    crashed = false;
   }
 
   public void setMaxSizeInBytes(long maxSize) {
@@ -126,24 +182,41 @@ void maybeThrowIOException() throws IOException {
   }
 
   public synchronized void deleteFile(String name) throws IOException {
+    deleteFile(name, false);
+  }
+
+  private synchronized void deleteFile(String name, boolean forced) throws IOException {
+    if (crashed && !forced)
+      throw new IOException("cannot delete after crash");
+
+    if (unSyncedFiles.contains(name))
+      unSyncedFiles.remove(name);
+    if (!forced) {
     synchronized(openFiles) {
       if (noDeleteOpenFile && openFiles.containsKey(name)) {
         throw new IOException("MockRAMDirectory: file \"" + name + "\" is still open: cannot delete");
       }
     }
+    }
     super.deleteFile(name);
   }
 
   public IndexOutput createOutput(String name) throws IOException {
-    if (openFiles == null) {
-      openFiles = new HashMap();
-    }
+    if (crashed)
+      throw new IOException("cannot createOutput after crash");
+    init();
     synchronized(openFiles) {
+      if (preventDoubleWrite && createdFiles.contains(name))
+        throw new IOException("file \"" + name + "\" was already written to");
       if (noDeleteOpenFile && openFiles.containsKey(name))
        throw new IOException("MockRAMDirectory: file \"" + name + "\" is still open: cannot overwrite");
     }
     RAMFile file = new RAMFile(this);
     synchronized (this) {
+      if (crashed)
+        throw new IOException("cannot createOutput after crash");
+      unSyncedFiles.add(name);
+      createdFiles.add(name);
       RAMFile existing = (RAMFile)fileMap.get(name);
       // Enforce write once:
       if (existing!=null && !name.equals("segments.gen"))
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMInputStream.java b/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMInputStream.java
index 2fa07fb8..540de741 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMInputStream.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMInputStream.java
@@ -45,6 +45,8 @@ public void close() {
     if (!isClone) {
       synchronized(dir.openFiles) {
         Integer v = (Integer) dir.openFiles.get(name);
+        // Could be null when MockRAMDirectory.crash() was called
+        if (v != null) {
         if (v.intValue() == 1) {
           dir.openFiles.remove(name);
         } else {
@@ -54,6 +56,7 @@ public void close() {
       }
     }
   }
+  }
 
   public Object clone() {
     MockRAMInputStream clone = (MockRAMInputStream) super.clone();
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMOutputStream.java b/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMOutputStream.java
index 7c114f78..c17e354a 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMOutputStream.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/store/MockRAMOutputStream.java
@@ -63,6 +63,11 @@ public void writeBytes(byte[] b, int offset, int len) throws IOException {
     long freeSpace = dir.maxSize - dir.sizeInBytes();
     long realUsage = 0;
 
+    // If MockRAMDir crashed since we were opened, then
+    // don't write anything:
+    if (dir.crashed)
+      throw new IOException("MockRAMDirectory was crashed");
+
     // Enforce disk full:
     if (dir.maxSize != 0 && freeSpace <= len) {
       // Compute the real disk free.  This will greatly slow
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/util/LuceneTestCase.java b/lucene/java/trunk/src/test/org/apache/lucene/util/LuceneTestCase.java
index 0a84d9d8..8420853d 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/util/LuceneTestCase.java
@@ -46,6 +46,9 @@ protected void setUp() throws Exception {
 
   protected void tearDown() throws Exception {
     if (ConcurrentMergeScheduler.anyUnhandledExceptions()) {
+      // Clear the failure so that we don't just keep
+      // failing subsequent test cases
+      ConcurrentMergeScheduler.clearUnhandledExceptions();
       fail("ConcurrentMergeScheduler hit unhandled exceptions");
     }
   }
