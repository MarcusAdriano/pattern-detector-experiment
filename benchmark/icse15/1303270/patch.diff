diff --git a/lucene/dev/branches/branch_3x/lucene/core/src/test/org/apache/lucene/analysis/TestDuelingAnalyzers.java b/lucene/dev/branches/branch_3x/lucene/core/src/test/org/apache/lucene/analysis/TestDuelingAnalyzers.java
index f4e7ac99..3ab303d8 100644
--- a/lucene/dev/branches/branch_3x/lucene/core/src/test/org/apache/lucene/analysis/TestDuelingAnalyzers.java
+++ b/lucene/dev/branches/branch_3x/lucene/core/src/test/org/apache/lucene/analysis/TestDuelingAnalyzers.java
@@ -22,6 +22,7 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockReaderWrapper;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
@@ -50,8 +51,63 @@ protected TokenStreamComponents createComponents(String fieldName, Reader reader
     };
     for (int i = 0; i < 10000; i++) {
       String s = _TestUtil.randomSimpleString(random);
-      assertEquals(s, left.tokenStream("foo", new StringReader(s)), 
-                   right.tokenStream("foo", new StringReader(s)));
+      assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
+                   right.tokenStream("foo", newStringReader(s)));
+    }
+  }
+  
+  // not so useful since its all one token?!
+  public void testLetterAsciiHuge() throws Exception {
+    int maxLength = 8192; // CharTokenizer.IO_BUFFER_SIZE*2
+    MockAnalyzer left = new MockAnalyzer(random, MockTokenizer.SIMPLE, false);
+    left.setMaxTokenLength(255); // match CharTokenizer's max token length
+    Analyzer right = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    int numIterations = atLeast(100);
+    for (int i = 0; i < numIterations; i++) {
+      String s = _TestUtil.randomSimpleString(random, maxLength);
+      assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
+                   right.tokenStream("foo", newStringReader(s)));
+    }
+  }
+  
+  public void testLetterHtmlish() throws Exception {
+    Analyzer left = new MockAnalyzer(random, MockTokenizer.SIMPLE, false);
+    Analyzer right = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    for (int i = 0; i < 10000; i++) {
+      String s = _TestUtil.randomHtmlishString(random, 20);
+      assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
+                   right.tokenStream("foo", newStringReader(s)));
+    }
+  }
+  
+  public void testLetterHtmlishHuge() throws Exception {
+    int maxLength = 2048; // this is number of elements, not chars!
+    MockAnalyzer left = new MockAnalyzer(random, MockTokenizer.SIMPLE, false);
+    left.setMaxTokenLength(255); // match CharTokenizer's max token length
+    Analyzer right = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    int numIterations = atLeast(100);
+    for (int i = 0; i < numIterations; i++) {
+      String s = _TestUtil.randomHtmlishString(random, maxLength);
+      assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
+                   right.tokenStream("foo", newStringReader(s)));
     }
   }
   
@@ -66,8 +122,27 @@ protected TokenStreamComponents createComponents(String fieldName, Reader reader
     };
     for (int i = 0; i < 10000; i++) {
       String s = _TestUtil.randomUnicodeString(random);
-      assertEquals(s, left.tokenStream("foo", new StringReader(s)), 
-                   right.tokenStream("foo", new StringReader(s)));
+      assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
+                   right.tokenStream("foo", newStringReader(s)));
+    }
+  }
+  
+  public void testLetterUnicodeHuge() throws Exception {
+    int maxLength = 8192; // CharTokenizer.IO_BUFFER_SIZE*2
+    MockAnalyzer left = new MockAnalyzer(random, MockTokenizer.SIMPLE, false);
+    left.setMaxTokenLength(255); // match CharTokenizer's max token length
+    Analyzer right = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    int numIterations = atLeast(100);
+    for (int i = 0; i < numIterations; i++) {
+      String s = _TestUtil.randomUnicodeString(random, maxLength);
+      assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
+                   right.tokenStream("foo", newStringReader(s)));
     }
   }
   
@@ -97,4 +172,13 @@ public void assertEquals(String s, TokenStream left, TokenStream right) throws E
     left.close();
     right.close();
   }
+  
+  // TODO: maybe push this out to _TestUtil or LuceneTestCase and always use it instead?
+  private static Reader newStringReader(String s) {
+    Reader r = new StringReader(s);
+    if (random.nextBoolean()) {
+      r = new MockReaderWrapper(random, r);
+    }
+    return r;
+  }
 }
diff --git a/lucene/dev/branches/branch_3x/lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java b/lucene/dev/branches/branch_3x/lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
index 857c0955..6f51ee94 100644
--- a/lucene/dev/branches/branch_3x/lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
+++ b/lucene/dev/branches/branch_3x/lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
@@ -50,6 +50,7 @@
   private final Random random;
   private Map<String,Integer> previousMappings = new HashMap<String,Integer>();
   private boolean enableChecks = true;
+  private int maxTokenLength = MockTokenizer.DEFAULT_MAX_TOKEN_LENGTH;
 
   /**
    * Creates a new MockAnalyzer.
@@ -88,7 +89,7 @@ public MockAnalyzer(Random random) {
 
   @Override
   public TokenStream tokenStream(String fieldName, Reader reader) {
-    MockTokenizer tokenizer = new MockTokenizer(reader, pattern, lowerCase);
+    MockTokenizer tokenizer = new MockTokenizer(reader, pattern, lowerCase, maxTokenLength);
     tokenizer.setEnableChecks(enableChecks);
     StopFilter filt = new StopFilter(LuceneTestCase.TEST_VERSION_CURRENT, tokenizer, filter);
     filt.setEnablePositionIncrements(enablePositionIncrements);
@@ -112,7 +113,7 @@ public TokenStream reusableTokenStream(String fieldName, Reader reader)
     SavedStreams saved = map.get(fieldName);
     if (saved == null) {
       saved = new SavedStreams();
-      saved.tokenizer = new MockTokenizer(reader, pattern, lowerCase);
+      saved.tokenizer = new MockTokenizer(reader, pattern, lowerCase, maxTokenLength);
       saved.tokenizer.setEnableChecks(enableChecks);
       StopFilter filt = new StopFilter(LuceneTestCase.TEST_VERSION_CURRENT, saved.tokenizer, filter);
       filt.setEnablePositionIncrements(enablePositionIncrements);
@@ -167,4 +168,11 @@ public int getPositionIncrementGap(String fieldName){
   public void setEnableChecks(boolean enableChecks) {
     this.enableChecks = enableChecks;
   }
+  
+  /** 
+   * Toggle maxTokenLength for MockTokenizer
+   */
+  public void setMaxTokenLength(int length) {
+    this.maxTokenLength = length;
+  }
 }
