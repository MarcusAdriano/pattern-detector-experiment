diff --git a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/IdentityReducer.java b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/IdentityReducer.java
index f81b76c4..e69de29b 100644
--- a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/IdentityReducer.java
+++ b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/IdentityReducer.java
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop;
-
-import org.apache.hadoop.mapred.MapReduceBase;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-/** Copied from Hadoop 0.19. Replace when Hadoop 0.20+ makes Reducer non-abstract. */
-public class IdentityReducer<K, V> extends MapReduceBase implements Reducer<K, V, K, V> {
-
-  @Override
-  public void reduce(K key,
-                     Iterator<V> values,
-                     OutputCollector<K, V> output,
-                     Reporter reporter) throws IOException {
-    while (values.hasNext()) {
-      output.collect(key, values.next());
-    }
-  }
-
-}
diff --git a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/ItemItemWritable.java b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/ItemItemWritable.java
index 923e399d..0c432fd9 100644
--- a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/ItemItemWritable.java
+++ b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/ItemItemWritable.java
@@ -67,16 +67,12 @@ public static ItemItemWritable read(DataInput in) throws IOException {
 
   @Override
   public int compareTo(ItemItemWritable that) {
-    if (this == that) {
-      return 0;
-    }
-    if (itemAID < that.getItemAID()) {
-      return -1;
-    } else if (itemAID > that.getItemAID()) {
-      return 1;
-    } else {
-      return itemBID < that.getItemBID() ? -1 : itemBID > that.getItemBID() ? 1 : 0;
+    int aCompare = compare(itemAID, that.getItemAID());
+    return aCompare == 0 ? compare(itemBID, that.getItemBID()) : aCompare;
     }
+
+  private static int compare(long a, long b) {
+    return a < b ? -1 : a > b ? 1 : 0;
   }
 
   @Override
diff --git a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommendedItemsWritable.java b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommendedItemsWritable.java
index 19ef2d42..6e7440d0 100644
--- a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommendedItemsWritable.java
+++ b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommendedItemsWritable.java
@@ -64,11 +64,15 @@ public void readFields(DataInput in) throws IOException {
       do {
         long itemID = in.readLong();
         float value = in.readFloat();
+        if (!Float.isNaN(value)) {
         RecommendedItem recommendedItem = new GenericRecommendedItem(itemID, value);
         recommended.add(recommendedItem);
+        }
       } while (true);
     } catch (EOFException eofe) {
       // continue; done
+    } catch (ArrayIndexOutOfBoundsException aiooe) {
+      // bizarre ByteArrayInputStream bug? sometimes throws from read(); done
     }
   }
 
@@ -80,7 +84,7 @@ public static RecommendedItemsWritable read(DataInput in) throws IOException {
 
   @Override
   public String toString() {
-    StringBuilder result = new StringBuilder();
+    StringBuilder result = new StringBuilder(100);
     result.append('[');
     boolean first = true;
     for (RecommendedItem item : recommended) {
diff --git a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommenderJob.java b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommenderJob.java
index 10ac2195..5d0cc9da 100644
--- a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommenderJob.java
+++ b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommenderJob.java
@@ -36,6 +36,7 @@
 import org.apache.hadoop.mapred.Reducer;
 import org.apache.hadoop.mapred.TextInputFormat;
 import org.apache.hadoop.mapred.TextOutputFormat;
+import org.apache.hadoop.mapred.lib.IdentityReducer;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.mahout.cf.taste.recommender.Recommender;
 import org.apache.mahout.common.CommandLineUtil;
@@ -60,12 +61,14 @@
  *  <li>Location of a data model file containing preference data,
  *   suitable for use with {@link org.apache.mahout.cf.taste.impl.model.file.FileDataModel}</li>
  *  <li>Output path where reducer output should go</li>
+ *  <li>JAR file containing implementation code</li>
  * </ol>
  *
  * <p>Example arguments:</p>
  *
- * <p><code>org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender 10 path/to/users.txt
- * path/to/data.csv path/to/reducerOutputDir</code></p>
+ * <p><code>--recommenderClassName org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender
+ * --userRec 10 --userIdFile path/to/users.txt --dataModelFile path/to/data.csv
+ * --output path/to/reducerOutputDir --jarFile recommender.jar</code></p>
  *
  * <p>
  * Set up Hadoop in a pseudo-distributed manner: http://hadoop.apache.org/common/docs/current/quickstart.html
@@ -89,22 +92,29 @@
  * {@code
  * hadoop fs -put input.csv input/input.csv
  * hadoop fs -put users.txt input/users.txt
- * hadoop fs -mkdir output/
  * }
  *
  * <p>Now build Mahout code using your IDE, or Maven. Note where the compiled classes go. If you built with
  * Maven, it'll be like (Mahout directory)/core/target/classes/. Prepare a .jar file for Hadoop:</p>
  *
  * {@code
- * jar cvf recommender.jar -C (classes directory) .
+ * jar cf recommender.jar -C (Mahout classes directory) .
+ * }
+ *
+ * <p>Now add your own custom recommender code and dependencies. Your IDE produced compiled .class
+ * files somewhere and they need to be packaged up as well:</p>
+ *
+ * {@code
+ * jar uf recommender.jar -C (your classes directory) .
  * }
  *
  * <p>And launch:</p>
  *
  * {@code
  * hadoop jar recommender.jar org.apache.mahout.cf.taste.hadoop.RecommenderJob \
- *   org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender \
- *   10 input/users.txt input/input.csv recommender.jar output
+ *   --recommenderClassName org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender \
+ *   --userRec 10 --userIdFile input/users.txt --dataModelFile input/input.csv \
+ *   --output output --jarFile recommender.jar
  * }
  */
 public final class RecommenderJob {
@@ -143,7 +153,8 @@ public static void main(String[] args) throws Exception {
     Option helpOpt = DefaultOptionCreator.helpOption(obuilder);
 
     Group group = gbuilder.withName("Options").withOption(recommendClassOpt).withOption(userRecommendOpt)
-      .withOption(userIDFileOpt).withOption(dataModelFileOpt).withOption(outputOpt).withOption(helpOpt).create();
+      .withOption(userIDFileOpt).withOption(dataModelFileOpt).withOption(outputOpt)
+      .withOption(jarFileOpt).withOption(helpOpt).create();
 
 
     try {
diff --git a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommenderMapper.java b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommenderMapper.java
index 12a69fd6..768fc134 100644
--- a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommenderMapper.java
+++ b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/RecommenderMapper.java
@@ -20,6 +20,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.MapReduceBase;
 import org.apache.hadoop.mapred.Mapper;
 import org.apache.hadoop.mapred.OutputCollector;
@@ -34,6 +35,7 @@
 import java.io.IOException;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.InvocationTargetException;
+import java.util.Iterator;
 import java.util.List;
 
 /**
@@ -43,13 +45,13 @@
  *
  * <p>Note that there is no corresponding {@link org.apache.hadoop.mapreduce.Reducer}; this implementation can only
  * partially take advantage of the mapreduce paradigm and only really leverages it for easy parallelization. Therefore,
- * use the {@link IdentityReducer} when running this on Hadoop.</p>
+ * use the {@link org.apache.hadoop.mapred.lib.IdentityReducer} when running this on Hadoop.</p>
  *
  * @see RecommenderJob
  */
 public final class RecommenderMapper
     extends MapReduceBase
-    implements Mapper<LongWritable, LongWritable, LongWritable, RecommendedItemsWritable> {
+    implements Mapper<LongWritable, Text, LongWritable, RecommendedItemsWritable> {
 
   static final String RECOMMENDER_CLASS_NAME = "recommenderClassName";
   static final String RECOMMENDATIONS_PER_USER = "recommendationsPerUser";
@@ -96,18 +98,24 @@ public void configure(org.apache.hadoop.mapred.JobConf jobConf) {
 
   @Override
   public void map(LongWritable key,
-                  LongWritable value,
+                  Text value,
                   OutputCollector<LongWritable, RecommendedItemsWritable> output,
                   Reporter reporter) throws IOException {
-    long userID = value.get();
+    long userID = Long.parseLong(value.toString());
     List<RecommendedItem> recommendedItems;
     try {
       recommendedItems = recommender.recommend(userID, recommendationsPerUser);
     } catch (TasteException te) {
       throw new IllegalStateException(te);
     }
+    Iterator<RecommendedItem> it = recommendedItems.iterator();
+    while (it.hasNext()) {
+      if (Float.isNaN(it.next().getValue())) {
+        it.remove();
+      }
+    }
     RecommendedItemsWritable writable = new RecommendedItemsWritable(recommendedItems);
-    output.collect(value, writable);
+    output.collect(new LongWritable(userID), writable);
     reporter.getCounter(ReducerMetrics.USERS_PROCESSED).increment(1L);
     reporter.getCounter(ReducerMetrics.RECOMMENDATIONS_MADE).increment(recommendedItems.size());
   }
diff --git a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/SlopeOneDiffsToAveragesJob.java b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/SlopeOneDiffsToAveragesJob.java
index 14f2c04f..8c901797 100644
--- a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/SlopeOneDiffsToAveragesJob.java
+++ b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/SlopeOneDiffsToAveragesJob.java
@@ -36,6 +36,7 @@
 import org.apache.hadoop.mapred.Reducer;
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
 import org.apache.hadoop.mapred.TextOutputFormat;
+import org.apache.hadoop.mapred.lib.IdentityMapper;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.mahout.common.CommandLineUtil;
 import org.apache.mahout.common.commandline.DefaultOptionCreator;
@@ -112,7 +113,7 @@ public static JobConf buildJobConf(String prefsFile,
     jobConf.setClass("mapred.input.format.class", SequenceFileInputFormat.class, InputFormat.class);
     jobConf.set("mapred.input.dir", StringUtils.escapeString(prefsFilePath.toString()));
 
-    jobConf.setClass("mapred.mapper.class", Mapper.class, Mapper.class);
+    jobConf.setClass("mapred.mapper.class", IdentityMapper.class, Mapper.class);
     jobConf.setClass("mapred.mapoutput.key.class", ItemItemWritable.class, Object.class);
     jobConf.setClass("mapred.mapoutput.value.class", FloatWritable.class, Object.class);
 
diff --git a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/SlopeOnePrefsToDiffsJob.java b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/SlopeOnePrefsToDiffsJob.java
index 57070715..d7404541 100644
--- a/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/SlopeOnePrefsToDiffsJob.java
+++ b/lucene/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/SlopeOnePrefsToDiffsJob.java
@@ -29,6 +29,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobClient;
@@ -116,7 +117,7 @@ public static JobConf buildJobConf(String prefsFile,
     jobConf.set("mapred.input.dir", StringUtils.escapeString(prefsFilePath.toString()));
 
     jobConf.setClass("mapred.mapper.class", SlopeOnePrefsToDiffsMapper.class, Mapper.class);
-    jobConf.setClass("mapred.mapoutput.key.class", Text.class, Object.class);
+    jobConf.setClass("mapred.mapoutput.key.class", LongWritable.class, Object.class);
     jobConf.setClass("mapred.mapoutput.value.class", ItemPrefWritable.class, Object.class);
 
     jobConf.setClass("mapred.reducer.class", SlopeOnePrefsToDiffsReducer.class, Reducer.class);
