diff --git a/mahout/trunk/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputFormatTest.java b/mahout/trunk/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputFormatTest.java
index 70bc811e..1d22a8d1 100644
--- a/mahout/trunk/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputFormatTest.java
+++ b/mahout/trunk/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputFormatTest.java
@@ -28,6 +28,7 @@
 import org.junit.Test;
 
 import java.io.IOException;
+import java.lang.reflect.InvocationTargetException;
 import java.util.Collections;
 import java.util.List;
 
@@ -38,12 +39,12 @@
   private Configuration conf;
 
   @Before
-  public void before() throws IOException {
+  public void before() throws Exception {
     inputFormat = new LuceneSegmentInputFormat();
     LuceneStorageConfiguration lucene2SeqConf = new LuceneStorageConfiguration(new Configuration(), Collections.singletonList(indexPath1), new Path("output"), "id", Collections.singletonList("field"));
     conf = lucene2SeqConf.serialize();
 
-    jobContext = new JobContext(conf, new JobID());
+    jobContext = getJobContext(conf, new JobID());
   }
 
   @After
@@ -65,4 +66,19 @@ public void testGetSplits() throws IOException, InterruptedException {
     List<LuceneSegmentInputSplit> splits = inputFormat.getSplits(jobContext);
     Assert.assertEquals(3, splits.size());
   }
+
+  // Use reflection to abstract this incompatibility between Hadoop 1 & 2 APIs.
+  private JobContext getJobContext(Configuration conf, JobID jobID) throws
+      ClassNotFoundException, NoSuchMethodException, IllegalAccessException,
+      InvocationTargetException, InstantiationException {
+    Class<? extends JobContext> clazz = null;
+    if (!JobContext.class.isInterface()) {
+      clazz = JobContext.class;
+    } else {
+      clazz = (Class<? extends JobContext>)
+          Class.forName("org.apache.hadoop.mapreduce.task.JobContextImpl");
+    }
+    return clazz.getConstructor(Configuration.class, JobID.class)
+        .newInstance(conf, jobID);
+  }
 }
diff --git a/mahout/trunk/integration/src/test/java/org/apache/mahout/text/LuceneSegmentRecordReaderTest.java b/mahout/trunk/integration/src/test/java/org/apache/mahout/text/LuceneSegmentRecordReaderTest.java
index 680c8dd1..8a772884 100644
--- a/mahout/trunk/integration/src/test/java/org/apache/mahout/text/LuceneSegmentRecordReaderTest.java
+++ b/mahout/trunk/integration/src/test/java/org/apache/mahout/text/LuceneSegmentRecordReaderTest.java
@@ -29,6 +29,7 @@
 import org.junit.Test;
 
 import java.io.IOException;
+import java.lang.reflect.InvocationTargetException;
 
 import static java.util.Arrays.asList;
 
@@ -58,7 +59,7 @@ public void testKey() throws Exception {
     for (SegmentInfoPerCommit segmentInfo : segmentInfos) {
       int docId = 0;
       LuceneSegmentInputSplit inputSplit = new LuceneSegmentInputSplit(getIndexPath1(), segmentInfo.info.name, segmentInfo.sizeInBytes());
-      TaskAttemptContext context = new TaskAttemptContext(configuration, new TaskAttemptID());
+      TaskAttemptContext context = getTaskAttemptContext(configuration, new TaskAttemptID());
       recordReader.initialize(inputSplit, context);
       for (int i = 0; i < 500; i++){
         recordReader.nextKeyValue();
@@ -69,4 +70,19 @@ public void testKey() throws Exception {
       }
     }
   }
+
+  // Use reflection to abstract this incompatibility between Hadoop 1 & 2 APIs.
+  private TaskAttemptContext getTaskAttemptContext(Configuration conf, TaskAttemptID jobID) throws
+      ClassNotFoundException, NoSuchMethodException, IllegalAccessException,
+      InvocationTargetException, InstantiationException {
+    Class<? extends TaskAttemptContext> clazz = null;
+    if (!TaskAttemptContext.class.isInterface()) {
+      clazz = TaskAttemptContext.class;
+    } else {
+      clazz = (Class<? extends TaskAttemptContext>)
+          Class.forName("org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl");
+    }
+    return clazz.getConstructor(Configuration.class, TaskAttemptID.class)
+        .newInstance(conf, jobID);
+  }
 }
