diff --git a/db/derby/code/trunk/java/engine/org/apache/derby/iapi/sql/compile/Optimizer.java b/db/derby/code/trunk/java/engine/org/apache/derby/iapi/sql/compile/Optimizer.java
index eb1aca73..41fe722f 100644
--- a/db/derby/code/trunk/java/engine/org/apache/derby/iapi/sql/compile/Optimizer.java
+++ b/db/derby/code/trunk/java/engine/org/apache/derby/iapi/sql/compile/Optimizer.java
@@ -69,6 +69,12 @@ Property name for controlling the maximum size of memory (in KB)
 	 */
 	String MAX_MEMORY_PER_TABLE = "derby.language.maxMemoryPerTable";
 
+	/**
+		Maximum size of dynamically created materialized rows. Caching large results
+		use lot of memory and can cause stack overflow. See DERBY-634
+	*/
+	int MAX_DYNAMIC_MATERIALIZED_ROWS = 512;
+
 	/**
 	   Property name for disabling statistics use for all queries.
 	*/
diff --git a/db/derby/code/trunk/java/engine/org/apache/derby/impl/sql/execute/BaseActivation.java b/db/derby/code/trunk/java/engine/org/apache/derby/impl/sql/execute/BaseActivation.java
index 577b172e..0d67e081 100644
--- a/db/derby/code/trunk/java/engine/org/apache/derby/impl/sql/execute/BaseActivation.java
+++ b/db/derby/code/trunk/java/engine/org/apache/derby/impl/sql/execute/BaseActivation.java
@@ -1409,6 +1409,19 @@ protected void checkPositionedStatement(String cursorName, String psName)
 	 * in-memory converted resultset, or the original result set if not converted.
 	 * See beetle 4373 for details.
 	 *
+	 * Optimization implemented as part of Beetle: 4373 can cause severe stack overflow
+	 * problems. See JIRA entry DERBY-634. With default MAX_MEMORY_PER_TABLE of 1MG, it is
+	 * possible that this optimization could attempt to cache upto 250K rows as nested
+	 * union results. At runtime, this would cause stack overflow.
+	 *
+	 * As Jeff mentioned in DERBY-634, right way to optimize original problem would have been
+	 * to address subquery materialization during optimization phase, through hash joins.
+	 * Recent Army's optimizer work through DEBRY-781 and related work introduced a way to
+	 * materialize subquery results correctly and needs to be extended to cover this case.
+	 * While his optimization needs to be made more generic and stable, I propose to avoid
+	 * this regression by limiting size of the materialized resultset created here to be
+	 * less than MAX_MEMORY_PER_TABLE and MAX_DYNAMIC_MATERIALIZED_ROWS.
+	 *
 	 *	@param	rs	input result set
 	 *	@return	materialized resultset, or original rs if it can't be materialized
 	 */
@@ -1432,7 +1445,8 @@ public NoPutResultSet materializeResultSetIfPossible(NoPutResultSet rs)
 		while (aRow != null)
 		{
 			cacheSize += aRow.getColumn(1).getLength();
-			if (cacheSize > maxMemoryPerTable)
+			if (cacheSize > maxMemoryPerTable ||
+					rowCache.size() > Optimizer.MAX_DYNAMIC_MATERIALIZED_ROWS)
 				break;
 			rowCache.addElement(aRow.getClone(toClone));
 			aRow = rs.getNextRowCore();
