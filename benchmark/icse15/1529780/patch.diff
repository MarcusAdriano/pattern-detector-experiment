diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java
index e5b05c3c..8fc6c8ed 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java
@@ -34,6 +34,7 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefHash;
 import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.IntsRef;
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util.fst.ByteSequenceOutputs;
@@ -307,7 +308,9 @@ public Parser(boolean dedup, Analyzer analyzer) {
      *  separates by {@link SynonymMap#WORD_SEPARATOR}.
      *  reuse and its chars must not be null. */
     public CharsRef analyze(String text, CharsRef reuse) throws IOException {
+      IOException priorException = null;
       TokenStream ts = analyzer.tokenStream("", text);
+      try {
       CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
       PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);
       ts.reset();
@@ -330,7 +333,11 @@ public CharsRef analyze(String text, CharsRef reuse) throws IOException {
         reuse.length += length;
       }
       ts.end();
-      ts.close();
+      } catch (IOException e) {
+        priorException = e;
+      } finally {
+        IOUtils.closeWhileHandlingException(priorException, ts);
+      }
       if (reuse.length == 0) {
         throw new IllegalArgumentException("term: " + text + " was completely eliminated by analyzer");
       }
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
index 1b07d201..3733a7eb 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
@@ -37,6 +37,7 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util._TestUtil;
 
 public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
@@ -118,11 +119,17 @@ public void testMutipleDocument() throws Exception {
   // LUCENE-1441
   public void testOffsets() throws Exception {
     TokenStream stream = new KeywordAnalyzer().tokenStream("field", new StringReader("abcd"));
+    try {
     OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);
     stream.reset();
     assertTrue(stream.incrementToken());
     assertEquals(0, offsetAtt.startOffset());
     assertEquals(4, offsetAtt.endOffset());
+      assertFalse(stream.incrementToken());
+      stream.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(stream);
+    }
   }
   
   /** blast some random strings through the analyzer */
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
index 9f9c2714..7e6c5511 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
@@ -22,6 +22,7 @@
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
 
 import java.io.IOException;
@@ -47,6 +48,7 @@ public void setUp() throws Exception {
   public void testDefaults() throws IOException {
     assertTrue(stop != null);
     TokenStream stream = stop.tokenStream("test", "This is a test of the english stop analyzer");
+    try {
     assertTrue(stream != null);
     CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
     stream.reset();
@@ -54,12 +56,17 @@ public void testDefaults() throws IOException {
     while (stream.incrementToken()) {
       assertFalse(inValidTokens.contains(termAtt.toString()));
     }
+      stream.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(stream);
+    }
   }
 
   public void testStopList() throws IOException {
     CharArraySet stopWordsSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("good", "test", "analyzer"), false);
     StopAnalyzer newStop = new StopAnalyzer(Version.LUCENE_40, stopWordsSet);
     TokenStream stream = newStop.tokenStream("test", "This is a good test of the english stop analyzer");
+    try {
     assertNotNull(stream);
     CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
     
@@ -68,6 +75,10 @@ public void testStopList() throws IOException {
       String text = termAtt.toString();
       assertFalse(stopWordsSet.contains(text));
     }
+      stream.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(stream);
+    }
   }
 
   public void testStopListPositions() throws IOException {
@@ -76,6 +87,7 @@ public void testStopListPositions() throws IOException {
     String s =             "This is a good test of the english stop analyzer with positions";
     int expectedIncr[] =  { 1,   1, 1,          3, 1,  1,      1,            2,   1};
     TokenStream stream = newStop.tokenStream("test", s);
+    try {
     assertNotNull(stream);
     int i = 0;
     CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
@@ -87,6 +99,10 @@ public void testStopListPositions() throws IOException {
       assertFalse(stopWordsSet.contains(text));
       assertEquals(expectedIncr[i++],posIncrAtt.getPositionIncrement());
     }
+      stream.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(stream);
+    }
   }
 
 }
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java
index ce0cd745..8db95bf3 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java
@@ -9,6 +9,7 @@
 import org.apache.lucene.analysis.core.SimpleAnalyzer;
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.IOUtils;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -38,6 +39,7 @@ public void testPerField() throws Exception {
               new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), analyzerPerField);
 
     TokenStream tokenStream = analyzer.tokenStream("field", text);
+    try {
     CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);
     tokenStream.reset();
 
@@ -45,15 +47,26 @@ public void testPerField() throws Exception {
     assertEquals("WhitespaceAnalyzer does not lowercase",
                  "Qwerty",
                  termAtt.toString());
+      assertFalse(tokenStream.incrementToken());
+      tokenStream.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(tokenStream);
+    }
 
     tokenStream = analyzer.tokenStream("special", text);
-    termAtt = tokenStream.getAttribute(CharTermAttribute.class);
+    try {
+      CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);
     tokenStream.reset();
 
     assertTrue(tokenStream.incrementToken());
     assertEquals("SimpleAnalyzer lowercases",
                  "qwerty",
                  termAtt.toString());
+      assertFalse(tokenStream.incrementToken());
+      tokenStream.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(tokenStream);
+    }
   }
   
   public void testCharFilters() throws Exception {
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
index 855a14e3..7f645020 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
@@ -34,6 +34,7 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.*;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
 
 /**
  * A test class for ShingleAnalyzerWrapper as regards queries and scoring.
@@ -96,6 +97,7 @@ public void testShingleAnalyzerWrapperPhraseQuery() throws Exception {
     PhraseQuery q = new PhraseQuery();
 
     TokenStream ts = analyzer.tokenStream("content", "this sentence");
+    try {
     int j = -1;
     
     PositionIncrementAttribute posIncrAtt = ts.addAttribute(PositionIncrementAttribute.class);
@@ -107,6 +109,10 @@ public void testShingleAnalyzerWrapperPhraseQuery() throws Exception {
       String termText = termAtt.toString();
       q.add(new Term("content", termText), j);
     }
+      ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
 
     ScoreDoc[] hits = searcher.search(q, null, 1000).scoreDocs;
     int[] ranks = new int[] { 0 };
@@ -122,16 +128,19 @@ public void testShingleAnalyzerWrapperBooleanQuery() throws Exception {
     BooleanQuery q = new BooleanQuery();
 
     TokenStream ts = analyzer.tokenStream("content", "test sentence");
-    
+    try {
     CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
     
     ts.reset();
-
     while (ts.incrementToken()) {
       String termText =  termAtt.toString();
       q.add(new TermQuery(new Term("content", termText)),
             BooleanClause.Occur.SHOULD);
     }
+      ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
 
     ScoreDoc[] hits = searcher.search(q, null, 1000).scoreDocs;
     int[] ranks = new int[] { 1, 2, 0 };
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java
index e8880dfe..91d69bdc 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java
@@ -29,6 +29,7 @@
 import org.apache.lucene.analysis.core.LetterTokenizer;
 import org.apache.lucene.analysis.core.LowerCaseTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util._TestUtil;
 
 
@@ -124,6 +125,7 @@ protected int normalize(int c) {
     for (int i = 0; i < num; i++) {
       String s = _TestUtil.randomUnicodeString(random());
       TokenStream ts = analyzer.tokenStream("foo", s);
+      try {
       ts.reset();
       OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
       while (ts.incrementToken()) {
@@ -134,7 +136,9 @@ protected int normalize(int c) {
         }
       }
       ts.end();
-      ts.close();
+      } finally {
+        IOUtils.closeWhileHandlingException(ts);
+      }
     }
     // just for fun
     checkRandomData(random(), analyzer, num);
@@ -162,6 +166,7 @@ protected int normalize(int c) {
     for (int i = 0; i < num; i++) {
       String s = _TestUtil.randomUnicodeString(random());
       TokenStream ts = analyzer.tokenStream("foo", s);
+      try {
       ts.reset();
       OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
       while (ts.incrementToken()) {
@@ -172,7 +177,9 @@ protected int normalize(int c) {
         }
       }
       ts.end();
-      ts.close();
+      } finally {
+        IOUtils.closeWhileHandlingException(ts);
+      }
     }
     // just for fun
     checkRandomData(random(), analyzer, num);
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java b/lucene/dev/branches/branch_4x/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
index f6649f3b..12f1fe99 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
@@ -24,6 +24,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.icu.ICUNormalizer2Filter;
 import org.apache.lucene.analysis.icu.tokenattributes.ScriptAttribute;
+import org.apache.lucene.util.IOUtils;
 
 import com.ibm.icu.lang.UScript;
 
@@ -250,6 +251,7 @@ public void testRandomHugeStrings() throws Exception {
   
   public void testTokenAttributes() throws Exception {
     TokenStream ts = a.tokenStream("dummy", "This is a test");
+    try {
     ScriptAttribute scriptAtt = ts.addAttribute(ScriptAttribute.class);
     ts.reset();
     while (ts.incrementToken()) {
@@ -259,6 +261,8 @@ public void testTokenAttributes() throws Exception {
       assertTrue(ts.reflectAsString(false).contains("script=Latin"));
     }
     ts.end();
-    ts.close();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 }
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestExtendedMode.java b/lucene/dev/branches/branch_4x/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestExtendedMode.java
index 250f26e2..47c1611a 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestExtendedMode.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestExtendedMode.java
@@ -27,6 +27,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.ja.JapaneseTokenizer.Mode;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.util.LuceneTestCase.Slow;
@@ -54,13 +55,16 @@ public void testSurrogates2() throws IOException {
     for (int i = 0; i < numIterations; i++) {
       String s = _TestUtil.randomUnicodeString(random(), 100);
       TokenStream ts = analyzer.tokenStream("foo", s);
+      try {
       CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
       ts.reset();
       while (ts.incrementToken()) {
         assertTrue(UnicodeUtil.validUTF16String(termAtt));
       }
       ts.end();
-      ts.close();
+      } finally {
+        IOUtils.closeWhileHandlingException(ts);
+      }
     }
   }
   
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java b/lucene/dev/branches/branch_4x/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
index 019e9ddd..15ebc8ea 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
@@ -143,12 +143,15 @@ public void testDecomposition4() throws Exception {
    */
   public void testDecomposition5() throws Exception {
     TokenStream ts = analyzer.tokenStream("bogus", "くよくよくよくよくよくよくよくよくよくよくよくよくよくよくよくよくよくよくよくよ");
+    try {
     ts.reset();
     while (ts.incrementToken()) {
       
     }
     ts.end();
-    ts.close();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   /*
@@ -215,11 +218,14 @@ public void testLargeDocReliability() throws Exception {
     for (int i = 0; i < 100; i++) {
       String s = _TestUtil.randomUnicodeString(random(), 10000);
       TokenStream ts = analyzer.tokenStream("foo", s);
+      try {
       ts.reset();
       while (ts.incrementToken()) {
       }
       ts.end();
-      ts.close();
+      } finally {
+        IOUtils.closeWhileHandlingException(ts);
+      }
     }
   }
   
@@ -238,28 +244,39 @@ public void testSurrogates2() throws IOException {
       }
       String s = _TestUtil.randomUnicodeString(random(), 100);
       TokenStream ts = analyzer.tokenStream("foo", s);
+      try {
       CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
       ts.reset();
       while (ts.incrementToken()) {
         assertTrue(UnicodeUtil.validUTF16String(termAtt));
       }
       ts.end();
-      ts.close();
+      } finally {
+        IOUtils.closeWhileHandlingException(ts);
+      }
     }
   }
 
   public void testOnlyPunctuation() throws IOException {
     TokenStream ts = analyzerNoPunct.tokenStream("foo", "。、。。");
+    try {
     ts.reset();
     assertFalse(ts.incrementToken());
     ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   public void testOnlyPunctuationExtended() throws IOException {
     TokenStream ts = extendedModeAnalyzerNoPunct.tokenStream("foo", "......");
+    try {
     ts.reset();
     assertFalse(ts.incrementToken());
     ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
   
   // note: test is kinda silly since kuromoji emits punctuation tokens.
@@ -371,6 +388,7 @@ protected TokenStreamComponents createComponents(String fieldName, Reader reader
 
   private void assertReadings(String input, String... readings) throws IOException {
     TokenStream ts = analyzer.tokenStream("ignored", input);
+    try {
     ReadingAttribute readingAtt = ts.addAttribute(ReadingAttribute.class);
     ts.reset();
     for(String reading : readings) {
@@ -379,10 +397,14 @@ private void assertReadings(String input, String... readings) throws IOException
     }
     assertFalse(ts.incrementToken());
     ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   private void assertPronunciations(String input, String... pronunciations) throws IOException {
     TokenStream ts = analyzer.tokenStream("ignored", input);
+    try {
     ReadingAttribute readingAtt = ts.addAttribute(ReadingAttribute.class);
     ts.reset();
     for(String pronunciation : pronunciations) {
@@ -391,10 +413,14 @@ private void assertPronunciations(String input, String... pronunciations) throws
     }
     assertFalse(ts.incrementToken());
     ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
   
   private void assertBaseForms(String input, String... baseForms) throws IOException {
     TokenStream ts = analyzer.tokenStream("ignored", input);
+    try {
     BaseFormAttribute baseFormAtt = ts.addAttribute(BaseFormAttribute.class);
     ts.reset();
     for(String baseForm : baseForms) {
@@ -403,10 +429,14 @@ private void assertBaseForms(String input, String... baseForms) throws IOExcepti
     }
     assertFalse(ts.incrementToken());
     ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   private void assertInflectionTypes(String input, String... inflectionTypes) throws IOException {
     TokenStream ts = analyzer.tokenStream("ignored", input);
+    try {
     InflectionAttribute inflectionAtt = ts.addAttribute(InflectionAttribute.class);
     ts.reset();
     for(String inflectionType : inflectionTypes) {
@@ -415,10 +445,14 @@ private void assertInflectionTypes(String input, String... inflectionTypes) thro
     }
     assertFalse(ts.incrementToken());
     ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   private void assertInflectionForms(String input, String... inflectionForms) throws IOException {
     TokenStream ts = analyzer.tokenStream("ignored", input);
+    try {
     InflectionAttribute inflectionAtt = ts.addAttribute(InflectionAttribute.class);
     ts.reset();
     for(String inflectionForm : inflectionForms) {
@@ -427,10 +461,14 @@ private void assertInflectionForms(String input, String... inflectionForms) thro
     }
     assertFalse(ts.incrementToken());
     ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
   
   private void assertPartsOfSpeech(String input, String... partsOfSpeech) throws IOException {
     TokenStream ts = analyzer.tokenStream("ignored", input);
+    try {
     PartOfSpeechAttribute partOfSpeechAtt = ts.addAttribute(PartOfSpeechAttribute.class);
     ts.reset();
     for(String partOfSpeech : partsOfSpeech) {
@@ -439,6 +477,9 @@ private void assertPartsOfSpeech(String input, String... partsOfSpeech) throws I
     }
     assertFalse(ts.incrementToken());
     ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
   
   public void testReadings() throws Exception {
@@ -632,11 +673,14 @@ private void doTestBocchan(int numIterations) throws Exception {
 
     long totalStart = System.currentTimeMillis();
     for (int i = 0; i < numIterations; i++) {
-      final TokenStream ts = analyzer.tokenStream("ignored", line);
+      TokenStream ts = analyzer.tokenStream("ignored", line);
+      try {
       ts.reset();
       while(ts.incrementToken());
       ts.end();
-      ts.close();
+      } finally {
+        IOUtils.closeWhileHandlingException(ts);
+      }
     }
     String[] sentences = line.split("、|。");
     if (VERBOSE) {
@@ -646,11 +690,14 @@ private void doTestBocchan(int numIterations) throws Exception {
     totalStart = System.currentTimeMillis();
     for (int i = 0; i < numIterations; i++) {
       for (String sentence: sentences) {
-        final TokenStream ts = analyzer.tokenStream("ignored", sentence);
+        TokenStream ts = analyzer.tokenStream("ignored", sentence);
+        try {
         ts.reset();
         while(ts.incrementToken());
         ts.end();
-        ts.close();
+        } finally {
+          IOUtils.closeWhileHandlingException(ts);
+        }
       }
     }
     if (VERBOSE) {
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java b/lucene/dev/branches/branch_4x/lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java
index 366355b7..97b053fc 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java
@@ -30,6 +30,7 @@
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
 
 /**
@@ -73,6 +74,7 @@ public final void testMultipleTokens() throws IOException {
   @SuppressWarnings("unused")
   private void dumpTokens(String input) throws IOException {
     TokenStream ts = getTestAnalyzer().tokenStream("dummy", input);
+    try {
     ts.reset();
 
     MorphosyntacticTagsAttribute attribute = ts.getAttribute(MorphosyntacticTagsAttribute.class);
@@ -80,26 +82,36 @@ private void dumpTokens(String input) throws IOException {
     while (ts.incrementToken()) {
       System.out.println(charTerm.toString() + " => " + attribute.getTags());
     }
+      ts.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   /** Test reuse of MorfologikFilter with leftover stems. */
   public final void testLeftoverStems() throws IOException {
     Analyzer a = getTestAnalyzer();
     TokenStream ts_1 = a.tokenStream("dummy", "liście");
+    try {
     CharTermAttribute termAtt_1 = ts_1.getAttribute(CharTermAttribute.class);
     ts_1.reset();
     ts_1.incrementToken();
     assertEquals("first stream", "liście", termAtt_1.toString());
     ts_1.end();
-    ts_1.close();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts_1);
+    }
 
     TokenStream ts_2 = a.tokenStream("dummy", "danych");
+    try {
     CharTermAttribute termAtt_2 = ts_2.getAttribute(CharTermAttribute.class);
     ts_2.reset();
     ts_2.incrementToken();
     assertEquals("second stream", "dany", termAtt_2.toString());
     ts_2.end();
-    ts_2.close();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts_2);
+    }
   }
 
   /** Test stemming of mixed-case tokens. */
@@ -141,7 +153,7 @@ private void assertPOSToken(TokenStream ts, String term, String... tags) throws
   /** Test morphosyntactic annotations. */
   public final void testPOSAttribute() throws IOException {
     TokenStream ts = getTestAnalyzer().tokenStream("dummy", "liście");
-
+    try {
     ts.reset();
     assertPOSToken(ts, "liście",  
         "subst:sg:acc:n2",
@@ -161,7 +173,9 @@ public final void testPOSAttribute() throws IOException {
         "subst:sg:dat:f",
         "subst:sg:loc:f");
     ts.end();
-    ts.close();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   /** */
diff --git a/lucene/dev/branches/branch_4x/lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java b/lucene/dev/branches/branch_4x/lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java
index 1cb8e9b6..8c5e3d46 100644
--- a/lucene/dev/branches/branch_4x/lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java
+++ b/lucene/dev/branches/branch_4x/lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java
@@ -29,6 +29,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
 
 public class TestSmartChineseAnalyzer extends BaseTokenStreamTestCase {
@@ -185,9 +186,14 @@ public void testLargeDocument() throws Exception {
     }
     Analyzer analyzer = new SmartChineseAnalyzer(TEST_VERSION_CURRENT);
     TokenStream stream = analyzer.tokenStream("", sb.toString());
+    try {
     stream.reset();
     while (stream.incrementToken()) {
     }
+      stream.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(stream);
+    }
   }
   
   // LUCENE-3026
@@ -198,9 +204,14 @@ public void testLargeSentence() throws Exception {
     }
     Analyzer analyzer = new SmartChineseAnalyzer(TEST_VERSION_CURRENT);
     TokenStream stream = analyzer.tokenStream("", sb.toString());
+    try {
     stream.reset();
     while (stream.incrementToken()) {
     }
+      stream.end();
+    } finally {
+      IOUtils.closeWhileHandlingException(stream);
+    }
   }
   
   // LUCENE-3642
diff --git a/lucene/dev/branches/branch_4x/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java b/lucene/dev/branches/branch_4x/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java
index 2fe5c832..b5629cbb 100644
--- a/lucene/dev/branches/branch_4x/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java
+++ b/lucene/dev/branches/branch_4x/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java
@@ -31,6 +31,7 @@
 import org.apache.lucene.search.TotalHitCountCollector;
 import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 
 import java.io.IOException;
 import java.util.Collection;
@@ -86,13 +87,16 @@ private int countDocsWithClass() throws IOException {
   private String[] tokenizeDoc(String doc) throws IOException {
     Collection<String> result = new LinkedList<String>();
     TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc);
+    try {
     CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);
     tokenStream.reset();
     while (tokenStream.incrementToken()) {
       result.add(charTermAttribute.toString());
     }
     tokenStream.end();
-    tokenStream.close();
+    } finally {
+      IOUtils.closeWhileHandlingException(tokenStream);
+    }
     return result.toArray(new String[result.size()]);
   }
 
diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
index 3cf14079..8fc17c75 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
@@ -6,6 +6,7 @@
 import java.util.Arrays;
 import java.util.Random;
 
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.util.automaton.Automaton;
 import org.apache.lucene.util.automaton.BasicAutomata;
@@ -99,13 +100,19 @@ public void testLUCENE_3042() throws Exception {
     String testString = "t";
     
     Analyzer analyzer = new MockAnalyzer(random());
+    Exception priorException = null;
     TokenStream stream = analyzer.tokenStream("dummy", testString);
+    try {
     stream.reset();
     while (stream.incrementToken()) {
       // consume
     }
     stream.end();
-    stream.close();
+    } catch (Exception e) {
+      priorException = e;
+    } finally {
+      IOUtils.closeWhileHandlingException(priorException, stream);
+    }
     
     assertAnalyzesTo(analyzer, testString, new String[] { "t" });
   }
@@ -122,13 +129,19 @@ public void testForwardOffsets() throws Exception {
       StringReader reader = new StringReader(s);
       MockCharFilter charfilter = new MockCharFilter(reader, 2);
       MockAnalyzer analyzer = new MockAnalyzer(random());
+      Exception priorException = null;
       TokenStream ts = analyzer.tokenStream("bogus", charfilter);
+      try {
       ts.reset();
       while (ts.incrementToken()) {
         ;
       }
       ts.end();
-      ts.close();
+      } catch (Exception e) {
+        priorException = e;
+      } finally {
+        IOUtils.closeWhileHandlingException(priorException, ts);
+      }
     }
   }
   
diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
index 34f0a590..78637215 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
@@ -30,6 +30,7 @@
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.FixedBitSet;
@@ -47,7 +48,9 @@ private String getRandomTerm(String other) throws IOException {
       if (other != null && s.equals(other)) {
         continue;
       }
-      final TokenStream ts = a.tokenStream("foo", s);
+      IOException priorException = null;
+      TokenStream ts = a.tokenStream("foo", s);
+      try {
       final TermToBytesRefAttribute termAtt = ts.getAttribute(TermToBytesRefAttribute.class);
       final BytesRef termBytes = termAtt.getBytesRef();
       ts.reset();
@@ -66,12 +69,15 @@ private String getRandomTerm(String other) throws IOException {
       }
 
       ts.end();
-      ts.close();
-
       // Did we iterate just once and the value was unchanged?
       if (!changed && count == 1) {
         return s;
       }
+      } catch (IOException e) {
+        priorException = e;
+      } finally {
+        IOUtils.closeWhileHandlingException(priorException, ts);
+      }
     }
   }
 
diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
index 37589097..f8aecd31 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
@@ -34,6 +34,7 @@
 import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 
 /** tests for writing term vectors */
@@ -174,17 +175,24 @@ public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));
     Document doc = new Document();
+    IOException priorException = null;
     TokenStream stream = analyzer.tokenStream("field", "abcd   ");
+    try {
     stream.reset(); // TODO: weird to reset before wrapping with CachingTokenFilter... correct?
-    stream = new CachingTokenFilter(stream);
+      TokenStream cachedStream = new CachingTokenFilter(stream);
     FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);
     customType.setStoreTermVectors(true);
     customType.setStoreTermVectorPositions(true);
     customType.setStoreTermVectorOffsets(true);
-    Field f = new Field("field", stream, customType);
+      Field f = new Field("field", cachedStream, customType);
     doc.add(f);
     doc.add(f);
     w.addDocument(doc);
+    } catch (IOException e) {
+      priorException = e;
+    } finally {
+      IOUtils.closeWhileHandlingException(priorException, stream);
+    }
     w.close();
 
     IndexReader r = DirectoryReader.open(dir);
diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery.java b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery.java
index 08db8d67..85797772 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery.java
@@ -624,7 +624,9 @@ public void testRandomPhrases() throws Exception {
               break;
             }
           }
+          IOException priorException = null;
           TokenStream ts = analyzer.tokenStream("ignore", term);
+          try {
           CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);
           ts.reset();
           while(ts.incrementToken()) {
@@ -633,7 +635,11 @@ public void testRandomPhrases() throws Exception {
             sb.append(text).append(' ');
           }
           ts.end();
-          ts.close();
+          } catch (IOException e) {
+            priorException = e;
+          } finally {
+            IOUtils.closeWhileHandlingException(priorException, ts);
+          }
         } else {
           // pick existing sub-phrase
           List<String> lastDoc = docs.get(r.nextInt(docs.size()));
diff --git a/lucene/dev/branches/branch_4x/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java b/lucene/dev/branches/branch_4x/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
index 0a27cd06..1c66a243 100644
--- a/lucene/dev/branches/branch_4x/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
+++ b/lucene/dev/branches/branch_4x/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
@@ -45,6 +45,7 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 
 public abstract class AbstractTestCase extends LuceneTestCase {
@@ -173,6 +174,7 @@ protected void assertCollectionQueries( Collection<Query> actual, Query... expec
     List<BytesRef> bytesRefs = new ArrayList<BytesRef>();
 
     TokenStream tokenStream = analyzer.tokenStream(field, text);
+    try {
     TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);
 
     BytesRef bytesRef = termAttribute.getBytesRef();
@@ -185,7 +187,9 @@ protected void assertCollectionQueries( Collection<Query> actual, Query... expec
     }
 
     tokenStream.end();
-    tokenStream.close();
+    } finally {
+      IOUtils.closeWhileHandlingException(tokenStream);
+    }
 
     return bytesRefs;
   }
diff --git a/lucene/dev/branches/branch_4x/lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java b/lucene/dev/branches/branch_4x/lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
index 99d79716..04732a15 100644
--- a/lucene/dev/branches/branch_4x/lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
+++ b/lucene/dev/branches/branch_4x/lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
@@ -35,6 +35,7 @@
 import org.apache.lucene.search.similarities.TFIDFSimilarity;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.PriorityQueue;
 import org.apache.lucene.util.UnicodeUtil;
 
@@ -777,6 +778,7 @@ private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String f
           "term vectors, you must provide an Analyzer");
     }
     TokenStream ts = analyzer.tokenStream(fieldName, r);
+    try {
     int tokenCount = 0;
     // for every token
     CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
@@ -800,7 +802,9 @@ private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String f
       }
     }
     ts.end();
-    ts.close();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
 
diff --git a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java
index 3ce2ae34..a5f5abfe 100644
--- a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java
+++ b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java
@@ -26,6 +26,7 @@
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
 
 /**
@@ -163,7 +164,7 @@ protected Query getFuzzyQuery(String field, String termStr, float minSimilarity)
   protected String analyzeSingleChunk(String field, String termStr, String chunk) throws ParseException{
     String analyzed = null;
     TokenStream stream = null;
-    try{
+    try {
       stream = getAnalyzer().tokenStream(field, chunk);
       stream.reset();
       CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
@@ -186,7 +187,6 @@ protected String analyzeSingleChunk(String field, String termStr, String chunk)
           multipleOutputs.append('"');
         }
         stream.end();
-        stream.close();
         if (null != multipleOutputs) {
           throw new ParseException(
               String.format(getLocale(),
@@ -196,12 +196,13 @@ protected String analyzeSingleChunk(String field, String termStr, String chunk)
         // nothing returned by analyzer.  Was it a stop word and the user accidentally
         // used an analyzer with stop words?
         stream.end();
-        stream.close();
         throw new ParseException(String.format(getLocale(), "Analyzer returned nothing for \"%s\"", chunk));
       }
     } catch (IOException e){
       throw new ParseException(
           String.format(getLocale(), "IO error while trying to analyze single term: \"%s\"", termStr));
+    } finally {
+      IOUtils.closeWhileHandlingException(stream);
     }
     return analyzed;
   }
diff --git a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
index 10da05a4..e47e2783 100644
--- a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
+++ b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
@@ -34,6 +34,7 @@
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.BooleanQuery.TooManyClauses;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
 
 /** This class is overridden by QueryParser in QueryParser.jj
@@ -501,21 +502,19 @@ protected Query getFieldQuery(String field, String queryText, boolean quoted) th
   protected Query newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)  throws ParseException {
     // Use the analyzer to get all the tokens, and then build a TermQuery,
     // PhraseQuery, or nothing based on the term count
-
-    TokenStream source;
-    try {
-      source = analyzer.tokenStream(field, queryText);
-      source.reset();
-    } catch (IOException e) {
-      ParseException p = new ParseException("Unable to initialize TokenStream to analyze query text");
-      p.initCause(e);
-      throw p;
-    }
-    CachingTokenFilter buffer = new CachingTokenFilter(source);
+    CachingTokenFilter buffer = null;
     TermToBytesRefAttribute termAtt = null;
     PositionIncrementAttribute posIncrAtt = null;
     int numTokens = 0;
+    int positionCount = 0;
+    boolean severalTokensAtSamePosition = false;
+    boolean hasMoreTokens = false;    
 
+    TokenStream source = null;
+    try {
+      source = analyzer.tokenStream(field, queryText);
+      source.reset();
+      buffer = new CachingTokenFilter(source);
     buffer.reset();
 
     if (buffer.hasAttribute(TermToBytesRefAttribute.class)) {
@@ -525,10 +524,6 @@ protected Query newFieldQuery(Analyzer analyzer, String field, String queryText,
       posIncrAtt = buffer.getAttribute(PositionIncrementAttribute.class);
     }
 
-    int positionCount = 0;
-    boolean severalTokensAtSamePosition = false;
-
-    boolean hasMoreTokens = false;
     if (termAtt != null) {
       try {
         hasMoreTokens = buffer.incrementToken();
@@ -546,19 +541,17 @@ protected Query newFieldQuery(Analyzer analyzer, String field, String queryText,
         // ignore
       }
     }
-    try {
-      // rewind the buffer stream
-      buffer.reset();
-
-      // close original stream - all tokens buffered
-      source.close();
-    }
-    catch (IOException e) {
-      ParseException p = new ParseException("Cannot close TokenStream analyzing query text");
+    } catch (IOException e) {
+      ParseException p = new ParseException("Eror analyzing query text");
       p.initCause(e);
       throw p;
+    } finally {
+      IOUtils.closeWhileHandlingException(source);
     }
 
+    // rewind the buffer stream
+    buffer.reset();
+
     BytesRef bytes = termAtt == null ? null : termAtt.getBytesRef();
 
     if (numTokens == 0)
@@ -843,38 +836,28 @@ private BytesRef analyzeMultitermTerm(String field, String part) {
   }
 
   protected BytesRef analyzeMultitermTerm(String field, String part, Analyzer analyzerIn) {
-    TokenStream source;
-
     if (analyzerIn == null) analyzerIn = analyzer;
 
+    TokenStream source = null;
     try {
       source = analyzerIn.tokenStream(field, part);
       source.reset();
-    } catch (IOException e) {
-      throw new RuntimeException("Unable to initialize TokenStream to analyze multiTerm term: " + part, e);
-    }
       
     TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
     BytesRef bytes = termAtt.getBytesRef();
 
-    try {
       if (!source.incrementToken())
         throw new IllegalArgumentException("analyzer returned no terms for multiTerm term: " + part);
       termAtt.fillBytesRef();
       if (source.incrementToken())
         throw new IllegalArgumentException("analyzer returned too many terms for multiTerm term: " + part);
-    } catch (IOException e) {
-      throw new RuntimeException("error analyzing range part: " + part, e);
-    }
-      
-    try {
       source.end();
-      source.close();
+      return BytesRef.deepCopyOf(bytes);
     } catch (IOException e) {
-      throw new RuntimeException("Unable to end & close TokenStream after analyzing multiTerm term: " + part, e);
+      throw new RuntimeException("Error analyzing multiTerm term: " + part, e);
+    } finally {
+      IOUtils.closeWhileHandlingException(source);
     }
-    
-    return BytesRef.deepCopyOf(bytes);
   }
 
   /**
diff --git a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
index 3190990f..5f4b6f52 100644
--- a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
+++ b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
@@ -45,6 +45,7 @@
 import org.apache.lucene.queryparser.flexible.standard.nodes.RegexpQueryNode;
 import org.apache.lucene.queryparser.flexible.standard.nodes.StandardBooleanQueryNode;
 import org.apache.lucene.queryparser.flexible.standard.nodes.WildcardQueryNode;
+import org.apache.lucene.util.IOUtils;
 
 /**
  * This processor verifies if {@link ConfigurationKeys#ANALYZER}
@@ -114,20 +115,18 @@ protected QueryNode postProcessNode(QueryNode node) throws QueryNodeException {
       String text = fieldNode.getTextAsString();
       String field = fieldNode.getFieldAsString();
 
-      TokenStream source;
-      try {
-        source = this.analyzer.tokenStream(field, text);
-        source.reset();
-      } catch (IOException e1) {
-        throw new RuntimeException(e1);
-      }
-      CachingTokenFilter buffer = new CachingTokenFilter(source);
-
+      CachingTokenFilter buffer = null;
       PositionIncrementAttribute posIncrAtt = null;
       int numTokens = 0;
       int positionCount = 0;
       boolean severalTokensAtSamePosition = false;
 
+      TokenStream source = null;
+      try {
+        source = this.analyzer.tokenStream(field, text);
+        source.reset();
+        buffer = new CachingTokenFilter(source);
+
       if (buffer.hasAttribute(PositionIncrementAttribute.class)) {
         posIncrAtt = buffer.getAttribute(PositionIncrementAttribute.class);
       }
@@ -150,17 +149,15 @@ protected QueryNode postProcessNode(QueryNode node) throws QueryNodeException {
       } catch (IOException e) {
         // ignore
       }
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      } finally {
+        IOUtils.closeWhileHandlingException(source);
+      }
 
-      try {
         // rewind the buffer stream
         buffer.reset();
 
-        // close original stream - all tokens buffered
-        source.close();
-      } catch (IOException e) {
-        // ignore
-      }
-
       if (!buffer.hasAttribute(CharTermAttribute.class)) {
         return new NoTokenFoundQueryNode();
       }
diff --git a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java
index 56cc66ef..91a7353a 100644
--- a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java
+++ b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java
@@ -13,6 +13,7 @@
 import org.apache.lucene.queries.mlt.MoreLikeThisQuery;
 import org.apache.lucene.queryparser.xml.QueryBuilder;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.queryparser.xml.DOMUtils;
 import org.apache.lucene.queryparser.xml.ParserException;
 import org.w3c.dom.Element;
@@ -73,18 +74,20 @@ public Query getQuery(Element e) throws ParserException {
     if ((stopWords != null) && (fields != null)) {
       stopWordsSet = new HashSet<String>();
       for (String field : fields) {
+        TokenStream ts = null;
         try {
-          TokenStream ts = analyzer.tokenStream(field, stopWords);
+          ts = analyzer.tokenStream(field, stopWords);
           CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
           ts.reset();
           while (ts.incrementToken()) {
             stopWordsSet.add(termAtt.toString());
           }
           ts.end();
-          ts.close();
         } catch (IOException ioe) {
           throw new ParserException("IoException parsing stop words list in "
               + getClass().getName() + ":" + ioe.getLocalizedMessage());
+        } finally {
+          IOUtils.closeWhileHandlingException(ts);
         }
       }
     }
diff --git a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrTermsBuilder.java b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrTermsBuilder.java
index ecda3113..82f8d903 100644
--- a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrTermsBuilder.java
+++ b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrTermsBuilder.java
@@ -8,6 +8,7 @@
 import org.apache.lucene.search.spans.SpanQuery;
 import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.queryparser.xml.DOMUtils;
 import org.apache.lucene.queryparser.xml.ParserException;
 import org.w3c.dom.Element;
@@ -49,9 +50,11 @@ public SpanQuery getSpanQuery(Element e) throws ParserException {
     String fieldName = DOMUtils.getAttributeWithInheritanceOrFail(e, "fieldName");
     String value = DOMUtils.getNonBlankTextOrFail(e);
 
-    try {
       List<SpanQuery> clausesList = new ArrayList<SpanQuery>();
-      TokenStream ts = analyzer.tokenStream(fieldName, value);
+
+    TokenStream ts = null;
+    try {
+      ts = analyzer.tokenStream(fieldName, value);
       TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
       BytesRef bytes = termAtt.getBytesRef();
       ts.reset();
@@ -61,13 +64,14 @@ public SpanQuery getSpanQuery(Element e) throws ParserException {
         clausesList.add(stq);
       }
       ts.end();
-      ts.close();
       SpanOrQuery soq = new SpanOrQuery(clausesList.toArray(new SpanQuery[clausesList.size()]));
       soq.setBoost(DOMUtils.getAttribute(e, "boost", 1.0f));
       return soq;
     }
     catch (IOException ioe) {
       throw new ParserException("IOException parsing value:" + value);
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
     }
   }
 
diff --git a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsFilterBuilder.java b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsFilterBuilder.java
index 65b13014..c3fddf61 100644
--- a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsFilterBuilder.java
+++ b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsFilterBuilder.java
@@ -6,6 +6,7 @@
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.queries.TermsFilter;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.queryparser.xml.DOMUtils;
 import org.apache.lucene.queryparser.xml.FilterBuilder;
 import org.apache.lucene.queryparser.xml.ParserException;
@@ -54,8 +55,9 @@ public Filter getFilter(Element e) throws ParserException {
     String text = DOMUtils.getNonBlankTextOrFail(e);
     String fieldName = DOMUtils.getAttributeWithInheritanceOrFail(e, "fieldName");
 
+    TokenStream ts = null;
     try {
-      TokenStream ts = analyzer.tokenStream(fieldName, text);
+      ts = analyzer.tokenStream(fieldName, text);
       TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
       BytesRef bytes = termAtt.getBytesRef();
       ts.reset();
@@ -64,10 +66,11 @@ public Filter getFilter(Element e) throws ParserException {
         terms.add(BytesRef.deepCopyOf(bytes));
       }
       ts.end();
-      ts.close();
     }
     catch (IOException ioe) {
       throw new RuntimeException("Error constructing terms from index:" + ioe);
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
     }
     return new TermsFilter(fieldName, terms);
   }
diff --git a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsQueryBuilder.java b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsQueryBuilder.java
index ed06d091..efec7360 100644
--- a/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsQueryBuilder.java
+++ b/lucene/dev/branches/branch_4x/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsQueryBuilder.java
@@ -9,6 +9,7 @@
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.queryparser.xml.DOMUtils;
 import org.apache.lucene.queryparser.xml.ParserException;
 import org.apache.lucene.queryparser.xml.QueryBuilder;
@@ -51,8 +52,9 @@ public Query getQuery(Element e) throws ParserException {
 
     BooleanQuery bq = new BooleanQuery(DOMUtils.getAttribute(e, "disableCoord", false));
     bq.setMinimumNumberShouldMatch(DOMUtils.getAttribute(e, "minimumNumberShouldMatch", 0));
+    TokenStream ts = null;
     try {
-      TokenStream ts = analyzer.tokenStream(fieldName, text);
+      ts = analyzer.tokenStream(fieldName, text);
       TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
       Term term = null;
       BytesRef bytes = termAtt.getBytesRef();
@@ -63,10 +65,11 @@ public Query getQuery(Element e) throws ParserException {
         bq.add(new BooleanClause(new TermQuery(term), BooleanClause.Occur.SHOULD));
       }
       ts.end();
-      ts.close();
     }
     catch (IOException ioe) {
       throw new RuntimeException("Error constructing terms from index:" + ioe);
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
     }
 
     bq.setBoost(DOMUtils.getAttribute(e, "boost", 1.0f));
diff --git a/lucene/dev/branches/branch_4x/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java b/lucene/dev/branches/branch_4x/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
index bbc65281..46409a8d 100644
--- a/lucene/dev/branches/branch_4x/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
+++ b/lucene/dev/branches/branch_4x/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
@@ -36,6 +36,7 @@
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.PriorityQueue;
 
 /**
@@ -194,16 +195,17 @@ public void addTerms(String queryString, String fieldName,float minSimilarity, i
 
   private void addTerms(IndexReader reader, FieldVals f) throws IOException {
     if (f.queryString == null) return;
+    final Terms terms = MultiFields.getTerms(reader, f.fieldName);
+    if (terms == null) {
+      return;
+    }
     TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString);
+    try {
     CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
 
     int corpusNumDocs = reader.numDocs();
     HashSet<String> processedTerms = new HashSet<String>();
     ts.reset();
-    final Terms terms = MultiFields.getTerms(reader, f.fieldName);
-    if (terms == null) {
-      return;
-    }
     while (ts.incrementToken()) {
       String term = termAtt.toString();
       if (!processedTerms.contains(term)) {
@@ -254,7 +256,9 @@ private void addTerms(IndexReader reader, FieldVals f) throws IOException {
       }
     }
     ts.end();
-    ts.close();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   @Override
diff --git a/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java b/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
index ae62acb6..0dba3566 100644
--- a/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
+++ b/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
@@ -352,9 +352,10 @@ protected Query getLastTokenQuery(String token) throws IOException {
       occur = BooleanClause.Occur.SHOULD;
     }
 
+    TokenStream ts = null;
     try {
+      ts = queryAnalyzer.tokenStream("", new StringReader(key.toString()));
       //long t0 = System.currentTimeMillis();
-      TokenStream ts = queryAnalyzer.tokenStream("", new StringReader(key.toString()));
       ts.reset();
       final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
       final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
@@ -450,6 +451,8 @@ protected Query getLastTokenQuery(String token) throws IOException {
       return results;
     } catch (IOException ioe) {
       throw new RuntimeException(ioe);
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
     }
   }
 
@@ -465,6 +468,7 @@ protected Query finishQuery(BooleanQuery in, boolean allTermsRequired) {
    *  LookupResult#highlightKey} member. */
   protected Object highlight(String text, Set<String> matchedTokens, String prefixToken) throws IOException {
     TokenStream ts = queryAnalyzer.tokenStream("text", new StringReader(text));
+    try {
     CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
     OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
     ts.reset();
@@ -495,9 +499,10 @@ protected Object highlight(String text, Set<String> matchedTokens, String prefix
     if (upto < endOffset) {
       addNonMatch(sb, text.substring(upto));
     }
-    ts.close();
-
     return sb.toString();
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   /** Called while highlighting a single result, to append a
diff --git a/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java b/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
index ba64403d..34b96025 100644
--- a/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
+++ b/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
@@ -828,13 +828,17 @@ protected boolean acceptResult(IntsRef input, Pair<Long,BytesRef> output) {
   
   final Set<IntsRef> toFiniteStrings(final BytesRef surfaceForm, final TokenStreamToAutomaton ts2a) throws IOException {
  // Analyze surface form:
+    Automaton automaton = null;
     TokenStream ts = indexAnalyzer.tokenStream("", surfaceForm.utf8ToString());
+    try {
 
     // Create corresponding automaton: labels are bytes
     // from each analyzed token, with byte 0 used as
     // separator between tokens:
-    Automaton automaton = ts2a.toAutomaton(ts);
-    ts.close();
+      automaton = ts2a.toAutomaton(ts);
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
 
     replaceSep(automaton);
     automaton = convertAutomaton(automaton);
@@ -854,9 +858,13 @@ protected boolean acceptResult(IntsRef input, Pair<Long,BytesRef> output) {
   final Automaton toLookupAutomaton(final CharSequence key) throws IOException {
     // TODO: is there a Reader from a CharSequence?
     // Turn tokenstream into automaton:
+    Automaton automaton = null;
     TokenStream ts = queryAnalyzer.tokenStream("", key.toString());
-    Automaton automaton = (getTokenStreamToAutomaton()).toAutomaton(ts);
-    ts.close();
+    try {
+      automaton = (getTokenStreamToAutomaton()).toAutomaton(ts);
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
 
     // TODO: we could use the end offset to "guess"
     // whether the final token was a partial token; this
diff --git a/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester.java b/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester.java
index bf103377..05f25407 100644
--- a/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester.java
+++ b/lucene/dev/branches/branch_4x/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester.java
@@ -450,6 +450,7 @@ private int countGrams(BytesRef token) {
   /** Retrieve suggestions. */
   public List<LookupResult> lookup(final CharSequence key, int num) throws IOException {
     TokenStream ts = queryAnalyzer.tokenStream("", key.toString());
+    try {
     TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);
     OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
     PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);
@@ -498,7 +499,6 @@ private int countGrams(BytesRef token) {
     // because we fill the unigram with an empty BytesRef
     // below:
     boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;
-    ts.close();
     //System.out.println("maxEndOffset=" + maxEndOffset + " vs " + offsetAtt.endOffset());
 
     if (lastTokenEnded) {
@@ -713,6 +713,9 @@ public int compare(LookupResult a, LookupResult b) {
     }
 
     return results;
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   /** weight -> cost */
diff --git a/lucene/dev/branches/branch_4x/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java b/lucene/dev/branches/branch_4x/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java
index 8f7dea99..77949c44 100644
--- a/lucene/dev/branches/branch_4x/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java
+++ b/lucene/dev/branches/branch_4x/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java
@@ -39,6 +39,7 @@
 import org.apache.lucene.search.suggest.TermFreqPayloadArrayIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
@@ -167,6 +168,7 @@ protected Directory getDirectory(File path) {
         @Override
         protected Object highlight(String text, Set<String> matchedTokens, String prefixToken) throws IOException {
           TokenStream ts = queryAnalyzer.tokenStream("text", new StringReader(text));
+          try {
           CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
           OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
           ts.reset();
@@ -200,9 +202,11 @@ protected Object highlight(String text, Set<String> matchedTokens, String prefix
           if (upto < endOffset) {
             fragments.add(new LookupHighlightFragment(text.substring(upto), false));
           }
-          ts.close();
 
           return fragments;
+          } finally {
+            IOUtils.closeWhileHandlingException(ts);
+          }
         }
       };
     suggester.build(new TermFreqPayloadArrayIterator(keys));
diff --git a/lucene/dev/branches/branch_4x/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java b/lucene/dev/branches/branch_4x/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
index 1c34a222..3ab05dc9 100644
--- a/lucene/dev/branches/branch_4x/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
+++ b/lucene/dev/branches/branch_4x/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
@@ -45,6 +45,7 @@
 import org.apache.lucene.search.TermRangeQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.IndexableBinaryStringTools;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
@@ -277,7 +278,9 @@ public void assertThreadSafe(final Analyzer analyzer) throws Exception {
 
     for (int i = 0; i < numTestPoints; i++) {
       String term = _TestUtil.randomSimpleString(random());
+      IOException priorException = null;
       TokenStream ts = analyzer.tokenStream("fake", term);
+      try {
       TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
       BytesRef bytes = termAtt.getBytesRef();
       ts.reset();
@@ -287,7 +290,11 @@ public void assertThreadSafe(final Analyzer analyzer) throws Exception {
       map.put(term, BytesRef.deepCopyOf(bytes));
       assertFalse(ts.incrementToken());
       ts.end();
-      ts.close();
+      } catch (IOException e) {
+        priorException = e;
+      } finally {
+        IOUtils.closeWhileHandlingException(priorException, ts);
+      }
     }
     
     Thread threads[] = new Thread[numThreads];
@@ -299,7 +306,9 @@ public void run() {
             for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {
               String term = mapping.getKey();
               BytesRef expected = mapping.getValue();
+              IOException priorException = null;
               TokenStream ts = analyzer.tokenStream("fake", term);
+              try {
               TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
               BytesRef bytes = termAtt.getBytesRef();
               ts.reset();
@@ -308,7 +317,11 @@ public void run() {
               assertEquals(expected, bytes);
               assertFalse(ts.incrementToken());
               ts.end();
-              ts.close();
+              } catch (IOException e) {
+                priorException = e;
+              } finally {
+                IOUtils.closeWhileHandlingException(priorException, ts);
+              }
             }
           } catch (IOException e) {
             throw new RuntimeException(e);
diff --git a/lucene/dev/branches/branch_4x/solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java b/lucene/dev/branches/branch_4x/solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java
index 59a611b1..059db55e 100644
--- a/lucene/dev/branches/branch_4x/solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java
+++ b/lucene/dev/branches/branch_4x/solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java
@@ -236,36 +236,27 @@ public Analyzer getQueryAnalyzer() {
    * simple (we already have a threadlocal clone in the reused TS)
    */
   private BytesRef analyzeRangePart(String field, String part) {
-    TokenStream source;
-      
+    TokenStream source = null;
     try {
       source = analyzer.tokenStream(field, part);
       source.reset();
-    } catch (IOException e) {
-      throw new RuntimeException("Unable to initialize TokenStream to analyze range part: " + part, e);
-    }
       
     TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
     BytesRef bytes = termAtt.getBytesRef();
 
     // we control the analyzer here: most errors are impossible
-    try {
       if (!source.incrementToken())
         throw new IllegalArgumentException("analyzer returned no terms for range part: " + part);
       termAtt.fillBytesRef();
       assert !source.incrementToken();
-    } catch (IOException e) {
-      throw new RuntimeException("error analyzing range part: " + part, e);
-    }
       
-    try {
       source.end();
-      source.close();
+      return BytesRef.deepCopyOf(bytes);
     } catch (IOException e) {
-      throw new RuntimeException("Unable to end & close TokenStream after analyzing range part: " + part, e);
+      throw new RuntimeException("Unable analyze range part: " + part, e);
+    } finally {
+      IOUtils.closeQuietly(source);
     }
-      
-    return BytesRef.deepCopyOf(bytes);
   }
   
   @Override
diff --git a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java
index 2b30c2e2..4b4f6412 100644
--- a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java
+++ b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java
@@ -88,12 +88,14 @@ public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throw
       TokenStream tokenStream = null;
       try {
         tokenStream = analyzer.tokenStream(context.getFieldName(), value);
-      } catch (IOException e) {
-        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);
-      }
       NamedList<List<NamedList>> namedList = new NamedList<List<NamedList>>();
       namedList.add(tokenStream.getClass().getName(), convertTokensToNamedLists(analyzeTokenStream(tokenStream), context));
       return namedList;
+      } catch (IOException e) {
+        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);
+      } finally {
+        IOUtils.closeWhileHandlingException(tokenStream);
+      }
     }
 
     TokenizerChain tokenizerChain = (TokenizerChain) analyzer;
@@ -141,8 +143,8 @@ public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throw
   protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {
     TokenStream tokenStream = null;
     try {
-      final Set<BytesRef> tokens = new HashSet<BytesRef>();
       tokenStream = analyzer.tokenStream("", query);
+      final Set<BytesRef> tokens = new HashSet<BytesRef>();
       final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);
       final BytesRef bytes = bytesAtt.getBytesRef();
 
diff --git a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
index 87bd7481..673fc69c 100644
--- a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
+++ b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
@@ -38,6 +38,7 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.SentinelIntSet;
 import org.apache.solr.cloud.ZkController;
 import org.apache.solr.common.SolrException;
@@ -345,6 +346,7 @@ String getAnalyzedQuery(String query) throws IOException {
     }
     StringBuilder norm = new StringBuilder();
     TokenStream tokens = analyzer.tokenStream("", query);
+    try {
     tokens.reset();
 
     CharTermAttribute termAtt = tokens.addAttribute(CharTermAttribute.class);
@@ -352,8 +354,10 @@ String getAnalyzedQuery(String query) throws IOException {
       norm.append(termAtt.buffer(), 0, termAtt.length());
     }
     tokens.end();
-    tokens.close();
     return norm.toString();
+    } finally {
+      IOUtils.closeWhileHandlingException(tokens);
+    }
   }
 
   //---------------------------------------------------------------------------------
diff --git a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
index 1deec69d..7b3e4be5 100644
--- a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
+++ b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
@@ -43,6 +43,7 @@
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.spell.SuggestMode;
 import org.apache.lucene.search.spell.SuggestWord;
+import org.apache.lucene.util.IOUtils;
 import org.apache.solr.client.solrj.response.SpellCheckResponse;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.CommonParams;
@@ -464,6 +465,7 @@ private void collectShardCollations(SpellCheckMergeData mergeData, NamedList spe
     Collection<Token> result = new ArrayList<Token>();
     assert analyzer != null;
     TokenStream ts = analyzer.tokenStream("", q);
+    try {
     ts.reset();
     // TODO: support custom attributes
     CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
@@ -484,8 +486,10 @@ private void collectShardCollations(SpellCheckMergeData mergeData, NamedList spe
       result.add(token);
     }
     ts.end();
-    ts.close();
     return result;
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
+    }
   }
 
   protected SolrSpellChecker getSpellChecker(SolrParams params) {
diff --git a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
index 863b03ee..990c737b 100644
--- a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
+++ b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
@@ -38,6 +38,7 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.ToStringUtils;
 import org.apache.lucene.util.Version;
 import org.apache.lucene.util.automaton.Automaton;
@@ -403,18 +404,18 @@ protected Query newFieldQuery(Analyzer analyzer, String field, String queryText,
     // Use the analyzer to get all the tokens, and then build a TermQuery,
     // PhraseQuery, or nothing based on the term count
 
-    TokenStream source;
-    try {
-      source = analyzer.tokenStream(field, queryText);
-      source.reset();
-    } catch (IOException e) {
-      throw new SyntaxError("Unable to initialize TokenStream to analyze query text", e);
-    }
-    CachingTokenFilter buffer = new CachingTokenFilter(source);
+    CachingTokenFilter buffer = null;
     TermToBytesRefAttribute termAtt = null;
     PositionIncrementAttribute posIncrAtt = null;
     int numTokens = 0;
+    int positionCount = 0;
+    boolean severalTokensAtSamePosition = false;
 
+    TokenStream source = null;
+    try {
+      source = analyzer.tokenStream(field, queryText);
+      source.reset();
+      buffer = new CachingTokenFilter(source);
     buffer.reset();
 
     if (buffer.hasAttribute(TermToBytesRefAttribute.class)) {
@@ -424,9 +425,6 @@ protected Query newFieldQuery(Analyzer analyzer, String field, String queryText,
       posIncrAtt = buffer.getAttribute(PositionIncrementAttribute.class);
     }
 
-    int positionCount = 0;
-    boolean severalTokensAtSamePosition = false;
-
     boolean hasMoreTokens = false;
     if (termAtt != null) {
       try {
@@ -445,17 +443,15 @@ protected Query newFieldQuery(Analyzer analyzer, String field, String queryText,
         // ignore
       }
     }
-    try {
+    } catch (IOException e) {
+      throw new SyntaxError("Error analyzing query text", e);
+    } finally {
+      IOUtils.closeWhileHandlingException(source);
+    }
+    
       // rewind the buffer stream
       buffer.reset();
 
-      // close original stream - all tokens buffered
-      source.close();
-    }
-    catch (IOException e) {
-      throw new SyntaxError("Cannot close TokenStream analyzing query text", e);
-    }
-
     BytesRef bytes = termAtt == null ? null : termAtt.getBytesRef();
 
     if (numTokens == 0)
diff --git a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/schema/CollationField.java b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/schema/CollationField.java
index 58da79c6..87758d6f 100644
--- a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/schema/CollationField.java
+++ b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/schema/CollationField.java
@@ -210,36 +210,26 @@ public Analyzer getQueryAnalyzer() {
    * simple (we already have a threadlocal clone in the reused TS)
    */
   private BytesRef analyzeRangePart(String field, String part) {
-    TokenStream source;
-      
+    TokenStream source = null;
     try {
       source = analyzer.tokenStream(field, part);
       source.reset();
-    } catch (IOException e) {
-      throw new RuntimeException("Unable to initialize TokenStream to analyze range part: " + part, e);
-    }
-      
     TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
     BytesRef bytes = termAtt.getBytesRef();
 
     // we control the analyzer here: most errors are impossible
-    try {
       if (!source.incrementToken())
         throw new IllegalArgumentException("analyzer returned no terms for range part: " + part);
       termAtt.fillBytesRef();
       assert !source.incrementToken();
-    } catch (IOException e) {
-      throw new RuntimeException("error analyzing range part: " + part, e);
-    }
       
-    try {
       source.end();
-      source.close();
+      return BytesRef.deepCopyOf(bytes);
     } catch (IOException e) {
-      throw new RuntimeException("Unable to end & close TokenStream after analyzing range part: " + part, e);
+      throw new RuntimeException("Unable to analyze range part: " + part, e);
+    } finally {
+      IOUtils.closeQuietly(source);
     }
-      
-    return BytesRef.deepCopyOf(bytes);
   }
   
   @Override
diff --git a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/schema/TextField.java b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/schema/TextField.java
index 33105601..77075c27 100644
--- a/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/schema/TextField.java
+++ b/lucene/dev/branches/branch_4x/solr/core/src/java/org/apache/solr/schema/TextField.java
@@ -27,6 +27,7 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.response.TextResponseWriter;
 import org.apache.solr.search.QParser;
@@ -138,35 +139,27 @@ public Query getRangeQuery(QParser parser, SchemaField field, String part1, Stri
   public static BytesRef analyzeMultiTerm(String field, String part, Analyzer analyzerIn) {
     if (part == null || analyzerIn == null) return null;
 
-    TokenStream source;
+    TokenStream source = null;
     try {
       source = analyzerIn.tokenStream(field, part);
       source.reset();
-    } catch (IOException e) {
-      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "Unable to initialize TokenStream to analyze multiTerm term: " + part, e);
-    }
 
     TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
     BytesRef bytes = termAtt.getBytesRef();
 
-    try {
       if (!source.incrementToken())
         throw  new SolrException(SolrException.ErrorCode.BAD_REQUEST,"analyzer returned no terms for multiTerm term: " + part);
       termAtt.fillBytesRef();
       if (source.incrementToken())
         throw  new SolrException(SolrException.ErrorCode.BAD_REQUEST,"analyzer returned too many terms for multiTerm term: " + part);
-    } catch (IOException e) {
-      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,"error analyzing range part: " + part, e);
-    }
 
-    try {
       source.end();
-      source.close();
+      return BytesRef.deepCopyOf(bytes);
     } catch (IOException e) {
-      throw new RuntimeException("Unable to end & close TokenStream after analyzing multiTerm term: " + part, e);
+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,"error analyzing range part: " + part, e);
+    } finally {
+      IOUtils.closeWhileHandlingException(source);
     }
-
-    return BytesRef.deepCopyOf(bytes);
   }
 
 
@@ -178,17 +171,18 @@ static Query parseFieldQuery(QParser parser, Analyzer analyzer, String field, St
     // Use the analyzer to get all the tokens, and then build a TermQuery,
     // PhraseQuery, or nothing based on the term count
 
-    TokenStream source;
-    try {
-      source = analyzer.tokenStream(field, queryText);
-      source.reset();
-    } catch (IOException e) {
-      throw new RuntimeException("Unable to initialize TokenStream to analyze query text", e);
-    }
-    CachingTokenFilter buffer = new CachingTokenFilter(source);
+    CachingTokenFilter buffer = null;
     CharTermAttribute termAtt = null;
     PositionIncrementAttribute posIncrAtt = null;
     int numTokens = 0;
+    int positionCount = 0;
+    boolean severalTokensAtSamePosition = false;
+
+    TokenStream source = null;
+    try {
+      source = analyzer.tokenStream(field, queryText);
+      source.reset();
+      buffer = new CachingTokenFilter(source);
 
     buffer.reset();
 
@@ -199,9 +193,6 @@ static Query parseFieldQuery(QParser parser, Analyzer analyzer, String field, St
       posIncrAtt = buffer.getAttribute(PositionIncrementAttribute.class);
     }
 
-    int positionCount = 0;
-    boolean severalTokensAtSamePosition = false;
-
     boolean hasMoreTokens = false;
     if (termAtt != null) {
       try {
@@ -220,17 +211,15 @@ static Query parseFieldQuery(QParser parser, Analyzer analyzer, String field, St
         // ignore
       }
     }
-    try {
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    } finally {
+      IOUtils.closeWhileHandlingException(source);
+    }
+
       // rewind the buffer stream
       buffer.reset();
 
-      // close original stream - all tokens buffered
-      source.close();
-    }
-    catch (IOException e) {
-      // ignore
-    }
-
     if (numTokens == 0)
       return null;
     else if (numTokens == 1) {
diff --git a/lucene/dev/branches/branch_4x/solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java b/lucene/dev/branches/branch_4x/solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
index 2410baff..eafe4433 100644
--- a/lucene/dev/branches/branch_4x/solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
+++ b/lucene/dev/branches/branch_4x/solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
@@ -26,6 +26,7 @@
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Version;
 
 import java.util.Collection;
@@ -41,10 +42,12 @@
 
   @Override
   public Collection<Token> convert(String origQuery) {
-    try {
       Collection<Token> result = new HashSet<Token>();
       WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);
-      TokenStream ts = analyzer.tokenStream("", origQuery);
+    
+    TokenStream ts = null;
+    try {
+      ts = analyzer.tokenStream("", origQuery);
       // TODO: support custom attributes
       CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
       OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
@@ -66,11 +69,11 @@
         result.add(tok);
       }
       ts.end();
-      ts.close();
-      
       return result;
     } catch (IOException e) {
       throw new RuntimeException(e);
+    } finally {
+      IOUtils.closeWhileHandlingException(ts);
     }
   }
 }
