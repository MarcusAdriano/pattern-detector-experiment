diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BBtJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BBtJob.java
index 128299fa..176dcfcd 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BBtJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BBtJob.java
@@ -106,8 +106,7 @@ protected void map(IntWritable key, VectorWritable value, Context context)
         // this approach should reduce GC churn rate
         double mul = btVec.getQuick(i);
         for (int j = i; j < kp; j++) {
-          bbtPartial.setQuick(i, j,
-                              bbtPartial.getQuick(i, j) + mul * btVec.getQuick(j));
+          bbtPartial.setQuick(i, j, bbtPartial.getQuick(i, j) + mul * btVec.getQuick(j));
         }
       }
     }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java
index b5d70eb3..ccdcfd52 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java
@@ -44,16 +44,18 @@
 import org.apache.mahout.common.iterator.CopyConstructorIterator;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
 import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.SequentialAccessSparseVector;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
 import org.apache.mahout.math.hadoop.stochasticsvd.QJob.QJobKeyWritable;
 
 /**
- * Bt job. For details, see working notes in MAHOUT-376. 
+ * Bt job. For details, see working notes in MAHOUT-376. <P>
+ * 
+ * Uses hadoop deprecated API wherever new api has not been updated (MAHOUT-593), 
+ * hence @SuppressWarning("deprecation"). <P>
  *
  */
+@SuppressWarnings("deprecation")
 public final class BtJob {
 
   public static final String OUTPUT_Q = "Q";
@@ -76,7 +78,8 @@ private BtJob() {
     private final VectorWritable btValue = new VectorWritable();
     private int kp;
     private final VectorWritable qRowValue = new VectorWritable();
-    //private int qCount; // debug
+
+    // private int qCount; // debug
 
     void loadNextQt() throws IOException {
       Writable key = new QJobKeyWritable();
@@ -85,8 +88,8 @@ void loadNextQt() throws IOException {
       boolean more = qInput.next(key, v);
       assert more;
 
-      mQt = GivensThinSolver.computeQtHat(v.getBlock(), blockNum == 0 ? 0
-          : 1, new CopyConstructorIterator<UpperTriangular>(mRs.iterator()));
+      mQt = GivensThinSolver.computeQtHat(v.getBlock(), blockNum == 0 ? 0 : 1,
+          new CopyConstructorIterator<UpperTriangular>(mRs.iterator()));
       r = mQt[0].length;
       kp = mQt.length;
       if (btValue.get() == null) {
@@ -114,7 +117,7 @@ void loadNextQt() throws IOException {
       // // it doesn't matter if it overflows.
       // m_outputs.write( OUTPUT_Q, oKey, oV);
       // }
-      //qCount++;
+      // qCount++;
     }
 
     @Override
@@ -128,6 +131,11 @@ protected void cleanup(Context context) throws IOException, InterruptedException
       super.cleanup(context);
     }
 
+    @SuppressWarnings("unchecked")
+    private void outputQRow(Writable key, Writable value) throws IOException {
+      outputs.getCollector(OUTPUT_Q, null).collect(key, value);
+    }
+    
     @Override
     protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException {
       if (mQt != null && cnt++ == r) {
@@ -147,11 +155,11 @@ protected void map(Writable key, VectorWritable value, Context context) throws I
         qRow.setQuick(j, mQt[j][qRowIndex]);
       }
 
-      outputs.getCollector(OUTPUT_Q, null).collect(key, qRowValue);
       // make sure Qs are inheriting A row labels.
+      outputQRow(key,qRowValue);
 
       Vector btRow = btValue.get();
-      if ((aRow instanceof SequentialAccessSparseVector) || (aRow instanceof RandomAccessSparseVector)) {
+      if (!aRow.isDense()) {
         for (Vector.Element el : aRow) {
           double mul = el.get();
           for (int j = 0; j < kp; j++) {
@@ -203,8 +211,8 @@ protected void setup(Context context) throws IOException, InterruptedException {
 
       int block = 0;
       for (FileStatus fstat : rFiles) {
-        SequenceFileValueIterator<VectorWritable> iterator =
-            new SequenceFileValueIterator<VectorWritable>(fstat.getPath(), true, context.getConfiguration());
+        SequenceFileValueIterator<VectorWritable> iterator = new SequenceFileValueIterator<VectorWritable>(
+            fstat.getPath(), true, context.getConfiguration());
         VectorWritable rValue;
         try {
           rValue = iterator.next();
@@ -212,8 +220,7 @@ protected void setup(Context context) throws IOException, InterruptedException {
           iterator.close();
         }
         if (block < blockNum && block > 0) {
-          GivensThinSolver.mergeR(mRs.get(0),
-                                  new UpperTriangular(rValue.get()));
+          GivensThinSolver.mergeR(mRs.get(0), new UpperTriangular(rValue.get()));
         } else {
           mRs.add(new UpperTriangular(rValue.get()));
         }
@@ -229,8 +236,8 @@ protected void setup(Context context) throws IOException, InterruptedException {
     private DenseVector accum;
 
     @Override
-    protected void reduce(IntWritable key, Iterable<VectorWritable> values,
-        Context ctx) throws IOException, InterruptedException {
+    protected void reduce(IntWritable key, Iterable<VectorWritable> values, Context ctx) throws IOException,
+      InterruptedException {
       Iterator<VectorWritable> vwIter = values.iterator();
 
       Vector vec = vwIter.next().get();
@@ -257,13 +264,12 @@ public static void run(Configuration conf,
                          int k,
                          int p,
                          int numReduceTasks,
-                         Class<? extends Writable> labelClass)
-    throws ClassNotFoundException, InterruptedException, IOException {
+                         Class<? extends Writable> labelClass) throws ClassNotFoundException, InterruptedException,
+    IOException {
 
     JobConf oldApiJob = new JobConf(conf);
-    MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_Q,
-        org.apache.hadoop.mapred.SequenceFileOutputFormat.class, labelClass,
-        VectorWritable.class);
+    MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_Q, org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
+        labelClass, VectorWritable.class);
 
     Job job = new Job(oldApiJob);
     job.setJobName("Bt-job");
@@ -285,8 +291,7 @@ public static void run(Configuration conf,
     job.getConfiguration().set("mapreduce.output.basename", OUTPUT_BT);
     FileOutputFormat.setCompressOutput(job, true);
     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
-    SequenceFileOutputFormat.setOutputCompressionType(job,
-        CompressionType.BLOCK);
+    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
 
     job.setMapOutputKeyClass(IntWritable.class);
     job.setMapOutputValueClass(VectorWritable.class);
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/Omega.java b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/Omega.java
index a0aa0d65..18e491ad 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/Omega.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/Omega.java
@@ -20,8 +20,6 @@
 import java.util.Arrays;
 import java.util.Random;
 
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.SequentialAccessSparseVector;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.Vector.Element;
 
@@ -61,7 +59,7 @@ public void computeYRow(Vector aRow, double[] yRow) {
     assert yRow.length == kp;
 
     Arrays.fill(yRow, 0);
-    if ((aRow instanceof SequentialAccessSparseVector) || (aRow instanceof RandomAccessSparseVector)) {
+    if (!aRow.isDense()) {
       int j = 0;
       for (Element el : aRow) {
         accumDots(j, el.get(), yRow);
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/QJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/QJob.java
index 716b8d9f..865f9910 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/QJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/QJob.java
@@ -51,11 +51,17 @@
 
 /**
  * Compute first level of QHat-transpose blocks.
+ * <P>
  * 
- * See Mahout-376 woking notes for details.
+ * See Mahout-376 working notes for details.
+ * <P>
  * 
+ * Uses some of Hadoop deprecated api wherever newer api is not available.
+ * Hence, @SuppressWarnings("deprecation") for imports (MAHOUT-593).
+ * <P>
  * 
  */
+@SuppressWarnings("deprecation")
 public final class QJob {
 
   public static final String PROP_OMEGA_SEED = "ssvd.omegaseed";
@@ -68,8 +74,9 @@
 
   private QJob() {
   }
+
   // public static final String OUTPUT_Q="Q";
-  //public static final String OUTPUT_BT = "Bt";
+  // public static final String OUTPUT_BT = "Bt";
 
   public static class QJobKeyWritable implements WritableComparable<QJobKeyWritable> {
 
@@ -148,17 +155,24 @@ private void flushQBlocks(Context ctx) throws IOException {
         // for efficiency in most cases. Sure mapper should be able to load
         // the entire split in memory -- and we don't require even that.
         value.setBlock(qSolver.getThinQtTilde());
-        outputs.getCollector(OUTPUT_QHAT, null).collect(key, value);
-        outputs.getCollector(OUTPUT_R, null).collect(
-            key,
-            new VectorWritable(new DenseVector(qSolver.getRTilde().getData(),
-                true)));
+        outputQHat(key, value);
+        outputR(key, new VectorWritable(new DenseVector(qSolver.getRTilde().getData(), true)));
 
       } else {
         secondPass(ctx);
       }
     }
 
+    @SuppressWarnings("unchecked")
+    private void outputQHat(Writable key, Writable value) throws IOException {
+      outputs.getCollector(OUTPUT_QHAT, null).collect(key, value);
+    }
+
+    @SuppressWarnings("unchecked")
+    private void outputR(Writable key, Writable value) throws IOException {
+      outputs.getCollector(OUTPUT_R, null).collect(key, value);
+    }
+
     private void secondPass(Context ctx) throws IOException {
       qSolver = null; // release mem
       FileSystem localFs = FileSystem.getLocal(ctx.getConfiguration());
@@ -166,8 +180,7 @@ private void secondPass(Context ctx) throws IOException {
       closeables.addFirst(tempQr);
       int qCnt = 0;
       while (tempQr.next(tempKey, value)) {
-        value.setBlock(GivensThinSolver.computeQtHat(value.getBlock(),
-                                                     qCnt,
+        value.setBlock(GivensThinSolver.computeQtHat(value.getBlock(), qCnt,
                                                      new CopyConstructorIterator<UpperTriangular>(rSubseq.iterator())));
         if (qCnt == 1) {
           // just merge r[0] <- r[1] so it doesn't have to repeat
@@ -176,22 +189,18 @@ private void secondPass(Context ctx) throws IOException {
         } else {
           qCnt++;
         }
-        outputs.getCollector(OUTPUT_QHAT, null).collect(key, value);
+        outputQHat(key, value);
       }
 
       assert rSubseq.size() == 1;
 
       // m_value.setR(m_rSubseq.get(0));
-      outputs.getCollector(OUTPUT_R, null).collect(
-          key,
-          new VectorWritable(new DenseVector(rSubseq.get(0).getData(),
-                                             true)));
+      outputR(key, new VectorWritable(new DenseVector(rSubseq.get(0).getData(), true)));
 
     }
 
     @Override
-    protected void map(Writable key, VectorWritable value, Context context)
-      throws IOException, InterruptedException {
+    protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException {
       double[] yRow;
       if (yLookahead.size() == kp) {
         if (qSolver.isFull()) {
@@ -274,8 +283,7 @@ protected void cleanup(Context context) throws IOException, InterruptedException
         String taskTmpDir = System.getProperty("java.io.tmpdir");
         FileSystem localFs = FileSystem.getLocal(context.getConfiguration());
         tempQPath = new Path(new Path(taskTmpDir), "q-temp.seq");
-        tempQw = SequenceFile.createWriter(localFs,
-            context.getConfiguration(), tempQPath, IntWritable.class,
+        tempQw = SequenceFile.createWriter(localFs, context.getConfiguration(), tempQPath, IntWritable.class,
             DenseBlockWritable.class, CompressionType.BLOCK);
         closeables.addFirst(tempQw);
         closeables.addFirst(new IOUtils.DeleteFileOnClose(new File(tempQPath.toString())));
@@ -295,11 +303,9 @@ public static void run(Configuration conf,
                          int numReduceTasks) throws ClassNotFoundException, InterruptedException, IOException {
 
     JobConf oldApiJob = new JobConf(conf);
-    MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_QHAT,
-        org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
+    MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_QHAT, org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
         QJobKeyWritable.class, DenseBlockWritable.class);
-    MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_R,
-        org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
+    MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_R, org.apache.hadoop.mapred.SequenceFileOutputFormat.class,
         QJobKeyWritable.class, VectorWritable.class);
 
     Job job = new Job(oldApiJob);
@@ -316,8 +322,7 @@ public static void run(Configuration conf,
 
     FileOutputFormat.setCompressOutput(job, true);
     FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);
-    SequenceFileOutputFormat.setOutputCompressionType(job,
-        CompressionType.BLOCK);
+    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
 
     job.setMapOutputKeyClass(QJobKeyWritable.class);
     job.setMapOutputValueClass(VectorWritable.class);
@@ -333,9 +338,8 @@ public static void run(Configuration conf,
     job.getConfiguration().setInt(PROP_P, p);
 
     // number of reduce tasks doesn't matter. we don't actually
-    // send anything to reducers. in fact, the only reason
-    // we need to configure reduce step is so that combiners can fire.
-    // so reduce here is purely symbolic.
+    // send anything to reducers.
+    
     job.setNumReduceTasks(0 /* numReduceTasks */);
 
     job.submit();
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java
index e69de29b..a2addcf0 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java
@@ -0,0 +1,177 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.hadoop.stochasticsvd;
+
+import java.io.Closeable;
+import java.io.File;
+import java.util.Deque;
+import java.util.LinkedList;
+import java.util.Random;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.RandomUtils;
+import org.apache.mahout.math.DenseMatrix;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.SingularValueDecomposition;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.junit.Test;
+
+/**
+ * 
+ * Tests SSVD solver with a made-up data running hadoop 
+ * solver in a local mode. It requests full-rank SSVD and 
+ * then compares singular values to that of Colt's SVD 
+ * asserting epsilon(precision) 1e-10 or whatever most recent 
+ * value configured. 
+ * 
+ */
+public class LocalSSVDSolverDenseTest extends MahoutTestCase {
+
+  private static final double s_epsilon = 1.0E-10d;
+
+  @Test
+  public void testSSVDSolver() throws Exception {
+
+    Configuration conf = new Configuration();
+    conf.set("mapred.job.tracker", "local");
+    conf.set("fs.default.name", "file:///");
+
+    // conf.set("mapred.job.tracker","localhost:11011");
+    // conf.set("fs.default.name","hdfs://localhost:11010/");
+
+    Deque<Closeable> closeables = new LinkedList<Closeable>();
+    Random rnd = RandomUtils.getRandom();
+
+    File tmpDir = getTestTempDir("svdtmp");
+    conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());
+
+    Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");
+
+    // create distributed row matrix-like struct
+    SequenceFile.Writer w = SequenceFile.createWriter(
+        FileSystem.getLocal(conf), conf, aLocPath, IntWritable.class,
+        VectorWritable.class, CompressionType.BLOCK, new DefaultCodec());
+    closeables.addFirst(w);
+
+    int n = 100;
+    double[] row = new double[n];
+    Vector dv = new DenseVector(row, true);
+    Writable vw = new VectorWritable(dv);
+    IntWritable roww = new IntWritable();
+
+    double muAmplitude = 50.0;
+    int m = 1000;
+    for (int i = 0; i < m; i++) {
+      for (int j = 0; j < n; j++) {
+        row[j] = muAmplitude * (rnd.nextDouble() - 0.5);
+      }
+      roww.set(i);
+      w.append(roww, vw);
+    }
+    closeables.remove(w);
+    w.close();
+
+    FileSystem fs = FileSystem.get(conf);
+
+    Path tempDirPath = getTestTempDirPath("svd-proc");
+    Path aPath = new Path(tempDirPath, "A/A.seq");
+    fs.copyFromLocalFile(aLocPath, aPath);
+
+    Path svdOutPath = new Path(tempDirPath, "SSVD-out");
+
+    // make sure we wipe out previous test results, just a convenience
+    fs.delete(svdOutPath, true);
+
+    int ablockRows = 251;
+    int p = 60;
+    int k = 40;
+    SSVDSolver ssvd = new SSVDSolver(conf, new Path[] { aPath }, svdOutPath,
+        ablockRows, k, p, 3);
+    // ssvd.setcUHalfSigma(true);
+    // ssvd.setcVHalfSigma(true);
+    ssvd.setOverwrite(true);
+    ssvd.run();
+
+    double[] stochasticSValues = ssvd.getSingularValues();
+    System.out.println("--SSVD solver singular values:");
+    dumpSv(stochasticSValues);
+    System.out.println("--Colt SVD solver singular values:");
+
+    // try to run the same thing without stochastic algo
+    double[][] a = SSVDSolver.loadDistributedRowMatrix(fs, aPath, conf);
+
+    // SingularValueDecompositionImpl svd=new SingularValueDecompositionImpl(new
+    // Array2DRowRealMatrix(a));
+    SingularValueDecomposition svd2 = new SingularValueDecomposition(
+        new DenseMatrix(a));
+
+    a = null;
+
+    double[] svalues2 = svd2.getSingularValues();
+    dumpSv(svalues2);
+
+    for (int i = 0; i < k + p; i++) {
+      Assert.assertTrue(Math.abs(svalues2[i] - stochasticSValues[i]) <= s_epsilon);
+    }
+
+    double[][] q = SSVDSolver.loadDistributedRowMatrix(fs, new Path(svdOutPath,
+        "Bt-job/" + BtJob.OUTPUT_Q + "-*"), conf);
+
+    SSVDPrototypeTest.assertOrthonormality(new DenseMatrix(q), false, s_epsilon);
+
+    double[][] u = SSVDSolver.loadDistributedRowMatrix(fs, new Path(svdOutPath,
+                                                                    "U/[^_]*"), conf);
+
+    SSVDPrototypeTest.assertOrthonormality(new DenseMatrix(u), false, s_epsilon);
+    double[][] v = SSVDSolver.loadDistributedRowMatrix(fs, new Path(svdOutPath,
+        "V/[^_]*"), conf);
+
+    SSVDPrototypeTest
+        .assertOrthonormality(new DenseMatrix(v), false, s_epsilon);
+  }
+
+  static void dumpSv(double[] s) {
+    System.out.printf("svs: ");
+    for (double value : s) {
+      System.out.printf("%f  ", value);
+    }
+    System.out.println();
+
+  }
+
+  static void dump(double[][] matrix) {
+    for (double[] aMatrix : matrix) {
+      for (double anAMatrix : aMatrix) {
+        System.out.printf("%f  ", anAMatrix);
+      }
+      System.out.println();
+    }
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverSparseSequentialTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverSparseSequentialTest.java
index e69de29b..fd6b88ce 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverSparseSequentialTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverSparseSequentialTest.java
@@ -0,0 +1,170 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.hadoop.stochasticsvd;
+
+import java.io.Closeable;
+import java.io.File;
+import java.util.Deque;
+import java.util.LinkedList;
+import java.util.Random;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.RandomUtils;
+import org.apache.mahout.math.DenseMatrix;
+import org.apache.mahout.math.SequentialAccessSparseVector;
+import org.apache.mahout.math.SingularValueDecomposition;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.junit.Test;
+
+/**
+ * 
+ * Tests SSVD solver with a made-up data running hadoop solver in a local mode.
+ * It requests full-rank SSVD and then compares singular values to that of
+ * Colt's SVD asserting epsilon(precision) 1e-10 or whatever most recent value
+ * configured.
+ * 
+ */
+public class LocalSSVDSolverSparseSequentialTest extends MahoutTestCase {
+
+  private static final double s_epsilon = 1.0E-10d;
+
+  @Test
+  public void testSSVDSolver() throws Exception {
+
+    Configuration conf = new Configuration();
+    conf.set("mapred.job.tracker", "local");
+    conf.set("fs.default.name", "file:///");
+
+    // conf.set("mapred.job.tracker","localhost:11011");
+    // conf.set("fs.default.name","hdfs://localhost:11010/");
+
+    Deque<Closeable> closeables = new LinkedList<Closeable>();
+    Random rnd = RandomUtils.getRandom();
+
+    File tmpDir = getTestTempDir("svdtmp");
+    conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());
+
+    Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");
+
+    // create distributed row matrix-like struct
+    SequenceFile.Writer w = SequenceFile.createWriter(FileSystem.getLocal(conf), conf, aLocPath, IntWritable.class,
+        VectorWritable.class, CompressionType.BLOCK, new DefaultCodec());
+    closeables.addFirst(w);
+
+    int n = 100;
+    Vector dv;
+    VectorWritable vw = new VectorWritable();
+    IntWritable roww = new IntWritable();
+
+    double muAmplitude = 50.0;
+    int m = 1000;
+    for (int i = 0; i < m; i++) {
+      dv=new SequentialAccessSparseVector(n);
+      for (int j = 0; j < n / 5; j++) {
+        dv.setQuick(rnd.nextInt(n), muAmplitude * (rnd.nextDouble() - 0.5));
+      }
+      roww.set(i);
+      vw.set(dv);
+      w.append(roww, vw);
+    }
+    closeables.remove(w);
+    w.close();
+
+    FileSystem fs = FileSystem.get(conf);
+
+    Path tempDirPath = getTestTempDirPath("svd-proc");
+    Path aPath = new Path(tempDirPath, "A/A.seq");
+    fs.copyFromLocalFile(aLocPath, aPath);
+
+    Path svdOutPath = new Path(tempDirPath, "SSVD-out");
+
+    // make sure we wipe out previous test results, just a convenience
+    fs.delete(svdOutPath, true);
+
+    int ablockRows = 251;
+    int p = 60;
+    int k = 40;
+    SSVDSolver ssvd = new SSVDSolver(conf, new Path[] { aPath }, svdOutPath, ablockRows, k, p, 3);
+    // ssvd.setcUHalfSigma(true);
+    // ssvd.setcVHalfSigma(true);
+    ssvd.setOverwrite(true);
+    ssvd.run();
+
+    double[] stochasticSValues = ssvd.getSingularValues();
+    System.out.println("--SSVD solver singular values:");
+    dumpSv(stochasticSValues);
+    System.out.println("--Colt SVD solver singular values:");
+
+    // try to run the same thing without stochastic algo
+    double[][] a = SSVDSolver.loadDistributedRowMatrix(fs, aPath, conf);
+
+    // SingularValueDecompositionImpl svd=new SingularValueDecompositionImpl(new
+    // Array2DRowRealMatrix(a));
+    SingularValueDecomposition svd2 = new SingularValueDecomposition(new DenseMatrix(a));
+
+    a = null;
+
+    double[] svalues2 = svd2.getSingularValues();
+    dumpSv(svalues2);
+
+    for (int i = 0; i < k + p; i++) {
+      Assert.assertTrue(Math.abs(svalues2[i] - stochasticSValues[i]) <= s_epsilon);
+    }
+
+    double[][] q = SSVDSolver.loadDistributedRowMatrix(fs, new Path(svdOutPath, "Bt-job/" + BtJob.OUTPUT_Q + "-*"),
+        conf);
+
+    SSVDPrototypeTest.assertOrthonormality(new DenseMatrix(q), false, s_epsilon);
+
+    double[][] u = SSVDSolver.loadDistributedRowMatrix(fs, new Path(svdOutPath, "U/[^_]*"), conf);
+
+    SSVDPrototypeTest.assertOrthonormality(new DenseMatrix(u), false, s_epsilon);
+    double[][] v = SSVDSolver.loadDistributedRowMatrix(fs, new Path(svdOutPath, "V/[^_]*"), conf);
+
+    SSVDPrototypeTest.assertOrthonormality(new DenseMatrix(v), false, s_epsilon);
+  }
+
+  static void dumpSv(double[] s) {
+    System.out.printf("svs: ");
+    for (double value : s) {
+      System.out.printf("%f  ", value);
+    }
+    System.out.println();
+
+  }
+
+  static void dump(double[][] matrix) {
+    for (double[] aMatrix : matrix) {
+      for (double anAMatrix : aMatrix) {
+        System.out.printf("%f  ", anAMatrix);
+      }
+      System.out.println();
+    }
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverTest.java
index 3ca27cc1..e69de29b 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverTest.java
@@ -1,176 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.hadoop.stochasticsvd;
-
-import java.io.Closeable;
-import java.io.File;
-import java.util.Deque;
-import java.util.LinkedList;
-import java.util.Random;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.SequenceFile.CompressionType;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.compress.DefaultCodec;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.math.DenseMatrix;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.SingularValueDecomposition;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Test;
-
-/**
- * 
- * Tests SSVD solver with a made-up data running hadoop 
- * solver in a local mode. It requests full-rank SSVD and 
- * then compares singular values to that of Colt's SVD 
- * asserting epsilon(precision) 1e-10 or whatever most recent 
- * value configured. 
- * 
- */
-public class LocalSSVDSolverTest extends MahoutTestCase {
-
-  private static final double s_epsilon = 1.0E-10d;
-
-  @Test
-  public void testSSVDSolver() throws Exception {
-
-    Configuration conf = new Configuration();
-    conf.set("mapred.job.tracker", "local");
-    conf.set("fs.default.name", "file:///");
-
-    // conf.set("mapred.job.tracker","localhost:11011");
-    // conf.set("fs.default.name","hdfs://localhost:11010/");
-
-    Deque<Closeable> closeables = new LinkedList<Closeable>();
-    Random rnd = RandomUtils.getRandom();
-
-    File tmpDir = getTestTempDir("svdtmp");
-    conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());
-
-    Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");
-
-    // create distributed row matrix-like struct
-    SequenceFile.Writer w = SequenceFile.createWriter(
-        FileSystem.getLocal(conf), conf, aLocPath, IntWritable.class,
-        VectorWritable.class, CompressionType.BLOCK, new DefaultCodec());
-    closeables.addFirst(w);
-
-    int n = 100;
-    double[] row = new double[n];
-    DenseVector dv = new DenseVector(row, true);
-    Writable vw = new VectorWritable(dv);
-    IntWritable roww = new IntWritable();
-
-    double muAmplitude = 50.0;
-    int m = 1000;
-    for (int i = 0; i < m; i++) {
-      for (int j = 0; j < n; j++) {
-        row[j] = muAmplitude * (rnd.nextDouble() - 0.5);
-      }
-      roww.set(i);
-      w.append(roww, vw);
-    }
-    closeables.remove(w);
-    w.close();
-
-    FileSystem fs = FileSystem.get(conf);
-
-    Path tempDirPath = getTestTempDirPath("svd-proc");
-    Path aPath = new Path(tempDirPath, "A/A.seq");
-    fs.copyFromLocalFile(aLocPath, aPath);
-
-    Path svdOutPath = new Path(tempDirPath, "SSVD-out");
-
-    // make sure we wipe out previous test results, just a convenience
-    fs.delete(svdOutPath, true);
-
-    int ablockRows = 251;
-    int p = 60;
-    int k = 40;
-    SSVDSolver ssvd = new SSVDSolver(conf, new Path[] { aPath }, svdOutPath,
-        ablockRows, k, p, 3);
-    // ssvd.setcUHalfSigma(true);
-    // ssvd.setcVHalfSigma(true);
-    ssvd.setOverwrite(true);
-    ssvd.run();
-
-    double[] stochasticSValues = ssvd.getSingularValues();
-    System.out.println("--SSVD solver singular values:");
-    dumpSv(stochasticSValues);
-    System.out.println("--Colt SVD solver singular values:");
-
-    // try to run the same thing without stochastic algo
-    double[][] a = SSVDSolver.loadDistributedRowMatrix(fs, aPath, conf);
-
-    // SingularValueDecompositionImpl svd=new SingularValueDecompositionImpl(new
-    // Array2DRowRealMatrix(a));
-    SingularValueDecomposition svd2 = new SingularValueDecomposition(
-        new DenseMatrix(a));
-
-    a = null;
-
-    double[] svalues2 = svd2.getSingularValues();
-    dumpSv(svalues2);
-
-    for (int i = 0; i < k + p; i++) {
-      Assert.assertTrue(Math.abs(svalues2[i] - stochasticSValues[i]) <= s_epsilon);
-    }
-
-    double[][] q = SSVDSolver.loadDistributedRowMatrix(fs, new Path(svdOutPath,
-        "Bt-job/" + BtJob.OUTPUT_Q + "-*"), conf);
-
-    SSVDPrototypeTest.assertOrthonormality(new DenseMatrix(q), false, s_epsilon);
-
-    double[][] u = SSVDSolver.loadDistributedRowMatrix(fs, new Path(svdOutPath,
-                                                                    "U/[^_]*"), conf);
-
-    SSVDPrototypeTest.assertOrthonormality(new DenseMatrix(u), false, s_epsilon);
-    double[][] v = SSVDSolver.loadDistributedRowMatrix(fs, new Path(svdOutPath,
-        "V/[^_]*"), conf);
-
-    SSVDPrototypeTest
-        .assertOrthonormality(new DenseMatrix(v), false, s_epsilon);
-  }
-
-  static void dumpSv(double[] s) {
-    System.out.printf("svs: ");
-    for (double value : s) {
-      System.out.printf("%f  ", value);
-    }
-    System.out.println();
-
-  }
-
-  static void dump(double[][] matrix) {
-    for (double[] aMatrix : matrix) {
-      for (double anAMatrix : aMatrix) {
-        System.out.printf("%f  ", anAMatrix);
-      }
-      System.out.println();
-    }
-  }
-
-}
