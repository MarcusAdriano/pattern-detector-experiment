diff --git a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestMappingCharFilter.java b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestMappingCharFilter.java
index d5f10ef0..a03d3466 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestMappingCharFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestMappingCharFilter.java
@@ -17,6 +17,7 @@
 
 package org.apache.lucene.analysis;
 
+import java.io.Reader;
 import java.io.StringReader;
 
 public class TestMappingCharFilter extends BaseTokenStreamTestCase {
@@ -59,55 +60,55 @@ public void testReaderReset() throws Exception {
   public void testNothingChange() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "x" ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
-    assertTokenStreamContents(ts, new String[]{"x"}, new int[]{0}, new int[]{1});
+    assertTokenStreamContents(ts, new String[]{"x"}, new int[]{0}, new int[]{1}, 1);
   }
 
   public void test1to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "h" ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
-    assertTokenStreamContents(ts, new String[]{"i"}, new int[]{0}, new int[]{1});
+    assertTokenStreamContents(ts, new String[]{"i"}, new int[]{0}, new int[]{1}, 1);
   }
 
   public void test1to2() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "j" ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
-    assertTokenStreamContents(ts, new String[]{"jj"}, new int[]{0}, new int[]{1});
+    assertTokenStreamContents(ts, new String[]{"jj"}, new int[]{0}, new int[]{1}, 1);
   }
 
   public void test1to3() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "k" ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
-    assertTokenStreamContents(ts, new String[]{"kkk"}, new int[]{0}, new int[]{1});
+    assertTokenStreamContents(ts, new String[]{"kkk"}, new int[]{0}, new int[]{1}, 1);
   }
 
   public void test2to4() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "ll" ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
-    assertTokenStreamContents(ts, new String[]{"llll"}, new int[]{0}, new int[]{2});
+    assertTokenStreamContents(ts, new String[]{"llll"}, new int[]{0}, new int[]{2}, 2);
   }
 
   public void test2to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "aa" ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
-    assertTokenStreamContents(ts, new String[]{"a"}, new int[]{0}, new int[]{2});
+    assertTokenStreamContents(ts, new String[]{"a"}, new int[]{0}, new int[]{2}, 2);
   }
 
   public void test3to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "bbb" ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
-    assertTokenStreamContents(ts, new String[]{"b"}, new int[]{0}, new int[]{3});
+    assertTokenStreamContents(ts, new String[]{"b"}, new int[]{0}, new int[]{3}, 3);
   }
 
   public void test4to2() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "cccc" ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
-    assertTokenStreamContents(ts, new String[]{"cc"}, new int[]{0}, new int[]{4});
+    assertTokenStreamContents(ts, new String[]{"cc"}, new int[]{0}, new int[]{4}, 4);
   }
 
   public void test5to0() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "empty" ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
-    assertTokenStreamContents(ts, new String[0]);
+    assertTokenStreamContents(ts, new String[0], new int[]{}, new int[]{}, 5);
   }
 
   //
@@ -129,12 +130,14 @@ public void test5to0() throws Exception {
   //   aa,20,22 =>    a,20,22
   //
   public void testTokenStream() throws Exception {
-    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "h i j k ll cccc bbb aa" ) ) );
+    String testString = "h i j k ll cccc bbb aa";
+    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( testString ) ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
     assertTokenStreamContents(ts,
       new String[]{"i","i","jj","kkk","llll","cc","b","a"},
       new int[]{0,2,4,6,8,11,16,20},
-      new int[]{1,3,5,7,10,15,19,22}
+      new int[]{1,3,5,7,10,15,19,22},
+      testString.length()
     );
   }
 
@@ -149,13 +152,34 @@ public void testTokenStream() throws Exception {
   //   ll,5,7 => llllllll,5,7
   //    h,8,9 => i,8,9
   public void testChained() throws Exception {
+    String testString = "aaaa ll h";
     CharStream cs = new MappingCharFilter( normMap,
-        new MappingCharFilter( normMap, CharReader.get( new StringReader( "aaaa ll h" ) ) ) );
+        new MappingCharFilter( normMap, CharReader.get( new StringReader( testString ) ) ) );
     TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);
     assertTokenStreamContents(ts,
       new String[]{"a","llllllll","i"},
       new int[]{0,5,8},
-      new int[]{4,7,9}
+      new int[]{4,7,9},
+      testString.length()
     );
   }
+  
+  public void testRandom() throws Exception {
+    Analyzer analyzer = new ReusableAnalyzerBase() {
+
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+
+      @Override
+      protected Reader initReader(Reader reader) {
+        return new MappingCharFilter(normMap, CharReader.get(reader));
+      }
+    };
+    
+    int numRounds = RANDOM_MULTIPLIER * 10000;
+    checkRandomData(random, analyzer, numRounds);
+  }
 }
diff --git a/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java b/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
index 93151a68..4c24a88e 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
@@ -246,7 +246,7 @@ public static void checkOneTermReuse(Analyzer a, final String input, final Strin
   }
   
   // simple utility method for blasting tokenstreams with data to make sure they don't do anything crazy
-
+  // TODO: add a MockCharStream, and use it here too, to ensure that correctOffset etc is being done by tokenizers.
   public static void checkRandomData(Random random, Analyzer a, int iterations) throws IOException {
     checkRandomData(random, a, iterations, 20);
   }
@@ -254,13 +254,16 @@ public static void checkRandomData(Random random, Analyzer a, int iterations) th
   public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {
     for (int i = 0; i < iterations; i++) {
       String text;
-      switch(_TestUtil.nextInt(random, 0, 3)) {
+      switch(_TestUtil.nextInt(random, 0, 4)) {
         case 0: 
           text = _TestUtil.randomSimpleString(random);
           break;
         case 1:
           text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);
           break;
+        case 2:
+          text = _TestUtil.randomHtmlishString(random, maxWordLength);
+          break;
         default:
           text = _TestUtil.randomUnicodeString(random, maxWordLength);
       }
diff --git a/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java b/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java
index 65f0a8b4..08df9471 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java
@@ -266,6 +266,31 @@ public static void randomFixedLengthUnicodeString(Random random, char[] chars, i
     }
   }
 
+  // TODO: make this more evil
+  public static String randomHtmlishString(Random random, int numElements) {
+    final int end = random.nextInt(numElements);
+    if (end == 0) {
+      // allow 0 length
+      return "";
+    }
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < end; i++) {
+      int val = random.nextInt(10);
+      switch(val) {
+        case 0: sb.append("<p>"); break;
+        case 1: sb.append("</p>"); break;
+        case 2: sb.append("<!--"); break;
+        case 3: sb.append("-->"); break;
+        case 4: sb.append("&#"); break;
+        case 5: sb.append(";"); break;
+        case 6: sb.append((char)_TestUtil.nextInt(random, '0', '9')); break;
+        default:
+          sb.append((char)_TestUtil.nextInt(random, 'a', 'z'));
+      }
+    }
+    return sb.toString();
+  }
+
   private static final int[] blockStarts = {
     0x0000, 0x0080, 0x0100, 0x0180, 0x0250, 0x02B0, 0x0300, 0x0370, 0x0400, 
     0x0500, 0x0530, 0x0590, 0x0600, 0x0700, 0x0750, 0x0780, 0x07C0, 0x0800, 
diff --git a/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/HTMLStripCharFilterTest.java b/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/HTMLStripCharFilterTest.java
index b5ee2917..b27e4bc9 100644
--- a/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/HTMLStripCharFilterTest.java
+++ b/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/HTMLStripCharFilterTest.java
@@ -26,22 +26,19 @@
 import java.util.HashSet;
 import java.util.Set;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharReader;
-import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.analysis.ReusableAnalyzerBase;
 
-import org.apache.solr.SolrTestCaseJ4;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
+import org.junit.Ignore;
 
-public class HTMLStripCharFilterTest extends LuceneTestCase {
+import org.apache.solr.SolrTestCaseJ4;
 
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-  }
+public class HTMLStripCharFilterTest extends BaseTokenStreamTestCase {
 
-  @Override
-  public void tearDown() throws Exception {
-    super.tearDown();
-  }
   //this is some text  here is a  link  and another  link . This is an entity: & plus a <.  Here is an &
   //
   public void test() throws IOException {
@@ -262,4 +259,23 @@ public void testOffsets() throws Exception {
     doTestOffsets("X < &zz >X &# < X > < &l > &g < X");
   }
 
+  @Ignore("broken offsets: see LUCENE-2208")
+  public void testRandom() throws Exception {
+    Analyzer analyzer = new ReusableAnalyzerBase() {
+
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+
+      @Override
+      protected Reader initReader(Reader reader) {
+        return new HTMLStripCharFilter(CharReader.get(new BufferedReader(reader)));
+      }
+    };
+    
+    int numRounds = RANDOM_MULTIPLIER * 10000;
+    checkRandomData(random, analyzer, numRounds);
+  }
 }
diff --git a/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilter.java b/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilter.java
index 36cb229c..1b41dbae 100644
--- a/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilter.java
+++ b/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilter.java
@@ -51,7 +51,8 @@ public void testNothingChange() throws IOException {
     assertTokenStreamContents(ts,
         new String[] { "this", "is", "test." },
         new int[] { 0, 5, 8 },
-        new int[] { 4, 7, 13 });
+        new int[] { 4, 7, 13 }, 
+        BLOCK.length());
   }
   
   // 012345678
@@ -84,7 +85,8 @@ public void test1block1matchSameLength() throws IOException {
     assertTokenStreamContents(ts,
         new String[] { "aa#bb#cc" },
         new int[] { 0 },
-        new int[] { 8 });
+        new int[] { 8 }, 
+        BLOCK.length());
   }
 
   //           11111
@@ -99,7 +101,8 @@ public void test1block1matchLonger() throws IOException {
     assertTokenStreamContents(ts,
         new String[] { "aa##bb###cc", "dd" },
         new int[] { 0, 9 },
-        new int[] { 8, 11 });
+        new int[] { 8, 11 },
+        BLOCK.length());
   }
 
   // 01234567
@@ -113,7 +116,8 @@ public void test1block2matchLonger() throws IOException {
     assertTokenStreamContents(ts,
         new String[] { "aa", "aa" },
         new int[] { 1, 4 },
-        new int[] { 2, 5 });
+        new int[] { 2, 5 },
+        BLOCK.length());
   }
 
   //           11111
@@ -128,7 +132,8 @@ public void test1block1matchShorter() throws IOException {
     assertTokenStreamContents(ts,
         new String[] { "aa#bb", "dd" },
         new int[] { 0, 12 },
-        new int[] { 11, 14 });
+        new int[] { 11, 14 },
+        BLOCK.length());
   }
 
   //           111111111122222222223333
@@ -143,7 +148,8 @@ public void test1blockMultiMatches() throws IOException {
     assertTokenStreamContents(ts,
         new String[] { "aa", "bb", "cc", "---", "aa", "bb", "aa", "bb", "cc" },
         new int[] { 2, 6, 9, 11, 15, 18, 21, 25, 29 },
-        new int[] { 4, 8, 10, 14, 17, 20, 23, 27, 33 });
+        new int[] { 4, 8, 10, 14, 17, 20, 23, 27, 33 },
+        BLOCK.length());
   }
 
   //           11111111112222222222333333333
@@ -158,7 +164,8 @@ public void test2blocksMultiMatches() throws IOException {
     assertTokenStreamContents(ts,
         new String[] { "aa##bb", "cc", "---", "aa##bb", "aa.", "bb", "aa##bb", "cc" },
         new int[] { 2, 8, 11, 15, 21, 25, 28, 36 },
-        new int[] { 7, 10, 14, 20, 24, 27, 35, 38 });
+        new int[] { 7, 10, 14, 20, 24, 27, 35, 38 },
+        BLOCK.length());
   }
 
   //           11111111112222222222333333333
@@ -175,7 +182,8 @@ public void testChain() throws IOException {
     assertTokenStreamContents(ts,
         new String[] { "aa", "b", "-", "c", ".", "---", "b", "aa", ".", "c", "c", "b" },
         new int[] { 1, 3, 6, 8, 12, 14, 18, 21, 23, 25, 29, 33 },
-        new int[] { 2, 5, 7, 11, 13, 17, 20, 22, 24, 28, 32, 35 });
+        new int[] { 2, 5, 7, 11, 13, 17, 20, 22, 24, 28, 32, 35 },
+        BLOCK.length());
   }
   
   private Pattern pattern( String p ){
diff --git a/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/TestPatternTokenizerFactory.java b/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/TestPatternTokenizerFactory.java
index 0227c6bf..e4db8b47 100644
--- a/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/TestPatternTokenizerFactory.java
+++ b/lucene/dev/branches/branch_3x/solr/core/src/test/org/apache/solr/analysis/TestPatternTokenizerFactory.java
@@ -97,7 +97,8 @@ public void testOffsetCorrection() throws Exception {
     assertTokenStreamContents(stream,
         new String[] { "Günther", "Günther", "is", "here" },
         new int[] { 0, 13, 26, 29 },
-        new int[] { 12, 25, 28, 33 });
+        new int[] { 12, 25, 28, 33 },
+        INPUT.length());
     
     charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );
     args.put( PatternTokenizerFactory.PATTERN, "Günther" );
@@ -108,7 +109,8 @@ public void testOffsetCorrection() throws Exception {
     assertTokenStreamContents(stream,
         new String[] { "Günther", "Günther" },
         new int[] { 0, 13 },
-        new int[] { 12, 25 });
+        new int[] { 12, 25 },
+        INPUT.length());
   }
   
   /** 
