diff --git a/lucene/dev/trunk/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java b/lucene/dev/trunk/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java
index a63370f5..a56baaa9 100644
--- a/lucene/dev/trunk/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java
+++ b/lucene/dev/trunk/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesProducer;
@@ -62,6 +63,7 @@
   private final Map<Integer,Bits> docsWithFieldInstances = new HashMap<Integer,Bits>();
   
   private final int maxDoc;
+  private final AtomicLong ramBytesUsed;
   
   static final byte NUMBER = 0;
   static final byte BYTES = 1;
@@ -76,6 +78,7 @@
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
     // read in the entries from the metadata file.
     IndexInput in = state.directory.openInput(metaName, state.context);
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
     boolean success = false;
     final int version;
     try {
@@ -178,8 +181,7 @@ private void readFields(IndexInput meta) throws IOException {
 
   @Override
   public long ramBytesUsed() {
-    // TODO: optimize me
-    return RamUsageEstimator.sizeOf(this);
+    return ramBytesUsed.get();
   }
   
   @Override
@@ -199,9 +201,8 @@ private NumericDocValues loadNumeric(NumericEntry entry) throws IOException {
     case 1:
       {
         final byte[] values = new byte[entry.count];
-        for(int i=0;i<entry.count;i++) {
-          values[i] = data.readByte();
-        }
+        data.readBytes(values, 0, entry.count);
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
         return new NumericDocValues() {
           @Override
           public long get(int idx) {
@@ -216,6 +217,7 @@ public long get(int idx) {
         for(int i=0;i<entry.count;i++) {
           values[i] = data.readShort();
         }
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
         return new NumericDocValues() {
           @Override
           public long get(int idx) {
@@ -230,6 +232,7 @@ public long get(int idx) {
         for(int i=0;i<entry.count;i++) {
           values[i] = data.readInt();
         }
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
         return new NumericDocValues() {
           @Override
           public long get(int idx) {
@@ -244,6 +247,7 @@ public long get(int idx) {
         for(int i=0;i<entry.count;i++) {
           values[i] = data.readLong();
         }
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
         return new NumericDocValues() {
           @Override
           public long get(int idx) {
@@ -280,6 +284,8 @@ private BinaryDocValues loadBinary(BinaryEntry entry) throws IOException {
     }
     address[entry.count] = data.readInt();
 
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes) + RamUsageEstimator.sizeOf(address));
+
     return new BinaryDocValues() {
       @Override
       public void get(int docID, BytesRef result) {
diff --git a/lucene/dev/trunk/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java b/lucene/dev/trunk/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java
index 54da809f..0264b1e3 100644
--- a/lucene/dev/trunk/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java
+++ b/lucene/dev/trunk/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesProducer;
@@ -75,7 +76,7 @@
   private final Map<Integer,Bits> docsWithFieldInstances = new HashMap<Integer,Bits>();
   
   private final int maxDoc;
-  
+  private final AtomicLong ramBytesUsed;
   
   static final byte NUMBER = 0;
   static final byte BYTES = 1;
@@ -107,7 +108,7 @@
       binaries = new HashMap<Integer,BinaryEntry>();
       fsts = new HashMap<Integer,FSTEntry>();
       readFields(in, state.fieldInfos);
-
+      ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
       success = true;
     } finally {
       if (success) {
@@ -204,8 +205,7 @@ public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOExcept
   
   @Override
   public long ramBytesUsed() {
-    // TODO: optimize me
-    return RamUsageEstimator.sizeOf(this);
+    return ramBytesUsed.get();
   }
   
   private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
@@ -224,6 +224,7 @@ private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
         final int formatID = data.readVInt();
         final int bitsPerValue = data.readVInt();
         final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), entry.packedIntsVersion, maxDoc, bitsPerValue);
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -233,10 +234,12 @@ public long get(int docID) {
       case DELTA_COMPRESSED:
         final int blockSize = data.readVInt();
         final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, blockSize, maxDoc, false);
+        ramBytesUsed.addAndGet(reader.ramBytesUsed());
         return reader;
       case UNCOMPRESSED:
         final byte bytes[] = new byte[maxDoc];
         data.readBytes(bytes, 0, bytes.length);
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -248,6 +251,7 @@ public long get(int docID) {
         final long mult = data.readLong();
         final int quotientBlockSize = data.readVInt();
         final BlockPackedReader quotientReader = new BlockPackedReader(data, entry.packedIntsVersion, quotientBlockSize, maxDoc, false);
+        ramBytesUsed.addAndGet(quotientReader.ramBytesUsed());
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -277,6 +281,7 @@ private BinaryDocValues loadBinary(FieldInfo field) throws IOException {
     final PagedBytes.Reader bytesReader = bytes.freeze(true);
     if (entry.minLength == entry.maxLength) {
       final int fixedLength = entry.minLength;
+      ramBytesUsed.addAndGet(bytes.ramBytesUsed());
       return new BinaryDocValues() {
         @Override
         public void get(int docID, BytesRef result) {
@@ -286,6 +291,7 @@ public void get(int docID, BytesRef result) {
     } else {
       data.seek(data.getFilePointer() + entry.missingBytes);
       final MonotonicBlockPackedReader addresses = new MonotonicBlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, maxDoc, false);
+      ramBytesUsed.addAndGet(bytes.ramBytesUsed() + addresses.ramBytesUsed());
       return new BinaryDocValues() {
         @Override
         public void get(int docID, BytesRef result) {
@@ -309,6 +315,7 @@ public SortedDocValues getSorted(FieldInfo field) throws IOException {
       if (instance == null) {
         data.seek(entry.offset);
         instance = new FST<Long>(data, PositiveIntOutputs.getSingleton());
+        ramBytesUsed.addAndGet(instance.sizeInBytes());
         fstInstances.put(field.number, instance);
       }
     }
@@ -383,6 +390,7 @@ public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
       if (instance == null) {
         data.seek(entry.offset);
         instance = new FST<Long>(data, PositiveIntOutputs.getSingleton());
+        ramBytesUsed.addAndGet(instance.sizeInBytes());
         fstInstances.put(field.number, instance);
       }
     }
diff --git a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
index 9c3143e9..9df57782 100644
--- a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
+++ b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesProducer;
@@ -62,10 +63,13 @@
   private final Map<Integer,SortedDocValues> sortedInstances = 
       new HashMap<Integer,SortedDocValues>();
   
+  private final AtomicLong ramBytesUsed;
+
   Lucene40DocValuesReader(SegmentReadState state, String filename, String legacyKey) throws IOException {
     this.state = state;
     this.legacyKey = legacyKey;
     this.dir = new CompoundFileDirectory(state.directory, filename, state.context, false);
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOf(getClass()));
   }
   
   @Override
@@ -128,6 +132,7 @@ private NumericDocValues loadVarIntsField(FieldInfo field, IndexInput input) thr
       for (int i = 0; i < values.length; i++) {
         values[i] = input.readLong();
       }
+      ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
       return new NumericDocValues() {
         @Override
         public long get(int docID) {
@@ -138,6 +143,7 @@ public long get(int docID) {
       final long minValue = input.readLong();
       final long defaultValue = input.readLong();
       final PackedInts.Reader reader = PackedInts.getReader(input);
+      ramBytesUsed.addAndGet(reader.ramBytesUsed());
       return new NumericDocValues() {
         @Override
         public long get(int docID) {
@@ -165,6 +171,7 @@ private NumericDocValues loadByteField(FieldInfo field, IndexInput input) throws
     int maxDoc = state.segmentInfo.getDocCount();
     final byte values[] = new byte[maxDoc];
     input.readBytes(values, 0, values.length);
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -186,6 +193,7 @@ private NumericDocValues loadShortField(FieldInfo field, IndexInput input) throw
     for (int i = 0; i < values.length; i++) {
       values[i] = input.readShort();
     }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -207,6 +215,7 @@ private NumericDocValues loadIntField(FieldInfo field, IndexInput input) throws
     for (int i = 0; i < values.length; i++) {
       values[i] = input.readInt();
     }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -228,6 +237,7 @@ private NumericDocValues loadLongField(FieldInfo field, IndexInput input) throws
     for (int i = 0; i < values.length; i++) {
       values[i] = input.readLong();
     }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -249,6 +259,7 @@ private NumericDocValues loadFloatField(FieldInfo field, IndexInput input) throw
     for (int i = 0; i < values.length; i++) {
       values[i] = input.readInt();
     }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -270,6 +281,7 @@ private NumericDocValues loadDoubleField(FieldInfo field, IndexInput input) thro
     for (int i = 0; i < values.length; i++) {
       values[i] = input.readLong();
     }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -319,6 +331,7 @@ private BinaryDocValues loadBytesFixedStraight(FieldInfo field) throws IOExcepti
         throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
       }
       success = true;
+      ramBytesUsed.addAndGet(bytes.ramBytesUsed());
       return new BinaryDocValues() {
         @Override
         public void get(int docID, BytesRef result) {
@@ -361,6 +374,7 @@ private BinaryDocValues loadBytesVarStraight(FieldInfo field) throws IOException
         throw new CorruptIndexException("did not read all bytes from file \"" + indexName + "\": read " + index.getFilePointer() + " vs size " + index.length() + " (resource: " + index + ")");
       }
       success = true;
+      ramBytesUsed.addAndGet(bytes.ramBytesUsed() + reader.ramBytesUsed());
       return new BinaryDocValues() {
         @Override
         public void get(int docID, BytesRef result) {
@@ -406,6 +420,7 @@ private BinaryDocValues loadBytesFixedDeref(FieldInfo field) throws IOException
       if (index.getFilePointer() != index.length()) {
         throw new CorruptIndexException("did not read all bytes from file \"" + indexName + "\": read " + index.getFilePointer() + " vs size " + index.length() + " (resource: " + index + ")");
       }
+      ramBytesUsed.addAndGet(bytes.ramBytesUsed() + reader.ramBytesUsed());
       success = true;
       return new BinaryDocValues() {
         @Override
@@ -450,6 +465,7 @@ private BinaryDocValues loadBytesVarDeref(FieldInfo field) throws IOException {
       if (index.getFilePointer() != index.length()) {
         throw new CorruptIndexException("did not read all bytes from file \"" + indexName + "\": read " + index.getFilePointer() + " vs size " + index.length() + " (resource: " + index + ")");
       }
+      ramBytesUsed.addAndGet(bytes.ramBytesUsed() + reader.ramBytesUsed());
       success = true;
       return new BinaryDocValues() {
         @Override
@@ -533,6 +549,7 @@ private SortedDocValues loadBytesFixedSorted(FieldInfo field, IndexInput data, I
     bytes.copy(data, fixedLength * (long) valueCount);
     final PagedBytes.Reader bytesReader = bytes.freeze(true);
     final PackedInts.Reader reader = PackedInts.getReader(index);
+    ramBytesUsed.addAndGet(bytes.ramBytesUsed() + reader.ramBytesUsed());
     
     return correctBuggyOrds(new SortedDocValues() {
       @Override
@@ -568,6 +585,7 @@ private SortedDocValues loadBytesVarSorted(FieldInfo field, IndexInput data, Ind
     final PackedInts.Reader ordsReader = PackedInts.getReader(index);
     
     final int valueCount = addressReader.size() - 1;
+    ramBytesUsed.addAndGet(bytes.ramBytesUsed() + addressReader.ramBytesUsed() + ordsReader.ramBytesUsed());
     
     return correctBuggyOrds(new SortedDocValues() {
       @Override
@@ -634,6 +652,6 @@ public void close() throws IOException {
 
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(this);
+    return ramBytesUsed.get();
   }
 }
diff --git a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
index c7117a19..ff555690 100644
--- a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
+++ b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesProducer;
@@ -73,7 +74,7 @@
       new HashMap<Integer,FST<Long>>();
   
   private final int maxDoc;
-  
+  private final AtomicLong ramBytesUsed;
   
   static final byte NUMBER = 0;
   static final byte BYTES = 1;
@@ -96,6 +97,7 @@
     // read in the entries from the metadata file.
     IndexInput in = state.directory.openInput(metaName, state.context);
     boolean success = false;
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
     final int version;
     try {
       version = CodecUtil.checkHeader(in, metaCodec, 
@@ -190,7 +192,7 @@ public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOExcept
   
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(this);
+    return ramBytesUsed.get();
   }
   
   private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
@@ -209,6 +211,7 @@ private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
         final int formatID = data.readVInt();
         final int bitsPerValue = data.readVInt();
         final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), entry.packedIntsVersion, maxDoc, bitsPerValue);
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -218,15 +221,12 @@ public long get(int docID) {
       case DELTA_COMPRESSED:
         final int blockSize = data.readVInt();
         final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, blockSize, maxDoc, false);
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return reader.get(docID);
-          }
-        };
+        ramBytesUsed.addAndGet(reader.ramBytesUsed());
+        return reader;
       case UNCOMPRESSED:
         final byte bytes[] = new byte[maxDoc];
         data.readBytes(bytes, 0, bytes.length);
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -238,6 +238,7 @@ public long get(int docID) {
         final long mult = data.readLong();
         final int quotientBlockSize = data.readVInt();
         final BlockPackedReader quotientReader = new BlockPackedReader(data, entry.packedIntsVersion, quotientBlockSize, maxDoc, false);
+        ramBytesUsed.addAndGet(quotientReader.ramBytesUsed());
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -267,6 +268,7 @@ private BinaryDocValues loadBinary(FieldInfo field) throws IOException {
     final PagedBytes.Reader bytesReader = bytes.freeze(true);
     if (entry.minLength == entry.maxLength) {
       final int fixedLength = entry.minLength;
+      ramBytesUsed.addAndGet(bytes.ramBytesUsed());
       return new BinaryDocValues() {
         @Override
         public void get(int docID, BytesRef result) {
@@ -275,6 +277,7 @@ public void get(int docID, BytesRef result) {
       };
     } else {
       final MonotonicBlockPackedReader addresses = new MonotonicBlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, maxDoc, false);
+      ramBytesUsed.addAndGet(bytes.ramBytesUsed() + addresses.ramBytesUsed());
       return new BinaryDocValues() {
         @Override
         public void get(int docID, BytesRef result) {
@@ -295,6 +298,7 @@ public SortedDocValues getSorted(FieldInfo field) throws IOException {
       if (instance == null) {
         data.seek(entry.offset);
         instance = new FST<Long>(data, PositiveIntOutputs.getSingleton());
+        ramBytesUsed.addAndGet(instance.sizeInBytes());
         fstInstances.put(field.number, instance);
       }
     }
@@ -369,6 +373,7 @@ public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
       if (instance == null) {
         data.seek(entry.offset);
         instance = new FST<Long>(data, PositiveIntOutputs.getSingleton());
+        ramBytesUsed.addAndGet(instance.sizeInBytes());
         fstInstances.put(field.number, instance);
       }
     }
