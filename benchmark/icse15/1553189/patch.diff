diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansDriver.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansDriver.java
index 33fc8107..ece3114c 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansDriver.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansDriver.java
@@ -407,7 +407,6 @@ public static void configureOptionsForWorkers(Configuration conf,
    * @param output the directory pathname for output points.
    * @return 0 on success, -1 on failure.
    */
-  @SuppressWarnings("unchecked")
   public static int run(Configuration conf, Path input, Path output)
       throws IOException, InterruptedException, ClassNotFoundException, ExecutionException {
     log.info("Starting StreamingKMeans clustering for vectors in {}; results are output to {}",
@@ -455,7 +454,6 @@ private static int runSequentially(Configuration conf, Path input, Path output)
     return 0;
   }
 
-  @SuppressWarnings("unchecked")
   public static int runMapReduce(Configuration conf, Path input, Path output)
     throws IOException, ClassNotFoundException, InterruptedException {
     // Prepare Job for submission.
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansThread.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansThread.java
index 55bdd5ea..acb2b56d 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansThread.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansThread.java
@@ -21,7 +21,6 @@
 import java.util.List;
 import java.util.concurrent.Callable;
 
-import com.google.common.collect.Iterators;
 import com.google.common.collect.Lists;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
@@ -60,28 +59,32 @@ public StreamingKMeansThread(Iterable<Centroid> dataPoints, Configuration conf)
         StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF);
 
     Iterator<Centroid> dataPointsIterator = dataPoints.iterator();
-    List<Centroid> dataPointsList = Lists.newArrayList();
+
     if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
       List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
       while (dataPointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
         Centroid centroid = dataPointsIterator.next();
         estimatePoints.add(centroid);
-        dataPointsList.add(centroid);
       }
 
       if (log.isInfoEnabled()) {
         log.info("Estimated Points: {}", estimatePoints.size());
       }
       estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
-
-    } else {
-      Iterators.addAll(dataPointsList, dataPointsIterator);
     }
 
     StreamingKMeans streamingKMeans = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
-    for (Centroid aDataPoints : dataPointsList) {
-      streamingKMeans.cluster(aDataPoints);
+
+    // datapointsIterator could be empty if no estimate distance was initially provided
+    // hence creating the iterator again here for the clustering
+    if (!dataPointsIterator.hasNext()) {
+      dataPointsIterator = dataPoints.iterator();
     }
+
+    while (dataPointsIterator.hasNext()) {
+      streamingKMeans.cluster(dataPointsIterator.next());
+    }
+
     streamingKMeans.reindexCentroids();
     return streamingKMeans;
   }
