diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/join/src/java/org/apache/lucene/search/join/BlockJoinQuery.java b/lucene/dev/branches/branch_3x/lucene/contrib/join/src/java/org/apache/lucene/search/join/BlockJoinQuery.java
index af80d45d..ea647399 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/join/src/java/org/apache/lucene/search/join/BlockJoinQuery.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/join/src/java/org/apache/lucene/search/join/BlockJoinQuery.java
@@ -193,7 +193,7 @@ public boolean scoresDocsOutOfOrder() {
     private final Scorer childScorer;
     private final FixedBitSet parentBits;
     private final ScoreMode scoreMode;
-    private int parentDoc;
+    private int parentDoc = -1;
     private float parentScore;
     private int nextChildDoc;
 
@@ -326,8 +326,15 @@ public int advance(int parentTarget) throws IOException {
         return parentDoc = NO_MORE_DOCS;
       }
 
-      // Every parent must have at least one child:
-      assert parentTarget != 0;
+      if (parentTarget == 0) {
+        // Callers should only be passing in a docID from
+        // the parent space, so this means this parent
+        // has no children (it got docID 0), so it cannot
+        // possibly match.  We must handle this case
+        // separately otherwise we pass invalid -1 to
+        // prevSetBit below:
+        return nextDoc();
+      }
 
       final int prevParentDoc = parentBits.prevSetBit(parentTarget-1);
 
diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/join/src/test/org/apache/lucene/search/TestBlockJoin.java b/lucene/dev/branches/branch_3x/lucene/contrib/join/src/test/org/apache/lucene/search/TestBlockJoin.java
index 27627a95..bddcfd56 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/join/src/test/org/apache/lucene/search/TestBlockJoin.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/join/src/test/org/apache/lucene/search/TestBlockJoin.java
@@ -21,10 +21,12 @@
 import java.util.Arrays;
 import java.util.List;
 
+import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericField;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LogDocMergePolicy;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause.Occur;
@@ -84,7 +86,7 @@ public void testSimple() throws Exception {
 
     IndexReader r = w.getReader();
     w.close();
-    IndexSearcher s = new IndexSearcher(r);
+    IndexSearcher s = newSearcher(r);
 
     // Create a filter that defines "parent" documents in the index - in this case resumes
     Filter parentsFilter = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("docType", "resume"))));
@@ -126,6 +128,7 @@ public void testSimple() throws Exception {
     Document parentDoc = s.doc(group.groupValue);
     assertEquals("Lisa", parentDoc.get("name"));
 
+    s.close();
     r.close();
     dir.close();
   }
@@ -281,10 +284,10 @@ public void testRandom() throws Exception {
       }
     }
 
-    final IndexSearcher s = new IndexSearcher(r);
+    final IndexSearcher s = newSearcher(r);
     s.setDefaultFieldSortScoring(true, true);
 
-    final IndexSearcher joinS = new IndexSearcher(joinR);
+    final IndexSearcher joinS = newSearcher(joinR);
 
     final Filter parentsFilter = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("isParent", "x"))));
 
@@ -454,7 +457,9 @@ public void testRandom() throws Exception {
       }
     }
 
+    s.close();
     r.close();
+    joinS.close();
     joinR.close();
     dir.close();
     joinDir.close();
@@ -515,7 +520,7 @@ public void testMultiChildTypes() throws Exception {
 
     IndexReader r = w.getReader();
     w.close();
-    IndexSearcher s = new IndexSearcher(r);
+    IndexSearcher s = newSearcher(r);
 
     // Create a filter that defines "parent" documents in the index - in this case resumes
     Filter parentsFilter = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("docType", "resume"))));
@@ -586,6 +591,67 @@ public void testMultiChildTypes() throws Exception {
     assertEquals("Lisa", parentDoc.get("name"));
 
 
+    s.close();
+    r.close();
+    dir.close();
+  }
+
+  public void testAdvanceSingleParentSingleChild() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random, dir);
+    Document childDoc = new Document();
+    childDoc.add(newField("child", "1", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    Document parentDoc = new Document();
+    parentDoc.add(newField("parent", "1", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    w.addDocuments(Arrays.asList(new Document[] {childDoc, parentDoc}));
+    IndexReader r = w.getReader();
+    w.close();
+    IndexSearcher s = newSearcher(r);
+    Query tq = new TermQuery(new Term("child", "1"));
+    Filter parentFilter = new CachingWrapperFilter(
+                            new QueryWrapperFilter(
+                              new TermQuery(new Term("parent", "1"))));
+
+    BlockJoinQuery q = new BlockJoinQuery(tq, parentFilter, BlockJoinQuery.ScoreMode.Avg);
+    Weight weight = s.createNormalizedWeight(q);
+    DocIdSetIterator disi = weight.scorer(s.getIndexReader().getSequentialSubReaders()[0], true, true);
+    assertEquals(1, disi.advance(1));
+    s.close();
+    r.close();
+    dir.close();
+  }
+
+  public void testAdvanceSingleParentNoChild() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(new LogDocMergePolicy()));
+    Document parentDoc = new Document();
+    parentDoc.add(newField("parent", "1", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    parentDoc.add(newField("isparent", "yes", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    w.addDocuments(Arrays.asList(new Document[] {parentDoc}));
+
+    // Add another doc so scorer is not null
+    parentDoc = new Document();
+    parentDoc.add(newField("parent", "2", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    parentDoc.add(newField("isparent", "yes", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    Document childDoc = new Document();
+    childDoc.add(newField("child", "2", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    w.addDocuments(Arrays.asList(new Document[] {childDoc, parentDoc}));
+
+    // Need single seg:
+    w.forceMerge(1);
+    IndexReader r = w.getReader();
+    w.close();
+    IndexSearcher s = newSearcher(r);
+    Query tq = new TermQuery(new Term("child", "2"));
+    Filter parentFilter = new CachingWrapperFilter(
+                            new QueryWrapperFilter(
+                              new TermQuery(new Term("isparent", "yes"))));
+
+    BlockJoinQuery q = new BlockJoinQuery(tq, parentFilter, BlockJoinQuery.ScoreMode.Avg);
+    Weight weight = s.createNormalizedWeight(q);
+    DocIdSetIterator disi = weight.scorer(s.getIndexReader().getSequentialSubReaders()[0], true, true);
+    assertEquals(2, disi.advance(0));
+    s.close();
     r.close();
     dir.close();
   }
