diff --git a/lucene/dev/trunk/lucene/backwards/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java b/lucene/dev/trunk/lucene/backwards/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
index 871d1891..d56ce917 100644
--- a/lucene/dev/trunk/lucene/backwards/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
+++ b/lucene/dev/trunk/lucene/backwards/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
@@ -18,8 +18,6 @@
  */
 
 import java.io.IOException;
-import java.text.Collator;
-import java.util.Locale;
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
@@ -66,9 +64,7 @@ public void testRangeFilterId() throws IOException {
     Query q = new TermQuery(new Term("body","body"));
 
     // test id, bounded on both ends
-    FieldCacheRangeFilter fcrf;
-    result = search.search(q,fcrf = FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,T,T), numDocs).scoreDocs;
-    assertTrue(fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q, FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
     result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,T,F), numDocs).scoreDocs;
@@ -213,9 +209,7 @@ public void testFieldCacheRangeFilterShorts() throws IOException {
     Query q = new TermQuery(new Term("body","body"));
 
     // test id, bounded on both ends
-    FieldCacheRangeFilter fcrf;
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newShortRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
-    assertTrue(fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newShortRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
     result = search.search(q,FieldCacheRangeFilter.newShortRange("id",minIdO,maxIdO,T,F), numDocs).scoreDocs;
@@ -305,9 +299,7 @@ public void testFieldCacheRangeFilterInts() throws IOException {
 
     // test id, bounded on both ends
         
-    FieldCacheRangeFilter fcrf;
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newIntRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
-    assertTrue(fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newIntRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
     result = search.search(q,FieldCacheRangeFilter.newIntRange("id",minIdO,maxIdO,T,F), numDocs).scoreDocs;
@@ -397,9 +389,7 @@ public void testFieldCacheRangeFilterLongs() throws IOException {
 
     // test id, bounded on both ends
         
-    FieldCacheRangeFilter fcrf;
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newLongRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
-    assertTrue(fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newLongRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
     result = search.search(q,FieldCacheRangeFilter.newLongRange("id",minIdO,maxIdO,T,F), numDocs).scoreDocs;
@@ -529,7 +519,7 @@ public void testFieldCacheRangeFilterDoubles() throws IOException {
     assertEquals("infinity special case", 0, result.length);
   }
   
-  // test using a sparse index (with deleted docs). The DocIdSet should be not cacheable, as it uses TermDocs if the range contains 0
+  // test using a sparse index (with deleted docs).
   public void testSparseIndex() throws IOException {
     RAMDirectory dir = new RAMDirectory();
     IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), T, IndexWriter.MaxFieldLength.LIMITED);
@@ -550,27 +540,21 @@ public void testSparseIndex() throws IOException {
     assertTrue(reader.hasDeletions());
 
     ScoreDoc[] result;
-    FieldCacheRangeFilter fcrf;
     Query q = new TermQuery(new Term("body","body"));
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
-    assertFalse("DocIdSet must be not cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
     assertEquals("find all", 40, result.length);
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) 0),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
-    assertFalse("DocIdSet must be not cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) 0),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
     assertEquals("find all", 20, result.length);
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) 0),T,T), 100).scoreDocs;
-    assertFalse("DocIdSet must be not cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) 0),T,T), 100).scoreDocs;
     assertEquals("find all", 20, result.length);
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) 10),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
-    assertTrue("DocIdSet must be cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) 10),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
     assertEquals("find all", 11, result.length);
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) -10),T,T), 100).scoreDocs;
-    assertTrue("DocIdSet must be cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) -10),T,T), 100).scoreDocs;
     assertEquals("find all", 11, result.length);
   }
   
diff --git a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/CheckIndex.java
index 5ece0f60..ab862142 100644
--- a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/CheckIndex.java
@@ -282,7 +282,7 @@ public Status checkIndex() throws IOException {
     return checkIndex(null);
   }
 
-  protected Status checkIndex(List<String> onlySegments) throws IOException {
+  public Status checkIndex(List<String> onlySegments) throws IOException {
     return checkIndex(onlySegments, CodecProvider.getDefault());
   }
   
@@ -298,7 +298,7 @@ protected Status checkIndex(List<String> onlySegments) throws IOException {
    *  <p><b>WARNING</b>: make sure
    *  you only call this when the index is not opened by any
    *  writer. */
-  protected Status checkIndex(List<String> onlySegments, CodecProvider codecs) throws IOException {
+  public Status checkIndex(List<String> onlySegments, CodecProvider codecs) throws IOException {
     NumberFormat nf = NumberFormat.getInstance();
     SegmentInfos sis = new SegmentInfos();
     Status result = new Status();
diff --git a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
index d111fbfe..598b53d0 100644
--- a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -1050,8 +1050,12 @@ private final synchronized boolean applyDeletes(IndexReader reader, int docIDSta
 
     // Delete by term
     if (deletesFlushed.terms.size() > 0) {
-      try {
         Fields fields = reader.fields();
+      if (fields == null) {
+        // This reader has no postings
+        return false;
+      }
+
         TermsEnum termsEnum = null;
         
         String currentField = null;
@@ -1098,10 +1102,8 @@ private final synchronized boolean applyDeletes(IndexReader reader, int docIDSta
             }
           }
         }
-      } finally {
-        //docs.close();
-      }
     }
+
     // Delete by docID
     for (Integer docIdInt : deletesFlushed.docIDs) {
       int docID = docIdInt.intValue();
diff --git a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/MultiFields.java b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/MultiFields.java
index d0f1c6ac..1a26f8b6 100644
--- a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/MultiFields.java
+++ b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/MultiFields.java
@@ -77,9 +77,12 @@ public static Fields getFields(IndexReader r) throws IOException {
         new ReaderUtil.Gather(r) {
           @Override
           protected void add(int base, IndexReader r) throws IOException {
-            fields.add(r.fields());
+            final Fields f = r.fields();
+            if (f != null) {
+              fields.add(f);
             slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));
           }
+          }
         }.run();
 
         if (fields.size() == 0) {
diff --git a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
index 4369901c..47a383ef 100644
--- a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -572,12 +572,15 @@ private final void mergeTerms() throws CorruptIndexException, IOException {
       docBase = new ReaderUtil.Gather(readers.get(i)) {
           @Override
           protected void add(int base, IndexReader r) throws IOException {
+            final Fields f = r.fields();
+            if (f != null) {
             subReaders.add(r);
-            fields.add(r.fields());
+              fields.add(f);
             slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));
             bits.add(r.getDeletedDocs());
             bitsStarts.add(base);
           }
+          }
         }.run(docBase);
     }
 
diff --git a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
index 5e98be5a..f94d6bc9 100644
--- a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
+++ b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
@@ -26,9 +26,6 @@
 import org.apache.lucene.index.codecs.CodecProvider;
 
 /**
- * This class is not meant for public usage; it's only
- * public in order to expose access across packages.  It's
- * used internally when updating the index.
  * @lucene.experimental
  */
 public class SegmentWriteState {
diff --git a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/codecs/sep/SingleIntIndexInput.java b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/codecs/sep/SingleIntIndexInput.java
index c2e03c68..482036b8 100644
--- a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/codecs/sep/SingleIntIndexInput.java
+++ b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/codecs/sep/SingleIntIndexInput.java
@@ -66,17 +66,13 @@ public int next() throws IOException {
   
   class Index extends IntIndexInput.Index {
     private long fp;
-    // nocmmit: only for asserts
-    boolean first = true;
 
     @Override
     public void read(IndexInput indexIn, boolean absolute)
       throws IOException {
       if (absolute) {
         fp = indexIn.readVLong();
-        first = false;
       } else {
-        assert !first;
         fp += indexIn.readVLong();
       }
     }
@@ -84,7 +80,6 @@ public void read(IndexInput indexIn, boolean absolute)
     @Override
     public void set(IntIndexInput.Index other) {
       fp = ((Index) other).fp;
-      first = false;
     }
 
     @Override
@@ -100,7 +95,6 @@ public String toString() {
     @Override
     public Object clone() {
       Index other = new Index();
-      other.first = first;
       other.fp = fp;
       return other;
     }
diff --git a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
index 8003c81d..b280ee8a 100644
--- a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
+++ b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
@@ -504,7 +504,7 @@ public final int hashCode() {
   
   static abstract class FieldCacheDocIdSet extends DocIdSet {
     private final IndexReader reader;
-    private boolean canIgnoreDeletedDocs;
+    private final boolean canIgnoreDeletedDocs;
 
     FieldCacheDocIdSet(IndexReader reader, boolean canIgnoreDeletedDocs) {
       this.reader = reader;
@@ -518,33 +518,21 @@ public final int hashCode() {
     abstract boolean matchDoc(int doc) throws ArrayIndexOutOfBoundsException;
 
     /**
-     * this DocIdSet is cacheable, if it can ignore deletions
+     * this DocIdSet is always cacheable (does not go back
+     * to the reader for iteration)
      */
     @Override
     public boolean isCacheable() {
-      return canIgnoreDeletedDocs || !reader.hasDeletions();
+      return true;
     }
 
     @Override
     public DocIdSetIterator iterator() throws IOException {
-      // Synchronization needed because deleted docs BitVector
-      // can change after call to hasDeletions until TermDocs creation.
-      // We only use an iterator with termDocs, when this was requested (e.g.
-      // range contains 0)
-      // and the index has deletions
 
-      final Bits skipDocs;
-      synchronized (reader) {
-        if (isCacheable()) {
-          skipDocs = null;
-        } else {
-          skipDocs = MultiFields.getDeletedDocs(reader);
-        }
-      }
-      final int maxDoc = reader.maxDoc();
+      final Bits skipDocs = canIgnoreDeletedDocs ? null : MultiFields.getDeletedDocs(reader);
 
-      // a DocIdSetIterator generating docIds by
-      // incrementing a variable & checking skipDocs -
+      if (skipDocs == null) {
+        // Specialization optimization disregard deletions
       return new DocIdSetIterator() {
         private int doc = -1;
         @Override
@@ -557,8 +545,7 @@ public int nextDoc() {
           try {
             do {
               doc++;
-            } while ((skipDocs != null && doc < maxDoc && skipDocs.get(doc))
-                || !matchDoc(doc));
+              } while (!matchDoc(doc));
             return doc;
           } catch (ArrayIndexOutOfBoundsException e) {
             return doc = NO_MORE_DOCS;
@@ -576,10 +563,44 @@ public int advance(int target) {
           } catch (ArrayIndexOutOfBoundsException e) {
             return doc = NO_MORE_DOCS;
           }
-
         }
       };
+      } else {
+        // Must consult deletions
+
+        final int maxDoc = reader.maxDoc();
+
+        // a DocIdSetIterator generating docIds by
+        // incrementing a variable & checking skipDocs -
+        return new DocIdSetIterator() {
+          private int doc = -1;
+          @Override
+            public int docID() {
+            return doc;
     }
+        
+          @Override
+          public int nextDoc() {
+            do {
+              doc++;
+              if (doc >= maxDoc) {
+                return doc = NO_MORE_DOCS;
+              }
+            } while (skipDocs.get(doc) || !matchDoc(doc));
+            return doc;
   }
 
+          @Override
+          public int advance(int target) {
+            for(doc=target;doc<maxDoc;doc++) {
+              if (!skipDocs.get(doc) && matchDoc(doc)) {
+                return doc;
+              }
+            }
+            return doc = NO_MORE_DOCS;
+          }
+        };
+      }
+    }
+  }
 }
diff --git a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/search/TermScorer.java b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/search/TermScorer.java
index 6c7ff6bc..efec1ab6 100644
--- a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/search/TermScorer.java
+++ b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/search/TermScorer.java
@@ -160,7 +160,7 @@ public int advance(int target) throws IOException {
     // not found in readahead cache, seek underlying stream
     int newDoc = docsEnum.advance(target);
     //System.out.println("ts.advance docsEnum=" + docsEnum);
-    if (newDoc != DocsEnum.NO_MORE_DOCS) {
+    if (newDoc != NO_MORE_DOCS) {
       doc = newDoc;
       freq = docsEnum.freq();
     } else {
diff --git a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/util/UnicodeUtil.java b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/util/UnicodeUtil.java
index 109b1888..3995a44f 100644
--- a/lucene/dev/trunk/lucene/src/java/org/apache/lucene/util/UnicodeUtil.java
+++ b/lucene/dev/trunk/lucene/src/java/org/apache/lucene/util/UnicodeUtil.java
@@ -79,12 +79,6 @@
     public int[] offsets = new int[10];
     public int length;
 
-    /*
-    public String toString() {
-      return new String(result, 0, length);
-    }
-    */
-
     public void setLength(int newLength) {
       if (result.length < newLength)
         result = ArrayUtil.grow(result, newLength);
diff --git a/lucene/dev/trunk/lucene/src/test/org/apache/lucene/index/TestIndexReader.java b/lucene/dev/trunk/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
index e1cac39d..a2a55d11 100644
--- a/lucene/dev/trunk/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
+++ b/lucene/dev/trunk/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
@@ -1570,7 +1570,6 @@ public void testNoDupCommitFileNames() throws Throwable {
   // LUCENE-1579: Ensure that on a cloned reader, segments
   // reuse the doc values arrays in FieldCache
   public void testFieldCacheReuseAfterClone() throws Exception {
-    //Codec.DEBUG = true;
     Directory dir = new MockRAMDirectory();
     IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));
     Document doc = new Document();
diff --git a/lucene/dev/trunk/lucene/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java b/lucene/dev/trunk/lucene/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
index 21cdef2a..871a3d79 100644
--- a/lucene/dev/trunk/lucene/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
+++ b/lucene/dev/trunk/lucene/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
@@ -66,9 +66,7 @@ public void testRangeFilterId() throws IOException {
     Query q = new TermQuery(new Term("body","body"));
 
     // test id, bounded on both ends
-    FieldCacheRangeFilter<String> fcrf;
-    result = search.search(q,fcrf = FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,T,T), numDocs).scoreDocs;
-    assertTrue(fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q, FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
     result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,T,F), numDocs).scoreDocs;
@@ -213,9 +211,7 @@ public void testFieldCacheRangeFilterShorts() throws IOException {
     Query q = new TermQuery(new Term("body","body"));
 
     // test id, bounded on both ends
-    FieldCacheRangeFilter<Short> fcrf;
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newShortRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
-    assertTrue(fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newShortRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
     result = search.search(q,FieldCacheRangeFilter.newShortRange("id",minIdO,maxIdO,T,F), numDocs).scoreDocs;
@@ -305,9 +301,7 @@ public void testFieldCacheRangeFilterInts() throws IOException {
 
     // test id, bounded on both ends
         
-    FieldCacheRangeFilter<Integer> fcrf;
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newIntRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
-    assertTrue(fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newIntRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
     result = search.search(q,FieldCacheRangeFilter.newIntRange("id",minIdO,maxIdO,T,F), numDocs).scoreDocs;
@@ -397,9 +391,7 @@ public void testFieldCacheRangeFilterLongs() throws IOException {
 
     // test id, bounded on both ends
         
-    FieldCacheRangeFilter<Long> fcrf;
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newLongRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
-    assertTrue(fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newLongRange("id",minIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
     result = search.search(q,FieldCacheRangeFilter.newLongRange("id",minIdO,maxIdO,T,F), numDocs).scoreDocs;
@@ -529,7 +521,7 @@ public void testFieldCacheRangeFilterDoubles() throws IOException {
     assertEquals("infinity special case", 0, result.length);
   }
   
-  // test using a sparse index (with deleted docs). The DocIdSet should be not cacheable, as it uses TermDocs if the range contains 0
+  // test using a sparse index (with deleted docs).
   public void testSparseIndex() throws IOException {
     RAMDirectory dir = new RAMDirectory();
     IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));
@@ -550,27 +542,21 @@ public void testSparseIndex() throws IOException {
     assertTrue(reader.hasDeletions());
 
     ScoreDoc[] result;
-    FieldCacheRangeFilter<Byte> fcrf;
     Query q = new TermQuery(new Term("body","body"));
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
-    assertFalse("DocIdSet must be not cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
     assertEquals("find all", 40, result.length);
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) 0),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
-    assertFalse("DocIdSet must be not cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) 0),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
     assertEquals("find all", 20, result.length);
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) 0),T,T), 100).scoreDocs;
-    assertFalse("DocIdSet must be not cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) 0),T,T), 100).scoreDocs;
     assertEquals("find all", 20, result.length);
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) 10),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
-    assertTrue("DocIdSet must be cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) 10),Byte.valueOf((byte) 20),T,T), 100).scoreDocs;
     assertEquals("find all", 11, result.length);
 
-    result = search.search(q,fcrf=FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) -10),T,T), 100).scoreDocs;
-    assertTrue("DocIdSet must be cacheable", fcrf.getDocIdSet(reader.getSequentialSubReaders()[0]).isCacheable());
+    result = search.search(q,FieldCacheRangeFilter.newByteRange("id",Byte.valueOf((byte) -20),Byte.valueOf((byte) -10),T,T), 100).scoreDocs;
     assertEquals("find all", 11, result.length);
   }
   
