diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java
index f403846c..7a9e4afc 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -2615,6 +2615,15 @@ boolean updateDocument(Document doc, Analyzer analyzer, Term delTerm)
       } finally {
         if (!success) {
           synchronized(this) {
+
+            // If this thread state had decided to flush, we
+            // must clear it so another thread can flush
+            if (state.doFlushAfter) {
+              state.doFlushAfter = false;
+              flushPending = false;
+              notifyAll();
+            }
+
             // Immediately mark this document as deleted
             // since likely it was partially added.  This
             // keeps indexing as "all or none" (atomic) when
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java
index f610d05f..917ce03f 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -3159,4 +3159,34 @@ public void testExceptionDocumentsWriterInit() throws IOException {
     _TestUtil.checkIndex(dir);
     dir.close();
   }
+
+  // LUCENE-1208
+  public void testExceptionJustBeforeFlush() throws IOException {
+    MockRAMDirectory dir = new MockRAMDirectory();
+    MockIndexWriter w = new MockIndexWriter(dir, false, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    w.setMaxBufferedDocs(2);
+    Document doc = new Document();
+    doc.add(new Field("field", "a field", Field.Store.YES,
+                      Field.Index.TOKENIZED));
+    w.addDocument(doc);
+
+    Analyzer analyzer = new Analyzer() {
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        return new CrashingFilter(fieldName, new WhitespaceTokenizer(reader));
+      }
+    };
+
+    Document crashDoc = new Document();
+    crashDoc.add(new Field("crash", "do it on token 4", Field.Store.YES,
+                           Field.Index.TOKENIZED));
+    try {
+      w.addDocument(crashDoc, analyzer);
+      fail("did not hit exxpected exception");
+    } catch (IOException ioe) {
+      // expected
+    }
+    w.addDocument(doc);
+    w.close();
+    dir.close();
+  }    
 }
