diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterCountReader.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterCountReader.java
index d816fadb..e4c87bdf 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterCountReader.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterCountReader.java
@@ -17,18 +17,21 @@
 
 package org.apache.mahout.clustering.topdown.postprocessor;
 
-import java.io.IOException;
-import java.util.Iterator;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Writable;
+import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.common.iterator.sequencefile.PathFilters;
 import org.apache.mahout.common.iterator.sequencefile.PathType;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
 
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+
 /**
  * Reads the number of clusters produced by the clustering algorithm.
  */
@@ -40,10 +43,8 @@ private ClusterCountReader() {
   /**
    * Reads the number of clusters present by reading the clusters-*-final file.
    * 
-   * @param clusterOutputPath
-   *          The output path provided to the clustering algorithm.
-   * @param conf
-   *          The hadoop configuration.
+   * @param clusterOutputPath The output path provided to the clustering algorithm.
+   * @param conf              The hadoop configuration.
    * @return the number of final clusters.
    */
   public static int getNumberOfClusters(Path clusterOutputPath, Configuration conf) throws IOException {
@@ -63,4 +64,38 @@ public static int getNumberOfClusters(Path clusterOutputPath, Configuration conf
     return numberOfClusters;
   }
 
+  /**
+   * Generates a list of all cluster ids by reading the clusters-*-final file.
+   *
+   * @param clusterOutputPath The output path provided to the clustering algorithm.
+   * @param conf              The hadoop configuration.
+   * @return An ArrayList containing the final cluster ids.
+   */
+  public static Map<Integer, Integer> getClusterIDs(Path clusterOutputPath, Configuration conf, boolean keyIsClusterId) throws IOException {
+    Map<Integer, Integer> clusterIds = new HashMap<Integer, Integer>();
+    FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);
+    FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());
+    //System.out.println("LOOK HERE: " + clusterOutputPath);
+    Iterator<ClusterWritable> it = new SequenceFileDirValueIterator<ClusterWritable>(clusterFiles[0].getPath(),
+            PathType.LIST,
+            PathFilters.partFilter(),
+            null,
+            true,
+            conf);
+    int i = 0;
+    while (it.hasNext()) {
+      Integer key, value;
+      if (keyIsClusterId == true) { // key is the cluster id, value is i, the index we will use
+        key = it.next().getValue().getId();
+        value = i;
+      } else {
+        key = i;
+        value = it.next().getValue().getId();
+      }
+      clusterIds.put(key, value);
+      i++;
+    }
+    return clusterIds;
+  }
+
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessor.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessor.java
index 750c3cb1..4737827b 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessor.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessor.java
@@ -17,10 +17,6 @@
 
 package org.apache.mahout.clustering.topdown.postprocessor;
 
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -37,14 +33,18 @@
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
 import org.apache.mahout.math.VectorWritable;
 
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
 /**
  * This class reads the output of any clustering algorithm, and, creates separate directories for different
  * clusters. Each cluster directory's name is its clusterId. Each and every point is written in the cluster
  * directory associated with that point.
- * 
+ * <p/>
  * This class incorporates a sequential algorithm and is appropriate for use for data which has been clustered
  * sequentially.
- * 
+ * <p/>
  * The sequential and non sequential version, both are being used from {@link ClusterOutputPostProcessorDriver}.
  */
 public final class ClusterOutputPostProcessor {
@@ -53,9 +53,9 @@
   private final FileSystem fileSystem;
   private final Configuration conf;
   private final Path clusterPostProcessorOutput;
-  private final Map<String,Path> postProcessedClusterDirectories = new HashMap<String,Path>();
+  private final Map<String, Path> postProcessedClusterDirectories = new HashMap<String, Path>();
   private long uniqueVectorId = 0L;
-  private final Map<String,SequenceFile.Writer> writersForClusters;
+  private final Map<String, SequenceFile.Writer> writersForClusters;
   
   public ClusterOutputPostProcessor(Path clusterOutputToBeProcessed,
                                     Path output,
@@ -63,7 +63,7 @@ public ClusterOutputPostProcessor(Path clusterOutputToBeProcessed,
     this.clusterPostProcessorOutput = output;
     this.clusteredPoints = PathDirectory.getClusterOutputClusteredPoints(clusterOutputToBeProcessed);
     this.conf = hadoopConfiguration;
-    this.writersForClusters = new HashMap<String,SequenceFile.Writer>();
+    this.writersForClusters = new HashMap<String, SequenceFile.Writer>();
     fileSystem = clusteredPoints.getFileSystem(conf);    
   }
   
@@ -73,8 +73,8 @@ public ClusterOutputPostProcessor(Path clusterOutputToBeProcessed,
    */
   public void process() throws IOException {
     createPostProcessDirectory();
-    for (Pair<?,WeightedVectorWritable> record
-        : new SequenceFileDirIterable<Writable,WeightedVectorWritable>(clusteredPoints,
+    for (Pair<?, WeightedVectorWritable> record
+            : new SequenceFileDirIterable<Writable, WeightedVectorWritable>(clusteredPoints,
                                                                       PathType.GLOB,
                                                                       PathFilters.partFilter(),
                                                                       null,
@@ -98,7 +98,6 @@ private void createPostProcessDirectory() throws IOException {
   }
   
   /**
-   * 
    * Finds out the cluster directory of the vector and writes it into the specified cluster.
    */
   private void putVectorInRespectiveCluster(String clusterId, WeightedVectorWritable point) throws IOException {
@@ -133,7 +132,7 @@ private void writeVectorToCluster(Writer writer, WeightedVectorWritable point) t
   /**
    * @return the set of all post processed cluster paths.
    */
-  public Map<String,Path> getPostProcessedClusterDirectories() {
+  public Map<String, Path> getPostProcessedClusterDirectories() {
     return postProcessedClusterDirectories;
   }
   
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorDriver.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorDriver.java
index 29622a79..f02224cd 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorDriver.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorDriver.java
@@ -17,13 +17,11 @@
 
 package org.apache.mahout.clustering.topdown.postprocessor;
 
-import java.io.IOException;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
@@ -32,11 +30,14 @@
 import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.common.AbstractJob;
+import org.apache.mahout.common.HadoopUtil;
 import org.apache.mahout.common.commandline.DefaultOptionCreator;
 import org.apache.mahout.common.iterator.sequencefile.PathFilters;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterator;
 import org.apache.mahout.math.VectorWritable;
 
+import java.io.IOException;
+
 /**
  * Post processes the output of clustering algorithms and groups them into respective clusters. Ideal to be
  * used for top down clustering. It can also be used if the clustering output needs to be grouped into their
@@ -50,21 +51,23 @@
    */
   @Override
   public int run(String[] args) throws Exception {
-    
     addInputOption();
     addOutputOption();
     addOption(DefaultOptionCreator.methodOption().create());
+    addOption(DefaultOptionCreator.overwriteOption().create());
 
     if (parseArguments(args) == null) {
       return -1;
     }
-    
     Path input = getInputPath();
     Path output = getOutputPath();
 
     if (getConf() == null) {
       setConf(new Configuration());
     }
+    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
+      HadoopUtil.delete(getConf(), output);
+    }
     boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(
       DefaultOptionCreator.SEQUENTIAL_METHOD);
     run(input, output, runSequential);
@@ -75,7 +78,8 @@ public int run(String[] args) throws Exception {
   /**
    * Constructor to be used by the ToolRunner.
    */
-  private ClusterOutputPostProcessorDriver() {}
+  private ClusterOutputPostProcessorDriver() {
+  }
   
   public static void main(String[] args) throws Exception {
     ToolRunner.run(new Configuration(), new ClusterOutputPostProcessorDriver(), args);
@@ -85,13 +89,10 @@ public static void main(String[] args) throws Exception {
    * Post processes the output of clustering algorithms and groups them into respective clusters. Each
    * cluster's vectors are written into a directory named after its clusterId.
    * 
-   * @param input
-   *          The output path provided to the clustering algorithm, whose would be post processed. Hint : The
+   * @param input         The output path provided to the clustering algorithm, whose would be post processed. Hint : The
    *          path of the directory containing clusters-*-final and clusteredPoints.
-   * @param output
-   *          The post processed data would be stored at this path.
-   * @param runSequential
-   *          If set to true, post processes it sequentially, else, uses. MapReduce. Hint : If the clustering
+   * @param output        The post processed data would be stored at this path.
+   * @param runSequential If set to true, post processes it sequentially, else, uses. MapReduce. Hint : If the clustering
    *          was done sequentially, make it sequential, else vice versa.
    */
   public static void run(Path input, Path output, boolean runSequential) throws IOException,
@@ -111,11 +112,9 @@ public static void run(Path input, Path output, boolean runSequential) throws IO
    * Process Sequentially. Reads the vectors one by one, and puts them into respective directory, named after
    * their clusterId.
    * 
-   * @param input
-   *          The output path provided to the clustering algorithm, whose would be post processed. Hint : The
+   * @param input  The output path provided to the clustering algorithm, whose would be post processed. Hint : The
    *          path of the directory containing clusters-*-final and clusteredPoints.
-   * @param output
-   *          The post processed data would be stored at this path.
+   * @param output The post processed data would be stored at this path.
    */
   private static void postProcessSeq(Path input, Path output) throws IOException {
     ClusterOutputPostProcessor clusterOutputPostProcessor = new ClusterOutputPostProcessor(input, output,
@@ -127,27 +126,26 @@ private static void postProcessSeq(Path input, Path output) throws IOException {
    * Process as a map reduce job. The numberOfReduceTasks is set to the number of clusters present in the
    * output. So that each cluster's vector is written in its own part file.
    * 
-   * @param conf
-   *          The hadoop configuration.
-   * @param input
-   *          The output path provided to the clustering algorithm, whose would be post processed. Hint : The
+   * @param conf   The hadoop configuration.
+   * @param input  The output path provided to the clustering algorithm, whose would be post processed. Hint : The
    *          path of the directory containing clusters-*-final and clusteredPoints.
-   * @param output
-   *          The post processed data would be stored at this path.
+   * @param output The post processed data would be stored at this path.
    */
   private static void postProcessMR(Configuration conf, Path input, Path output) throws IOException,
                                                                                 InterruptedException,
                                                                                 ClassNotFoundException {
+    System.out.println("WARNING: If you are running in Hadoop local mode, please use the --sequential option, as the MapReduce option will not work properly");
+    int numberOfClusters = ClusterCountReader.getNumberOfClusters(input, conf);
+    conf.set("clusterOutputPath", input.toString());
     Job job = new Job(conf, "ClusterOutputPostProcessor Driver running over input: " + input);
     job.setInputFormatClass(SequenceFileInputFormat.class);
     job.setOutputFormatClass(SequenceFileOutputFormat.class);
     job.setMapperClass(ClusterOutputPostProcessorMapper.class);
-    job.setMapOutputKeyClass(Text.class);
+    job.setMapOutputKeyClass(IntWritable.class);
     job.setMapOutputValueClass(VectorWritable.class);
     job.setReducerClass(ClusterOutputPostProcessorReducer.class);
-    job.setOutputKeyClass(Text.class);
+    job.setOutputKeyClass(IntWritable.class);
     job.setOutputValueClass(VectorWritable.class);
-    int numberOfClusters = ClusterCountReader.getNumberOfClusters(input, conf);
     job.setNumReduceTasks(numberOfClusters);
     job.setJarByClass(ClusterOutputPostProcessorDriver.class);
     
@@ -162,16 +160,14 @@ private static void postProcessMR(Configuration conf, Path input, Path output) t
    * The mapreduce version of the post processor writes different clusters into different part files. This
    * method reads the part files and moves them into directories named after their clusterIds.
    * 
-   * @param conf
-   *          The hadoop configuration.
-   * @param output
-   *          The post processed data would be stored at this path.
+   * @param conf   The hadoop configuration.
+   * @param output The post processed data would be stored at this path.
    */
   private static void movePartFilesToRespectiveDirectories(Configuration conf, Path output) throws IOException {
     FileSystem fileSystem = output.getFileSystem(conf);
     for (FileStatus fileStatus : fileSystem.listStatus(output, PathFilters.partFilter())) {
-      SequenceFileIterator<Writable,Writable> it =
-          new SequenceFileIterator<Writable,Writable>(fileStatus.getPath(), true, conf);
+      SequenceFileIterator<Writable, Writable> it =
+              new SequenceFileIterator<Writable, Writable>(fileStatus.getPath(), true, conf);
       if (it.hasNext()) {
         renameFile(it.next().getFirst(), fileStatus, conf);
       }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorMapper.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorMapper.java
index 96e74c86..15cdc6f8 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorMapper.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorMapper.java
@@ -17,26 +17,41 @@
 
 package org.apache.mahout.clustering.topdown.postprocessor;
 
-import java.io.IOException;
-
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.mahout.clustering.classify.WeightedVectorWritable;
 import org.apache.mahout.math.VectorWritable;
 
+import java.io.IOException;
+import java.util.Map;
+
 /**
  * Mapper for post processing cluster output.
  */
 public class ClusterOutputPostProcessorMapper extends
-    Mapper<IntWritable,WeightedVectorWritable,Text,VectorWritable> {
+        Mapper<IntWritable, WeightedVectorWritable, IntWritable, VectorWritable> {
+
+  private Map<Integer, Integer> newClusterMappings;
+  private VectorWritable outputVector;
+
+  //read the current cluster ids, and populate the cluster mapping hash table
+  @Override
+  public void setup(Context context) throws IOException {
+    Configuration conf = context.getConfiguration();
+    //this give the clusters-x-final directory where the cluster ids can be read
+    Path clusterOutputPath = new Path(conf.get("clusterOutputPath"));
+    //we want the key to be the cluster id, the value to be the index
+    newClusterMappings = ClusterCountReader.getClusterIDs(clusterOutputPath, conf, true);
+    outputVector = new VectorWritable();
+  }
   
-  /**
-   * The key is the cluster id and the value is the vector.
-   */
   @Override
-  protected void map(IntWritable key, WeightedVectorWritable vector, Context context) throws IOException,
-                                                                                     InterruptedException {
-    context.write(new Text(key.toString().trim()), new VectorWritable(vector.getVector()));
+  public void map(IntWritable key, WeightedVectorWritable val, Context context) throws IOException, InterruptedException {
+    //by pivoting on the cluster mapping value, we can make sure that each unique cluster goes to it's own reducer, since they
+    //are numbered from 0 to k-1, where k is the number of clusters
+    outputVector.set(val.getVector());
+    context.write(new IntWritable(newClusterMappings.get(key.get())), outputVector);
   }
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorReducer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorReducer.java
index 54936186..cf1f6809 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorReducer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorReducer.java
@@ -17,24 +17,45 @@
 
 package org.apache.mahout.clustering.topdown.postprocessor;
 
-import java.io.IOException;
-
-import org.apache.hadoop.io.Text;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.mapreduce.Reducer;
 import org.apache.mahout.math.VectorWritable;
 
+import java.io.IOException;
+import java.util.Map;
+
 /**
  * Reducer for post processing cluster output.
  */
-public class ClusterOutputPostProcessorReducer extends Reducer<Text,VectorWritable,Text,VectorWritable> {
+public class ClusterOutputPostProcessorReducer extends Reducer<IntWritable, VectorWritable, IntWritable, VectorWritable> {
+
+
+  private Map<Integer, Integer> reverseClusterMappings;
+
+  //read the current cluster ids, and populate the hash cluster mapping hash table
+  @Override
+  public void setup(Context context) throws IOException {
+    Configuration conf = context.getConfiguration();
+    Path clusterOutputPath = new Path(conf.get("clusterOutputPath"));
+    //we want to the key to be the index, the value to be the cluster id
+    reverseClusterMappings = ClusterCountReader.getClusterIDs(clusterOutputPath, conf, false);
+  }
+
   /**
-   * The key is the cluster id and the values contains the points in that cluster.
+   * The key is the remapped cluster id and the values contains the vectors in that cluster.
    */
   @Override
-  protected void reduce(Text key, Iterable<VectorWritable> values, Context context) throws IOException,
+  protected void reduce(IntWritable key, Iterable<VectorWritable> values, Context context) throws IOException,
                                                                                    InterruptedException {
+    //remap the cluster back to its original id
+    //and then output the vectors with their correct
+    //cluster id.
+    IntWritable outKey = new IntWritable(reverseClusterMappings.get(key.get()));
+    System.out.println(outKey + " this: " + this);
     for (VectorWritable value : values) {
-      context.write(key, value);
+      context.write(outKey, value);
     }
   }
   
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DictionaryVectorizer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DictionaryVectorizer.java
index 1e0cca2e..12c601aa 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DictionaryVectorizer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DictionaryVectorizer.java
@@ -52,6 +52,8 @@
 import org.apache.mahout.vectorizer.term.TermCountCombiner;
 import org.apache.mahout.vectorizer.term.TermCountMapper;
 import org.apache.mahout.vectorizer.term.TermCountReducer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * This class converts a set of input documents in the sequence file format to vectors. The Sequence file
@@ -60,6 +62,7 @@
  * This is a dictionary based Vectorizer.
  */
 public final class DictionaryVectorizer implements Vectorizer {
+  private static Logger log = LoggerFactory.getLogger(DictionaryVectorizer.class);
   
   public static final String DOCUMENT_VECTOR_OUTPUT_FOLDER = "tf-vectors";
   public static final String MIN_SUPPORT = "min.support";
@@ -167,6 +170,7 @@ public static void createTermFrequencyVectors(Path input,
     
     int[] maxTermDimension = new int[1];
     List<Path> dictionaryChunks;
+    log.info("Creating dictionary from {} and saving at {}", input, dictionaryJobPath);
     if (maxNGramSize == 1) {
       startWordCounting(input, dictionaryJobPath, baseConf, minSupport);
       dictionaryChunks =
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java
index 53787801..227cc8c9 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java
@@ -17,8 +17,6 @@
 
 package org.apache.mahout.vectorizer;
 
-import java.util.List;
-
 import org.apache.commons.cli2.CommandLine;
 import org.apache.commons.cli2.Group;
 import org.apache.commons.cli2.Option;
@@ -45,6 +43,8 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.List;
+
 /**
  * Converts a given set of sequence files into SparseVectors
  */
@@ -248,7 +248,7 @@ public int run(String[] args) throws Exception {
       if (cmdLine.hasOption(logNormalizeOpt)) {
         logNormalize = true;
       }
-
+      log.info("Tokenizing documents in {}", inputDir);
       Configuration conf = getConf();
       Path tokenizedPath = new Path(outputDir, DocumentProcessor.TOKENIZED_DOCUMENT_OUTPUT_FOLDER);
       //TODO: move this into DictionaryVectorizer , and then fold SparseVectorsFrom with EncodedVectorsFrom
@@ -268,7 +268,7 @@ public int run(String[] args) throws Exception {
       String tfDirName = shouldPrune
           ? DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER + "-toprune"
           : DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER;
-
+      log.info("Creating Term Frequency Vectors");
       if (processIdf) {
         DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath,
                                                         outputDir,
@@ -302,8 +302,9 @@ public int run(String[] args) throws Exception {
       Pair<Long[], List<Path>> docFrequenciesFeatures = null;
       // Should document frequency features be processed
       if (shouldPrune || processIdf) {
+        log.info("Calculating IDF");
         docFrequenciesFeatures =
-            TFIDFConverter.calculateDF(new Path(outputDir, tfDirName),outputDir, conf, chunkSize);
+                TFIDFConverter.calculateDF(new Path(outputDir, tfDirName), outputDir, conf, chunkSize);
       }
 
       long maxDF = maxDFPercent; //if we are pruning by std dev, then this will get changed
@@ -325,7 +326,7 @@ public int run(String[] args) throws Exception {
         Path prunedTFDir = new Path(outputDir, DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER);
         Path prunedPartialTFDir =
             new Path(outputDir, DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER + "-partial");
-
+        log.info("Pruning");
         if (processIdf) {
           HighDFWordsPruner.pruneVectors(tfDir,
                                          prunedTFDir,
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/term/TFPartialVectorReducer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/term/TFPartialVectorReducer.java
index 7d15c244..7b82ed66 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/term/TFPartialVectorReducer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/term/TFPartialVectorReducer.java
@@ -21,6 +21,8 @@
 import com.google.common.io.Closeables;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocalFileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.Text;
@@ -40,6 +42,8 @@
 import org.apache.mahout.math.map.OpenObjectIntHashMap;
 import org.apache.mahout.vectorizer.DictionaryVectorizer;
 import org.apache.mahout.vectorizer.common.PartialVectorMerger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.io.IOException;
 import java.net.URI;
@@ -49,7 +53,7 @@
  * Converts a document in to a sparse vector
  */
 public class TFPartialVectorReducer extends Reducer<Text, StringTuple, Text, VectorWritable> {
-
+  private transient static Logger log = LoggerFactory.getLogger(TFPartialVectorReducer.class);
   private final OpenObjectIntHashMap<String> dictionary = new OpenObjectIntHashMap<String>();
 
   private int dimension;
@@ -119,7 +123,18 @@ protected void setup(Context context) throws IOException, InterruptedException {
     Path[] localFiles = DistributedCache.getLocalCacheFiles(conf);
     Preconditions.checkArgument(localFiles != null && localFiles.length >= 1,
             "missing paths from the DistributedCache");
-
+    LocalFileSystem localFs = FileSystem.getLocal(conf);
+    if (!localFs.exists(localFiles[0])) {
+      log.info("Can't find dictionary dist. cache file, looking in .getCacheFiles");
+      URI[] filesURIs = DistributedCache.getCacheFiles(conf);
+      if (filesURIs == null) {
+        throw new IOException("Cannot read Frequency list from Distributed Cache");
+      }
+      if (filesURIs.length != 1) {
+        throw new IOException("Cannot read Frequency list from Distributed Cache (" + localFiles.length + ')');
+      }
+      localFiles[0] = new Path(filesURIs[0].getPath());
+    }
     dimension = conf.getInt(PartialVectorMerger.DIMENSION, Integer.MAX_VALUE);
     sequentialAccess = conf.getBoolean(PartialVectorMerger.SEQUENTIAL_ACCESS, false);
     namedVector = conf.getBoolean(PartialVectorMerger.NAMED_VECTOR, false);
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/tfidf/TFIDFPartialVectorReducer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/tfidf/TFIDFPartialVectorReducer.java
index 3ed0a72c..e7a8df07 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/tfidf/TFIDFPartialVectorReducer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/tfidf/TFIDFPartialVectorReducer.java
@@ -24,6 +24,8 @@
 import com.google.common.base.Preconditions;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocalFileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
@@ -103,7 +105,17 @@ protected void setup(Context context) throws IOException, InterruptedException {
     Path[] localFiles = DistributedCache.getLocalCacheFiles(conf);
     Preconditions.checkArgument(localFiles != null && localFiles.length >= 1, 
         "missing paths from the DistributedCache");
-
+    LocalFileSystem localFs = FileSystem.getLocal(conf);
+    if (!localFs.exists(localFiles[0])) {
+      URI[] filesURIs = DistributedCache.getCacheFiles(conf);
+      if (filesURIs == null) {
+        throw new IOException("Cannot read Frequency list from Distributed Cache");
+      }
+      if (filesURIs.length != 1) {
+        throw new IOException("Cannot read Frequency list from Distributed Cache (" + localFiles.length + ')');
+      }
+      localFiles[0] = new Path(filesURIs[0].getPath());
+    }
     vectorCount = conf.getLong(TFIDFConverter.VECTOR_COUNT, 1);
     featureCount = conf.getLong(TFIDFConverter.FEATURE_COUNT, 1);
     minDf = conf.getInt(TFIDFConverter.MIN_DF, 1);
