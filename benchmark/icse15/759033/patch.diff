diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java b/incubator/cassandra/trunk/src/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
index 47ef9d1f..83687666 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
@@ -57,23 +57,6 @@ public DebuggableThreadPoolExecutor(int corePoolSize,
     public void afterExecute(Runnable r, Throwable t)
     {
         super.afterExecute(r,t);
-
-        if (r instanceof FutureTask) {
-            assert t == null;
-            try
-            {
-                ((FutureTask)r).get();
-            }
-            catch (InterruptedException e)
-            {
-                throw new RuntimeException(e);
-            }
-            catch (ExecutionException e)
-            {
-                t = e;
-            }
-        }
-
         if ( t != null )
         {  
             Context ctx = ThreadLocalContext.get();
@@ -83,10 +66,20 @@ public void afterExecute(Runnable r, Throwable t)
                 
                 if ( object != null )
                 {
-                    logger_.error("In afterExecute() " + t.getClass().getName() + " occured while working with " + object);
+                    logger_.info("**** In afterExecute() " + t.getClass().getName() + " occured while working with " + object + " ****");
+                }
+                else
+                {
+                    logger_.info("**** In afterExecute() " + t.getClass().getName() + " occured ****");
                 }
             }
-            logger_.error("Error in ThreadPoolExecutor", t);
+            
+            Throwable cause = t.getCause();
+            if ( cause != null )
+            {
+                logger_.info( LogUtil.throwableToString(cause) );
+            }
+            logger_.info( LogUtil.throwableToString(t) );
         }
     }
 }
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/config/DatabaseDescriptor.java b/incubator/cassandra/trunk/src/org/apache/cassandra/config/DatabaseDescriptor.java
index e1eb5c5d..942d2c4d 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/config/DatabaseDescriptor.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/config/DatabaseDescriptor.java
@@ -18,24 +18,19 @@
 
 package org.apache.cassandra.config;
 
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.atomic.AtomicInteger;
+import java.io.*;
 
 import org.apache.cassandra.db.ColumnFamily;
-import org.apache.cassandra.db.SystemTable;
 import org.apache.cassandra.db.Table;
 import org.apache.cassandra.db.TypeInfo;
+import org.apache.cassandra.db.Table.TableMetadata;
 import org.apache.cassandra.utils.FileUtils;
 import org.apache.cassandra.utils.XMLUtils;
 import org.w3c.dom.Node;
 import org.w3c.dom.NodeList;
+import org.apache.cassandra.io.*;
 
 
 /**
@@ -89,7 +84,7 @@
     */
     private static Map<String, Map<String, CFMetaData>> tableToCFMetaDataMap_;
     /* Hashing strategy Random or OPHF */
-    private static String partitionerClass_;
+    private static String hashingStrategy_ = DatabaseDescriptor.random_;
     /* if the size of columns or super-columns are more than this, indexing will kick in */
     private static int columnIndexSizeInKB_;
     /* Size of touch key cache */
@@ -120,11 +115,22 @@
     // the path qualified config file (storage-conf.xml) name
     private static String configFileName_;
 
-    static
+    public static Map<String, Map<String, CFMetaData>> init(String filePath) throws Throwable
     {
-        try
+        /* Read the configuration file to retrieve DB related properties. */
+        String file = filePath + System.getProperty("file.separator") + "storage-conf.xml";
+        return initInternal(file);
+    }
+
+    public static Map<String, Map<String, CFMetaData>> init() throws Throwable
+    {
+        /* Read the configuration file to retrieve DB related properties. */
+        configFileName_ = System.getProperty("storage-config") + System.getProperty("file.separator") + "storage-conf.xml";
+        return initInternal(configFileName_);
+    }
+    
+    public static Map<String, Map<String, CFMetaData>> initInternal(String file) throws Throwable
         {
-            String file = System.getProperty("storage-config") + System.getProperty("file.separator") + "storage-conf.xml";
             String os = System.getProperty("os.name");
             XMLUtils xmlUtils = new XMLUtils(file);
 
@@ -138,7 +144,7 @@
             zkAddress_ = xmlUtils.getNodeValue("/Storage/ZookeeperAddress");
 
             /* Hashing strategy */
-            partitionerClass_ = xmlUtils.getNodeValue("/Storage/Partitioner");
+        hashingStrategy_ = xmlUtils.getNodeValue("/Storage/HashingStrategy");
             /* Callout location */
             calloutLocation_ = xmlUtils.getNodeValue("/Storage/CalloutLocation");
 
@@ -309,9 +315,6 @@
             /* Read the table related stuff from config */
             NodeList tables = xmlUtils.getRequestedNodeList("/Storage/Tables/Table");
             int size = tables.getLength();
-            if (size == 0) {
-                throw new UnsupportedOperationException("A Table must be configured");
-            }
             for ( int i = 0; i < size; ++i )
             {
                 Node table = tables.item(i);
@@ -336,10 +339,6 @@
                 {
                     Node columnFamily = columnFamilies.item(j);
                     String cName = XMLUtils.getAttributeValue(columnFamily, "Name");
-                    if (cName == null)
-                    {
-                        throw new IllegalArgumentException("ColumnFamily element missing Name attribute: " + columnFamily);
-                    }
                     String xqlCF = xqlTable + "ColumnFamily[@Name='" + cName + "']/";
 
                     /* squirrel away the application column families */
@@ -410,68 +409,12 @@
             {
                 seeds_.add( seeds[i] );
             }
+        return tableToCFMetaDataMap_;
         }
-        catch (Exception e)
-        {
-            throw new RuntimeException(e);
-        }
-
-        try
-        {
-            storeMetadata();
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /*
-     * Create the metadata tables. This table has information about
-     * the table name and the column families that make up the table.
-     * Each column family also has an associated ID which is an int.
-    */
-    private static void storeMetadata() throws IOException
-    {
-        AtomicInteger idGenerator = new AtomicInteger(0);
-        Set<String> tables = tableToCFMetaDataMap_.keySet();
-
-        for ( String table : tables )
-        {
-            Table.TableMetadata tmetadata = Table.TableMetadata.instance();
-            if (tmetadata.isEmpty())
-            {
-                tmetadata = Table.TableMetadata.instance();
-                /* Column families associated with this table */
-                Map<String, CFMetaData> columnFamilies = tableToCFMetaDataMap_.get(table);
-
-                for (String columnFamily : columnFamilies.keySet())
-                {
-                    tmetadata.add(columnFamily, idGenerator.getAndIncrement(), DatabaseDescriptor.getColumnType(columnFamily));
-                }
-
-                /*
-                 * Here we add all the system related column families.
-                */
-                /* Add the TableMetadata column family to this map. */
-                tmetadata.add(Table.TableMetadata.cfName_, idGenerator.getAndIncrement());
-                /* Add the LocationInfo column family to this map. */
-                tmetadata.add(SystemTable.cfName_, idGenerator.getAndIncrement());
-                /* Add the recycle column family to this map. */
-                tmetadata.add(Table.recycleBin_, idGenerator.getAndIncrement());
-                /* Add the Hints column family to this map. */
-                tmetadata.add(Table.hints_, idGenerator.getAndIncrement(), ColumnFamily.getColumnType("Super"));
-                tmetadata.apply();
-                idGenerator.set(0);
-            }
-        }
-    }
-
-
     
-    public static String getPartitionerClass()
+    public static String getHashingStrategy()
     {
-        return partitionerClass_;
+        return hashingStrategy_;
     }
     
     public static String getZkAddress()
@@ -793,13 +736,8 @@ public static TypeInfo getTypeInfo(String cfName)
         }
     }
 
-    public static Map<String, Map<String, CFMetaData>> getTableToColumnFamilyMap()
-    {
-        return tableToCFMetaDataMap_;
-    }
-
-    public static String getTableName()
+    public static void main(String[] args) throws Throwable
     {
-        return tables_.get(0);
+        DatabaseDescriptor.initInternal("C:\\Engagements\\Cassandra-Golden\\storage-conf.xml");
     }
 }
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/ColumnRangeQueryRSD.java b/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/ColumnRangeQueryRSD.java
index 758164f8..b103631a 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/ColumnRangeQueryRSD.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/ColumnRangeQueryRSD.java
@@ -33,6 +33,7 @@
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.LogUtil;
 import org.apache.log4j.Logger;
+import org.apache.cassandra.db.*;
 
 /**
  * A Row Source Defintion (RSD) for doing a range query on a column map
@@ -111,7 +112,7 @@ public ColumnRangeQueryRSD(CFMetaData cfMetaData, ConstantOperand rowKey, Consta
         List<Map<String, String>> rows = new LinkedList<Map<String, String>>();
         if (row != null)
         {
-            Map<String, ColumnFamily> cfMap = row.getColumnFamilyMap();
+            Map<String, ColumnFamily> cfMap = row.getColumnFamilies();
             if (cfMap != null && cfMap.size() > 0)
             {
                 ColumnFamily cfamily = cfMap.get(cfMetaData_.cfName);
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/SuperColumnRangeQueryRSD.java b/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/SuperColumnRangeQueryRSD.java
index 71a80658..e21c133d 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/SuperColumnRangeQueryRSD.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/SuperColumnRangeQueryRSD.java
@@ -18,6 +18,7 @@
 
 package org.apache.cassandra.cql.common;
 
+import java.util.ArrayList;
 import java.util.Collection;
 import java.util.HashMap;
 import java.util.LinkedList;
@@ -31,8 +32,11 @@
 import org.apache.cassandra.db.Row;
 import org.apache.cassandra.service.StorageProxy;
 import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.service.column_t;
+import org.apache.cassandra.service.superColumn_t;
 import org.apache.cassandra.utils.LogUtil;
 import org.apache.log4j.Logger;
+import org.apache.cassandra.db.*;
 
 /**
  * A Row Source Defintion (RSD) for doing a super column range query on a Super Column Family.
@@ -80,7 +84,7 @@ public SuperColumnRangeQueryRSD(CFMetaData cfMetaData, OperandDef rowKey, int of
         List<Map<String, String>> rows = new LinkedList<Map<String, String>>();
         if (row != null)
         {
-            Map<String, ColumnFamily> cfMap = row.getColumnFamilyMap();
+            Map<String, ColumnFamily> cfMap = row.getColumnFamilies();
             if (cfMap != null && cfMap.size() > 0)
             {
                 ColumnFamily cfamily = cfMap.get(cfMetaData_.cfName);
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/UniqueKeyQueryRSD.java b/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/UniqueKeyQueryRSD.java
index 3f3bd9f2..36553369 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/UniqueKeyQueryRSD.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/cql/common/UniqueKeyQueryRSD.java
@@ -33,6 +33,7 @@
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.LogUtil;
 import org.apache.log4j.Logger;
+import org.apache.cassandra.db.*;
 
 /**
  * A Row Source Defintion (RSD) for looking up a unique column within a column family.
@@ -95,7 +96,7 @@ public UniqueKeyQueryRSD(CFMetaData cfMetaData, OperandDef rowKey, OperandDef co
 
         if (row != null)
         {
-            Map<String, ColumnFamily> cfMap = row.getColumnFamilyMap();
+            Map<String, ColumnFamily> cfMap = row.getColumnFamilies();
             if (cfMap != null && cfMap.size() > 0)
             {
                 ColumnFamily cfamily = cfMap.get(cfMetaData_.cfName);
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/db/WriteResponse.java b/incubator/cassandra/trunk/src/org/apache/cassandra/db/WriteResponse.java
index 66433389..e69de29b 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/db/WriteResponse.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/db/WriteResponse.java
@@ -1,100 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.Serializable;
-
-import javax.xml.bind.annotation.XmlElement;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-
-
-/*
- * This message is sent back the row mutation verb handler 
- * and basically specifes if the write succeeded or not for a particular 
- * key in a table
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public class WriteResponse 
-{
-    private static WriteResponseSerializer serializer_ = new WriteResponseSerializer();
-
-    public static WriteResponseSerializer serializer()
-    {
-        return serializer_;
-    }
-
-    public static Message makeWriteResponseMessage(Message original, WriteResponse writeResponseMessage) throws IOException
-    {
-    	ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream( bos );
-        WriteResponse.serializer().serialize(writeResponseMessage, dos);
-        return original.getReply(StorageService.getLocalStorageEndPoint(), bos.toByteArray());
-    }
-
-	private final String table_;
-	private final String key_;
-	private final boolean status_;
-
-	public WriteResponse(String table, String key, boolean bVal) {
-		table_ = table;
-		key_ = key;
-		status_ = bVal;
-	}
-
-	public String table()
-	{
-		return table_;
-	}
-
-	public String key()
-	{
-		return key_;
-	}
-
-	public boolean isSuccess()
-	{
-		return status_;
-	}
-
-    public static class WriteResponseSerializer implements ICompactSerializer<WriteResponse>
-    {
-        public void serialize(WriteResponse wm, DataOutputStream dos) throws IOException
-        {
-            dos.writeUTF(wm.table());
-            dos.writeUTF(wm.key());
-            dos.writeBoolean(wm.isSuccess());
-        }
-
-        public WriteResponse deserialize(DataInputStream dis) throws IOException
-        {
-            String table = dis.readUTF();
-            String key = dis.readUTF();
-            boolean status = dis.readBoolean();
-            return new WriteResponse(table, key, status);
-        }
-    }
-}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/BigIntegerToken.java b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/BigIntegerToken.java
index 2615b2d9..e69de29b 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/BigIntegerToken.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/BigIntegerToken.java
@@ -1,16 +0,0 @@
-package org.apache.cassandra.dht;
-
-import java.math.BigInteger;
-
-public class BigIntegerToken extends Token<BigInteger>
-{
-    public BigIntegerToken(BigInteger token)
-    {
-        super(token);
-    }
-
-    // convenience method for testing
-    public BigIntegerToken(String token) {
-        this(new BigInteger(token));
-    }
-}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/IPartitioner.java b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/IPartitioner.java
index c4117eba..e69de29b 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/IPartitioner.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/IPartitioner.java
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.util.Comparator;
-
-public interface IPartitioner
-{
-    /** transform key to on-disk format s.t. keys are stored in node comparison order.
-     *  this lets bootstrap rip out parts of the sstable sequentially instead of doing random seeks. */
-    public String decorateKey(String key);
-
-    public String undecorateKey(String decoratedKey);
-
-    public Comparator<String> getDecoratedKeyComparator();
-
-    public Comparator<String> getReverseDecoratedKeyComparator();
-
-    public Token getTokenForKey(String key);
-
-    public Token getDefaultToken();
-
-    public Token.TokenFactory getTokenFactory();
-}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/OrderPreservingPartitioner.java b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/OrderPreservingPartitioner.java
index d0407fdc..e69de29b 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/OrderPreservingPartitioner.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/OrderPreservingPartitioner.java
@@ -1,117 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.io.UnsupportedEncodingException;
-import java.text.Collator;
-import java.util.Comparator;
-import java.util.Locale;
-import java.util.Random;
-
-public class OrderPreservingPartitioner implements IPartitioner
-{
-    // TODO make locale configurable.  But don't just leave it up to the OS or you could really screw
-    // people over if they deploy on nodes with different OS locales.
-    static final Collator collator = Collator.getInstance(new Locale("en", "US")); 
-
-    private static final Comparator<String> comparator = new Comparator<String>() {
-        public int compare(String o1, String o2)
-        {
-            return collator.compare(o1, o2);
-        }
-    };
-    private static final Comparator<String> reverseComparator = new Comparator<String>() {
-        public int compare(String o1, String o2)
-        {
-            return -comparator.compare(o1, o2);
-        }
-    };
-
-    public String decorateKey(String key)
-    {
-        return key;
-    }
-
-    public String undecorateKey(String decoratedKey)
-    {
-        return decoratedKey;
-    }
-
-    public Comparator<String> getDecoratedKeyComparator()
-    {
-        return comparator;
-    }
-
-    public Comparator<String> getReverseDecoratedKeyComparator()
-    {
-        return reverseComparator;
-    }
-
-    public StringToken getDefaultToken()
-    {
-        String chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";
-        Random r = new Random();
-        StringBuffer buffer = new StringBuffer();
-        for (int j = 0; j < 16; j++) {
-            buffer.append(chars.charAt(r.nextInt(chars.length())));
-        }
-        return new StringToken(buffer.toString());
-    }
-
-    private final Token.TokenFactory<String> tokenFactory = new Token.TokenFactory<String>() {
-        public byte[] toByteArray(Token<String> stringToken)
-        {
-            try
-            {
-                return stringToken.token.getBytes("UTF-8");
-            }
-            catch (UnsupportedEncodingException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-
-        public Token<String> fromByteArray(byte[] bytes)
-        {
-            try
-            {
-                return new StringToken(new String(bytes, "UTF-8"));
-            }
-            catch (UnsupportedEncodingException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-
-        public Token<String> fromString(String string)
-        {
-            return new StringToken(string);
-        }
-    };
-
-    public Token.TokenFactory<String> getTokenFactory()
-    {
-        return tokenFactory;
-    }
-
-    public Token getTokenForKey(String key)
-    {
-        return new StringToken(key);
-    }
-}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/RandomPartitioner.java b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/RandomPartitioner.java
index e047b527..e69de29b 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/RandomPartitioner.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/RandomPartitioner.java
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.math.BigInteger;
-import java.util.Comparator;
-
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.utils.GuidGenerator;
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.dht.BigIntegerToken;
-
-/**
- * This class generates a MD5 hash of the key. It uses the standard technique
- * used in all DHT's.
- * 
- * @author alakshman
- * 
- */
-public class RandomPartitioner implements IPartitioner
-{
-    private static final Comparator<String> comparator = new Comparator<String>() {
-        public int compare(String o1, String o2)
-        {
-            BigInteger i1 = new BigInteger(o1.split(":")[0]);
-            BigInteger i2 = new BigInteger(o2.split(":")[0]);
-            return i1.compareTo(i2);
-        }
-    };
-    private static final Comparator<String> reverseComparator = new Comparator<String>() {
-        public int compare(String o1, String o2)
-        {
-           return -comparator.compare(o1, o2);
-        }
-    };
-
-    public BigInteger hash(String key)
-	{
-		return FBUtilities.hash(key);
-	}
-
-    public String decorateKey(String key)
-    {
-        return hash(key).toString() + ":" + key;
-    }
-
-    public String undecorateKey(String decoratedKey)
-    {
-        return decoratedKey.split(":")[1];
-    }
-
-    public Comparator<String> getDecoratedKeyComparator()
-    {
-        return comparator;
-    }
-
-    public Comparator<String> getReverseDecoratedKeyComparator()
-    {
-        return reverseComparator;
-    }
-
-    public BigIntegerToken getDefaultToken()
-    {
-        String guid = GuidGenerator.guid();
-        BigInteger token = FBUtilities.hash(guid);
-        if ( token.signum() == -1 )
-            token = token.multiply(BigInteger.valueOf(-1L));
-        return new BigIntegerToken(token);
-    }
-
-    private final Token.TokenFactory<BigInteger> tokenFactory = new Token.TokenFactory<BigInteger>() {
-        public byte[] toByteArray(Token<BigInteger> bigIntegerToken)
-        {
-            return bigIntegerToken.token.toByteArray();
-        }
-
-        public Token<BigInteger> fromByteArray(byte[] bytes)
-        {
-            return new BigIntegerToken(new BigInteger(bytes));
-        }
-
-        public Token<BigInteger> fromString(String string)
-        {
-            return new BigIntegerToken(new BigInteger(string));
-        }
-    };
-
-    public Token.TokenFactory<BigInteger> getTokenFactory()
-    {
-        return tokenFactory;
-    }
-
-    public Token getTokenForKey(String key)
-    {
-        return new BigIntegerToken(FBUtilities.hash(key));
-    }
-}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/StringToken.java b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/StringToken.java
index a1939dda..e69de29b 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/StringToken.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/StringToken.java
@@ -1,14 +0,0 @@
-package org.apache.cassandra.dht;
-
-public class StringToken extends Token<String>
-{
-    protected StringToken(String token)
-    {
-        super(token);
-    }
-
-    public int compareTo(Token<String> o)
-    {
-        return OrderPreservingPartitioner.collator.compare(this.token, o.token);
-    }
-}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/Token.java b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/Token.java
index 03e3c334..e69de29b 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/dht/Token.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/dht/Token.java
@@ -1,77 +0,0 @@
-package org.apache.cassandra.dht;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.service.StorageService;
-
-public abstract class Token<T extends Comparable> implements Comparable<Token<T>>
-{
-    private static final TokenSerializer serializer = new TokenSerializer();
-    public static TokenSerializer serializer()
-    {
-        return serializer;
-    }
-
-    T token;
-
-    protected Token(T token)
-    {
-        this.token = token;
-    }
-
-    /**
-     * This determines the comparison for node destination purposes.
-     */
-    public int compareTo(Token<T> o)
-    {
-        return token.compareTo(o.token);
-    }
-
-    public String toString()
-    {
-        return "Token(" + token + ")";
-    }
-
-    public boolean equals(Object obj)
-    {
-        if (!(obj instanceof Token)) {
-            return false;
-        }
-        return token.equals(((Token)obj).token);
-    }
-
-    public int hashCode()
-    {
-        return token.hashCode();
-    }
-
-    public static abstract class TokenFactory<T extends Comparable>
-    {
-        public abstract byte[] toByteArray(Token<T> token);
-        public abstract Token<T> fromByteArray(byte[] bytes);
-        public abstract Token<T> fromString(String string);
-    }
-
-    public static class TokenSerializer implements ICompactSerializer<Token>
-    {
-        public void serialize(Token token, DataOutputStream dos) throws IOException
-        {
-            IPartitioner p = StorageService.getPartitioner();
-            byte[] b = p.getTokenFactory().toByteArray(token);
-            dos.writeInt(b.length);
-            dos.write(b);
-        }
-
-        public Token deserialize(DataInputStream dis) throws IOException
-        {
-            IPartitioner p = StorageService.getPartitioner();
-            int size = dis.readInt();
-            byte[] bytes = new byte[size];
-            dis.readFully(bytes);
-            return p.getTokenFactory().fromByteArray(bytes);
-        }
-    }
-}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/gms/FailureDetector.java b/incubator/cassandra/trunk/src/org/apache/cassandra/gms/FailureDetector.java
index 7d5152b9..4b21148e 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/gms/FailureDetector.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/gms/FailureDetector.java
@@ -177,7 +177,7 @@ public void intepret(EndPoint ep)
         /* We need this so that we do not suspect a convict. */
         boolean isConvicted = false;
         double phi = hbWnd.phi(now);
-        logger_.trace("PHI for " + ep + " : " + phi);
+        logger_.info("PHI for " + ep + " : " + phi);
         
         /*
         if ( phi > phiConvictThreshold_ )
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/gms/Gossiper.java b/incubator/cassandra/trunk/src/org/apache/cassandra/gms/Gossiper.java
index b5a34768..8260a92f 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/gms/Gossiper.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/gms/Gossiper.java
@@ -76,7 +76,7 @@ public void run()
                         if ( !bVal )
                             doGossipToSeed(message);
 
-                        logger_.trace("Performing status check ...");
+                        logger_.debug("Performing status check ...");
                         doStatusCheck();
                     }
                 }
@@ -344,7 +344,7 @@ void makeRandomGossipDigest(List<GossipDigest> gDigests)
             sb.append(gDigest);
             sb.append(" ");
         }
-        logger_.trace("Gossip Digests are : " + sb.toString());
+        logger_.debug("Gossip Digests are : " + sb.toString());
     }
 
     public int getCurrentGenerationNumber(EndPoint endpoint)
@@ -367,7 +367,7 @@ Message makeGossipDigestAckMessage(GossipDigestAckMessage gDigestAckMessage) thr
         ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
         DataOutputStream dos = new DataOutputStream(bos);
         GossipDigestAckMessage.serializer().serialize(gDigestAckMessage, dos);
-        logger_.trace("@@@@ Size of GossipDigestAckMessage is " + bos.toByteArray().length);
+        logger_.debug("@@@@ Size of GossipDigestAckMessage is " + bos.toByteArray().length);
         Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_ACK_VERB, new Object[]{bos.toByteArray()});
         return message;
     }
@@ -392,7 +392,7 @@ boolean sendGossipToLiveNode(Message message)
         }
 
         EndPoint to = eps.get(++rrIndex_);
-        logger_.trace("Sending a GossipDigestSynMessage to " + to + " ...");
+        logger_.info("Sending a GossipDigestSynMessage to " + to + " ...");
         MessagingService.getMessagingInstance().sendUdpOneWay(message, to);
         return seeds_.contains(to);
     }
@@ -411,7 +411,7 @@ boolean sendGossip(Message message, Set<EndPoint> epSet)
         List<EndPoint> liveEndPoints = new ArrayList<EndPoint>(epSet);
         int index = (size == 1) ? 0 : random_.nextInt(size);
         EndPoint to = liveEndPoints.get(index);
-        logger_.trace("Sending a GossipDigestSynMessage to " + to + " ...");
+        logger_.info("Sending a GossipDigestSynMessage to " + to + " ...");
         MessagingService.getMessagingInstance().sendUdpOneWay(message, to);
         return seeds_.contains(to);
     }
@@ -977,7 +977,7 @@ public void doVerb(Message message)
     public void doVerb(Message message)
     {
         EndPoint from = message.getFrom();
-        logger_.trace("Received a GossipDigestSynMessage from " + from);
+        logger_.info("Received a GossipDigestSynMessage from " + from);
 
         byte[] bytes = (byte[])message.getMessageBody()[0];
         DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
@@ -1001,7 +1001,7 @@ public void doVerb(Message message)
 
             GossipDigestAckMessage gDigestAck = new GossipDigestAckMessage(deltaGossipDigestList, deltaEpStateMap);
             Message gDigestAckMessage = Gossiper.instance().makeGossipDigestAckMessage(gDigestAck);
-            logger_.trace("Sending a GossipDigestAckMessage to " + from);
+            logger_.info("Sending a GossipDigestAckMessage to " + from);
             MessagingService.getMessagingInstance().sendUdpOneWay(gDigestAckMessage, from);
         }
         catch (IOException e)
@@ -1061,7 +1061,7 @@ private void doSort(List<GossipDigest> gDigestList)
     public void doVerb(Message message)
     {
         EndPoint from = message.getFrom();
-        logger_.trace("Received a GossipDigestAckMessage from " + from);
+        logger_.info("Received a GossipDigestAckMessage from " + from);
 
         byte[] bytes = (byte[])message.getMessageBody()[0];
         DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
@@ -1091,7 +1091,7 @@ public void doVerb(Message message)
 
             GossipDigestAck2Message gDigestAck2 = new GossipDigestAck2Message(deltaEpStateMap);
             Message gDigestAck2Message = Gossiper.instance().makeGossipDigestAck2Message(gDigestAck2);
-            logger_.trace("Sending a GossipDigestAck2Message to " + from);
+            logger_.info("Sending a GossipDigestAck2Message to " + from);
             MessagingService.getMessagingInstance().sendUdpOneWay(gDigestAck2Message, from);
         }
         catch ( IOException e )
@@ -1108,7 +1108,7 @@ public void doVerb(Message message)
     public void doVerb(Message message)
     {
         EndPoint from = message.getFrom();
-        logger_.trace("Received a GossipDigestAck2Message from " + from);
+        logger_.info("Received a GossipDigestAck2Message from " + from);
 
         byte[] bytes = (byte[])message.getMessageBody()[0];
         DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/io/Coordinate.java b/incubator/cassandra/trunk/src/org/apache/cassandra/io/Coordinate.java
index e5eaf6a8..b34b6cae 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/io/Coordinate.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/io/Coordinate.java
@@ -21,10 +21,10 @@
  * Section of a file that needs to be scanned
  * is represented by this class.
 */
-public class Coordinate
+class Coordinate
 {
-    public final long start_;
-    public final long end_;
+    long start_;
+    long end_;
     
     Coordinate(long start, long end)
     {
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/io/IFileReader.java b/incubator/cassandra/trunk/src/org/apache/cassandra/io/IFileReader.java
index 8086e6a8..337acf9c 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/io/IFileReader.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/io/IFileReader.java
@@ -90,6 +90,31 @@
     */
     public long next(DataOutputBuffer bufOut) throws IOException;
 
+    /**
+     * This method dumps the next key/value into the DataOuputStream
+     * passed in.
+     *
+     * @param key key we are interested in.
+     * @param dos DataOutputStream that needs to be filled.
+     * @param section region of the file that needs to be read
+     * @throws IOException
+     * @return the number of bytes read.
+    */
+    public long next(String key, DataOutputBuffer bufOut, Coordinate section) throws IOException;
+
+    /**
+     * This method dumps the next key/value into the DataOuputStream
+     * passed in.
+     *
+     * @param key key we are interested in.
+     * @param dos DataOutputStream that needs to be filled.
+     * @param column name of the column in our format.
+     * @param section region of the file that needs to be read
+     * @throws IOException
+     * @return number of bytes that were read.
+    */
+    public long next(String key, DataOutputBuffer bufOut, String column, Coordinate section) throws IOException;
+
     /**
      * This method dumps the next key/value into the DataOuputStream
      * passed in. Always use this method to query for application
@@ -105,7 +130,21 @@
      * @return number of bytes read.
      *
     */
-    public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, List<String> columnNames, IndexHelper.TimeRange timeRange, Coordinate section) throws IOException;
+    public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, List<String> columnNames, Coordinate section) throws IOException;
+    
+    /**
+     * This method dumps the next key/value into the DataOuputStream
+     * passed in.
+     *
+     * @param key key we are interested in.
+     * @param dos DataOutputStream that needs to be filled.
+     * @param column name of the column in our format.
+     * @param timeRange time range we are interested in.
+     * @param section region of the file that needs to be read
+     * @throws IOException
+     * @return number of bytes that were read.
+    */
+    public long next(String key, DataOutputBuffer bufOut, String column, IndexHelper.TimeRange timeRange, Coordinate section) throws IOException;
     
     /**
      * Close the file after reading.
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/io/IFileWriter.java b/incubator/cassandra/trunk/src/org/apache/cassandra/io/IFileWriter.java
index a5c3be48..13ab183c 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/io/IFileWriter.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/io/IFileWriter.java
@@ -18,8 +18,11 @@
 
 package org.apache.cassandra.io;
 
+import java.io.File;
 import java.io.IOException;
 
+import org.apache.cassandra.db.PrimaryKey;
+
 
 /**
  * An interface for writing into the SequenceFile abstraction.
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/io/SSTable.java b/incubator/cassandra/trunk/src/org/apache/cassandra/io/SSTable.java
index 526d8a10..195b2179 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/io/SSTable.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/io/SSTable.java
@@ -18,30 +18,24 @@
 
 package org.apache.cassandra.io;
 
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.Hashtable;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import org.apache.log4j.Logger;
+import java.io.*;
+import java.math.BigInteger;
+import java.nio.channels.FileChannel;
+import java.util.*;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock.ReadLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock.WriteLock;
 
 import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.service.PartitionerType;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.BasicUtilities;
 import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.utils.FileUtils;
 import org.apache.cassandra.utils.LogUtil;
-import org.apache.cassandra.dht.IPartitioner;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
 
 /**
  * This class is built on top of the SequenceFile. It stores
@@ -143,6 +137,37 @@ protected boolean removeEldestEntry(Map.Entry<String, Long> entry)
         }
     }
     
+    /**
+     * This compares two strings and does it in reverse
+     * order.
+     * 
+     * @author alakshman
+     *
+     */
+    private static class OrderPreservingPartitionerComparator implements Comparator<String>
+    {
+        public int compare(String c1, String c2) 
+        {
+            return c2.compareTo(c1);
+        } 
+    }
+
+    /**
+     * This class compares two BigInteger's passes in
+     * as strings and does so in reverse order.
+     * @author alakshman
+     *
+     */
+    private static class RandomPartitionerComparator implements Comparator<String>
+    {
+        public int compare(String c1, String c2) 
+        {
+            BigInteger b1 = new BigInteger(c1);
+            BigInteger b2 = new BigInteger(c2);
+            return b2.compareTo(b1);
+        } 
+    }
+    
     /**
      * This is a simple container for the index Key and its corresponding position
      * in the data file. Binary search is performed on a list of these objects
@@ -150,20 +175,25 @@ protected boolean removeEldestEntry(Map.Entry<String, Long> entry)
     */
     public static class KeyPositionInfo implements Comparable<KeyPositionInfo>
     {
-        private final String decoratedKey;
+        private String key_;
         private long position_;
 
-        public KeyPositionInfo(String decoratedKey)
+        public KeyPositionInfo(String key)
         {
-            this.decoratedKey = decoratedKey;
+            key_ = key;            
         }
 
-        public KeyPositionInfo(String decoratedKey, long position)
+        public KeyPositionInfo(String key, long position)
         {
-            this(decoratedKey);
+            this(key);
             position_ = position;
         }
 
+        public String key()
+        {
+            return key_;
+        }
+
         public long position()
         {
             return position_;
@@ -171,13 +201,25 @@ public long position()
 
         public int compareTo(KeyPositionInfo kPosInfo)
         {
-            IPartitioner p = StorageService.getPartitioner();
-            return p.getDecoratedKeyComparator().compare(decoratedKey, kPosInfo.decoratedKey);
+            int value = 0;
+            PartitionerType pType = StorageService.getPartitionerType();
+            switch( pType )
+            {
+                case OPHF:
+                    value = key_.compareTo(kPosInfo.key_);                    
+                    break;
+                    
+                default:
+                    BigInteger b = new BigInteger(key_);
+                    value = b.compareTo( new BigInteger(kPosInfo.key_) );
+                    break;
+            }
+            return value;
         }
 
         public String toString()
         {
-        	return decoratedKey + ":" + position_;
+        	return key_ + ":" + position_;
         }
     }
     
@@ -262,7 +304,7 @@ public static int getApproximateKeyCount( List<String> dataFiles)
         List<String> indexedKeys = new ArrayList<String>();
         for ( KeyPositionInfo keyPositionInfo : keyPositionInfos )
         {
-            indexedKeys.add(keyPositionInfo.decoratedKey);
+            indexedKeys.add(keyPositionInfo.key_);
         }
 
         Collections.sort(indexedKeys);
@@ -319,13 +361,13 @@ public static void removeAssociatedBloomFilter(String filename)
      * Determines if the given key is in the specified file. If the
      * key is not present then we skip processing this file.
     */
-    public static boolean isKeyInFile(String clientKey, String filename)
+    public static boolean isKeyInFile(String key, String filename)
     {
         boolean bVal = false;
         BloomFilter bf = bfs_.get(filename);
         if ( bf != null )
         {
-            bVal = bf.isPresent(clientKey);
+            bVal = bf.isPresent(key);
         }
         return bVal;
     }
@@ -358,11 +400,46 @@ public SSTable(String dataFileName) throws IOException
     public SSTable(String directory, String filename) throws IOException
     {  
         dataFile_ = directory + System.getProperty("file.separator") + filename + "-Data.db";                
-        blockIndex_ = new TreeMap<String, BlockMetadata>(StorageService.getPartitioner().getReverseDecoratedKeyComparator());
+        blockIndex_ = new TreeMap<String, BlockMetadata>(Collections.reverseOrder());
         blockIndexes_ = new ArrayList<SortedMap<String, BlockMetadata>>();
+        // dataWriter_ = SequenceFile.writer(dataFile_);
+        dataWriter_ = SequenceFile.bufferedWriter(dataFile_, 4*1024*1024);        
+        SSTable.positionAfterFirstBlockIndex_ = dataWriter_.getCurrentPosition(); 
+    } 
+    
+    private void initBlockIndex()
+    {
+        initBlockIndex(StorageService.getPartitionerType());
+    }
+    
+    private void initBlockIndex(PartitionerType pType)
+    {
+        switch ( pType )
+        {
+            case OPHF: 
+                blockIndex_ = new TreeMap<String, BlockMetadata>( new SSTable.OrderPreservingPartitionerComparator() );                
+               break;
+               
+            default:
+                blockIndex_ = new TreeMap<String, BlockMetadata>( new SSTable.RandomPartitionerComparator() );
+                break;
+        }
+    }
+    
+    /**
+     * This ctor is used for DB writes into the SSTable. Use this
+     * version to write to the SSTable.
+    */
+    public SSTable(String directory, String filename, PartitionerType pType) throws IOException
+    {        
+        dataFile_ = directory + System.getProperty("file.separator") + filename + "-Data.db";
+        // dataWriter_ = SequenceFile.writer(dataFile_);
         dataWriter_ = SequenceFile.bufferedWriter(dataFile_, 4*1024*1024);
         // dataWriter_ = SequenceFile.chksumWriter(dataFile_, 4*1024*1024);
         SSTable.positionAfterFirstBlockIndex_ = dataWriter_.getCurrentPosition();
+        /* set up the block index based on partition type */
+        initBlockIndex(pType);
+        blockIndexes_ = new ArrayList<SortedMap<String, BlockMetadata>>();
     } 
 
     private void loadBloomFilter(IFileReader indexReader, long size) throws IOException
@@ -381,8 +458,8 @@ private void loadBloomFilter(IFileReader indexReader, long size) throws IOExcept
         indexReader.next(bufOut);   
         bufOut.close();
         bufIn.reset(bufOut.getData(), bufOut.getLength());
-        String clientKey = bufIn.readUTF();
-        if ( clientKey.equals(SequenceFile.marker_) )
+        String key = bufIn.readUTF();
+        if ( key.equals(SequenceFile.marker_) )
         {
             /*
              * We are now reading the serialized Bloom Filter. We read
@@ -515,26 +592,31 @@ public String getDataFileLocation() throws IOException
         return getFile(dataFile_);
     }
 
+    public long lastModified()
+    {
+        return dataWriter_.lastModified();
+    }
+    
     /*
      * Seeks to the specified key on disk.
     */
-    public void touch(final String clientKey, boolean fData) throws IOException
+    public void touch(String key, boolean fData) throws IOException
     {
-        if (touchCache_.containsKey(dataFile_ + ":" + clientKey))
+        if ( touchCache_.containsKey(key) )
             return;
         
         IFileReader dataReader = SequenceFile.reader(dataFile_); 
         try
         {
         	/* Morph the key */
-            String decoratedKey = StorageService.getPartitioner().decorateKey(clientKey);
-            Coordinate fileCoordinate = getCoordinates(decoratedKey, dataReader);
+        	key = morphKey(key);
+            Coordinate fileCoordinate = getCoordinates(key, dataReader);
             /* Get offset of key from block Index */
             dataReader.seek(fileCoordinate.end_);
-            BlockMetadata blockMetadata = dataReader.getBlockMetadata(decoratedKey);
+            BlockMetadata blockMetadata = dataReader.getBlockMetadata(key);
             if ( blockMetadata.position_ != -1L )
             {
-                touchCache_.put(dataFile_ + ":" + clientKey, blockMetadata.position_);
+                touchCache_.put(dataFile_ + ":" + key, blockMetadata.position_);                  
             } 
             
             if ( fData )
@@ -557,30 +639,63 @@ public void touch(final String clientKey, boolean fData) throws IOException
         }
     }
 
-    private long beforeAppend(String decoratedKey) throws IOException
+    private long beforeAppend(String key) throws IOException
     {
-    	if (decoratedKey == null )
+    	if(key == null )
             throw new IOException("Keys must not be null.");
-        Comparator<String> c = StorageService.getPartitioner().getReverseDecoratedKeyComparator();
-        if ( lastWrittenKey_ != null && c.compare(lastWrittenKey_, decoratedKey) <= 0 )
+        if ( lastWrittenKey_ != null && key.compareTo(lastWrittenKey_) <= 0 )
         {
             logger_.info("Last written key : " + lastWrittenKey_);
-            logger_.info("Current key : " + decoratedKey);
+            logger_.info("Current key : " + key);
             logger_.info("Writing into file " + dataFile_);
             throw new IOException("Keys must be written in ascending order.");
         }
-        return (lastWrittenKey_ == null) ? SSTable.positionAfterFirstBlockIndex_ : dataWriter_.getCurrentPosition();
+        long currentPosition = (lastWrittenKey_ == null) ? SSTable.positionAfterFirstBlockIndex_ : dataWriter_.getCurrentPosition();
+        return currentPosition;
+    }
+
+    private long beforeAppend(BigInteger hash) throws IOException
+    {
+        if(hash == null )
+            throw new IOException("Keys must not be null.");
+        if ( lastWrittenKey_ != null )
+        {
+            BigInteger previousKey = new BigInteger(lastWrittenKey_);
+            if ( hash.compareTo(previousKey) <= 0 )
+            {
+                logger_.info("Last written key : " + previousKey);
+                logger_.info("Current key : " + hash);
+                logger_.info("Writing into file " + dataFile_);
+                throw new IOException("Keys must be written in ascending order.");
+            }
+        }
+        long currentPosition = (lastWrittenKey_ == null) ? SSTable.positionAfterFirstBlockIndex_ : dataWriter_.getCurrentPosition();
+        return currentPosition;
     }
 
-    private void afterAppend(String decoratedKey, long position, long size) throws IOException
+    private void afterAppend(String key, long position, long size) throws IOException
     {
         ++indexKeysWritten_;
-        lastWrittenKey_ = decoratedKey;
-        blockIndex_.put(decoratedKey, new BlockMetadata(position, size));
+        lastWrittenKey_ = key;
+        blockIndex_.put(key, new BlockMetadata(position, size));
         if ( indexKeysWritten_ == indexInterval_ )
         {
         	blockIndexes_.add(blockIndex_);
-        	blockIndex_ = new TreeMap<String, BlockMetadata>(StorageService.getPartitioner().getReverseDecoratedKeyComparator());
+        	blockIndex_ = new TreeMap<String, BlockMetadata>(Collections.reverseOrder());
+            indexKeysWritten_ = 0;
+        }                
+    }
+    
+    private void afterAppend(BigInteger hash, long position, long size) throws IOException
+    {
+        ++indexKeysWritten_;
+        String key = hash.toString();
+        lastWrittenKey_ = key;
+        blockIndex_.put(key, new BlockMetadata(position, size));
+        if ( indexKeysWritten_ == indexInterval_ )
+        {
+            blockIndexes_.add(blockIndex_);
+            initBlockIndex();
             indexKeysWritten_ = 0;
         }                
     }
@@ -615,10 +730,10 @@ private void dumpBlockIndex( SortedMap<String, BlockMetadata> blockIndex) throws
         Set<String> keys = blockIndex.keySet();                
         /* Number of keys in this block */
         bufOut.writeInt(keys.size());
-        for ( String decoratedKey : keys )
+        for ( String key : keys )
         {            
-            bufOut.writeUTF(decoratedKey);
-            BlockMetadata blockMetadata = blockIndex.get(decoratedKey);
+            bufOut.writeUTF(key);
+            BlockMetadata blockMetadata = blockIndex.get(key);
             /* position of the key as a relative offset */
             bufOut.writeLong(position - blockMetadata.position_);
             bufOut.writeLong(blockMetadata.size_);
@@ -637,31 +752,45 @@ private void dumpBlockIndex( SortedMap<String, BlockMetadata> blockIndex) throws
         blockIndex.clear();        
     }
 
-    public void append(String clientKey, DataOutputBuffer buffer) throws IOException
+    public void append(String key, DataOutputBuffer buffer) throws IOException
     {
-        String decoratedKey = StorageService.getPartitioner().decorateKey(clientKey);
-        long currentPosition = beforeAppend(decoratedKey);
-        dataWriter_.append(decoratedKey, buffer);
-        afterAppend(decoratedKey, currentPosition, buffer.getLength());
+        long currentPosition = beforeAppend(key);
+        dataWriter_.append(key, buffer);
+        afterAppend(key, currentPosition, buffer.getLength());
+    }
+    
+    public void append(String key, BigInteger hash, DataOutputBuffer buffer) throws IOException
+    {
+        long currentPosition = beforeAppend(hash);
+        /* Use as key - hash + ":" + key */
+        dataWriter_.append(hash + ":" + key, buffer);
+        afterAppend(hash, currentPosition, buffer.getLength());
     }
 
-    public void append(String clientKey, byte[] value) throws IOException
+    public void append(String key, byte[] value) throws IOException
     {
-        String decoratedKey = StorageService.getPartitioner().decorateKey(clientKey);
-        long currentPosition = beforeAppend(decoratedKey);
-        dataWriter_.append(decoratedKey, value);
-        afterAppend(decoratedKey, currentPosition, value.length );
+        long currentPosition = beforeAppend(key);
+        dataWriter_.append(key, value);
+        afterAppend(key, currentPosition, value.length );
     }
 
-    public static Coordinate getCoordinates(String decoratedKey, IFileReader dataReader) throws IOException
+    public void append(String key, BigInteger hash, byte[] value) throws IOException
     {
-    	List<KeyPositionInfo> indexInfo = indexMetadataMap_.get(dataReader.getFileName());
+        long currentPosition = beforeAppend(hash);
+        /* Use as key - hash + ":" + key */
+        dataWriter_.append(hash + ":" + key, value);
+        afterAppend(hash, currentPosition, value.length);
+    }
+
+    private Coordinate getCoordinates(String key, IFileReader dataReader) throws IOException
+    {
+    	List<KeyPositionInfo> indexInfo = indexMetadataMap_.get(dataFile_);
     	int size = (indexInfo == null) ? 0 : indexInfo.size();
     	long start = 0L;
     	long end = dataReader.getEOF();
         if ( size > 0 )
         {
-            int index = Collections.binarySearch(indexInfo, new KeyPositionInfo(decoratedKey));
+            int index = Collections.binarySearch(indexInfo, new KeyPositionInfo(key));
             if ( index < 0 )
             {
                 /*
@@ -711,30 +840,127 @@ public static Coordinate getCoordinates(String decoratedKey, IFileReader dataRea
         return new Coordinate(start, end);
     }
     
-    public DataInputBuffer next(final String clientKey, String cfName, List<String> columnNames) throws IOException
+    /**
+     * Convert the application key into the appropriate application
+     * key based on the partition type.
+     * 
+     * @param key the application key
+     * @return the appropriate key based on partition mechanism
+    */
+    private String morphKey(String key)
+    {
+        String internalKey = key;
+        PartitionerType pType = StorageService.getPartitionerType();
+        switch ( pType )
     {
-        return next(clientKey, cfName, columnNames, null);
+            case OPHF:
+                break;
+                
+            default:
+                internalKey = FBUtilities.hash(key).toString();
+                break;
+        }
+        return internalKey;
     }
 
-    public DataInputBuffer next(final String clientKey, String cfName, List<String> columnNames, IndexHelper.TimeRange timeRange) throws IOException
+    public DataInputBuffer next(String key, String cf, List<String> cNames) throws IOException
+    {
+    	DataInputBuffer bufIn = null;        
+        IFileReader dataReader = null;
+        try
+        {
+            dataReader = SequenceFile.reader(dataFile_);
+            /* Morph key into actual key based on the partition type. */ 
+            key = morphKey(key);
+            Coordinate fileCoordinate = getCoordinates(key, dataReader);    
+            /*
+             * we have the position we have to read from in order to get the
+             * column family, get the column family and column(s) needed.
+            */          
+            bufIn = getData(dataReader, key, cf, cNames, fileCoordinate);
+        }
+        finally
+        {
+            if ( dataReader != null )
+            {
+                dataReader.close();
+            }
+        }
+        return bufIn;
+    }
+    
+    public DataInputBuffer next(String key, String columnName) throws IOException
     {
+        DataInputBuffer bufIn = null;
         IFileReader dataReader = null;
         try
         {
             dataReader = SequenceFile.reader(dataFile_);
             // dataReader = SequenceFile.chksumReader(dataFile_, 4*1024*1024);
+            /* Morph key into actual key based on the partition type. */ 
+            key = morphKey(key);
+            Coordinate fileCoordinate = getCoordinates(key, dataReader);
+            /*
+             * we have the position we have to read from in order to get the
+             * column family, get the column family and column(s) needed.
+            */            
+            bufIn = getData(dataReader, key, columnName, fileCoordinate);
+        }
+        finally
+        {
+            if ( dataReader != null )
+            {
+                dataReader.close();
+            }
+        }
+        return bufIn;
+    }
 
+    public DataInputBuffer next(String key, String columnName, IndexHelper.TimeRange timeRange) throws IOException
+    {
+        DataInputBuffer bufIn = null;
+        IFileReader dataReader = null;
+        try
+        {
+            dataReader = SequenceFile.reader(dataFile_);
             /* Morph key into actual key based on the partition type. */
-            String decoratedKey = StorageService.getPartitioner().decorateKey(clientKey);
-            Coordinate fileCoordinate = getCoordinates(decoratedKey, dataReader);
+            key = morphKey(key);
+            Coordinate fileCoordinate = getCoordinates(key, dataReader);
             /*
              * we have the position we have to read from in order to get the
              * column family, get the column family and column(s) needed.
             */
+            bufIn = getData(dataReader, key, columnName, timeRange, fileCoordinate);
+        }
+        finally
+        {
+            if ( dataReader != null )
+            {
+                dataReader.close();
+            }
+        }
+        return bufIn;
+    }
+    
+    long getSeekPosition(String key, long start)
+    {
+        Long seekStart = touchCache_.get(dataFile_ + ":" + key);
+        if( seekStart != null)
+        {
+            return seekStart;
+        }
+        return start;
+    }
+        
+    /*
+     * Get the data for the key from the position passed in. 
+    */
+    private DataInputBuffer getData(IFileReader dataReader, String key, String column, Coordinate section) throws IOException
+    {
             DataOutputBuffer bufOut = new DataOutputBuffer();
             DataInputBuffer bufIn = new DataInputBuffer();
 
-            long bytesRead = dataReader.next(decoratedKey, bufOut, cfName, columnNames, timeRange, fileCoordinate);
+        long bytesRead = dataReader.next(key, bufOut, column, section);
             if ( bytesRead != -1L )
             {
                 if ( bufOut.getLength() > 0 )
@@ -745,23 +971,93 @@ public DataInputBuffer next(final String clientKey, String cfName, List<String>
                     bufIn.readInt();
                 }
             }
+        
             return bufIn;
         }
-        finally
+    
+    private DataInputBuffer getData(IFileReader dataReader, String key, String cf, List<String> columns, Coordinate section) throws IOException
         {
-            if ( dataReader != null )
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        DataInputBuffer bufIn = new DataInputBuffer();
+                  
+        long bytesRead = dataReader.next(key, bufOut, cf, columns, section);
+        if ( bytesRead != -1L )
             {
-                dataReader.close();
+            if ( bufOut.getLength() > 0 )
+            {                     
+                bufIn.reset(bufOut.getData(), bufOut.getLength());             
+                /* read the key even though we do not use it */
+                bufIn.readUTF();
+                bufIn.readInt();            
+            }        
+        }
+        return bufIn;
+    }
+    
+    /*
+     * Get the data for the key from the position passed in. 
+    */
+    private DataInputBuffer getData(IFileReader dataReader, String key, String column, IndexHelper.TimeRange timeRange, Coordinate section) throws IOException
+    {
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        DataInputBuffer bufIn = new DataInputBuffer();
+                
+        try
+        {
+            dataReader.next(key, bufOut, column, timeRange, section);
+            if ( bufOut.getLength() > 0 )
+            {                              
+                bufIn.reset(bufOut.getData(), bufOut.getLength());            
+                /* read the key even though we do not use it */
+                bufIn.readUTF();
+                bufIn.readInt();            
             }
         }
+        catch ( IOException ex )
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }
+        return bufIn;
     }
 
-    public DataInputBuffer next(String clientKey, String columnFamilyColumn) throws IOException
+    /*
+     * Given a key we are interested in this method gets the
+     * closest index before the key on disk.
+     *
+     *  param @ key - key we are interested in.
+     *  return position of the closest index before the key
+     *  on disk or -1 if this key is not on disk.
+    */
+    private long getClosestIndexPositionToKeyOnDisk(String key)
     {
-        String[] values = RowMutation.getColumnAndColumnFamily(columnFamilyColumn);
-        String columnFamilyName = values[0];
-        List<String> cnNames = (values.length == 1) ? null : Arrays.asList(values[1]);
-        return next(clientKey, columnFamilyName, cnNames);
+        long position = -1L;
+        List<KeyPositionInfo> indexInfo = indexMetadataMap_.get(dataFile_);
+        int size = indexInfo.size();
+        int index = Collections.binarySearch(indexInfo, new KeyPositionInfo(key));
+        if ( index < 0 )
+    {
+            /*
+             * We are here which means that the requested
+             * key is not an index.
+            */
+            index = (++index)*(-1);
+            /* this means key is not present at all */
+            if ( index >= size )
+                return position;
+            /* a scan is in order. */
+            position = (index == 0) ? 0 : indexInfo.get(index - 1).position();
+        }
+        else
+        {
+            /*
+             * If we are here that means the key is in the index file
+             * and we can retrieve it w/o a scan. In reality we would
+             * like to have a retreive(key, fromPosition) but for now
+             * we use scan(start, start + 1) - a hack.
+            */
+            position = indexInfo.get(index).position();
+        }
+        return position;
     }
 
     public void close() throws IOException
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/io/SequenceFile.java b/incubator/cassandra/trunk/src/org/apache/cassandra/io/SequenceFile.java
index ab9df94a..8b03a83e 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/io/SequenceFile.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/io/SequenceFile.java
@@ -213,10 +213,12 @@ public long getFileSize() throws IOException
 
     public static class BufferWriter extends Writer
     {
+        private int size_;
 
         BufferWriter(String filename, int size) throws IOException
         {
             super(filename, size);
+            size_ = size;
         }
         
         @Override
@@ -268,6 +270,145 @@ public void close() throws IOException
         }
     }
 
+    public static class ConcurrentWriter extends AbstractWriter
+    {
+        private FileChannel fc_;
+
+        public ConcurrentWriter(String filename) throws IOException
+        {
+            super(filename);
+            RandomAccessFile raf = new RandomAccessFile(filename, "rw");
+            fc_ = raf.getChannel();
+        }
+
+        public long getCurrentPosition() throws IOException
+        {
+            return fc_.position();
+        }
+
+        public void seek(long position) throws IOException
+        {
+            fc_.position(position);
+        }
+
+        public void append(DataOutputBuffer buffer) throws IOException
+        {
+            int length = buffer.getLength();
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(length);
+            byteBuffer.put(buffer.getData(), 0, length);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+        
+        public void append(DataOutputBuffer keyBuffer, DataOutputBuffer buffer) throws IOException
+        {
+            int keyBufLength = keyBuffer.getLength();
+            if ( keyBuffer == null || keyBufLength == 0 )
+                throw new IllegalArgumentException("Key cannot be NULL or of zero length.");
+
+            /* Size allocated "int" for key length + key + "int" for data length + data */
+            int length = buffer.getLength();
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect( 4 + keyBufLength + 4 + length );
+            byteBuffer.putInt(keyBufLength);
+            byteBuffer.put(keyBuffer.getData(), 0, keyBufLength);
+            byteBuffer.putInt(length);
+            byteBuffer.put(buffer.getData(), 0, length);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public void append(String key, DataOutputBuffer buffer) throws IOException
+        {
+            if ( key == null )
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            int length = buffer.getLength();
+            /* Size allocated : utfPrefix_ + key length + "int" for data size + data */
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect( SequenceFile.utfPrefix_ + key.length() + 4 + length);
+            SequenceFile.writeUTF(byteBuffer, key);
+            byteBuffer.putInt(length);
+            byteBuffer.put(buffer.getData(), 0, length);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public void append(String key, byte[] value) throws IOException
+        {
+            if ( key == null )
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            /* Size allocated key length + "int" for data size + data */
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(utfPrefix_ + key.length() + 4 + value.length);
+            SequenceFile.writeUTF(byteBuffer, key);
+            byteBuffer.putInt(value.length);
+            byteBuffer.put(value);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public void append(String key, long value) throws IOException
+        {
+            if ( key == null )
+                throw new IllegalArgumentException("Key cannot be NULL.");
+
+            /* Size allocated key length + a long */
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(SequenceFile.utfPrefix_ + key.length() + 8);
+            SequenceFile.writeUTF(byteBuffer, key);
+            byteBuffer.putLong(value);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        /*
+         * Be extremely careful while using this API. This currently
+         * used to write the commit log header in the commit logs.
+         * If not used carefully it could completely screw up reads
+         * of other key/value pairs that are written.
+        */
+        public long writeDirect(byte[] bytes) throws IOException
+        {
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(bytes.length);
+            byteBuffer.put(bytes);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+            return fc_.position();
+        }
+        
+        public void writeLong(long value) throws IOException
+        {
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(8);
+            byteBuffer.putLong(value);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);
+        }
+
+        public void close() throws IOException
+        {
+            fc_.close();
+        }
+
+        public void close(byte[] footer, int size) throws IOException
+        {
+            /* Size is marker length + "int" for size + footer data */
+            ByteBuffer byteBuffer = ByteBuffer.allocateDirect( utfPrefix_ + SequenceFile.marker_.length() + 4 + footer.length);
+            SequenceFile.writeUTF(byteBuffer, SequenceFile.marker_);
+            byteBuffer.putInt(size);
+            byteBuffer.put(footer);
+            byteBuffer.flip();
+            fc_.write(byteBuffer);            
+        }
+
+        public String getFileName()
+        {
+            return filename_;
+        }
+
+        public long getFileSize() throws IOException
+        {
+            return fc_.size();
+        }
+    }
+    
     public static class FastConcurrentWriter extends AbstractWriter
     {
         private FileChannel fc_;
@@ -422,6 +563,34 @@ public String getFileName()
             return filename_;
         }
 
+        /**
+         * Given the application key this method basically figures if
+         * the key is in the block. Key comparisons differ based on the
+         * partition function. In OPHF key is stored as is but in the
+         * case of a Random hash key used internally is hash(key):key.
+         * @param key which we are looking for
+         * @param in DataInput stream into which we are looking for the key.
+         * @return true if key is found and false otherwise.
+         * @throws IOException
+         */
+        protected boolean isKeyInBlock(String key, DataInput in) throws IOException
+        {
+            boolean bVal = false;            
+            String keyInBlock = in.readUTF();
+            PartitionerType pType = StorageService.getPartitionerType();
+            switch ( pType )
+            {
+                case OPHF:
+                    bVal = keyInBlock.equals(key);
+                    break;
+                    
+                default:                    
+                    bVal = keyInBlock.split(":")[0].equals(key);
+                    break;
+            }
+            return bVal;
+        }
+       
         /**
          * Return the position of the given key from the block index.
          * @param key the key whose offset is to be extracted from the current block index
@@ -498,8 +667,7 @@ public long getPositionFromBlockIndex(String key) throws IOException
             int keys = bufIn.readInt();
             for ( int i = 0; i < keys; ++i )
             {
-                String keyInBlock = bufIn.readUTF();
-                if (keyInBlock.equals(key))
+                if ( isKeyInBlock(key, bufIn) )
                 {
                     long position = bufIn.readLong();
                     long dataSize = bufIn.readLong();
@@ -657,6 +825,272 @@ private int handleColumnTimeIndexes(String cfName, List<IndexHelper.ColumnIndexI
             return totalBytesRead;
         }
         
+        /**
+         * This is useful in figuring out the key in system. If an OPHF 
+         * is used then the "key" is the application supplied key. If a random
+         * partitioning mechanism is used then the key is of the form 
+         * hash:key where hash is used internally as the key.
+         * 
+         * @param in the DataInput stream from which the key needs to be read
+         * @return the appropriate key based on partitioning type
+         * @throws IOException
+         */
+        protected String readKeyFromDisk(DataInput in) throws IOException
+        {
+            String keyInDisk = null;
+            PartitionerType pType = StorageService.getPartitionerType();
+            switch( pType )
+            {
+                case OPHF:
+                    keyInDisk = in.readUTF();                  
+                    break;
+                    
+                default:
+                    keyInDisk = in.readUTF().split(":")[0];
+                    break;
+            }
+            return keyInDisk;
+        }
+
+        /**
+         * This method dumps the next key/value into the DataOuputStream
+         * passed in. Always use this method to query for application
+         * specific data as it will have indexes.
+         *
+         * @param key key we are interested in.
+         * @param dos DataOutputStream that needs to be filled.
+         * @param cf the IColumn we want to read
+         * @param section region of the file that needs to be read
+         * @return total number of bytes read/considered
+        */
+        public long next(String key, DataOutputBuffer bufOut, String cf, Coordinate section) throws IOException
+        {
+    		String[] values = RowMutation.getColumnAndColumnFamily(cf);
+    		String columnFamilyName = values[0];    		
+    		String columnName = (values.length == 1) ? null : values[1];
+
+            long bytesRead = -1L;
+            if ( isEOF() )
+                return bytesRead;
+            seekTo(key, section);            
+            /* note the position where the key starts */
+            long startPosition = file_.getFilePointer();
+            String keyInDisk = readKeyFromDisk(file_);
+            if ( keyInDisk != null )
+            {
+                /*
+                 * If key on disk is greater than requested key
+                 * we can bail out since we exploit the property
+                 * of the SSTable format.
+                */
+                if ( keyInDisk.compareTo(key) > 0 )
+                    return bytesRead;
+
+                /*
+                 * If we found the key then we populate the buffer that
+                 * is passed in. If not then we skip over this key and
+                 * position ourselves to read the next one.
+                */
+                int dataSize = file_.readInt();
+                if ( keyInDisk.equals(key) )
+                {
+                    /* write the key into buffer */
+                    bufOut.writeUTF( keyInDisk );                    
+                    
+                    if(columnName == null)
+                    {
+                    	int bytesSkipped = IndexHelper.skipBloomFilterAndIndex(file_);
+	                    /*
+	                     * read the correct number of bytes for the column family and
+	                     * write data into buffer. Substract from dataSize the bloom
+                         * filter size.
+	                    */                        
+                    	dataSize -= bytesSkipped;
+                    	/* write the data size */
+                    	bufOut.writeInt(dataSize);
+	                    /* write the data into buffer, except the boolean we have read */
+	                    bufOut.write(file_, dataSize);
+                    }
+                    else
+                    {
+                        /* Read the bloom filter for the column summarization */
+                        long preBfPos = file_.getFilePointer();
+                        BloomFilter bf = defreezeBloomFilter();
+                        /* column does not exist in this file */
+                        if ( !bf.isPresent(columnName) ) 
+                            return bytesRead;
+                        long postBfPos = file_.getFilePointer();
+                        dataSize -= (postBfPos - preBfPos);
+                        
+                        List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
+                        /* Read the name indexes if present */
+                        int totalBytesRead = handleColumnNameIndexes(columnFamilyName, columnIndexList);                    	
+                    	dataSize -= totalBytesRead;
+
+                        /* read the column family name */
+                        String cfName = file_.readUTF();
+                        dataSize -= (utfPrefix_ + cfName.length());
+
+                        /* read if this cf is marked for delete */
+                        boolean markedForDelete = file_.readBoolean();
+                        dataSize -= 1;
+
+                        /* read the total number of columns */
+                        int totalNumCols = file_.readInt();
+                        dataSize -= 4;
+                                                
+                        /* get the column range we have to read */
+                        IndexHelper.ColumnIndexInfo cIndexInfo = new IndexHelper.ColumnNameIndexInfo(columnName);
+                        IndexHelper.ColumnRange columnRange = IndexHelper.getColumnRangeFromNameIndex(cIndexInfo, columnIndexList, dataSize, totalNumCols);
+
+                        Coordinate coordinate = columnRange.coordinate();
+                		/* seek to the correct offset to the data, and calculate the data size */
+                        file_.skipBytes((int)coordinate.start_);                        
+                        dataSize = (int)(coordinate.end_ - coordinate.start_);
+                        
+                        /*
+                         * write the number of columns in the column family we are returning:
+                         * 	dataSize that we are reading +
+                         * 	length of column family name +
+                         * 	one booleanfor deleted or not +
+                         * 	one int for number of columns
+                        */
+                        bufOut.writeInt(dataSize + utfPrefix_+cfName.length() + 4 + 1);
+                        /* write the column family name */
+                        bufOut.writeUTF(cfName);
+                        /* write if this cf is marked for delete */
+                        bufOut.writeBoolean(markedForDelete);
+                        /* write number of columns */
+                        bufOut.writeInt(columnRange.count());
+                        /* now write the columns */
+                        bufOut.write(file_, dataSize);
+                    }
+                }
+                else
+                {
+                    /* skip over data portion */
+                	file_.seek(dataSize + file_.getFilePointer());
+                }
+                
+                long endPosition = file_.getFilePointer();
+                bytesRead = endPosition - startPosition;                 
+            }
+
+            return bytesRead;
+        }
+        
+        /**
+         * This method dumps the next key/value into the DataOuputStream
+         * passed in. Always use this method to query for application
+         * specific data as it will have indexes.
+         
+         * @param key key we are interested in.
+         * @param dos DataOutputStream that needs to be filled.
+         * @param column name of the column in our format.
+         * @param timeRange time range we are interested in.
+         * @param section region of the file that needs to be read
+         * @throws IOException
+         * @return number of bytes that were read.
+        */
+        public long next(String key, DataOutputBuffer bufOut, String cf, IndexHelper.TimeRange timeRange, Coordinate section) throws IOException
+        {
+            String[] values = RowMutation.getColumnAndColumnFamily(cf);
+            String columnFamilyName = values[0];            
+            String columnName = (values.length == 1) ? null : values[1];
+
+            long bytesRead = -1L;
+            if ( isEOF() )
+                return bytesRead;                                
+            seekTo(key, section);            
+            /* note the position where the key starts */
+            long startPosition = file_.getFilePointer();
+            String keyInDisk = readKeyFromDisk(file_);
+            if ( keyInDisk != null )
+            {
+                /*
+                 * If key on disk is greater than requested key
+                 * we can bail out since we exploit the property
+                 * of the SSTable format.
+                */
+                if ( keyInDisk.compareTo(key) > 0 )
+                    return bytesRead;
+
+                /*
+                 * If we found the key then we populate the buffer that
+                 * is passed in. If not then we skip over this key and
+                 * position ourselves to read the next one.
+                */
+                int dataSize = file_.readInt();
+                if ( keyInDisk.equals(key) )
+                {
+                    /* write the key into buffer */
+                    bufOut.writeUTF( keyInDisk );                    
+                    
+                    if(columnName == null)
+                    {
+                        int bytesSkipped = IndexHelper.skipBloomFilter(file_);
+                        /*
+                         * read the correct number of bytes for the column family and
+                         * write data into buffer. Substract from dataSize the bloom
+                         * filter size.
+                        */                        
+                        dataSize -= bytesSkipped;
+                        List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
+                        /* Read the times indexes if present */
+                        int totalBytesRead = handleColumnTimeIndexes(columnFamilyName, columnIndexList);                        
+                        dataSize -= totalBytesRead;
+                        
+                        /* read the column family name */
+                        String cfName = file_.readUTF();
+                        dataSize -= (utfPrefix_ + cfName.length());
+
+                        /* read if this cf is marked for delete */
+                        boolean markedForDelete = file_.readBoolean();
+                        dataSize -= 1;
+
+                        /* read the total number of columns */
+                        int totalNumCols = file_.readInt();
+                        dataSize -= 4;
+                                                
+                        /* get the column range we have to read */                        
+                        IndexHelper.ColumnRange columnRange = IndexHelper.getColumnRangeFromTimeIndex(timeRange, columnIndexList, dataSize, totalNumCols);
+
+                        Coordinate coordinate = columnRange.coordinate();
+                        /* seek to the correct offset to the data, and calculate the data size */
+                        file_.skipBytes((int)coordinate.start_);
+                        dataSize = (int)(coordinate.end_ - coordinate.start_);
+                        
+                        /*
+                         * write the number of columns in the column family we are returning:
+                         *  dataSize that we are reading +
+                         *  length of column family name +
+                         *  one booleanfor deleted or not +
+                         *  one int for number of columns
+                        */
+                        bufOut.writeInt(dataSize + utfPrefix_+cfName.length() + 4 + 1);
+                        /* write the column family name */
+                        bufOut.writeUTF(cfName);
+                        /* write if this cf is marked for delete */
+                        bufOut.writeBoolean(markedForDelete);
+                        /* write number of columns */
+                        bufOut.writeInt(columnRange.count());
+                        /* now write the columns */
+                        bufOut.write(file_, dataSize);
+                    }
+                }
+                else
+                {
+                    /* skip over data portion */
+                    file_.seek(dataSize + file_.getFilePointer());
+                }
+                
+                long endPosition = file_.getFilePointer();
+                bytesRead = endPosition - startPosition;                 
+            }
+
+            return bytesRead;
+        }
+
         /**
          * This method dumps the next key/value into the DataOuputStream
          * passed in. Always use this method to query for application
@@ -670,11 +1104,11 @@ private int handleColumnTimeIndexes(String cfName, List<IndexHelper.ColumnIndexI
          * @return total number of bytes read/considered
          *
         */
-        public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, List<String> columnNames, IndexHelper.TimeRange timeRange, Coordinate section) throws IOException
+        public long next(String key, DataOutputBuffer bufOut, String cf, List<String> columnNames, Coordinate section) throws IOException
         {
-            assert timeRange == null || columnNames == null; // at most one may be non-null
-            
-            List<String> cNames = columnNames == null ? null : new ArrayList<String>(columnNames);
+        	String[] values = RowMutation.getColumnAndColumnFamily(cf);
+    		String columnFamilyName = values[0];
+            List<String> cNames = new ArrayList<String>(columnNames);
 
             long bytesRead = -1L;
             if ( isEOF() )
@@ -683,7 +1117,7 @@ public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, L
             seekTo(key, section);            
             /* note the position where the key starts */
             long startPosition = file_.getFilePointer();
-            String keyInDisk = file_.readUTF();
+            String keyInDisk = readKeyFromDisk(file_);
             if ( keyInDisk != null )
             {
                 /*
@@ -737,9 +1171,7 @@ public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, L
                         
                         List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
                         /* read the column name indexes if present */
-                        int totalBytesRead = (timeRange == null)
-                                           ? handleColumnNameIndexes(columnFamilyName, columnIndexList)
-                                           : handleColumnTimeIndexes(columnFamilyName, columnIndexList);
+                        int totalBytesRead = handleColumnNameIndexes(columnFamilyName, columnIndexList);                        
                     	dataSize -= totalBytesRead;
 
                         /* read the column family name */
@@ -747,8 +1179,8 @@ public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, L
                         dataSize -= (utfPrefix_ + cfName.length());
 
                         /* read if this cf is marked for delete */
-                        long markedForDeleteAt = file_.readLong();
-                        dataSize -= 8;
+                        boolean markedForDelete = file_.readBoolean();
+                        dataSize -= 1;
 
                         /* read the total number of columns */
                         int totalNumCols = file_.readInt();
@@ -758,15 +1190,7 @@ public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, L
                         /* sort the required list of columns */
                         Collections.sort(cNames);
                         /* get the various column ranges we have to read */
-                        List<IndexHelper.ColumnRange> columnRanges;
-                        if (timeRange == null)
-                        {
-                            columnRanges = IndexHelper.getMultiColumnRangesFromNameIndex(cNames, columnIndexList, dataSize, totalNumCols);
-                        }
-                        else
-                        {
-                            columnRanges = Arrays.asList(IndexHelper.getColumnRangeFromTimeIndex(timeRange, columnIndexList, dataSize, totalNumCols));
-                        }
+                        List<IndexHelper.ColumnRange> columnRanges = IndexHelper.getMultiColumnRangesFromNameIndex(cNames, columnIndexList, dataSize, totalNumCols);
 
                         /* calculate the data size */
                         int numColsReturned = 0;
@@ -789,7 +1213,7 @@ public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, L
                         /* write the column family name */
                         bufOut.writeUTF(cfName);
                         /* write if this cf is marked for delete */
-                        bufOut.writeLong(markedForDeleteAt);
+                        bufOut.writeBoolean(markedForDelete);
                         /* write number of columns */
                         bufOut.writeInt(numColsReturned);
                         int prevPosition = 0;
@@ -856,6 +1280,63 @@ public long next(DataOutputBuffer bufOut) throws IOException
                 bytesRead = -1L;
             return bytesRead;
         }
+
+        /**
+         * This method dumps the next key/value into the DataOuputStream
+         * passed in.
+         *
+         * @param key - key we are interested in.
+         * @param dos - DataOutputStream that needs to be filled.
+         * @param section region of the file that needs to be read
+         * @return total number of bytes read/considered
+         */
+        public long next(String key, DataOutputBuffer bufOut, Coordinate section) throws IOException
+        {
+            long bytesRead = -1L;
+            if ( isEOF() )
+                return bytesRead;
+                   
+            seekTo(key, section);            
+            /* note the position where the key starts */
+            long startPosition = file_.getFilePointer(); 
+            String keyInDisk = readKeyFromDisk(file_);
+            if ( keyInDisk != null )
+            {
+                /*
+                 * If key on disk is greater than requested key
+                 * we can bail out since we exploit the property
+                 * of the SSTable format.
+                */
+                if ( keyInDisk.compareTo(key) > 0 )
+                    return bytesRead;
+
+                /*
+                 * If we found the key then we populate the buffer that
+                 * is passed in. If not then we skip over this key and
+                 * position ourselves to read the next one.
+                */
+                int dataSize = file_.readInt();
+                if ( keyInDisk.equals(key) )
+                {
+                    /* write the key into buffer */
+                    bufOut.writeUTF( keyInDisk );
+                    /* write data size into buffer */
+                    bufOut.writeInt(dataSize);
+                    /* write the data into buffer */
+                    bufOut.write(file_, dataSize);
+                }
+                else
+                {
+                    /* skip over data portion */
+                	file_.seek(dataSize + file_.getFilePointer());
+                }
+
+                long endPosition = file_.getFilePointer();
+                bytesRead = endPosition - startPosition;
+            }
+
+            return bytesRead;
+        }
     }
 
     public static class Reader extends AbstractReader
@@ -951,6 +1432,7 @@ protected void init(String filename) throws IOException
     }
     
     private static Logger logger_ = Logger.getLogger( SequenceFile.class ) ;
+    public static final short utfPrefix_ = 2;
     public static final String marker_ = "Bloom-Filter";
 
     public static IFileWriter writer(String filename) throws IOException
@@ -968,6 +1450,11 @@ public static IFileWriter chksumWriter(String filename, int size) throws IOExcep
         return new ChecksumWriter(filename, size);
     }
 
+    public static IFileWriter concurrentWriter(String filename) throws IOException
+    {
+        return new ConcurrentWriter(filename);
+    }
+    
     public static IFileWriter fastWriter(String filename, int size) throws IOException
     {
         return new FastConcurrentWriter(filename, size);
@@ -988,6 +1475,11 @@ public static IFileReader chksumReader(String filename, int size) throws IOExcep
         return new ChecksumReader(filename, size);
     }
 
+    public static boolean readBoolean(ByteBuffer buffer)
+    {
+        return ( buffer.get() == 1 ? true : false );
+    }
+
     /**
      * Efficiently writes a UTF8 string to the buffer.
      * Assuming all Strings that are passed in have length
@@ -1056,4 +1548,83 @@ else if (c > 0x07FF)
         buffer.put(bytearr, 0, utflen + 2);
     }
 
+    /**
+     * Read a UTF8 string from a serialized buffer.
+     * @param buffer buffer from which a UTF8 string is read
+     * @return a Java String
+    */
+    protected static String readUTF(ByteBuffer in) throws IOException
+    {
+        int utflen = in.getShort();
+        byte[] bytearr = new byte[utflen];
+        char[] chararr = new char[utflen];
+
+        int c, char2, char3;
+        int count = 0;
+        int chararr_count = 0;
+
+        in.get(bytearr, 0, utflen);
+
+        while (count < utflen)
+        {
+            c = (int) bytearr[count] & 0xff;
+            if (c > 127)
+                break;
+            count++;
+            chararr[chararr_count++] = (char) c;
+        }
+
+        while (count < utflen)
+        {
+            c = (int) bytearr[count] & 0xff;
+            switch (c >> 4)
+            {
+            case 0:
+            case 1:
+            case 2:
+            case 3:
+            case 4:
+            case 5:
+            case 6:
+            case 7:
+                /* 0xxxxxxx */
+                count++;
+                chararr[chararr_count++] = (char) c;
+                break;
+            case 12:
+            case 13:
+                /* 110x xxxx 10xx xxxx */
+                count += 2;
+                if (count > utflen)
+                    throw new UTFDataFormatException(
+                    "malformed input: partial character at end");
+                char2 = (int) bytearr[count - 1];
+                if ((char2 & 0xC0) != 0x80)
+                    throw new UTFDataFormatException(
+                            "malformed input around byte " + count);
+                chararr[chararr_count++] = (char) (((c & 0x1F) << 6) | (char2 & 0x3F));
+                break;
+            case 14:
+                /* 1110 xxxx 10xx xxxx 10xx xxxx */
+                count += 3;
+                if (count > utflen)
+                    throw new UTFDataFormatException(
+                    "malformed input: partial character at end");
+                char2 = (int) bytearr[count - 2];
+                char3 = (int) bytearr[count - 1];
+                if (((char2 & 0xC0) != 0x80) || ((char3 & 0xC0) != 0x80))
+                    throw new UTFDataFormatException(
+                            "malformed input around byte " + (count - 1));
+                chararr[chararr_count++] = (char) (((c & 0x0F) << 12)
+                        | ((char2 & 0x3F) << 6) | ((char3 & 0x3F) << 0));
+                break;
+            default:
+                /* 10xx xxxx, 1111 xxxx */
+                throw new UTFDataFormatException("malformed input around byte "
+                        + count);
+            }
+        }
+        // The number of chars produced may be less than utflen
+        return new String(chararr, 0, chararr_count);
+    }
 }
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/AbstractStrategy.java b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/AbstractStrategy.java
index fdc43409..0e1c107e 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/AbstractStrategy.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/AbstractStrategy.java
@@ -1,5 +1,6 @@
 package org.apache.cassandra.locator;
 
+import java.math.BigInteger;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -7,12 +8,11 @@
 import java.util.List;
 import java.util.Map;
 
-import org.apache.log4j.Logger;
-
 import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.dht.Token;
 import org.apache.cassandra.gms.FailureDetector;
 import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.log4j.Logger;
 
 /**
  * This class contains a helper method that will be used by
@@ -45,10 +45,10 @@ protected void retrofitPorts(List<EndPoint> eps)
     protected EndPoint getNextAvailableEndPoint(EndPoint startPoint, List<EndPoint> topN, List<EndPoint> liveNodes)
     {
         EndPoint endPoint = null;
-        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-        List tokens = new ArrayList(tokenToEndPointMap.keySet());
+        Map<BigInteger, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List<BigInteger> tokens = new ArrayList<BigInteger>(tokenToEndPointMap.keySet());
         Collections.sort(tokens);
-        Token token = tokenMetadata_.getToken(startPoint);
+        BigInteger token = tokenMetadata_.getToken(startPoint);
         int index = Collections.binarySearch(tokens, token);
         if(index < 0)
         {
@@ -76,7 +76,7 @@ protected EndPoint getNextAvailableEndPoint(EndPoint startPoint, List<EndPoint>
      * endpoint which is in the top N.
      * Get the map of top N to the live nodes currently.
      */
-    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(Token token)
+    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(BigInteger token)
     {
         List<EndPoint> liveList = new ArrayList<EndPoint>();
         Map<EndPoint, EndPoint> map = new HashMap<EndPoint, EndPoint>();
@@ -107,6 +107,6 @@ protected EndPoint getNextAvailableEndPoint(EndPoint startPoint, List<EndPoint>
         return map;
     }
 
-    public abstract EndPoint[] getStorageEndPoints(Token token);
+    public abstract EndPoint[] getStorageEndPoints(BigInteger token);
 
 }
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/IReplicaPlacementStrategy.java b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/IReplicaPlacementStrategy.java
index a2cef1fd..ff8a806d 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/IReplicaPlacementStrategy.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/IReplicaPlacementStrategy.java
@@ -18,9 +18,9 @@
 
 package org.apache.cassandra.locator;
 
+import java.math.BigInteger;
 import java.util.Map;
 
-import org.apache.cassandra.dht.Token;
 import org.apache.cassandra.net.EndPoint;
 
 
@@ -32,7 +32,8 @@
  */
 public interface IReplicaPlacementStrategy
 {
-	public EndPoint[] getStorageEndPoints(Token token);
-    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap);
-    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(Token token);    
+	public EndPoint[] getStorageEndPoints(BigInteger token);
+	public Map<String, EndPoint[]> getStorageEndPoints(String[] keys);
+    public EndPoint[] getStorageEndPoints(BigInteger token, Map<BigInteger, EndPoint> tokenToEndPointMap);
+    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(BigInteger token);    
 }
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/RackAwareStrategy.java b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/RackAwareStrategy.java
index 36aa44b3..b710cee5 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/RackAwareStrategy.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/RackAwareStrategy.java
@@ -1,13 +1,14 @@
 package org.apache.cassandra.locator;
 
+import java.math.BigInteger;
 import java.net.UnknownHostException;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
 import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.dht.Token;
 import org.apache.cassandra.net.EndPoint;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.LogUtil;
@@ -27,7 +28,7 @@ public RackAwareStrategy(TokenMetadata tokenMetadata)
         super(tokenMetadata);
     }
     
-    public EndPoint[] getStorageEndPoints(Token token)
+    public EndPoint[] getStorageEndPoints(BigInteger token)
     {
         int startIndex = 0 ;
         List<EndPoint> list = new ArrayList<EndPoint>();
@@ -35,8 +36,8 @@ public RackAwareStrategy(TokenMetadata tokenMetadata)
         boolean bOtherRack = false;
         int foundCount = 0;
         int N = DatabaseDescriptor.getReplicationFactor();
-        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-        List tokens = new ArrayList(tokenToEndPointMap.keySet());
+        Map<BigInteger, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List<BigInteger> tokens = new ArrayList<BigInteger>(tokenToEndPointMap.keySet());
         Collections.sort(tokens);
         int index = Collections.binarySearch(tokens, token);
         if(index < 0)
@@ -107,7 +108,97 @@ public RackAwareStrategy(TokenMetadata tokenMetadata)
         return list.toArray(new EndPoint[0]);
     }
 
-    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap)
+    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys)
+    {
+    	Map<String, EndPoint[]> results = new HashMap<String, EndPoint[]>();
+    	List<EndPoint> list = new ArrayList<EndPoint>();
+    	int startIndex = 0 ;
+    	int foundCount = 0;
+    	boolean bDataCenter = false;
+        boolean bOtherRack = false;
+    	
+    	Map<BigInteger, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+    	int N = DatabaseDescriptor.getReplicationFactor();
+        List<BigInteger> tokens = new ArrayList<BigInteger>(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        
+        for ( String key : keys )
+        {
+        	BigInteger token = StorageService.hash(key);
+        	int index = Collections.binarySearch(tokens, token);
+            if(index < 0)
+            {
+                index = (index + 1) * (-1);
+                if (index >= tokens.size())
+                    index = 0;
+            }
+            int totalNodes = tokens.size();
+            // Add the node at the index by default
+            list.add(tokenToEndPointMap.get(tokens.get(index)));
+            foundCount++;
+            if( N == 1 )
+            {
+            	results.put( key, list.toArray(new EndPoint[0]) );
+                return results;
+            }
+            startIndex = (index + 1)%totalNodes;
+            IEndPointSnitch endPointSnitch = StorageService.instance().getEndPointSnitch();
+            
+            for (int i = startIndex, count = 1; count < totalNodes && foundCount < N; ++count, i = (i+1)%totalNodes)
+            {
+                try
+                {
+                    // First try to find one in a different data center
+                    if(!endPointSnitch.isInSameDataCenter(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))))
+                    {
+                        // If we have already found something in a diff datacenter no need to find another
+                        if( !bDataCenter )
+                        {
+                            list.add(tokenToEndPointMap.get(tokens.get(i)));
+                            bDataCenter = true;
+                            foundCount++;
+                        }
+                        continue;
+                    }
+                    // Now  try to find one on a different rack
+                    if(!endPointSnitch.isOnSameRack(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))) &&
+                            endPointSnitch.isInSameDataCenter(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))))
+                    {
+                        // If we have already found something in a diff rack no need to find another
+                        if( !bOtherRack )
+                        {
+                            list.add(tokenToEndPointMap.get(tokens.get(i)));
+                            bOtherRack = true;
+                            foundCount++;
+                        }
+                        continue;
+                    }
+                }
+                catch (UnknownHostException e)
+                {
+                    logger_.debug(LogUtil.throwableToString(e));
+                }
+
+            }
+            // If we found N number of nodes we are good. This loop wil just exit. Otherwise just
+            // loop through the list and add until we have N nodes.
+            for (int i = startIndex, count = 1; count < totalNodes && foundCount < N; ++count, i = (i+1)%totalNodes)
+            {
+                if( ! list.contains(tokenToEndPointMap.get(tokens.get(i))))
+                {
+                    list.add(tokenToEndPointMap.get(tokens.get(i)));
+                    foundCount++;
+                    continue;
+                }
+            }
+            retrofitPorts(list);
+            results.put(key, list.toArray(new EndPoint[0]));
+        }
+        
+        return results;
+    }
+    
+    public EndPoint[] getStorageEndPoints(BigInteger token, Map<BigInteger, EndPoint> tokenToEndPointMap)
     {
         throw new UnsupportedOperationException("This operation is not currently supported");
     }
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/RackUnawareStrategy.java b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/RackUnawareStrategy.java
index 50839e36..568be708 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/RackUnawareStrategy.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/RackUnawareStrategy.java
@@ -1,14 +1,18 @@
 package org.apache.cassandra.locator;
 
+import java.math.BigInteger;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.dht.Range;
 import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
 
 
 /**
@@ -21,24 +25,25 @@
 {   
     /* Use this flag to check if initialization is in order. */
     private AtomicBoolean initialized_ = new AtomicBoolean(false);
+    private Map<Range, List<EndPoint>> rangeToEndPointMap_;
 
     public RackUnawareStrategy(TokenMetadata tokenMetadata)
     {
         super(tokenMetadata);
     }
     
-    public EndPoint[] getStorageEndPoints(Token token)
+    public EndPoint[] getStorageEndPoints(BigInteger token)
     {
         return getStorageEndPoints(token, tokenMetadata_.cloneTokenEndPointMap());            
     }
     
-    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap)
+    public EndPoint[] getStorageEndPoints(BigInteger token, Map<BigInteger, EndPoint> tokenToEndPointMap)
     {
         int startIndex = 0 ;
         List<EndPoint> list = new ArrayList<EndPoint>();
         int foundCount = 0;
         int N = DatabaseDescriptor.getReplicationFactor();
-        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
+        List<BigInteger> tokens = new ArrayList<BigInteger>(tokenToEndPointMap.keySet());
         Collections.sort(tokens);
         int index = Collections.binarySearch(tokens, token);
         if(index < 0)
@@ -67,4 +72,83 @@ public RackUnawareStrategy(TokenMetadata tokenMetadata)
         return list.toArray(new EndPoint[0]);
     }
 
+    private void doInitialization()
+    {
+        if ( !initialized_.get() )
+        {
+            /* construct the mapping from the ranges to the replicas responsible for them */
+            rangeToEndPointMap_ = StorageService.instance().getRangeToEndPointMap();            
+            initialized_.set(true);
+        }
+    }
+    
+    /**
+     * This method determines which range in the array actually contains
+     * the hash of the key
+     * @param ranges
+     * @param key
+     * @return
+     */
+    private int findRangeIndexForKey(Range[] ranges, String key)
+    {
+        int index = 0;
+        BigInteger hash = StorageService.hash(key);
+        for ( int i = 0; i < ranges.length; ++i )
+        {
+            if ( ranges[i].contains(hash) )
+            {
+                index = i;
+                break;
+            }
+        }
+        
+        return index;
+    }
+    
+    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys)
+    {              
+        Arrays.sort(keys);
+        Range[] ranges = StorageService.instance().getAllRanges();
+        
+    	Map<String, EndPoint[]> results = new HashMap<String, EndPoint[]>();
+    	List<EndPoint> list = new ArrayList<EndPoint>();
+    	int startIndex = 0 ;
+    	int foundCount = 0;
+    	
+    	Map<BigInteger, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+    	int N = DatabaseDescriptor.getReplicationFactor();
+        List<BigInteger> tokens = new ArrayList<BigInteger>(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        for ( String key : keys )
+        {
+        	BigInteger token = StorageService.hash(key);
+        	int index = Collections.binarySearch(tokens, token);
+            if(index < 0)
+            {
+                index = (index + 1) * (-1);
+                if (index >= tokens.size())
+                    index = 0;
+            }
+            int totalNodes = tokens.size();
+            // Add the node at the index by default
+            list.add(tokenToEndPointMap.get(tokens.get(index)));
+            foundCount++;
+            startIndex = (index + 1)%totalNodes;
+            // If we found N number of nodes we are good. This loop will just exit. Otherwise just
+            // loop through the list and add until we have N nodes.
+            for (int i = startIndex, count = 1; count < totalNodes && foundCount < N; ++count, i = (i+1)%totalNodes)
+            {
+                if( ! list.contains(tokenToEndPointMap.get(tokens.get(i))))
+                {
+                    list.add(tokenToEndPointMap.get(tokens.get(i)));
+                    foundCount++;
+                    continue;
+                }
+            }
+            retrofitPorts(list);
+            results.put(key, list.toArray(new EndPoint[0]));
+        }
+        
+        return results;
+    }
 }
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/TokenMetadata.java b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/TokenMetadata.java
index 17ef95ae..801dff0b 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/locator/TokenMetadata.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/locator/TokenMetadata.java
@@ -18,13 +18,18 @@
 
 package org.apache.cassandra.locator;
 
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.math.BigInteger;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.locks.ReadWriteLock;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 
-import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.CompactEndPointSerializationHelper;
 import org.apache.cassandra.net.EndPoint;
 
 
@@ -34,19 +39,29 @@
 
 public class TokenMetadata
 {
+    private static ICompactSerializer<TokenMetadata> serializer_ = new TokenMetadataSerializer();
+    
+    public static ICompactSerializer<TokenMetadata> serializer()
+    {
+        return serializer_;
+    }
+    
     /* Maintains token to endpoint map of every node in the cluster. */
-    private Map<Token, EndPoint> tokenToEndPointMap_ = new HashMap<Token, EndPoint>();
+    private Map<BigInteger, EndPoint> tokenToEndPointMap_ = new HashMap<BigInteger, EndPoint>();    
     /* Maintains a reverse index of endpoint to token in the cluster. */
-    private Map<EndPoint, Token> endPointToTokenMap_ = new HashMap<EndPoint, Token>();
+    private Map<EndPoint, BigInteger> endPointToTokenMap_ = new HashMap<EndPoint, BigInteger>();
     
     /* Use this lock for manipulating the token map */
-    private final ReadWriteLock lock_ = new ReentrantReadWriteLock(true);
+    private ReadWriteLock lock_ = new ReentrantReadWriteLock(true);
 
+    /*
+     * For JAXB purposes. 
+    */
     public TokenMetadata()
     {
     }
 
-    private TokenMetadata(Map<Token, EndPoint> tokenToEndPointMap, Map<EndPoint, Token> endPointToTokenMap)
+    protected TokenMetadata(Map<BigInteger, EndPoint> tokenToEndPointMap, Map<EndPoint, BigInteger> endPointToTokenMap)
     {
         tokenToEndPointMap_ = tokenToEndPointMap;
         endPointToTokenMap_ = endPointToTokenMap;
@@ -54,18 +69,20 @@ private TokenMetadata(Map<Token, EndPoint> tokenToEndPointMap, Map<EndPoint, Tok
     
     public TokenMetadata cloneMe()
     {
-        return new TokenMetadata(cloneTokenEndPointMap(), cloneEndPointTokenMap());
+        Map<BigInteger, EndPoint> tokenToEndPointMap = cloneTokenEndPointMap();
+        Map<EndPoint, BigInteger> endPointToTokenMap = cloneEndPointTokenMap();
+        return new TokenMetadata( tokenToEndPointMap, endPointToTokenMap );
     }
     
     /**
      * Update the two maps in an safe mode. 
     */
-    public void update(Token token, EndPoint endpoint)
+    public void update(BigInteger token, EndPoint endpoint)
     {
         lock_.writeLock().lock();
         try
         {            
-            Token oldToken = endPointToTokenMap_.get(endpoint);
+            BigInteger oldToken = endPointToTokenMap_.get(endpoint);
             if ( oldToken != null )
                 tokenToEndPointMap_.remove(oldToken);
             tokenToEndPointMap_.put(token, endpoint);
@@ -86,7 +103,7 @@ public void remove(EndPoint endpoint)
         lock_.writeLock().lock();
         try
         {            
-            Token oldToken = endPointToTokenMap_.get(endpoint);
+            BigInteger oldToken = endPointToTokenMap_.get(endpoint);
             if ( oldToken != null )
                 tokenToEndPointMap_.remove(oldToken);            
             endPointToTokenMap_.remove(endpoint);
@@ -97,7 +114,7 @@ public void remove(EndPoint endpoint)
         }
     }
     
-    public Token getToken(EndPoint endpoint)
+    public BigInteger getToken(EndPoint endpoint)
     {
         lock_.readLock().lock();
         try
@@ -126,12 +143,12 @@ public boolean isKnownEndPoint(EndPoint ep)
     /*
      * Returns a safe clone of tokenToEndPointMap_.
     */
-    public Map<Token, EndPoint> cloneTokenEndPointMap()
+    public Map<BigInteger, EndPoint> cloneTokenEndPointMap()
     {
         lock_.readLock().lock();
         try
         {            
-            return new HashMap<Token, EndPoint>( tokenToEndPointMap_ );
+            return new HashMap<BigInteger, EndPoint>( tokenToEndPointMap_ );
         }
         finally
         {
@@ -142,12 +159,12 @@ public boolean isKnownEndPoint(EndPoint ep)
     /*
      * Returns a safe clone of endPointTokenMap_.
     */
-    public Map<EndPoint, Token> cloneEndPointTokenMap()
+    public Map<EndPoint, BigInteger> cloneEndPointTokenMap()
     {
         lock_.readLock().lock();
         try
         {            
-            return new HashMap<EndPoint, Token>( endPointToTokenMap_ );
+            return new HashMap<EndPoint, BigInteger>( endPointToTokenMap_ );
         }
         finally
         {
@@ -171,3 +188,51 @@ public String toString()
         return sb.toString();
     }
 }
+
+class TokenMetadataSerializer implements ICompactSerializer<TokenMetadata>
+{
+    public void serialize(TokenMetadata tkMetadata, DataOutputStream dos) throws IOException
+    {        
+        Map<BigInteger, EndPoint> tokenToEndPointMap = tkMetadata.cloneTokenEndPointMap();
+        Set<BigInteger> tokens = tokenToEndPointMap.keySet();
+        /* write the size */
+        dos.writeInt(tokens.size());        
+        for ( BigInteger token : tokens )
+        {
+            byte[] bytes = token.toByteArray();
+            /* Convert the BigInteger to byte[] and persist */
+            dos.writeInt(bytes.length);
+            dos.write(bytes); 
+            /* Write the endpoint out */
+            CompactEndPointSerializationHelper.serialize(tokenToEndPointMap.get(token), dos);
+        }
+    }
+    
+    public TokenMetadata deserialize(DataInputStream dis) throws IOException
+    {
+        TokenMetadata tkMetadata = null;
+        int size = dis.readInt();
+        
+        if ( size > 0 )
+        {
+            Map<BigInteger, EndPoint> tokenToEndPointMap = new HashMap<BigInteger, EndPoint>();
+            Map<EndPoint, BigInteger> endPointToTokenMap = new HashMap<EndPoint, BigInteger>();
+            
+            for ( int i = 0; i < size; ++i )
+            {
+                /* Read the byte[] and convert to BigInteger */
+                byte[] bytes = new byte[dis.readInt()];
+                dis.readFully(bytes);
+                BigInteger token = new BigInteger(bytes);
+                /* Read the endpoint out */
+                EndPoint endpoint = CompactEndPointSerializationHelper.deserialize(dis);
+                tokenToEndPointMap.put(token, endpoint);
+                endPointToTokenMap.put(endpoint, token);
+            }
+            
+            tkMetadata = new TokenMetadata( tokenToEndPointMap, endPointToTokenMap );
+        }
+        
+        return tkMetadata;
+    }
+}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/net/Message.java b/incubator/cassandra/trunk/src/org/apache/cassandra/net/Message.java
index 304803c9..d33702e7 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/net/Message.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/net/Message.java
@@ -53,20 +53,27 @@
     Header header_;
     private Object[] body_ = new Object[0];
     
-    protected Message(String id, EndPoint from, String messageType, String verb, Object... body)
+    /* Ctor for JAXB. DO NOT DELETE */
+    private Message()
     {
-        this(new Header(id, from, messageType, verb), body);
     }
     
-    protected Message(Header header, Object... body)
+    protected Message(String id, EndPoint from, String messageType, String verb, Object[] body)
+    {
+        header_ = new Header(id, from, messageType, verb);
+        body_ = body;
+    }
+    
+    protected Message(Header header, Object[] body)
     {
         header_ = header;
         body_ = body;
     }
 
-    public Message(EndPoint from, String messageType, String verb, Object... body)
+    public Message(EndPoint from, String messageType, String verb, Object[] body)
     {
-        this(new Header(from, messageType, verb), body);
+        header_ = new Header(from, messageType, verb);
+        body_ = body;
     }    
     
     public byte[] getHeader(Object key)
@@ -157,7 +164,7 @@ void setMessageId(String id)
         header_.setMessageId(id);
     }    
 
-    public Message getReply(EndPoint from, Object... args)
+    public Message getReply(EndPoint from, Object[] args)
     {        
         Message response = new Message(getMessageId(),
                                        from,
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/test/DBTest.java b/incubator/cassandra/trunk/src/org/apache/cassandra/test/DBTest.java
index 5949b5ed..bcd47ef8 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/test/DBTest.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/test/DBTest.java
@@ -182,6 +182,7 @@ public static void main(String[] args) throws Throwable
         //doWrites();
         //doRead("543");
         
+        DatabaseDescriptor.init();
         DBTest.doTest();
     }
 }
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/test/DataImporter.java b/incubator/cassandra/trunk/src/org/apache/cassandra/test/DataImporter.java
index 6fd7fabe..5a74aa16 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/test/DataImporter.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/test/DataImporter.java
@@ -20,8 +20,12 @@
 
 import com.facebook.thrift.transport.TTransport;
 import com.facebook.thrift.transport.TSocket;
+import com.facebook.thrift.transport.THttpClient;
+import com.facebook.thrift.transport.TFramedTransport;
 import com.facebook.thrift.protocol.TBinaryProtocol;
-
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.net.*;
+import org.apache.cassandra.service.*;
 import java.io.BufferedReader;
 import java.io.File;
 import java.io.FileInputStream;
@@ -29,17 +33,25 @@
 import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.StringReader;
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.ResultSet;
+import java.sql.Statement;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
 import java.util.StringTokenizer;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.ThreadFactory;
+import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
 import org.apache.cassandra.concurrent.ThreadFactoryImpl;
 import org.apache.cassandra.db.ColumnFamily;
 import org.apache.cassandra.db.IColumn;
@@ -58,7 +70,8 @@
 import org.apache.cassandra.service.batch_mutation_t;
 import org.apache.cassandra.service.column_t;
 import org.apache.cassandra.service.superColumn_t;
-
+import org.apache.cassandra.utils.BasicUtilities;
+import org.apache.cassandra.utils.LogUtil;
 import org.apache.log4j.Logger;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Token;
@@ -894,7 +907,7 @@ public void testRead(String filepath) throws Throwable {
                 Thread.sleep(1000/requestsPerSecond_, 1000%requestsPerSecond_);
 				errorCount_++;
 			} else {
-				Map<String, ColumnFamily> cfMap = row.getColumnFamilyMap();
+				Map<String, ColumnFamily> cfMap = row.getColumnFamilies();
 				if (cfMap == null || cfMap.size() == 0) {
 					logger_
 							.debug("ERROR ColumnFamil map is missing.....: "
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/test/SSTableTest.java b/incubator/cassandra/trunk/src/org/apache/cassandra/test/SSTableTest.java
index 39f31979..b9134d41 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/test/SSTableTest.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/test/SSTableTest.java
@@ -20,13 +20,22 @@
 
 import java.io.FileInputStream;
 import java.io.FileOutputStream;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 import java.util.Random;
 
+import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.PrimaryKey;
 import org.apache.cassandra.io.DataInputBuffer;
 import org.apache.cassandra.io.DataOutputBuffer;
 import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.service.PartitionerType;
 import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.FBUtilities;
 
 
 public class SSTableTest
@@ -44,14 +53,79 @@ private static void rawSSTableWrite() throws Throwable
             ColumnFamily cf = new ColumnFamily("Test", "Standard");
             bufOut.reset();           
             // random.nextBytes(bytes);
-            cf.addColumn("C", "Avinash Lakshman is a good man".getBytes(), i);
-            ColumnFamily.serializerWithIndexes().serialize(cf, bufOut);
+            cf.createColumn("C", "Avinash Lakshman is a good man".getBytes(), i);
+            ColumnFamily.serializer2().serialize(cf, bufOut);
             ssTable.append(key, bufOut);            
             bf.fill(key);
         }
         ssTable.close(bf);
     }
     
+    private static void hashSSTableWrite() throws Throwable
+    {        
+        Map<String, ColumnFamily> columnFamilies = new HashMap<String, ColumnFamily>();                
+        byte[] bytes = new byte[64*1024];
+        Random random = new Random();
+        for ( int i = 100; i < 1000; ++i )
+        {
+            String key = Integer.toString(i);
+            ColumnFamily cf = new ColumnFamily("Test", "Standard");                      
+            // random.nextBytes(bytes);
+            cf.createColumn("C", "Avinash Lakshman is a good man".getBytes(), i);
+            columnFamilies.put(key, cf);
+        } 
+        flushForRandomPartitioner(columnFamilies);
+    }
+    
+    private static void flushForRandomPartitioner(Map<String, ColumnFamily> columnFamilies) throws Throwable
+    {
+        SSTable ssTable = new SSTable("C:\\Engagements\\Cassandra", "Table-Test-1", PartitionerType.RANDOM);
+        /* List of primary keys in sorted order */
+        List<PrimaryKey> pKeys = PrimaryKey.create( columnFamilies.keySet() );
+        DataOutputBuffer buffer = new DataOutputBuffer();
+        /* Use this BloomFilter to decide if a key exists in a SSTable */
+        BloomFilter bf = new BloomFilter(pKeys.size(), 15);
+        for ( PrimaryKey pKey : pKeys )
+        {
+            buffer.reset();
+            ColumnFamily columnFamily = columnFamilies.get(pKey.key());
+            if ( columnFamily != null )
+            {
+                /* serialize the cf with column indexes */
+                ColumnFamily.serializer2().serialize( columnFamily, buffer );
+                /* Now write the key and value to disk */
+                ssTable.append(pKey.key(), pKey.hash(), buffer);
+                bf.fill(pKey.key());                
+            }
+        }
+        ssTable.close(bf);
+    }
+    
+    private static void readSSTable() throws Throwable
+    {
+        SSTable ssTable = new SSTable("C:\\Engagements\\Cassandra\\Table-Test-1-Data.db");  
+        for ( int i = 100; i < 1000; ++i )
+        {
+            String key = Integer.toString(i);            
+            DataInputBuffer bufIn = ssTable.next(key, "Test:C");
+            ColumnFamily cf = ColumnFamily.serializer().deserialize(bufIn);
+            if ( cf != null )
+            {            
+                System.out.println("KEY:" + key);
+                System.out.println(cf.name());
+                Collection<IColumn> columns = cf.getAllColumns();
+                for ( IColumn column : columns )
+                {
+                    System.out.println(column.name());
+                }
+            }
+            else
+            {
+                System.out.println("CF doesn't exist for key " + key);
+            }                             
+        }
+    }
+    
     public static void main(String[] args) throws Throwable
     {
         BloomFilter bf = new BloomFilter(1024*1024, 15);
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/ClusterTool.java b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/ClusterTool.java
index e69de29b..0b963e1a 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/ClusterTool.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/ClusterTool.java
@@ -0,0 +1,107 @@
+package org.apache.cassandra.tools;
+
+import java.io.IOException;
+import java.math.BigInteger;
+import java.net.InetAddress;
+
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+
+
+public class ClusterTool
+{
+	public static final String SET_TOKEN = "settoken";
+	public static final String HASH_KEY = "hash";
+	public static final String BUILD_INDEX = "build_index";
+	public static final String READ_TEST = "read_test";
+	public static final String WRITE_TEST = "write_test";
+
+    public static void applyToken(String serverName, BigInteger token) throws IOException
+    {
+        try
+        {
+        	EndPoint from = new EndPoint(InetAddress.getLocalHost().getHostName(), 7000);
+        	System.out.println("Updating token of server " + serverName + " with token " + token);
+            Message message = new Message(from, "", StorageService.tokenVerbHandler_, new Object[]{ token.toByteArray() });
+            EndPoint ep = new EndPoint(serverName, 7000);
+        	MessagingService.getMessagingInstance().sendOneWay(message, ep);
+        	Thread.sleep(1000);
+	    	System.out.println("Successfully calibrated " + serverName);
+        }
+        catch (Exception e)
+        {
+            e.printStackTrace(System.out);
+        }
+    }
+
+    public static void printUsage()
+    {
+		System.out.println("Usage: java -jar <cassandra-tools.jar> <command> <options>");
+		System.out.println("Commands:");
+		System.out.println("\t" + SET_TOKEN + " <server> <token>");
+		System.out.println("\t" + HASH_KEY + " <key>");
+		System.out.println("\t" + BUILD_INDEX + "  <full path to the data file>");
+		System.out.println("\t" + READ_TEST + " <number of threads> <requests per sec per thread> <machine(s) to read (':' separated list)>");
+		System.out.println("\t" + WRITE_TEST + " <number of threads> <requests per sec per thread> <machine(s) to write (':' separated list)>");
+    }
+
+    public static void main(String[] args) throws Exception
+    {
+    	if(args.length < 2)
+    	{
+    		printUsage();
+    		return;
+    	}
+
+    	int argc = 0;
+    	try
+    	{
+    		/* set the token for a particular node in the Cassandra cluster */
+	    	if(SET_TOKEN.equals(args[argc]))
+	    	{
+		    	String serverName = args[argc + 1];
+		    	BigInteger token = new BigInteger(args[argc + 2]);
+		    	//System.out.println("Calibrating " + serverName + " with token " + token);
+		    	applyToken(serverName, token);
+	    	}
+	    	/* Print the hash of a given key */
+	    	else if(HASH_KEY.equals(args[argc]))
+	    	{
+	    		System.out.println("Hash = [" + StorageService.hash(args[argc + 1]) + "]");
+	    	}
+	    	/* build indexes given the data file */
+	    	else if(BUILD_INDEX.equals(args[argc]))
+	    	{
+	    		IndexBuilder.main(args);
+	    	}
+	    	/* test reads */
+	    	else if(READ_TEST.equals(args[argc]))
+	    	{
+	    		System.out.println("Testing reads...");
+	    		int numThreads = Integer.parseInt(args[argc + 1]);
+				int rpsPerThread = Integer.parseInt(args[argc + 2]);
+				String machinesToRead = args[argc + 3];
+//				ReadTest.runReadTest(numThreads, rpsPerThread, machinesToRead);
+	    	}
+	    	/* test writes */
+	    	else if(WRITE_TEST.equals(args[argc]))
+	    	{
+	    		System.out.println("Testing writes...");
+	    		int numThreads = Integer.parseInt(args[argc + 1]);
+				int rpsPerThread = Integer.parseInt(args[argc + 2]);
+				String machinesToWrite = args[argc + 3];
+//				WriteTest.runWriteTest(numThreads, rpsPerThread, machinesToWrite);
+	    	}
+    	} catch(Exception e)
+    	{
+    		System.err.println("Exception " + e.getMessage());
+			e.printStackTrace(System.err);
+    		printUsage();
+    	}
+
+    	System.exit(0);
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/FileSizeTokenGenerator.java b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/FileSizeTokenGenerator.java
index e69de29b..ddcba8b0 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/FileSizeTokenGenerator.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/FileSizeTokenGenerator.java
@@ -0,0 +1,150 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.File;
+import java.io.IOException;
+import java.math.BigInteger;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.SequenceFile;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.io.*;
+import org.apache.cassandra.utils.*;
+
+public class FileSizeTokenGenerator
+{
+    private static Logger logger_ = Logger.getLogger(IndexBuilder.class);
+    
+    public static void main(String[] args)
+    {
+        if ( args.length != 4 )
+        {
+            System.out.println("Usage : java com.facebook.infrastructure.tools.IndexBuilder <full path to the data file> < split factor>");
+            System.exit(1);
+        }
+        
+        try
+        {
+            int splitCount = Integer.parseInt(args[3]);
+            BigInteger l = new BigInteger(args[1]);
+            BigInteger h = new BigInteger(args[2]);
+            long totalSize = getTotalSize(args[0], l, h);
+        	System.out.println(" Total Size :  " + totalSize);
+            BigInteger[] tokens = generateTokens(args[0], l, h, totalSize, splitCount);
+            int i = 0 ;
+            for( BigInteger token : tokens)
+            {
+            	System.out.println(i++ + " th Token " + token);
+            }
+        }
+        catch( Throwable th )
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }
+    }
+
+    private static long  getTotalSize(String dataFile, BigInteger l , BigInteger h) throws IOException
+    {
+        final int bufferSize = 64*1024;
+        
+        IFileReader dataReader = SequenceFile.bufferedReader(dataFile, bufferSize);
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        DataInputBuffer bufIn = new DataInputBuffer();
+        long totalSize = 0;
+        try
+        {                                            
+            while ( !dataReader.isEOF() )
+            {                
+                bufOut.reset();                
+                /* Record the position of the key. */
+                dataReader.next(bufOut);
+                bufIn.reset(bufOut.getData(), bufOut.getLength());
+                /* Key just read */
+                String key = bufIn.readUTF();    
+                if ( !key.equals(SSTable.blockIndexKey_) && l.compareTo(StorageService.hash(key)) < 0 && h.compareTo(StorageService.hash(key)) > 0 )
+                {                                        
+                    int sz = bufIn.readInt();
+                	byte[] keyData = new byte[sz];
+                	bufIn.read(keyData, 0, sz);
+                    totalSize= totalSize + sz;
+                }
+            }
+        }
+        finally
+        {
+            dataReader.close();
+        }
+        return totalSize;
+    }
+    
+    
+    private static BigInteger[] generateTokens(String dataFile,BigInteger l , BigInteger h, long totalSize, int splitCount) throws IOException
+    {
+        final int bufferSize = 64*1024;
+        
+        IFileReader dataReader = SequenceFile.bufferedReader(dataFile, bufferSize);
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        DataInputBuffer bufIn = new DataInputBuffer();
+        long splitFactor = totalSize/(splitCount+1);
+        long curSize = 0;
+        BigInteger[] tokens = new BigInteger[splitCount];
+        int k = 0 ;
+        try
+        {                                            
+            while ( !dataReader.isEOF())
+            {                
+                bufOut.reset();                
+                /* Record the position of the key. */
+                dataReader.next(bufOut);
+                bufIn.reset(bufOut.getData(), bufOut.getLength());
+                /* Key just read */
+                String key = bufIn.readUTF();       
+                if ( !key.equals(SSTable.blockIndexKey_) && l.compareTo(StorageService.hash(key)) < 0 && h.compareTo(StorageService.hash(key)) > 0 )
+                {                                        
+                        int sz = bufIn.readInt();
+                        curSize = curSize + sz;
+                    	byte[] keyData = new byte[sz];
+                    	bufIn.read(keyData, 0, sz);
+                        
+                        if( curSize > splitFactor)
+                        {
+                        	tokens[k++] = StorageService.hash(key);
+                        	curSize = 0 ;
+                        	if( k == splitCount)
+                        	{
+                        		break;
+                        	}
+                        }
+                    }
+                }
+        }
+        finally
+        {
+            dataReader.close();
+        }
+        return tokens;
+    }
+        
+}
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/MembershipCleaner.java b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/MembershipCleaner.java
index 3399ce9e..b89b58e6 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/MembershipCleaner.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/MembershipCleaner.java
@@ -18,6 +18,7 @@
 
 package org.apache.cassandra.tools;
 
+import java.io.BufferedInputStream;
 import java.io.BufferedReader;
 import java.io.ByteArrayOutputStream;
 import java.io.DataInputStream;
@@ -26,6 +27,7 @@
 import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.Serializable;
+import java.math.BigInteger;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.cassandra.io.ICompactSerializer;
@@ -34,6 +36,8 @@
 import org.apache.cassandra.net.MessagingService;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.io.*;
+import org.apache.cassandra.utils.*;
 
 public class MembershipCleaner
 {
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java
index 5b20ddcd..4c929ce8 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java
@@ -18,20 +18,25 @@
 
 package org.apache.cassandra.tools;
 
+import java.util.*;
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
 import java.io.IOException;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.log4j.Logger;
+import java.math.BigInteger;
 
 import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.gms.Gossiper;
 import org.apache.cassandra.io.DataInputBuffer;
 import org.apache.cassandra.net.EndPoint;
 import org.apache.cassandra.net.IVerbHandler;
 import org.apache.cassandra.net.Message;
 import org.apache.cassandra.net.MessagingService;
 import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.tools.TokenUpdater.TokenInfoMessage;
 import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.io.*;
+import org.apache.cassandra.config.*;
 
 /**
  * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/TokenUpdateVerbHandler.java b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/TokenUpdateVerbHandler.java
index 37e41057..67dadbba 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/TokenUpdateVerbHandler.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/TokenUpdateVerbHandler.java
@@ -18,24 +18,24 @@
 
 package org.apache.cassandra.tools;
 
+import java.util.*;
 import java.io.ByteArrayOutputStream;
 import java.io.DataOutputStream;
 import java.io.IOException;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.log4j.Logger;
+import java.math.BigInteger;
 
 import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.dht.Token;
 import org.apache.cassandra.io.DataInputBuffer;
 import org.apache.cassandra.net.EndPoint;
 import org.apache.cassandra.net.IVerbHandler;
 import org.apache.cassandra.net.Message;
 import org.apache.cassandra.net.MessagingService;
 import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.tools.TokenUpdater.TokenInfoMessage;
 import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.io.*;
+import org.apache.cassandra.config.*;
 
 /**
  * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
@@ -54,8 +54,9 @@ public void doVerb(Message message)
             DataInputBuffer bufIn = new DataInputBuffer();
             bufIn.reset(body, body.length);
             /* Deserialize to get the token for this endpoint. */
-            Token token = Token.serializer().deserialize(bufIn);
+            TokenUpdater.TokenInfoMessage tiMessage = TokenUpdater.TokenInfoMessage.serializer().deserialize(bufIn);
 
+            BigInteger token = tiMessage.getToken();
             logger_.info("Updating the token to [" + token + "]");
             StorageService.instance().updateToken(token);
             
@@ -65,19 +66,19 @@ public void doVerb(Message message)
             logger_.debug("Number of nodes in the header " + headers.size());
             Set<String> nodes = headers.keySet();
             
-            IPartitioner p = StorageService.getPartitioner();
             for ( String node : nodes )
             {            
                 logger_.debug("Processing node " + node);
                 byte[] bytes = headers.remove(node);
                 /* Send a message to this node to update its token to the one retreived. */
                 EndPoint target = new EndPoint(node, DatabaseDescriptor.getStoragePort());
-                token = p.getTokenFactory().fromByteArray(bytes);
+                token = new BigInteger(bytes);
                 
-                /* Reset the new Message */
+                /* Reset the new TokenInfoMessage */
+                tiMessage = new TokenUpdater.TokenInfoMessage(target, token );
                 ByteArrayOutputStream bos = new ByteArrayOutputStream();
                 DataOutputStream dos = new DataOutputStream(bos);
-                Token.serializer().serialize(token, dos);
+                TokenInfoMessage.serializer().serialize(tiMessage, dos);
                 message.setMessageBody(new Object[]{bos.toByteArray()});
                 
                 logger_.debug("Sending a token update message to " + target + " to update it to " + token);
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/TokenUpdater.java b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/TokenUpdater.java
index 59cff0d7..09da1671 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/tools/TokenUpdater.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/tools/TokenUpdater.java
@@ -18,19 +18,26 @@
 
 package org.apache.cassandra.tools;
 
+import java.io.BufferedInputStream;
 import java.io.BufferedReader;
 import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
 import java.io.DataOutputStream;
 import java.io.FileInputStream;
+import java.io.IOException;
 import java.io.InputStreamReader;
+import java.io.Serializable;
+import java.math.BigInteger;
+import java.util.concurrent.atomic.AtomicInteger;
 
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.ICompactSerializer;
 import org.apache.cassandra.net.EndPoint;
 import org.apache.cassandra.net.Message;
 import org.apache.cassandra.net.MessagingService;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.io.*;
+import org.apache.cassandra.utils.*;
 
 public class TokenUpdater
 {
@@ -46,17 +53,16 @@ public static void main(String[] args) throws Throwable
         }
         
         String ipPort = args[0];
-        IPartitioner p = StorageService.getPartitioner();
-        Token token = p.getTokenFactory().fromString(args[1]);
+        String token = args[1];
         String file = args[2];
         
         String[] ipPortPair = ipPort.split(":");
         EndPoint target = new EndPoint(ipPortPair[0], Integer.valueOf(ipPortPair[1]));
+        TokenInfoMessage tiMessage = new TokenInfoMessage( target, new BigInteger(token) );
 
         ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream(bos);
-        Token.serializer().serialize(token, dos);
-
+        TokenInfoMessage.serializer().serialize(tiMessage, dos);
         /* Construct the token update message to be sent */
         Message tokenUpdateMessage = new Message( new EndPoint(FBUtilities.getHostName(), port_), "", StorageService.tokenVerbHandler_, new Object[]{bos.toByteArray()} );
         
@@ -67,8 +73,8 @@ public static void main(String[] args) throws Throwable
         {
             String[] nodeTokenPair = line.split(" ");
             /* Add the node and the token pair into the header of this message. */
-            Token nodeToken = p.getTokenFactory().fromString(nodeTokenPair[1]);
-            tokenUpdateMessage.addHeader(nodeTokenPair[0], p.getTokenFactory().toByteArray(nodeToken));
+            BigInteger nodeToken = new BigInteger(nodeTokenPair[1]);
+            tokenUpdateMessage.addHeader(nodeTokenPair[0], nodeToken.toByteArray());
         }
         
         System.out.println("Sending a token update message to " + target);
@@ -77,4 +83,63 @@ public static void main(String[] args) throws Throwable
         System.out.println("Done sending the update message");
     }
 
+    public static class TokenInfoMessage implements Serializable
+    {
+        private static ICompactSerializer<TokenInfoMessage> serializer_;
+        private static AtomicInteger idGen_ = new AtomicInteger(0);
+        
+        static
+        {
+            serializer_ = new TokenInfoMessageSerializer();            
+        }
+        
+        static ICompactSerializer<TokenInfoMessage> serializer()
+        {
+            return serializer_;
+        }
+
+        private EndPoint target_;
+        private BigInteger token_;
+        
+        TokenInfoMessage(EndPoint target, BigInteger token)
+        {
+            target_ = target;
+            token_ = token;
+        }
+        
+        EndPoint getTarget()
+        {
+            return target_;
+        }
+        
+        BigInteger getToken()
+        {
+            return token_;
+        }
+    }
+    
+    public static class TokenInfoMessageSerializer implements ICompactSerializer<TokenInfoMessage>
+    {
+        public void serialize(TokenInfoMessage tiMessage, DataOutputStream dos) throws IOException
+        {
+            byte[] node = EndPoint.toBytes( tiMessage.getTarget() );
+            dos.writeInt(node.length);
+            dos.write(node);
+            
+            byte[] token = tiMessage.getToken().toByteArray();
+            dos.writeInt( token.length );
+            dos.write(token);
+        }
+        
+        public TokenInfoMessage deserialize(DataInputStream dis) throws IOException
+        {
+            byte[] target = new byte[dis.readInt()];
+            dis.readFully(target);
+            
+            byte[] token = new byte[dis.readInt()];
+            dis.readFully(token);
+            
+            return new TokenInfoMessage(EndPoint.fromBytes(target), new BigInteger(token));
+        }
+    }
 }
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/utils/BasicUtilities.java b/incubator/cassandra/trunk/src/org/apache/cassandra/utils/BasicUtilities.java
index 604ff263..56dfc0fc 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/utils/BasicUtilities.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/utils/BasicUtilities.java
@@ -18,6 +18,10 @@
 
 package org.apache.cassandra.utils;
 
+import java.util.*;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.ThreadPoolExecutor; 
+import java.math.BigInteger;
 import java.nio.ByteBuffer;
 
 /**
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/utils/DestructivePQIterator.java b/incubator/cassandra/trunk/src/org/apache/cassandra/utils/DestructivePQIterator.java
index 0ed96f83..e69de29b 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/utils/DestructivePQIterator.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/utils/DestructivePQIterator.java
@@ -1,25 +0,0 @@
-package org.apache.cassandra.utils;
-
-import java.util.Iterator;
-import java.util.PriorityQueue;
-
-public class DestructivePQIterator<T> implements Iterator<T> {
-    private PriorityQueue<T> pq;
-
-    public DestructivePQIterator(PriorityQueue<T> pq) {
-        this.pq = pq;
-    }
-
-    public boolean hasNext() {
-        return pq.size() > 0;
-    }
-
-    public T next() {
-        return pq.poll();
-    }
-
-    public void remove() {
-        throw new UnsupportedOperationException();
-    }
-}
-
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/utils/FBUtilities.java b/incubator/cassandra/trunk/src/org/apache/cassandra/utils/FBUtilities.java
index 4d675f4e..1300969a 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/utils/FBUtilities.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/utils/FBUtilities.java
@@ -18,28 +18,19 @@
 
 package org.apache.cassandra.utils;
 
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-import java.io.PrintWriter;
-import java.io.StringWriter;
-import java.io.UnsupportedEncodingException;
-import java.math.BigInteger;
-import java.net.InetAddress;
-import java.net.UnknownHostException;
+import java.util.*;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.zip.Deflater;
+import java.util.zip.DataFormatException;
+import java.util.zip.Inflater;
 import java.security.MessageDigest;
 import java.text.DateFormat;
 import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.List;
-import java.util.StringTokenizer;
-import java.util.zip.DataFormatException;
-import java.util.zip.Deflater;
-import java.util.zip.Inflater;
-
+import java.io.*;
+import java.net.*;
+import java.nio.channels.SocketChannel;
+import java.nio.ByteBuffer;
+import java.math.BigInteger;
 /**
  * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
  */
diff --git a/incubator/cassandra/trunk/src/org/apache/cassandra/utils/FastObjectHash.java b/incubator/cassandra/trunk/src/org/apache/cassandra/utils/FastObjectHash.java
index b434c6a4..c46e4031 100644
--- a/incubator/cassandra/trunk/src/org/apache/cassandra/utils/FastObjectHash.java
+++ b/incubator/cassandra/trunk/src/org/apache/cassandra/utils/FastObjectHash.java
@@ -187,7 +187,7 @@ else if (cur != REMOVED && cur.equals(obj))
         }
         else
         { // already FULL or REMOVED, must probe
-            // compute the double token
+            // compute the double hash
             final int probe = 1 + (hash % (length - 2));
 
             // if the slot we landed on is FULL (but not removed), probe
