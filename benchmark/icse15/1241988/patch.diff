diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/DatasetSplitter.java b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/DatasetSplitter.java
index 3ec039df..0178128a 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/DatasetSplitter.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/DatasetSplitter.java
@@ -86,17 +86,23 @@ public int run(String[] args) throws Exception {
         Text.class, Text.class, SequenceFileOutputFormat.class);
     markPreferences.getConfiguration().set(TRAINING_PERCENTAGE, String.valueOf(trainingPercentage));
     markPreferences.getConfiguration().set(PROBE_PERCENTAGE, String.valueOf(probePercentage));
-    markPreferences.waitForCompletion(true);
+    boolean succeeded = markPreferences.waitForCompletion(true);
+    if (!succeeded) 
+      return -1;
 
     Job createTrainingSet = prepareJob(markedPrefs, trainingSetPath, SequenceFileInputFormat.class,
         WritePrefsMapper.class, NullWritable.class, Text.class, TextOutputFormat.class);
     createTrainingSet.getConfiguration().set(PART_TO_USE, INTO_TRAINING_SET.toString());
-    createTrainingSet.waitForCompletion(true);
+    succeeded = createTrainingSet.waitForCompletion(true);
+    if (!succeeded) 
+      return -1;
 
     Job createProbeSet = prepareJob(markedPrefs, probeSetPath, SequenceFileInputFormat.class,
         WritePrefsMapper.class, NullWritable.class, Text.class, TextOutputFormat.class);
     createProbeSet.getConfiguration().set(PART_TO_USE, INTO_PROBE_SET.toString());
-    createProbeSet.waitForCompletion(true);
+    succeeded = createProbeSet.waitForCompletion(true);
+    if (!succeeded) 
+      return -1;
 
     return 0;
   }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/FactorizationEvaluator.java b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/FactorizationEvaluator.java
index c2605799..faa16fc5 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/FactorizationEvaluator.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/FactorizationEvaluator.java
@@ -86,7 +86,9 @@ public int run(String[] args) throws Exception {
         DoubleWritable.class, NullWritable.class, SequenceFileOutputFormat.class);
     predictRatings.getConfiguration().set(USER_FEATURES_PATH, parsedArgs.get("--userFeatures"));
     predictRatings.getConfiguration().set(ITEM_FEATURES_PATH, parsedArgs.get("--itemFeatures"));
-    predictRatings.waitForCompletion(true);
+    boolean succeeded = predictRatings.waitForCompletion(true);
+    if (!succeeded) 
+      return -1;
 
     BufferedWriter writer  = null;
     try {
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJob.java
index dea1d17d..618ac9eb 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJob.java
@@ -134,21 +134,27 @@ public int run(String[] args) throws Exception {
         VectorWritable.class, VectorSumReducer.class, IntWritable.class,
         VectorWritable.class, SequenceFileOutputFormat.class);
     itemRatings.setCombinerClass(VectorSumReducer.class);
-    itemRatings.waitForCompletion(true);
+    boolean succeeded = itemRatings.waitForCompletion(true);
+    if (!succeeded) 
+      return -1;
 
     /* create A */
     Job userRatings = prepareJob(pathToItemRatings(), pathToUserRatings(),
         TransposeMapper.class, IntWritable.class, VectorWritable.class, MergeVectorsReducer.class, IntWritable.class,
         VectorWritable.class);
     userRatings.setCombinerClass(MergeVectorsCombiner.class);
-    userRatings.waitForCompletion(true);
+    succeeded = userRatings.waitForCompletion(true);
+    if (!succeeded) 
+      return -1;
 
     //TODO this could be fiddled into one of the upper jobs
     Job averageItemRatings = prepareJob(pathToItemRatings(), getTempPath("averageRatings"),
         AverageRatingMapper.class, IntWritable.class, VectorWritable.class, MergeVectorsReducer.class,
         IntWritable.class, VectorWritable.class);
     averageItemRatings.setCombinerClass(MergeVectorsCombiner.class);
-    averageItemRatings.waitForCompletion(true);
+    succeeded = averageItemRatings.waitForCompletion(true);
+    if (!succeeded) 
+      return -1;
 
     Vector averageRatings = ALSUtils.readFirstRow(getTempPath("averageRatings"), getConf());
 
@@ -219,7 +225,9 @@ private void runSolver(Path ratings, Path output, Path pathToUorI)
     solverConf.set(ALPHA, String.valueOf(alpha));
     solverConf.setInt(NUM_FEATURES, numFeatures);
     solverConf.set(FEATURE_MATRIX, pathToUorI.toString());
-    solverForUorI.waitForCompletion(true);
+    boolean succeeded = solverForUorI.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
   }
 
   static class SolveExplicitFeedbackMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/RecommenderJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/RecommenderJob.java
index c079f717..07df26c3 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/RecommenderJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/RecommenderJob.java
@@ -92,7 +92,9 @@ public int run(String[] args) throws Exception {
     prediction.getConfiguration().set(USER_FEATURES_PATH, parsedArgs.get("--userFeatures"));
     prediction.getConfiguration().set(ITEM_FEATURES_PATH, parsedArgs.get("--itemFeatures"));
     prediction.getConfiguration().set(MAX_RATING, parsedArgs.get("--maxRating"));
-    prediction.waitForCompletion(true);
+    boolean succeeded = prediction.waitForCompletion(true);
+    if (!succeeded) 
+      return -1;
 
     return 0;
   }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java
index 36b3ed5c..348dba1d 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java
@@ -191,7 +191,9 @@ public int run(String[] args) throws Exception {
               SimilarityMatrixRowWrapperMapper.class, VarIntWritable.class, VectorOrPrefWritable.class,
               Reducer.class, VarIntWritable.class, VectorOrPrefWritable.class,
               SequenceFileOutputFormat.class);
-      prePartialMultiply1.waitForCompletion(true);
+      boolean succeeded = prePartialMultiply1.waitForCompletion(true);
+      if (!succeeded) 
+        return -1;
       //continue the multiplication
       Job prePartialMultiply2 = prepareJob(new Path(prepPath, PreparePreferenceMatrixJob.USER_VECTORS),
               prePartialMultiplyPath2, SequenceFileInputFormat.class, UserVectorSplitterMapper.class, VarIntWritable.class,
@@ -202,7 +204,9 @@ public int run(String[] args) throws Exception {
       }
       prePartialMultiply2.getConfiguration().setInt(UserVectorSplitterMapper.MAX_PREFS_PER_USER_CONSIDERED,
               maxPrefsPerUser);
-      prePartialMultiply2.waitForCompletion(true);
+      succeeded = prePartialMultiply2.waitForCompletion(true);
+      if (!succeeded) 
+        return -1;
       //finish the job
       Job partialMultiply = prepareJob(
               new Path(prePartialMultiplyPath1 + "," + prePartialMultiplyPath2), partialMultiplyPath,
@@ -210,7 +214,9 @@ public int run(String[] args) throws Exception {
               ToVectorAndPrefReducer.class, VarIntWritable.class, VectorAndPrefsWritable.class,
               SequenceFileOutputFormat.class);
       setS3SafeCombinedInputPath(partialMultiply, getTempPath(), prePartialMultiplyPath1, prePartialMultiplyPath2);
-      partialMultiply.waitForCompletion(true);
+      succeeded = partialMultiply.waitForCompletion(true);
+      if (!succeeded) 
+        return -1;
     }
 
     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
@@ -221,7 +227,9 @@ public int run(String[] args) throws Exception {
                 ItemFilterMapper.class, VarLongWritable.class, VarLongWritable.class,
                 ItemFilterAsVectorAndPrefsReducer.class, VarIntWritable.class, VectorAndPrefsWritable.class,
                 SequenceFileOutputFormat.class);
-        itemFiltering.waitForCompletion(true);
+        boolean succeeded = itemFiltering.waitForCompletion(true);
+        if (!succeeded) 
+          return -1;
       }
 
       String aggregateAndRecommendInput = partialMultiplyPath.toString();
@@ -247,7 +255,9 @@ public int run(String[] args) throws Exception {
               new Path(prepPath, PreparePreferenceMatrixJob.ITEMID_INDEX).toString());
       aggregateAndRecommendConf.setInt(AggregateAndRecommendReducer.NUM_RECOMMENDATIONS, numRecommendations);
       aggregateAndRecommendConf.setBoolean(BOOLEAN_DATA, booleanData);
-      aggregateAndRecommend.waitForCompletion(true);
+      boolean succeeded = aggregateAndRecommend.waitForCompletion(true);
+      if (!succeeded) 
+        return -1;
     }
 
     return 0;
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/PreparePreferenceMatrixJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/PreparePreferenceMatrixJob.java
index a70f4767..13405d8b 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/PreparePreferenceMatrixJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/PreparePreferenceMatrixJob.java
@@ -75,7 +75,10 @@ public int run(String[] args) throws Exception {
             ItemIDIndexMapper.class, VarIntWritable.class, VarLongWritable.class, ItemIDIndexReducer.class,
             VarIntWritable.class, VarLongWritable.class, SequenceFileOutputFormat.class);
     itemIDIndex.setCombinerClass(ItemIDIndexReducer.class);
-    itemIDIndex.waitForCompletion(true);
+    boolean succeeded = itemIDIndex.waitForCompletion(true);
+    if (!succeeded) {
+      return -1;
+    }
     //convert user preferences into a vector per user
     Job toUserVectors = prepareJob(getInputPath(), getOutputPath(USER_VECTORS), TextInputFormat.class,
             ToItemPrefsMapper.class, VarLongWritable.class, booleanData ? VarLongWritable.class : EntityPrefWritable.class,
@@ -83,7 +86,10 @@ public int run(String[] args) throws Exception {
     toUserVectors.getConfiguration().setBoolean(RecommenderJob.BOOLEAN_DATA, booleanData);
     toUserVectors.getConfiguration().setInt(ToUserVectorsReducer.MIN_PREFERENCES_PER_USER, minPrefsPerUser);
     toUserVectors.getConfiguration().set(ToEntityPrefsMapper.RATING_SHIFT, String.valueOf(ratingShift));
-    toUserVectors.waitForCompletion(true);
+    succeeded = toUserVectors.waitForCompletion(true);
+    if (!succeeded) {
+      return -1;
+    }
     //we need the number of users later
     int numberOfUsers = (int) toUserVectors.getCounters().findCounter(ToUserVectorsReducer.Counters.USERS).getValue();
     HadoopUtil.writeInt(numberOfUsers, getOutputPath(NUM_USERS), getConf());
@@ -99,7 +105,10 @@ public int run(String[] args) throws Exception {
       toItemVectors.getConfiguration().setInt(ToItemVectorsMapper.SAMPLE_SIZE, samplingSize);
     }
 
-    toItemVectors.waitForCompletion(true);
+    succeeded = toItemVectors.waitForCompletion(true);
+    if (!succeeded) {
+      return -1;
+    }
 
     return 0;
   }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderJob.java
index 79e2cd1b..a18310d4 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderJob.java
@@ -139,8 +139,8 @@ public int run(String[] args) throws IOException, ClassNotFoundException, Interr
     jobConf.setInt(RecommenderReducer.RECOMMENDATIONS_PER_USER, recommendationsPerUser);
     jobConf.set(RecommenderReducer.DATA_MODEL_FILE, inputFile.toString());
 
-    job.waitForCompletion(true);
-    return 0;
+    boolean succeeded = job.waitForCompletion(true);
+    return succeeded ? 0 : -1;
   }
   
   public static void main(String[] args) throws Exception {
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJob.java
index 1e14b4c4..948621df 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJob.java
@@ -160,7 +160,10 @@ public int run(String[] args) throws Exception {
       mostSimilarItemsConf.set(ITEM_ID_INDEX_PATH_STR,
           new Path(prepPath, PreparePreferenceMatrixJob.ITEMID_INDEX).toString());
       mostSimilarItemsConf.setInt(MAX_SIMILARITIES_PER_ITEM, maxSimilarItemsPerItem);
-      mostSimilarItems.waitForCompletion(true);
+      boolean succeeded = mostSimilarItems.waitForCompletion(true);
+      if (!succeeded) {
+        return -1;
+      }
     }
 
     return 0;
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneAverageDiffsJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneAverageDiffsJob.java
index a8c95f95..70ae1101 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneAverageDiffsJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneAverageDiffsJob.java
@@ -68,7 +68,9 @@ public int run(String[] args) throws IOException, ClassNotFoundException, Interr
                                        EntityEntityWritable.class,
                                        FloatWritable.class,
                                        SequenceFileOutputFormat.class);
-      prefsToDiffsJob.waitForCompletion(true);
+      boolean succeeded = prefsToDiffsJob.waitForCompletion(true);
+      if (!succeeded) 
+        return -1;
     }
 
 
@@ -84,7 +86,9 @@ public int run(String[] args) throws IOException, ClassNotFoundException, Interr
                                           FullRunningAverageAndStdDevWritable.class,
                                           TextOutputFormat.class);
       FileOutputFormat.setOutputCompressorClass(diffsToAveragesJob, GzipCodec.class);
-      diffsToAveragesJob.waitForCompletion(true);
+      boolean succeeded = diffsToAveragesJob.waitForCompletion(true);
+      if (!succeeded)
+        return -1;
     }
     return 0;
   }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/Classifier.java b/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/Classifier.java
index 387ffbb6..cbced7c5 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/Classifier.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/Classifier.java
@@ -125,8 +125,7 @@ public void run() throws IOException, ClassNotFoundException, InterruptedExcepti
 
     log.info("Running the job...");
     if (!job.waitForCompletion(true)) {
-      log.error("Job failed!");
-      return;
+      throw new IllegalStateException("Job failed!");
     }
 
     parseOutput(job);
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/df/tools/FrequenciesJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/df/tools/FrequenciesJob.java
index 1d544bf7..d02d9741 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/df/tools/FrequenciesJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/df/tools/FrequenciesJob.java
@@ -111,7 +111,10 @@ public FrequenciesJob(Path base, Path dataPath, Path datasetPath) {
     job.setOutputFormatClass(SequenceFileOutputFormat.class);
     
     // run the job
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
     
     int[][] counts = parseOutput(job);
 
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java
index 3631ff93..0e0e7154 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java
@@ -97,14 +97,20 @@ public int run(String[] args) throws Exception {
             IndexInstancesMapper.class, IntWritable.class, VectorWritable.class, VectorSumReducer.class, IntWritable.class,
             VectorWritable.class, SequenceFileOutputFormat.class);
     indexInstances.setCombinerClass(VectorSumReducer.class);
-    indexInstances.waitForCompletion(true);
+    boolean succeeded = indexInstances.waitForCompletion(true);
+    if (!succeeded) {
+      return -1;
+    }
     //sum up all the weights from the previous step, per label and per feature
     Job weightSummer = prepareJob(getTempPath(SUMMED_OBSERVATIONS), getTempPath(WEIGHTS),
             SequenceFileInputFormat.class, WeightsMapper.class, Text.class, VectorWritable.class, VectorSumReducer.class,
             Text.class, VectorWritable.class, SequenceFileOutputFormat.class);
     weightSummer.getConfiguration().set(WeightsMapper.NUM_LABELS, String.valueOf(labelSize));
     weightSummer.setCombinerClass(VectorSumReducer.class);
-    weightSummer.waitForCompletion(true);
+    succeeded = weightSummer.waitForCompletion(true);
+    if (!succeeded) {
+      return -1;
+    }
     //put the per label and per feature vectors into the cache
     HadoopUtil.cacheFiles(getTempPath(WEIGHTS), getConf());
     //calculate the Thetas, write out to LABEL_THETA_NORMALIZER vectors -- TODO: add reference here to the part of the Rennie paper that discusses this
@@ -114,7 +120,10 @@ public int run(String[] args) throws Exception {
     thetaSummer.setCombinerClass(VectorSumReducer.class);
     thetaSummer.getConfiguration().setFloat(ThetaMapper.ALPHA_I, alphaI);
     thetaSummer.getConfiguration().setBoolean(ThetaMapper.TRAIN_COMPLEMENTARY, trainComplementary);
-    thetaSummer.waitForCompletion(true);
+    succeeded = thetaSummer.waitForCompletion(true);
+    if (!succeeded) {
+      return -1;
+    }
     //validate our model and then write it out to the official output
     NaiveBayesModel naiveBayesModel = BayesUtils.readModelFromDir(getTempPath(), getConf());
     naiveBayesModel.validate();
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputJob.java
index 63917093..d1e4be88 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputJob.java
@@ -61,7 +61,10 @@ public static void runJob(Path input, Path output, int rows, int cols)
 
     job.setJarByClass(AffinityMatrixInputJob.class);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
 
   /**
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/MatrixDiagonalizeJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/MatrixDiagonalizeJob.java
index 58625e88..52abb5e6 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/MatrixDiagonalizeJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/MatrixDiagonalizeJob.java
@@ -69,7 +69,10 @@ public static Vector runJob(Path affInput, int dimensions)
     
     job.setJarByClass(MatrixDiagonalizeJob.class);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
     
     // read the results back from the path
     return VectorCache.load(conf, new Path(diagOutput, "part-r-00000"));
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/UnitVectorizerJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/UnitVectorizerJob.java
index 6bb13d44..f61102b3 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/UnitVectorizerJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/UnitVectorizerJob.java
@@ -63,7 +63,10 @@ public static void runJob(Path input, Path output)
 
     job.setJarByClass(UnitVectorizerJob.class);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
   
   public static class UnitVectorizerMapper
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorMatrixMultiplicationJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorMatrixMultiplicationJob.java
index 3d87d5fb..ff654016 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorMatrixMultiplicationJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorMatrixMultiplicationJob.java
@@ -87,7 +87,10 @@ public static DistributedRowMatrix runJob(Path markovPath, Vector diag, Path out
 
     job.setJarByClass(VectorMatrixMultiplicationJob.class);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
 
     // build the resulting DRM from the results
     return new DistributedRowMatrix(outputPath, tmpPath,
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsAffinityCutsJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsAffinityCutsJob.java
index 14c18411..1e15a420 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsAffinityCutsJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsAffinityCutsJob.java
@@ -80,7 +80,10 @@ public static long runjob(Path currentAffinity, Path cutMatrix, Path nextAffinit
     FileInputFormat.addInputPath(job, cutMatrix);
     FileOutputFormat.setOutputPath(job, nextAffinity);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
     
     return job.getCounters().findCounter(CUTSCOUNTER.NUM_CUTS).getValue();
   }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityJob.java
index 35e437b6..c6037e85 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityJob.java
@@ -119,6 +119,9 @@ public static void runJob(Vector eigenvalues,
     FileInputFormat.addInputPath(job, eigenvectors);
     FileOutputFormat.setOutputPath(job, output);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }  
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
index 3aedeec3..eaeebd95 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
@@ -268,7 +268,10 @@ public static void startAggregating(Parameters params, Configuration conf)
     job.setOutputFormatClass(SequenceFileOutputFormat.class);
     
     HadoopUtil.delete(conf, outPath);
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
   
   /**
@@ -300,7 +303,10 @@ public static void startParallelCounting(Parameters params, Configuration conf)
     job.setReducerClass(ParallelCountingReducer.class);
     job.setOutputFormatClass(SequenceFileOutputFormat.class);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
     
   }
   
@@ -334,6 +340,9 @@ public static void startParallelFPGrowth(Parameters params, Configuration conf)
     job.setReducerClass(ParallelFPGrowthReducer.class);
     job.setOutputFormatClass(SequenceFileOutputFormat.class);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/ga/watchmaker/MahoutEvaluator.java b/mahout/trunk/core/src/main/java/org/apache/mahout/ga/watchmaker/MahoutEvaluator.java
index 7c03f63c..78f9ad75 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/ga/watchmaker/MahoutEvaluator.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/ga/watchmaker/MahoutEvaluator.java
@@ -75,7 +75,10 @@ public static void evaluate(FitnessEvaluator<?> evaluator,
     storePopulation(fs, new Path(input, "population"), population);
 
     configureJob(job, conf, evaluator, input, output);
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
     
     OutputUtils.importEvaluations(fs, conf, output, evaluations);
   }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/graph/AdjacencyMatrixJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/graph/AdjacencyMatrixJob.java
index 0c21644c..ce2deb99 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/graph/AdjacencyMatrixJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/graph/AdjacencyMatrixJob.java
@@ -118,9 +118,10 @@ public int run(String[] args) throws Exception {
     createAdjacencyMatrixConf.set(NUM_VERTICES_PARAM, String.valueOf(numVertices));
     createAdjacencyMatrixConf.set(VERTEX_INDEX_PARAM, getOutputPath(VERTEX_INDEX).toString());
     createAdjacencyMatrixConf.setBoolean(SYMMETRIC_PARAM, symmetric);
-    createAdjacencyMatrix.waitForCompletion(true);
 
-    return 0;
+    boolean succeeded = createAdjacencyMatrix.waitForCompletion(true);
+
+    return succeeded ? 0 : -1;
   }
 
   //TODO do this in parallel?
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/graph/linkanalysis/RandomWalk.java b/mahout/trunk/core/src/main/java/org/apache/mahout/graph/linkanalysis/RandomWalk.java
index 31d67d82..8b8ebf84 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/graph/linkanalysis/RandomWalk.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/graph/linkanalysis/RandomWalk.java
@@ -101,7 +101,10 @@ public final int run(String[] args) throws Exception {
     createTransitionMatrix.setCombinerClass(MergeVectorsCombiner.class);
     createTransitionMatrix.getConfiguration().set(NUM_VERTICES_PARAM, String.valueOf(numVertices));
     createTransitionMatrix.getConfiguration().set(STAYING_PROBABILITY_PARAM, String.valueOf(stayingProbability));
-    createTransitionMatrix.waitForCompletion(true);
+    boolean succeeded = createTransitionMatrix.waitForCompletion(true);
+    if (!succeeded) {
+      return -1;
+    }
 
     DistributedRowMatrix transitionMatrix = new DistributedRowMatrix(transitionMatrixPath, getTempPath(),
         numVertices, numVertices);
@@ -121,9 +124,12 @@ public final int run(String[] args) throws Exception {
         RankPerVertexMapper.class, LongWritable.class, DoubleWritable.class, TextOutputFormat.class);
     vertexWithPageRank.getConfiguration().set(RankPerVertexMapper.RANK_PATH_PARAM,
         getTempPath(RANK_VECTOR).toString());
-    vertexWithPageRank.waitForCompletion(true);
+    succeeded = vertexWithPageRank.waitForCompletion(true);
+    if (!succeeded) {
+      return -1;
+    }
 
-    return 1;
+    return 0;
   }
 
   static void persistVector(Configuration conf, Path path, Vector vector) throws IOException {
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJob.java b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJob.java
index 929f702a..a88b8742 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJob.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJob.java
@@ -121,7 +121,10 @@ public int run(String[] args) throws Exception {
       normsAndTransposeConf.set(NUM_NON_ZERO_ENTRIES_PATH, numNonZeroEntriesPath.toString());
       normsAndTransposeConf.set(MAXVALUES_PATH, maxValuesPath.toString());
       normsAndTransposeConf.set(SIMILARITY_CLASSNAME, similarityClassname);
-      normsAndTranspose.waitForCompletion(true);
+      boolean succeeded = normsAndTranspose.waitForCompletion(true);
+      if (!succeeded) {
+        return -1;
+      }
     }
 
     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
@@ -136,7 +139,10 @@ public int run(String[] args) throws Exception {
       pairwiseConf.set(SIMILARITY_CLASSNAME, similarityClassname);
       pairwiseConf.setInt(NUMBER_OF_COLUMNS, numberOfColumns);
       pairwiseConf.setBoolean(EXCLUDE_SELF_SIMILARITY, excludeSelfSimilarity);
-      pairwiseSimilarity.waitForCompletion(true);
+      boolean succeeded = pairwiseSimilarity.waitForCompletion(true);
+      if (!succeeded) {
+        return -1;
+      }
     }
 
     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
@@ -145,7 +151,10 @@ public int run(String[] args) throws Exception {
           VectorWritable.class);
       asMatrix.setCombinerClass(MergeToTopKSimilaritiesReducer.class);
       asMatrix.getConfiguration().setInt(MAX_SIMILARITIES_PER_ROW, maxSimilaritiesPerRow);
-      asMatrix.waitForCompletion(true);
+      boolean succeeded = asMatrix.waitForCompletion(true);
+      if (!succeeded) {
+        return -1;
+      }
     }
 
     return 0;
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stats/BasicStats.java b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stats/BasicStats.java
index 68d13e48..a77ada49 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stats/BasicStats.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stats/BasicStats.java
@@ -85,7 +85,10 @@ private static VarianceTotals computeVarianceTotals(Path input, Path output,
             IntWritable.class, DoubleWritable.class, SequenceFileOutputFormat.class, conf);
     HadoopUtil.delete(conf, output);
     job.setCombinerClass(StandardDeviationCalculatorReducer.class);
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
 
     // Now extract the computed sum
     Path filesPattern = new Path(output, "part-*");
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/math/stats/entropy/ConditionalEntropy.java b/mahout/trunk/core/src/main/java/org/apache/mahout/math/stats/entropy/ConditionalEntropy.java
index d098933a..cd175fa8 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/math/stats/entropy/ConditionalEntropy.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/math/stats/entropy/ConditionalEntropy.java
@@ -84,7 +84,10 @@ private void groupAndCountByKeyAndValue() throws IOException, ClassNotFoundExcep
         GroupAndCountByKeyAndValueMapper.class, StringTuple.class, VarIntWritable.class, VarIntSumReducer.class,
         StringTuple.class, VarIntWritable.class, SequenceFileOutputFormat.class);
     job.setCombinerClass(VarIntSumReducer.class);
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
 
     numberItems =
         job.getCounters().findCounter("org.apache.hadoop.mapred.Task$Counter", "MAP_INPUT_RECORDS").getValue();
@@ -102,7 +105,10 @@ private void calculateSpecificConditionalEntropy() throws IOException, ClassNotF
         SpecificConditionalEntropyReducer.class, Text.class, DoubleWritable.class,
         SequenceFileOutputFormat.class);
     job.getConfiguration().set(NUMBER_ITEMS_PARAM, String.valueOf(numberItems));
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
 
   }
 
@@ -116,7 +122,10 @@ private void calculateConditionalEntropy() throws IOException, ClassNotFoundExce
         DoubleSumReducer.class, NullWritable.class, DoubleWritable.class,
         SequenceFileOutputFormat.class);
     job.setCombinerClass(DoubleSumReducer.class);
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
 
   }
 
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/math/stats/entropy/Entropy.java b/mahout/trunk/core/src/main/java/org/apache/mahout/math/stats/entropy/Entropy.java
index e0d9ac5b..ba28bfac 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/math/stats/entropy/Entropy.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/math/stats/entropy/Entropy.java
@@ -111,7 +111,10 @@ private void groupAndCount() throws IOException, ClassNotFoundException, Interru
         VarIntWritable.class, VarIntSumReducer.class, Text.class, VarIntWritable.class,
         SequenceFileOutputFormat.class);
     job.setCombinerClass(VarIntSumReducer.class);
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
 
     numberItems =
         job.getCounters().findCounter("org.apache.hadoop.mapred.Task$Counter", "MAP_INPUT_RECORDS").getValue();
@@ -135,7 +138,10 @@ private void calculateEntropy() throws IOException, ClassNotFoundException, Inte
         DoubleWritable.class, SequenceFileOutputFormat.class);
     job.getConfiguration().set(NUMBER_ITEMS_PARAM, String.valueOf(numberItems));
     job.setCombinerClass(DoubleSumReducer.class);
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
 
   }
 
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DictionaryVectorizer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DictionaryVectorizer.java
index b1398a93..cea28b38 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DictionaryVectorizer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DictionaryVectorizer.java
@@ -324,7 +324,9 @@ private static void makePartialVectors(Path input,
 
     HadoopUtil.delete(conf, output);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
   }
   
   /**
@@ -360,6 +362,8 @@ private static void startWordCounting(Path input, Path output, Configuration bas
     
     HadoopUtil.delete(conf, output);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
   }
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DocumentProcessor.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DocumentProcessor.java
index b2d8bd54..e985aec5 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DocumentProcessor.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/DocumentProcessor.java
@@ -90,6 +90,9 @@ public static void tokenizeDocuments(Path input,
     job.setOutputFormatClass(SequenceFileOutputFormat.class);
     HadoopUtil.delete(conf, output);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
+
   }
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/HighDFWordsPruner.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/HighDFWordsPruner.java
index 3262340d..6ad0c9c1 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/HighDFWordsPruner.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/HighDFWordsPruner.java
@@ -92,7 +92,9 @@ private static void pruneVectorsPartial(Path input, Path output, Path dictionary
 
     HadoopUtil.delete(conf, output);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
   }
 
   public static void mergePartialVectors(Iterable<Path> partialVectorPaths,
@@ -129,7 +131,9 @@ public static void mergePartialVectors(Iterable<Path> partialVectorPaths,
 
     HadoopUtil.delete(conf, output);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
   }
 
   private static String getCommaSeparatedPaths(Iterable<Path> paths) {
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SimpleTextEncodingVectorizer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SimpleTextEncodingVectorizer.java
index a5b8773d..504d96cd 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SimpleTextEncodingVectorizer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SimpleTextEncodingVectorizer.java
@@ -63,7 +63,8 @@ public void createVectors(Path input, Path output, VectorizerConfig config)
     boolean finished = job.waitForCompletion(true);
 
     log.info("result of run: {}", finished);
-    //TODO: something useful w/ this result should it be meaningful.
+    if (!finished) 
+      throw new IllegalStateException("Job failed!");
   }
 
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/collocations/llr/CollocDriver.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/collocations/llr/CollocDriver.java
index 28a33a69..4daf3a69 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/collocations/llr/CollocDriver.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/collocations/llr/CollocDriver.java
@@ -233,7 +233,9 @@ private static long generateCollocations(Path input,
     job.setReducerClass(CollocReducer.class);
     job.setNumReduceTasks(reduceTasks);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
 
     return job.getCounters().findCounter(CollocMapper.Count.NGRAM_TOTAL).getValue();
   }
@@ -273,6 +275,8 @@ private static void computeNGramsPruneByLLR(Path output,
     job.setReducerClass(LLRReducer.class);
     job.setNumReduceTasks(reduceTasks);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
   }
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/common/PartialVectorMerger.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/common/PartialVectorMerger.java
index 09473345..243729e2 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/common/PartialVectorMerger.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/common/PartialVectorMerger.java
@@ -126,7 +126,9 @@ public static void mergePartialVectors(Iterable<Path> partialVectorPaths,
 
     HadoopUtil.delete(conf, output);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
   }
 
   private static String getCommaSeparatedPaths(Iterable<Path> paths) {
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/tfidf/TFIDFConverter.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/tfidf/TFIDFConverter.java
index c59461f7..5a5156bc 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/tfidf/TFIDFConverter.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/tfidf/TFIDFConverter.java
@@ -329,7 +329,9 @@ private static void makePartialVectors(Path input,
 
     HadoopUtil.delete(conf, output);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
   }
 
   /**
@@ -363,6 +365,8 @@ private static void startDFCounting(Path input, Path output, Configuration baseC
 
     HadoopUtil.delete(conf, output);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) 
+      throw new IllegalStateException("Job failed!");
   }
 }
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/cf/taste/example/email/MailToPrefsDriver.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/cf/taste/example/email/MailToPrefsDriver.java
index 6e389dab..ebca50a1 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/cf/taste/example/email/MailToPrefsDriver.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/cf/taste/example/email/MailToPrefsDriver.java
@@ -122,7 +122,11 @@ public int run(String[] args) throws Exception {
               Text.class,
               VarIntWritable.class,
               SequenceFileOutputFormat.class);
-      createMsgIdDictionary.waitForCompletion(true);
+
+      boolean succeeded = createMsgIdDictionary.waitForCompletion(true);
+      if (!succeeded) {
+        return -1;
+      }
       //write out the dictionary at the top level
       msgIdChunks = createDictionaryChunks(msgIdsPath, output, "msgIds-dictionary-", createMsgIdDictionary.getConfiguration(), chunkSize, msgDim);
     }
@@ -145,7 +149,10 @@ public int run(String[] args) throws Exception {
               VarIntWritable.class,
               SequenceFileOutputFormat.class);
       createFromIdDictionary.getConfiguration().set(EmailUtility.SEPARATOR, separator);
-      createFromIdDictionary.waitForCompletion(true);
+      boolean succeeded = createFromIdDictionary.waitForCompletion(true);
+      if (!succeeded) {
+        return -1;
+      }
       //write out the dictionary at the top level
       int[] fromDim = new int[1];
       fromChunks = createDictionaryChunks(fromIdsPath, output, "fromIds-dictionary-", createFromIdDictionary.getConfiguration(), chunkSize, fromDim);
@@ -177,7 +184,10 @@ public int run(String[] args) throws Exception {
                   MailToRecMapper.class, Text.class, LongWritable.class, MailToRecReducer.class, Text.class, NullWritable.class,
                   TextOutputFormat.class);
           createRecMatrix.getConfiguration().set("mapred.output.compress", "false");
-          createRecMatrix.waitForCompletion(true);
+          boolean succeeded = createRecMatrix.waitForCompletion(true);
+          if (!succeeded) {
+            return -1;
+          }
           //copy the results up a level
           //HadoopUtil.copyMergeSeqFiles(out.getFileSystem(conf), out, vecPath.getFileSystem(conf), outPath, true, conf, "");
           FileStatus[] fs = HadoopUtil.getFileStatus(new Path(out, "*"), PathType.GLOB, PathFilters.partFilter(), null, conf);
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/classifier/bayes/WikipediaDatasetCreatorDriver.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/classifier/bayes/WikipediaDatasetCreatorDriver.java
index c48a6a5f..1411ec07 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/classifier/bayes/WikipediaDatasetCreatorDriver.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/classifier/bayes/WikipediaDatasetCreatorDriver.java
@@ -184,6 +184,9 @@ public static void runJob(String input,
     FileOutputFormat.setOutputPath(job, outPath);
     HadoopUtil.delete(conf, outPath);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
 }
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/classifier/email/PrepEmailVectorsDriver.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/classifier/email/PrepEmailVectorsDriver.java
index f9e60ef7..0d33f203 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/classifier/email/PrepEmailVectorsDriver.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/classifier/email/PrepEmailVectorsDriver.java
@@ -66,7 +66,8 @@ public int run(String[] args) throws Exception {
             Text.class, VectorWritable.class, PrepEmailReducer.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class);
     convertJob.getConfiguration().set(ITEMS_PER_CLASS, parsedArgs.get("--maxItemsPerLabel"));
     convertJob.getConfiguration().set(USE_LIST_NAME, String.valueOf(parsedArgs.containsKey("--useListName")));
-    convertJob.waitForCompletion(true);
-    return 0;
+
+    boolean succeeded = convertJob.waitForCompletion(true);
+    return succeeded ? 0 : -1;
   }
 }
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleGrouper.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleGrouper.java
index 4cb25485..da3e4414 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleGrouper.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleGrouper.java
@@ -69,6 +69,9 @@ public static void startJob(Parameters params) throws IOException,
     job.setReducerClass(KeyBasedStringTupleReducer.class);
     job.setOutputFormatClass(TextOutputFormat.class);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
 }
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/ga/watchmaker/cd/hadoop/CDMahoutEvaluator.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/ga/watchmaker/cd/hadoop/CDMahoutEvaluator.java
index bce4dc3b..159796e5 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/ga/watchmaker/cd/hadoop/CDMahoutEvaluator.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/ga/watchmaker/cd/hadoop/CDMahoutEvaluator.java
@@ -77,7 +77,10 @@ public static void evaluate(List<? extends Rule> rules,
     Job job = new Job(conf);
 
     configureJob(job, rules, target, inpath, output, split);
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
 
     importEvaluations(fs, conf, output, evaluations);
   }
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/ga/watchmaker/cd/tool/CDInfosTool.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/ga/watchmaker/cd/tool/CDInfosTool.java
index 1858686c..1c187291 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/ga/watchmaker/cd/tool/CDInfosTool.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/ga/watchmaker/cd/tool/CDInfosTool.java
@@ -84,7 +84,10 @@ public static void gatherInfos(Descriptors descriptors, Path inpath, Path output
     Job job = new Job(conf);
 
     configureJob(job, descriptors, inpath, output);
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
 
     importDescriptions(fs, conf, output, descriptions);
   }
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/text/WikipediaToSequenceFile.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/text/WikipediaToSequenceFile.java
index 22fb4127..dc12dba3 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/text/WikipediaToSequenceFile.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/text/WikipediaToSequenceFile.java
@@ -193,6 +193,9 @@ public static void runJob(String input,
     
     conf.set("wikipedia.categories", categoriesStr);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
 }
diff --git a/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/conversion/InputDriver.java b/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/conversion/InputDriver.java
index fd3160e3..6a2b376b 100644
--- a/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/conversion/InputDriver.java
+++ b/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/conversion/InputDriver.java
@@ -105,7 +105,10 @@ public static void runJob(Path input, Path output, String vectorClassName)
     FileInputFormat.addInputPath(job, input);
     FileOutputFormat.setOutputPath(job, output);
     
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
   
 }
diff --git a/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputDriver.java b/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputDriver.java
index 67b1103b..6f636061 100644
--- a/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputDriver.java
+++ b/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputDriver.java
@@ -92,6 +92,9 @@ public static void runJob(Path input, Path output) throws IOException, Interrupt
     FileInputFormat.setInputPaths(job, input);
     FileOutputFormat.setOutputPath(job, output);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
 }
diff --git a/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/evaluation/RepresentativePointsDriver.java b/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/evaluation/RepresentativePointsDriver.java
index 60a7f2a3..7f882eb6 100644
--- a/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/evaluation/RepresentativePointsDriver.java
+++ b/mahout/trunk/integration/src/main/java/org/apache/mahout/clustering/evaluation/RepresentativePointsDriver.java
@@ -239,6 +239,9 @@ private static void runIterationMR(Configuration conf,
     job.setInputFormatClass(SequenceFileInputFormat.class);
     job.setOutputFormatClass(SequenceFileOutputFormat.class);
 
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
 }
diff --git a/mahout/trunk/integration/src/main/java/org/apache/mahout/text/TextParagraphSplittingJob.java b/mahout/trunk/integration/src/main/java/org/apache/mahout/text/TextParagraphSplittingJob.java
index 3ac73ad9..cacfd223 100644
--- a/mahout/trunk/integration/src/main/java/org/apache/mahout/text/TextParagraphSplittingJob.java
+++ b/mahout/trunk/integration/src/main/java/org/apache/mahout/text/TextParagraphSplittingJob.java
@@ -46,8 +46,8 @@ public int run(String[] strings) throws Exception {
                          Text.class,
                          SequenceFileOutputFormat.class);
     job.setNumReduceTasks(0);
-    job.waitForCompletion(true);
-    return 1;
+    boolean succeeded = job.waitForCompletion(true);
+    return succeeded ? 0 : -1;
   }
 
   public static class SplitMap extends Mapper<Text,Text,Text,Text> {
diff --git a/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/SplitInputJob.java b/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/SplitInputJob.java
index 20a83010..9d30889d 100644
--- a/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/SplitInputJob.java
+++ b/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/SplitInputJob.java
@@ -125,7 +125,10 @@ public static void run(Configuration initialConf, Path inputPath,
     job.setOutputKeyClass(keyClass);
     job.setOutputValueClass(valueClass);
     job.submit();
-    job.waitForCompletion(true);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
   }
 
   /**
diff --git a/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/regex/RegexConverterDriver.java b/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/regex/RegexConverterDriver.java
index aab13fdf..b9d16bed 100644
--- a/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/regex/RegexConverterDriver.java
+++ b/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/regex/RegexConverterDriver.java
@@ -90,9 +90,8 @@ public int run(String[] args) throws Exception {
             LongWritable.class,
             Text.class,
             TextOutputFormat.class);
-    job.waitForCompletion(true);
-
-    return 0;
+    boolean succeeded = job.waitForCompletion(true);
+    return succeeded ? 0 : -1;
   }
 
   public static void main(String[] args) throws Exception {
