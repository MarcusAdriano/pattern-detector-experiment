diff --git a/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/KeywordAnalyzer.java b/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/KeywordAnalyzer.java
index 0b2f1191..b0e983f9 100644
--- a/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/KeywordAnalyzer.java
+++ b/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/KeywordAnalyzer.java
@@ -19,7 +19,8 @@
 import java.io.Reader;
 
 /**
- * "Tokenizes" the entire stream as a single token.
+ * "Tokenizes" the entire stream as a single token. This is useful
+ * for data like zip codes, ids, and some product names.
  */
 public class KeywordAnalyzer extends Analyzer {
   public TokenStream tokenStream(String fieldName,
diff --git a/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java b/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
index 7929f6fc..aceab49e 100644
--- a/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
+++ b/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
@@ -31,16 +31,16 @@
 import java.util.Set;
 
 /**
- * Analyzer for brazilian language. Supports an external list of stopwords (words that
+ * Analyzer for Brazilian language. Supports an external list of stopwords (words that
  * will not be indexed at all) and an external list of exclusions (word that will
  * not be stemmed, but indexed).
  *
- * @author    Jo�o Kramer
+ * @author    Jo&atilde;o Kramer
  */
 public final class BrazilianAnalyzer extends Analyzer {
 
 	/**
-	 * List of typical german stopwords.
+	 * List of typical Brazilian stopwords.
 	 */
 	public final static String[] BRAZILIAN_STOP_WORDS = {
       "a","ainda","alem","ambas","ambos","antes",
@@ -68,6 +68,7 @@
 	 * Contains the stopwords used with the StopFilter.
 	 */
 	private Set stoptable = new HashSet();
+	
 	/**
 	 * Contains words that should be indexed but not stemmed.
 	 */
diff --git a/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java b/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
index 95e7ea36..15129fc4 100644
--- a/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
+++ b/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
@@ -105,7 +105,7 @@ public CzechAnalyzer( File stopwords ) throws IOException {
     /**
      * Loads stopwords hash from resource stream (file, database...).
      * @param   wordfile    File containing the wordlist
-     * @param   encoding    Encoding used (win-1250, iso-8859-2, ...}, null for default system encoding
+     * @param   encoding    Encoding used (win-1250, iso-8859-2, ...), null for default system encoding
      */
     public void loadStopWords( InputStream wordfile, String encoding ) {
         if ( wordfile == null ) {
@@ -122,7 +122,6 @@ public void loadStopWords( InputStream wordfile, String encoding ) {
             else
                 isr = new InputStreamReader(wordfile, encoding);
 
-
             LineNumberReader lnr = new LineNumberReader(isr);
             String word;
             while ( ( word = lnr.readLine() ) != null ) {
@@ -138,7 +137,7 @@ public void loadStopWords( InputStream wordfile, String encoding ) {
 	 * Creates a TokenStream which tokenizes all the text in the provided Reader.
 	 *
 	 * @return  A TokenStream build from a StandardTokenizer filtered with
-	 * 			StandardFilter, StopFilter, GermanStemFilter and LowerCaseFilter
+	 * 			StandardFilter, LowerCaseFilter, and StopFilter
 	 */
 	public final TokenStream tokenStream( String fieldName, Reader reader ) {
 		TokenStream result = new StandardTokenizer( reader );
diff --git a/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java b/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
index 70b08cad..ec4f50ca 100644
--- a/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
+++ b/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
@@ -32,19 +32,19 @@
 import java.util.Set;
 
 /**
- * Analyzer for french language. Supports an external list of stopwords (words that
+ * Analyzer for French language. Supports an external list of stopwords (words that
  * will not be indexed at all) and an external list of exclusions (word that will
  * not be stemmed, but indexed).
- * A default set of stopwords is used unless an other list is specified, the
- * exclusionlist is empty by default.
+ * A default set of stopwords is used unless an alternative list is specified, the
+ * exclusion list is empty by default.
  *
- * @author Patrick Talbot (based on Gerhard Schwarz work for German)
+ * @author Patrick Talbot (based on Gerhard Schwarz's work for German)
  * @version $Id$
  */
 public final class FrenchAnalyzer extends Analyzer {
 
   /**
-   * Extended list of typical french stopwords.
+   * Extended list of typical French stopwords.
    */
   public final static String[] FRENCH_STOP_WORDS = {
     "a", "afin", "ai", "ainsi", "après", "attendu", "au", "aujourd", "auquel", "aussi",
@@ -142,7 +142,7 @@ public void setStemExclusionTable(File exclusionlist) throws IOException {
   public final TokenStream tokenStream(String fieldName, Reader reader) {
 
     if (fieldName == null) throw new IllegalArgumentException("fieldName must not be null");
-    if (reader == null) throw new IllegalArgumentException("readermust not be null");
+    if (reader == null) throw new IllegalArgumentException("reader must not be null");
 
     TokenStream result = new StandardTokenizer(reader);
     result = new StandardFilter(result);
