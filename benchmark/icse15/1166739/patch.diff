diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java
index ea2fb570..16eff1be 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java
@@ -197,9 +197,9 @@ protected void decomposeInternal(final Token token) {
     
     char[] lowerCaseTermBuffer=makeLowerCaseCopy(token.buffer());
     
-    for (int i=0;i<token.length()-this.minSubwordSize;++i) {
+    for (int i=0;i<=token.length()-this.minSubwordSize;++i) {
         Token longestMatchToken=null;
-        for (int j=this.minSubwordSize-1;j<this.maxSubwordSize;++j) {
+        for (int j=this.minSubwordSize;j<=this.maxSubwordSize;++j) {
             if(i+j>token.length()) {
                 break;
             }
diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
index 6a39a03d..8aa633bb 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
@@ -162,6 +162,49 @@ public void testDumbCompoundWordsSELongestMatch() throws Exception {
         0, 0 });
   }
   
+  public void testTokenEndingWithWordComponentOfMinimumLength() throws Exception {
+    String[] dict = {"ab", "cd", "ef"};
+
+    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT,
+			new WhitespaceTokenizer(TEST_VERSION_CURRENT,
+				new StringReader(
+					"abcdef")
+				),
+			dict,
+			CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
+			CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
+			CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);
+
+    assertTokenStreamContents(tf,
+			new String[] { "abcdef", "ab", "cd", "ef" },
+			new int[] { 0, 0, 2, 4},
+			new int[] { 6, 2, 4, 6},
+			new int[] { 1, 0, 0, 0}
+			);
+  }
+
+  public void testWordComponentWithLessThanMinimumLength() throws Exception {
+    String[] dict = {"abc", "d", "efg"};
+
+    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT,
+			new WhitespaceTokenizer(TEST_VERSION_CURRENT,
+				new StringReader(
+					"abcdefg")
+				),
+			dict,
+			CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
+			CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
+			CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);
+
+	// since "d" is shorter than the minimum subword size, it should not be added to the token stream
+    assertTokenStreamContents(tf,
+			new String[] { "abcdefg", "abc", "efg" },
+			new int[] { 0, 0, 4},
+			new int[] { 7, 3, 7},
+			new int[] { 1, 0, 0}
+			);
+  }
+  
   public void testReset() throws Exception {
     String[] dict = { "Rind", "Fleisch", "Draht", "Schere", "Gesetz",
         "Aufgabe", "Ãœberwachung" };
