diff --git a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/CassandraServiceController.java b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/CassandraServiceController.java
index 3f24d583..23859acf 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/CassandraServiceController.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/CassandraServiceController.java
@@ -1 +1,334 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.URI;
+import java.util.*;
+
+import org.apache.cassandra.thrift.Cassandra;
+import org.apache.cassandra.thrift.TokenRange;
+import org.apache.cassandra.utils.KeyPair;
+import org.apache.cassandra.utils.BlobUtils;
+import org.apache.cassandra.utils.Pair;
+
+import org.apache.commons.configuration.Configuration;
+import org.apache.commons.configuration.CompositeConfiguration;
+import org.apache.commons.configuration.PropertiesConfiguration;
+
+import org.apache.thrift.TException;
+import org.apache.thrift.protocol.*;
+import org.apache.thrift.transport.*;
+
+import org.apache.whirr.service.Cluster;
+import org.apache.whirr.service.Cluster.Instance;
+import org.apache.whirr.service.ClusterSpec;
+import org.apache.whirr.service.ComputeServiceContextBuilder;
+import org.apache.whirr.service.Service;
+import org.apache.whirr.service.ServiceFactory;
+import org.apache.whirr.service.cassandra.CassandraService;
+import org.apache.whirr.service.cassandra.CassandraClusterActionHandler;
+import org.apache.whirr.service.jclouds.RunUrlStatement;
+
+import org.jclouds.blobstore.domain.BlobMetadata;
+
+import org.jclouds.compute.ComputeService;
+import org.jclouds.compute.domain.NodeMetadata;
+import org.jclouds.compute.options.RunScriptOptions;
+import org.jclouds.domain.Credentials;
+import org.jclouds.io.Payload;
+import org.jclouds.scriptbuilder.domain.OsFamily;
+import org.jclouds.ssh.ExecResponse;
+import static org.jclouds.io.Payloads.newStringPayload;
+
+import com.google.common.base.Predicate;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import static org.junit.Assert.assertThat;
+
+public class CassandraServiceController
+{
+    private static final Logger LOG =
+        LoggerFactory.getLogger(CassandraServiceController.class);
+
+    protected static int CLIENT_PORT    = 9160;
+    protected static int JMX_PORT       = 8080;
+
+    private static final CassandraServiceController INSTANCE =
+        new CassandraServiceController();
+    
+    public static CassandraServiceController getInstance()
+    {
+        return INSTANCE;
+    }
+    
+    private boolean     running;
+
+    private ClusterSpec         clusterSpec;
+    private CassandraService    service;
+    private Cluster             cluster;
+    private ComputeService      computeService;
+    private Credentials         credentials;
+    private CompositeConfiguration config;
+    private BlobMetadata        tarball;
+    private List<InetAddress>   hosts;
+    
+    private CassandraServiceController()
+    {
+    }
+
+    public Cassandra.Client createClient(InetAddress addr)
+        throws TTransportException, TException
+    {
+        TTransport transport    = new TSocket(
+                                    addr.getHostAddress(),
+                                    CLIENT_PORT,
+                                    200000);
+        transport               = new TFramedTransport(transport);
+        TProtocol  protocol     = new TBinaryProtocol(transport);
+
+        Cassandra.Client client = new Cassandra.Client(protocol);
+        transport.open();
+
+        return client;
+    }
+
+    private void waitForClusterInitialization()
+    {
+        for (InetAddress host : hosts)
+            waitForNodeInitialization(host);
+    }
+    
+    private void waitForNodeInitialization(InetAddress addr)
+    {
+        while (true)
+        {
+            try
+            {
+                Cassandra.Client client = createClient(addr);
+
+                client.describe_cluster_name();
+                break;
+            }
+            catch (TException e)
+            {
+                try
+                {
+                    Thread.sleep(1000);
+                }
+                catch (InterruptedException ie)
+                {
+                    break;
+                }
+            }
+        }
+    }
+
+    public synchronized void startup() throws Exception
+    {
+        LOG.info("Starting up cluster...");
+
+        config = new CompositeConfiguration();
+        if (System.getProperty("whirr.config") != null)
+        {
+            config.addConfiguration(
+                new PropertiesConfiguration(System.getProperty("whirr.config")));
+        }
+        config.addConfiguration(new PropertiesConfiguration("whirr-default.properties"));
+
+        clusterSpec = new ClusterSpec(config);
+        if (clusterSpec.getPrivateKey() == null)
+        {
+            Map<String, String> pair = KeyPair.generate();
+            clusterSpec.setPublicKey(pair.get("public"));
+            clusterSpec.setPrivateKey(pair.get("private"));
+        }
+
+        // if a local tarball is available deploy it to the blobstore where it will be available to cassandra
+        if (System.getProperty("whirr.cassandra_tarball") != null)
+        {
+            Pair<BlobMetadata,URI> blob = BlobUtils.storeBlob(config, clusterSpec, System.getProperty("whirr.cassandra_tarball"));
+            tarball = blob.left;
+            config.setProperty(CassandraClusterActionHandler.BIN_TARBALL, blob.right.toURL().toString());
+            // TODO: parse the CassandraVersion property file instead
+            config.setProperty(CassandraClusterActionHandler.MAJOR_VERSION, "0.7");
+        }
+
+        service = (CassandraService)new ServiceFactory().create(clusterSpec.getServiceName());
+        cluster = service.launchCluster(clusterSpec);
+        computeService = ComputeServiceContextBuilder.build(clusterSpec).getComputeService();
+        hosts = new ArrayList<InetAddress>();
+        for (Instance instance : cluster.getInstances())
+        {
+            hosts.add(instance.getPublicAddress());
+            credentials = instance.getLoginCredentials();
+        }
+
+        waitForClusterInitialization();
+
+        ShutdownHook shutdownHook = new ShutdownHook(this);
+        Runtime.getRuntime().addShutdownHook(shutdownHook);
+
+        running = true;
+    }
+
+    public synchronized void shutdown()
+    {
+        // catch and log errors, we're in a runtime shutdown hook
+        try
+        {
+            LOG.info("Shutting down cluster...");
+            if (service != null)
+                service.destroyCluster(clusterSpec);
+            if (tarball != null)
+                BlobUtils.deleteBlob(config, clusterSpec, tarball);
+            running = false;
+        }
+        catch (Exception e)
+        {
+            LOG.error(String.format("Error shutting down cluster: %s", e));
+        }
+    }
+
+    public class ShutdownHook extends Thread
+    {
+        private CassandraServiceController controller;
+
+        public ShutdownHook(CassandraServiceController controller)
+        {
+            this.controller = controller;
+        }
+
+        public void run()
+        {
+            controller.shutdown();
+        }
+    }
+
+    public synchronized boolean ensureClusterRunning() throws Exception
+    {
+        if (running)
+        {
+            LOG.info("Cluster already running.");
+            return false;
+        }
+        else
+        {
+            startup();
+            return true;
+        }
+    }
+
+    /**
+     * Execute nodetool with args against localhost from the given host.
+     */
+    public void nodetool(String args, InetAddress... hosts)
+    {
+        callOnHosts(String.format("apache/cassandra/nodetool %s", args), hosts);
+    }
+
+    /**
+     * Wipes all persisted state for the given node, leaving it as if it had just started.
+     */
+    public void wipeHosts(InetAddress... hosts)
+    {
+        callOnHosts("apache/cassandra/wipe-state", hosts);
+    }
+
+    public Failure failHosts(List<InetAddress> hosts)
+    {
+        return new Failure(hosts.toArray(new InetAddress[hosts.size()])).trigger();
+    }
+
+    public Failure failHosts(InetAddress... hosts)
+    {
+        return new Failure(hosts).trigger();
+    }
+
+    /** TODO: Move to CassandraService? */
+    protected void callOnHosts(String payload, InetAddress... hosts)
+    {
+        final Set<String> hostset = new HashSet<String>();
+        for (InetAddress host : hosts)
+            hostset.add(host.getHostAddress());
+        Map<? extends NodeMetadata,ExecResponse> results;
+        try
+        {
+            results = computeService.runScriptOnNodesMatching(new Predicate<NodeMetadata>()
+            {
+                public boolean apply(NodeMetadata node)
+                {
+                    Set<String> intersection = new HashSet<String>(hostset);
+                    intersection.retainAll(node.getPublicAddresses());
+                    return !intersection.isEmpty();
+                }
+            }, newStringPayload(new RunUrlStatement(clusterSpec.getRunUrlBase(), payload).render(OsFamily.UNIX)),
+            RunScriptOptions.Builder.overrideCredentialsWith(credentials));
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+        if (results.size() != hostset.size())
+            throw new RuntimeException(results.size() + " hosts matched " + hostset + ": " + results);
+        for (ExecResponse response : results.values())
+            if (response.getExitCode() != 0)
+                throw new RuntimeException("Call " + payload + " failed on at least one of " + hostset + ": " + results.values());
+    }
+
+    public List<InetAddress> getHosts()
+    {
+        return hosts;
+    }
+
+    class Failure
+    {
+        private InetAddress[] hosts;
+
+        public Failure(InetAddress... hosts)
+        {
+            this.hosts = hosts;
+        }
+        
+        public Failure trigger()
+        {
+            callOnHosts("apache/cassandra/stop", hosts);
+            return this;
+        }
+
+        public void resolve()
+        {
+            callOnHosts("apache/cassandra/start", hosts);
+            for (InetAddress host : hosts)
+                waitForNodeInitialization(host);
+        }
+    }
+
+    public InetAddress getPublicHost(InetAddress privateHost)
+    {
+        for (Instance instance : cluster.getInstances())
+            if (privateHost.equals(instance.getPrivateAddress()))
+                return instance.getPublicAddress();
+        throw new RuntimeException("No public host for private host " + privateHost);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/MovementTest.java b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/MovementTest.java
index e69de29b..0fa3c5e7 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/MovementTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/MovementTest.java
@@ -0,0 +1,128 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra;
+
+import java.io.BufferedReader;
+import java.io.InputStreamReader;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.Writer;
+import java.net.InetAddress;
+import java.nio.ByteBuffer;
+import java.util.*;
+
+import org.apache.cassandra.thrift.*;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.utils.WrappedRunnable;
+
+import org.apache.cassandra.CassandraServiceController.Failure;
+
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import static junit.framework.Assert.assertEquals;
+import static junit.framework.Assert.assertNull;
+
+public class MovementTest extends TestBase
+{
+    private static final String STANDARD_CF = "Standard1";
+    private static final ColumnParent STANDARD = new ColumnParent(STANDARD_CF);
+
+    /** Inserts 1000 keys with names such that at least 1 key ends up on each host. */
+    private static Map<ByteBuffer,List<ColumnOrSuperColumn>> insertBatch(Cassandra.Client client) throws Exception
+    {
+        final int N = 1000;
+        Column col1 = new Column(
+            ByteBufferUtil.bytes("c1"),
+            ByteBufferUtil.bytes("v1"),
+            0
+            );
+        Column col2 = new Column(
+            ByteBufferUtil.bytes("c2"),
+            ByteBufferUtil.bytes("v2"),
+            0
+            );
+
+        // build N rows
+        Map<ByteBuffer,List<ColumnOrSuperColumn>> rows = new HashMap<ByteBuffer, List<ColumnOrSuperColumn>>();
+        Map<ByteBuffer,Map<String,List<Mutation>>> batch = new HashMap<ByteBuffer,Map<String,List<Mutation>>>();
+        for (int i = 0; i < N; i++)
+        {
+            String rawKey = String.format("test.key.%d", i);
+            ByteBuffer key = ByteBufferUtil.bytes(rawKey);
+            Mutation m1 = (new Mutation()).setColumn_or_supercolumn((new ColumnOrSuperColumn()).setColumn(col1));
+            Mutation m2 = (new Mutation()).setColumn_or_supercolumn((new ColumnOrSuperColumn()).setColumn(col2));
+            rows.put(key, Arrays.asList(m1.getColumn_or_supercolumn(),
+                                        m2.getColumn_or_supercolumn()));
+
+            // add row to batch
+            Map<String,List<Mutation>> rowmap = new HashMap<String,List<Mutation>>();
+            rowmap.put(STANDARD_CF, Arrays.asList(m1, m2));
+            batch.put(key, rowmap);
+        }
+        // insert the batch
+        client.batch_mutate(batch, ConsistencyLevel.ONE);
+        return rows;
+    }
+
+    private static void verifyBatch(Cassandra.Client client, Map<ByteBuffer,List<ColumnOrSuperColumn>> batch) throws Exception
+    {
+        for (Map.Entry<ByteBuffer,List<ColumnOrSuperColumn>> entry : batch.entrySet())
+        {
+            // verify slice
+            SlicePredicate sp = new SlicePredicate();
+            sp.setSlice_range(
+                new SliceRange(
+                    ByteBuffer.wrap(new byte[0]),
+                    ByteBuffer.wrap(new byte[0]),
+                    false,
+                    1000
+                    )
+                );
+            assertEquals(client.get_slice(entry.getKey(), STANDARD, sp, ConsistencyLevel.ONE),
+                         entry.getValue());
+        }
+    }
+
+    @Test
+    public void testLoadbalance() throws Exception
+    {
+        final String keyspace = "TestLoadbalance";
+        addKeyspace(keyspace, 1);
+        List<InetAddress> hosts = controller.getHosts();
+        Cassandra.Client client = controller.createClient(hosts.get(0));
+        client.set_keyspace(keyspace);
+
+        // add keys to each node
+        Map<ByteBuffer,List<ColumnOrSuperColumn>> rows = insertBatch(client);
+
+        Thread.sleep(100);
+
+        // ask a node to move to a new location
+        controller.nodetool("loadbalance", hosts.get(0));
+
+        // trigger cleanup on all nodes
+        for (InetAddress host : hosts)
+            controller.nodetool("cleanup", host);
+
+        // check that all keys still exist
+        verifyBatch(client, rows);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/MutationTest.java b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/MutationTest.java
index e69de29b..26514251 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/MutationTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/MutationTest.java
@@ -0,0 +1,376 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra;
+
+import java.io.BufferedReader;
+import java.io.InputStreamReader;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.IOException;
+import java.io.Writer;
+import java.net.InetAddress;
+import java.nio.ByteBuffer;
+import java.util.*;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.thrift.*;
+import org.apache.cassandra.tools.NodeProbe;
+import org.apache.cassandra.utils.WrappedRunnable;
+import org.apache.thrift.TException;
+import org.apache.cassandra.client.*;
+import org.apache.cassandra.dht.RandomPartitioner;
+import org.apache.cassandra.service.StorageService;
+
+import org.apache.cassandra.CassandraServiceController.Failure;
+
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import static junit.framework.Assert.assertEquals;
+import static junit.framework.Assert.assertNull;
+
+public class MutationTest extends TestBase
+{
+    private static final Logger logger = LoggerFactory.getLogger(MutationTest.class);
+
+    @Test
+    public void testInsert() throws Exception
+    {
+        List<InetAddress> hosts = controller.getHosts();
+        final String keyspace = "TestInsert";
+        addKeyspace(keyspace, 3);
+        Cassandra.Client client = controller.createClient(hosts.get(0));
+        client.set_keyspace(keyspace);
+
+        ByteBuffer key = newKey();
+
+        insert(client, key, "Standard1", "c1", "v1", 0, ConsistencyLevel.ONE);
+        insert(client, key, "Standard1", "c2", "v2", 0, ConsistencyLevel.ONE);
+
+        // block until the column is available
+        new Get(client, "Standard1", key).name("c1").value("v1").perform(ConsistencyLevel.ONE);
+        new Get(client, "Standard1", key).name("c2").value("v2").perform(ConsistencyLevel.ONE);
+
+        List<ColumnOrSuperColumn> coscs = get_slice(client, key, "Standard1", ConsistencyLevel.ONE);
+        assertColumnEqual("c1", "v1", 0, coscs.get(0).column);
+        assertColumnEqual("c2", "v2", 0, coscs.get(1).column);
+    }
+
+    @Test
+    public void testWriteAllReadOne() throws Exception
+    {
+        List<InetAddress> hosts = controller.getHosts();
+        Cassandra.Client client = controller.createClient(hosts.get(0));
+
+        final String keyspace = "TestWriteAllReadOne";
+        addKeyspace(keyspace, 3);
+        client.set_keyspace(keyspace);
+
+        ByteBuffer key = newKey();
+
+        insert(client, key, "Standard1", "c1", "v1", 0, ConsistencyLevel.ALL);
+        // should be instantly available
+        assertColumnEqual("c1", "v1", 0, getColumn(client, key, "Standard1", "c1", ConsistencyLevel.ONE));
+
+        List<InetAddress> endpoints = endpointsForKey(hosts.get(0), key, keyspace);
+        InetAddress coordinator = nonEndpointForKey(hosts.get(0), key, keyspace);
+        Failure failure = controller.failHosts(endpoints.subList(1, endpoints.size()));
+
+        try {
+            client = controller.createClient(coordinator);
+            client.set_keyspace(keyspace);
+
+            new Get(client, "Standard1", key).name("c1").value("v1")
+                .perform(ConsistencyLevel.ONE);
+
+            new Insert(client, "Standard1", key).name("c3").value("v3")
+                .expecting(UnavailableException.class).perform(ConsistencyLevel.ALL);
+        } finally {
+            failure.resolve();
+            Thread.sleep(10000);
+        }
+    }
+
+    @Test
+    public void testWriteQuorumReadQuorum() throws Exception
+    {
+        List<InetAddress> hosts = controller.getHosts();
+        Cassandra.Client client = controller.createClient(hosts.get(0));
+
+        final String keyspace = "TestWriteQuorumReadQuorum";
+        addKeyspace(keyspace, 3);
+        client.set_keyspace(keyspace);
+
+        ByteBuffer key = newKey();
+
+        // with quorum-1 nodes up
+        List<InetAddress> endpoints = endpointsForKey(hosts.get(0), key, keyspace);
+        InetAddress coordinator = nonEndpointForKey(hosts.get(0), key, keyspace);
+        Failure failure = controller.failHosts(endpoints.subList(1, endpoints.size())); //kill all but one nodes
+
+        client = controller.createClient(coordinator);
+        client.set_keyspace(keyspace);
+        try {
+            new Insert(client, "Standard1", key).name("c1").value("v1")
+                .expecting(UnavailableException.class).perform(ConsistencyLevel.QUORUM);
+        } finally {
+            failure.resolve();
+        }
+
+        // with all nodes up
+        new Insert(client, "Standard1", key).name("c2").value("v2").perform(ConsistencyLevel.QUORUM);
+
+        failure = controller.failHosts(endpoints.get(0));
+        try {
+            new Get(client, "Standard1", key).name("c2").value("v2").perform(ConsistencyLevel.QUORUM);
+        } finally {
+            failure.resolve();
+            Thread.sleep(10000);
+        }
+    }
+
+    @Test
+    public void testWriteOneReadAll() throws Exception
+    {
+        List<InetAddress> hosts = controller.getHosts();
+        Cassandra.Client client = controller.createClient(hosts.get(0));
+
+        final String keyspace = "TestWriteOneReadAll";
+        addKeyspace(keyspace, 3);
+        client.set_keyspace(keyspace);
+
+        ByteBuffer key = newKey();
+
+        List<InetAddress> endpoints = endpointsForKey(hosts.get(0), key, keyspace);
+        InetAddress coordinator = nonEndpointForKey(hosts.get(0), key, keyspace);
+        client = controller.createClient(coordinator);
+        client.set_keyspace(keyspace);
+
+        insert(client, key, "Standard1", "c1", "v1", 0, ConsistencyLevel.ONE);
+        assertColumnEqual("c1", "v1", 0, getColumn(client, key, "Standard1", "c1", ConsistencyLevel.ALL));
+
+        // with each of HH, read repair and proactive repair:
+            // with one node up
+            // write with one (success)
+            // read with all (failure)
+            // bring nodes up
+            // repair
+            // read with all (success)
+
+        Failure failure = controller.failHosts(endpoints);
+        try {
+            new Insert(client, "Standard1", key).name("c2").value("v2")
+                .expecting(UnavailableException.class).perform(ConsistencyLevel.ONE);
+        } finally {
+            failure.resolve();
+        }
+    }
+
+    protected void insert(Cassandra.Client client, ByteBuffer key, String cf, String name, String value, long timestamp, ConsistencyLevel cl)
+        throws InvalidRequestException, UnavailableException, TimedOutException, TException
+    {
+        Column col = new Column(
+             ByteBufferUtil.bytes(name),
+             ByteBufferUtil.bytes(value),
+             timestamp
+             );
+        client.insert(key, new ColumnParent(cf), col, cl);
+    }
+
+    protected Column getColumn(Cassandra.Client client, ByteBuffer key, String cf, String col, ConsistencyLevel cl)
+        throws InvalidRequestException, UnavailableException, TimedOutException, TException, NotFoundException
+    {
+        ColumnPath cpath = new ColumnPath(cf);
+        cpath.setColumn(col.getBytes());
+        return client.get(key, cpath, cl).column;
+    }
+
+    protected class Get extends RetryingAction
+    {
+        public Get(Cassandra.Client client, String cf, ByteBuffer key)
+        {
+            super(client, cf, key);
+        }
+
+        public void tryPerformAction(ConsistencyLevel cl) throws Exception
+        {
+            assertColumnEqual(name, value, timestamp, getColumn(client, key, cf, name, cl));
+        }
+    }
+
+    protected class Insert extends RetryingAction
+    {
+        public Insert(Cassandra.Client client, String cf, ByteBuffer key)
+        {
+            super(client, cf, key);
+        }
+
+        public void tryPerformAction(ConsistencyLevel cl) throws Exception
+        {
+            insert(client, key, cf, name, value, timestamp, cl);
+        }
+    }
+
+    /** Performs an action repeatedly until timeout, success or failure. */
+    protected abstract class RetryingAction
+    {
+        protected Cassandra.Client client;
+        protected String cf;
+        protected ByteBuffer key;
+        protected String name;
+        protected String value;
+        protected long timestamp;
+
+        private Set<Class<Exception>> expected = new HashSet<Class<Exception>>();
+        private long timeout = StorageService.RING_DELAY;
+
+        public RetryingAction(Cassandra.Client client, String cf, ByteBuffer key)
+        {
+            this.client = client;
+            this.cf = cf;
+            this.key = key;
+            this.timestamp = 0;
+        }
+
+        public RetryingAction name(String name)
+        {
+            this.name = name; return this;
+        }
+
+        /** The value to expect for the return column, or null to expect the column to be missing. */
+        public RetryingAction value(String value)
+        {
+            this.value = value; return this;
+        }
+        
+        /** The total time to allow before failing. */
+        public RetryingAction timeout(long timeout)
+        {
+            this.timeout = timeout; return this;
+        }
+
+        /** The expected timestamp of the returned column. */
+        public RetryingAction timestamp(long timestamp)
+        {
+            this.timestamp = timestamp; return this;
+        }
+
+        /** The exception classes that indicate success. */
+        public RetryingAction expecting(Class... tempExceptions)
+        {
+            this.expected.clear();
+            for (Class exclass : tempExceptions)
+                expected.add((Class<Exception>)exclass);
+            return this;
+        }
+
+        public void perform(ConsistencyLevel cl) throws AssertionError
+        {
+            long deadline = System.currentTimeMillis() + timeout;
+            int attempts = 0;
+            String template = "%s for " + this + " after %d attempt(s) with %d ms to spare.";
+            Exception e = null;
+            while(deadline > System.currentTimeMillis())
+            {
+                try
+                {
+                    attempts++;
+                    tryPerformAction(cl);
+                    logger.info(String.format(template, "Succeeded", attempts, deadline - System.currentTimeMillis()));
+                    return;
+                }
+                catch (Exception ex)
+                {
+                    e = ex;
+                    if (!expected.contains(ex.getClass()))
+                        continue;
+                    logger.info(String.format(template, "Caught expected exception: " + e, attempts, deadline - System.currentTimeMillis()));
+                    return;
+                }
+            }
+            String err = String.format(template, "Caught unexpected: " + e, attempts, deadline - System.currentTimeMillis());
+            logger.error(err);
+            throw new AssertionError(err);
+        }
+        
+        public String toString()
+        {
+            return this.getClass() + "(" + key + "," + name + ")";
+        }
+
+        protected abstract void tryPerformAction(ConsistencyLevel cl) throws Exception;
+    }
+
+    protected List<ColumnOrSuperColumn> get_slice(Cassandra.Client client, ByteBuffer key, String cf, ConsistencyLevel cl)
+      throws InvalidRequestException, UnavailableException, TimedOutException, TException
+    {
+        SlicePredicate sp = new SlicePredicate();
+        sp.setSlice_range(
+            new SliceRange(
+                ByteBuffer.wrap(new byte[0]),
+                ByteBuffer.wrap(new byte[0]),
+                false,
+                1000
+                )
+            );
+        return client.get_slice(key, new ColumnParent(cf), sp, cl);
+    }
+
+    protected void assertColumnEqual(String name, String value, long timestamp, Column col)
+    {
+        assertEquals(ByteBufferUtil.bytes(name), col.name);
+        assertEquals(ByteBufferUtil.bytes(value), col.value);
+        assertEquals(timestamp, col.timestamp);
+    }
+
+    protected List<InetAddress> endpointsForKey(InetAddress seed, ByteBuffer key, String keyspace)
+        throws IOException
+    {
+        RingCache ring = new RingCache(keyspace, new RandomPartitioner(), seed.getHostAddress(), 9160);
+        List<InetAddress> privateendpoints = ring.getEndpoint(key);
+        List<InetAddress> endpoints = new ArrayList<InetAddress>();
+        for (InetAddress endpoint : privateendpoints)
+        {
+            endpoints.add(controller.getPublicHost(endpoint));
+        }
+        return endpoints;
+    }
+
+    protected InetAddress nonEndpointForKey(InetAddress seed, ByteBuffer key, String keyspace)
+        throws IOException
+    {
+        List<InetAddress> endpoints = endpointsForKey(seed, key, keyspace);
+        for (InetAddress host : controller.getHosts())
+        {
+            if (!endpoints.contains(host))
+            {
+                return host;
+            }
+        }
+        return null;
+    }
+
+    protected ByteBuffer newKey()
+    {
+        return ByteBufferUtil.bytes(String.format("test.key.%d", System.currentTimeMillis()));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/TestBase.java b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/TestBase.java
index e69de29b..53c242cb 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/TestBase.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/TestBase.java
@@ -0,0 +1,114 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra;
+
+import java.io.BufferedReader;
+import java.io.InputStreamReader;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.Writer;
+import java.net.InetAddress;
+import java.util.LinkedList;
+import java.util.List;
+
+import org.apache.thrift.TException;
+
+import org.apache.cassandra.thrift.*;
+
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import static junit.framework.Assert.assertEquals;
+import static junit.framework.Assert.assertNull;
+
+public abstract class TestBase
+{
+    protected static CassandraServiceController controller =
+        CassandraServiceController.getInstance();
+
+    protected static void addKeyspace(String name, int rf) throws Exception
+    {
+        List<CfDef> cfDefList = new LinkedList<CfDef>();
+
+        CfDef standard1 = new CfDef(name, "Standard1");
+        standard1.setComparator_type("BytesType");
+        standard1.setKey_cache_size(10000);
+        standard1.setRow_cache_size(1000);
+        standard1.setRow_cache_save_period_in_seconds(0);
+        standard1.setKey_cache_save_period_in_seconds(3600);
+        standard1.setMemtable_flush_after_mins(59);
+        standard1.setMemtable_throughput_in_mb(255);
+        standard1.setMemtable_operations_in_millions(0.29);
+        cfDefList.add(standard1);
+
+        List<InetAddress> hosts = controller.getHosts();
+        Cassandra.Client client = controller.createClient(hosts.get(0));
+
+        client.system_add_keyspace(
+            new KsDef(
+                name,
+                "org.apache.cassandra.locator.SimpleStrategy",
+                rf,
+                cfDefList));
+
+        // poll, until KS added
+        for (InetAddress host : hosts)
+        {
+            try
+            {
+                client = controller.createClient(host);
+                poll:
+                while (true)
+                {
+                    List<KsDef> ksDefList = client.describe_keyspaces();
+                    for (KsDef ks : ksDefList)
+                    {
+                        if (ks.name.equals(name))
+                            break poll;
+                    }
+
+                    try
+                    {
+                        Thread.sleep(1000);
+                    }
+                    catch (InterruptedException e)
+                    {
+                        break poll;
+                    }
+                }
+            }
+            catch (TException te)
+            {
+                continue;
+            }
+        }
+    }
+
+    @BeforeClass
+    public static void setUp() throws Exception
+    {
+        controller.ensureClusterRunning();
+    }
+
+    protected static String createTemporaryKey()
+    {
+        return String.format("test.key.%d", System.currentTimeMillis());
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/utils/BlobUtils.java b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/utils/BlobUtils.java
index e69de29b..94474cf0 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/utils/BlobUtils.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/utils/BlobUtils.java
@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.File;
+import java.net.URI;
+
+import org.apache.commons.configuration.Configuration;
+
+import org.apache.whirr.service.ClusterSpec;
+
+import org.jclouds.blobstore.BlobStore;
+import org.jclouds.blobstore.BlobStoreContext;
+import org.jclouds.blobstore.BlobStoreContextFactory;
+import org.jclouds.blobstore.domain.BlobMetadata;
+import org.jclouds.blobstore.InputStreamMap;
+
+import org.jclouds.aws.s3.S3Client;
+import org.jclouds.aws.s3.S3AsyncClient;
+import org.jclouds.aws.s3.domain.AccessControlList;
+import org.jclouds.aws.s3.domain.CannedAccessPolicy;
+
+import org.jclouds.rest.RestContext;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public final class BlobUtils
+{
+    private static final Logger LOG = LoggerFactory.getLogger(BlobUtils.class);
+
+    public static final String BLOB_PROVIDER = "whirr.blobstore.provider";
+    public static final String BLOB_CONTAINER = "whirr.blobstore.container";
+    
+    private static BlobStoreContext getContext(Configuration config, ClusterSpec spec)
+    {
+        return new BlobStoreContextFactory().createContext(getProvider(config), spec.getIdentity(), spec.getCredential());
+    }
+
+    private static String getProvider(Configuration config)
+    {
+        String provider = config.getString(BLOB_PROVIDER, null);
+        if (provider == null)
+            throw new RuntimeException("Please set " + BLOB_PROVIDER + " to a jclouds supported provider.");
+        return provider;
+    }
+
+    private static String getContainer(Configuration config)
+    {
+        String container = config.getString(BLOB_CONTAINER, null);
+        if (container == null)
+            throw new RuntimeException("Please set " + BLOB_CONTAINER + " to an existing container for your chosen provider.");
+        return container;
+    }
+
+    /**
+     * Stores the given local file as a public blob, and returns metadata for the blob.
+     */
+    public static Pair<BlobMetadata,URI> storeBlob(Configuration config, ClusterSpec spec, String filename)
+    {
+        File file = new File(filename);
+        String container = getContainer(config);
+        String provider = getProvider(config);
+        String blobName = System.nanoTime() + "/" + file.getName();
+        BlobStoreContext context = getContext(config, spec);
+        try
+        {
+            InputStreamMap map = context.createInputStreamMap(container);
+            map.putFile(blobName, file);
+            // TODO: magic! in order to expose the blob as public, we need to dive into provider specific APIs
+            // the hope is that permissions are encapsulated in jclouds in the future
+            if (provider.equals("s3"))
+            {
+                S3Client sss = context.<S3Client,S3AsyncClient>getProviderSpecificContext().getApi();
+                String ownerId = sss.getObjectACL(container, blobName).getOwner().getId();
+                sss.putObjectACL(container,
+                                 blobName,
+                                 AccessControlList.fromCannedAccessPolicy(CannedAccessPolicy.PUBLIC_READ, ownerId));
+            }
+            else
+                LOG.warn(provider + " may not be properly supported for tarball transfer.");
+            // resolve the full URI of the blob (see http://code.google.com/p/jclouds/issues/detail?id=431)
+            BlobMetadata blob = context.getBlobStore().blobMetadata(container, blobName);
+            URI uri = context.getProviderSpecificContext().getEndpoint().resolve("/" + container + "/" + blob.getName());
+            return new Pair<BlobMetadata, URI>(blob, uri);
+        }
+        finally
+        {
+            context.close();
+        }
+    }
+
+    public static void deleteBlob(Configuration config, ClusterSpec spec, BlobMetadata blob)
+    {
+        String container = getContainer(config);
+        BlobStoreContext context = getContext(config, spec);
+        try
+        {
+            context.getBlobStore().removeBlob(container, blob.getName());
+        }
+        finally
+        {
+            context.close();
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/utils/KeyPair.java b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/utils/KeyPair.java
index e69de29b..a8d90490 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/utils/KeyPair.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/distributed/org/apache/cassandra/utils/KeyPair.java
@@ -0,0 +1,49 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.ByteArrayOutputStream;
+import java.util.Map;
+
+import com.google.common.collect.ImmutableMap;
+import com.jcraft.jsch.JSch;
+import com.jcraft.jsch.JSchException;
+
+/**
+ * A convenience class for generating an RSA key pair.
+ */
+public class KeyPair {
+
+  /**
+   * return a "public" -> rsa public key, "private" -> its corresponding
+   *   private key
+   */
+  public static Map<String,String> generate() throws JSchException {
+    com.jcraft.jsch.KeyPair pair = com.jcraft.jsch.KeyPair.genKeyPair(
+        new JSch(),  com.jcraft.jsch.KeyPair.RSA);
+    ByteArrayOutputStream publicKeyOut = new ByteArrayOutputStream();
+    ByteArrayOutputStream privateKeyOut = new ByteArrayOutputStream();
+    pair.writePublicKey(publicKeyOut, "whirr");
+    pair.writePrivateKey(privateKeyOut);
+    String publicKey = new String(publicKeyOut.toByteArray());
+    String privateKey = new String(privateKeyOut.toByteArray());
+    return ImmutableMap.<String, String> of("public", publicKey,
+        "private", privateKey);
+  }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/db/LongCompactionSpeedTest.java b/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/db/LongCompactionSpeedTest.java
index e69de29b..4a01f778 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/db/LongCompactionSpeedTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/db/LongCompactionSpeedTest.java
@@ -0,0 +1,106 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.net.InetAddress;
+import java.util.*;
+
+import org.apache.cassandra.Util;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableUtils;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.utils.FBUtilities;
+import static junit.framework.Assert.assertEquals;
+
+public class LongCompactionSpeedTest extends CleanupHelper
+{
+    public static final String TABLE1 = "Keyspace1";
+    public static final InetAddress LOCAL = FBUtilities.getLocalAddress();
+
+    /**
+     * Test compaction with a very wide row.
+     */
+    @Test
+    public void testCompactionWide() throws Exception
+    {
+        testCompaction(2, 1, 200000);
+    }
+
+    /**
+     * Test compaction with lots of skinny rows.
+     */
+    @Test
+    public void testCompactionSlim() throws Exception
+    {
+        testCompaction(2, 200000, 1);
+    }
+
+    /**
+     * Test compaction with lots of small sstables.
+     */
+    @Test
+    public void testCompactionMany() throws Exception
+    {
+        testCompaction(100, 800, 5);
+    }
+
+    protected void testCompaction(int sstableCount, int rowsPerSSTable, int colsPerRow) throws Exception
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open(TABLE1);
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard1");
+
+        ArrayList<SSTableReader> sstables = new ArrayList<SSTableReader>();
+        for (int k = 0; k < sstableCount; k++)
+        {
+            SortedMap<String,ColumnFamily> rows = new TreeMap<String,ColumnFamily>();
+            for (int j = 0; j < rowsPerSSTable; j++)
+            {
+                String key = String.valueOf(j);
+                IColumn[] cols = new IColumn[colsPerRow];
+                for (int i = 0; i < colsPerRow; i++)
+                {
+                    // last sstable has highest timestamps
+                    cols[i] = Util.column(String.valueOf(i), String.valueOf(i), k);
+                }
+                rows.put(key, SSTableUtils.createCF(Long.MIN_VALUE, Integer.MIN_VALUE, cols));
+            }
+            SSTableReader sstable = SSTableUtils.prepare().write(rows);
+            sstables.add(sstable);
+            store.addSSTable(sstable);
+        }
+
+        // give garbage collection a bit of time to catch up
+        Thread.sleep(1000);
+
+        long start = System.currentTimeMillis();
+        CompactionManager.instance.doCompaction(store, sstables, (int) (System.currentTimeMillis() / 1000) - DatabaseDescriptor.getCFMetaData(TABLE1, "Standard1").getGcGraceSeconds());
+        System.out.println(String.format("%s: sstables=%d rowsper=%d colsper=%d: %d ms",
+                                         this.getClass().getName(),
+                                         sstableCount,
+                                         rowsPerSSTable,
+                                         colsPerRow,
+                                         System.currentTimeMillis() - start));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/db/LongTableTest.java b/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/db/LongTableTest.java
index e69de29b..fc093ff5 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/db/LongTableTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/db/LongTableTest.java
@@ -0,0 +1,85 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.nio.ByteBuffer;
+import java.nio.charset.CharacterCodingException;
+import java.text.DecimalFormat;
+import java.text.NumberFormat;
+import java.util.*;
+import java.io.IOException;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+import org.apache.commons.lang.StringUtils;
+import org.junit.Test;
+
+import static junit.framework.Assert.*;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.utils.WrappedRunnable;
+import static org.apache.cassandra.Util.column;
+import static org.apache.cassandra.Util.getBytes;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.io.sstable.IndexHelper;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.util.BufferedRandomAccessFile;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class LongTableTest extends CleanupHelper
+{
+    @Test
+    public void testGetRowMultiColumn() throws Throwable
+    {
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+
+        for (int i = 1; i < 5000; i += 100)
+        {
+            RowMutation rm = new RowMutation("Keyspace1", Util.dk("key" + i).key);
+            ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+            for (int j = 0; j < i; j++)
+                cf.addColumn(column("c" + j, "v" + j, 1L));
+            rm.add(cf);
+            rm.applyUnsafe();
+        }
+
+        Runnable verify = new WrappedRunnable()
+        {
+            public void runMayThrow() throws Exception
+            {
+                ColumnFamily cf;
+                for (int i = 1; i < 5000; i += 100)
+                {
+                    for (int j = 0; j < i; j++)
+                    {
+                        cf = cfStore.getColumnFamily(QueryFilter.getNamesFilter(Util.dk("key" + i), new QueryPath("Standard1"), ByteBufferUtil.bytes("c" + j)));
+                        TableTest.assertColumns(cf, "c" + j);
+                    }
+                }
+
+            }
+        };
+        TableTest.reTest(table.getColumnFamilyStore("Standard1"), verify);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/utils/LongBloomFilterTest.java b/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/utils/LongBloomFilterTest.java
index e69de29b..b88f74f0 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/utils/LongBloomFilterTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/utils/LongBloomFilterTest.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.cassandra.utils;
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.junit.Test;
+
+public class LongBloomFilterTest
+{
+    public BloomFilter bf;
+
+    /**
+     * NB: needs to run with -mx1G
+     */
+    @Test
+    public void testBigInt()
+    {
+        int size = 10 * 1000 * 1000;
+        bf = BloomFilter.getFilter(size, FilterTestHelper.spec.bucketsPerElement);
+        FilterTestHelper.testFalsePositives(bf,
+                                            new KeyGenerator.IntGenerator(size),
+                                            new KeyGenerator.IntGenerator(size, size * 2));
+    }
+
+    @Test
+    public void testBigRandom()
+    {
+        int size = 10 * 1000 * 1000;
+        bf = BloomFilter.getFilter(size, FilterTestHelper.spec.bucketsPerElement);
+        FilterTestHelper.testFalsePositives(bf,
+                                            new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size),
+                                            new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size));
+    }
+
+    @Test
+    public void timeit()
+    {
+        int size = 300 * FilterTestHelper.ELEMENTS;
+        bf = BloomFilter.getFilter(size, FilterTestHelper.spec.bucketsPerElement);
+        for (int i = 0; i < 10; i++)
+        {
+            FilterTestHelper.testFalsePositives(bf,
+                                                new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size),
+                                                new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size));
+            bf.clear();
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/utils/LongLegacyBloomFilterTest.java b/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/utils/LongLegacyBloomFilterTest.java
index e69de29b..ee7ee1d0 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/utils/LongLegacyBloomFilterTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/long/org/apache/cassandra/utils/LongLegacyBloomFilterTest.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.cassandra.utils;
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.junit.Test;
+
+public class LongLegacyBloomFilterTest
+{
+    public LegacyBloomFilter bf;
+
+    /**
+     * NB: needs to run with -mx1G
+     */
+    @Test
+    public void testBigInt()
+    {
+        int size = 10 * 1000 * 1000;
+        bf = LegacyBloomFilter.getFilter(size, FilterTestHelper.spec.bucketsPerElement);
+        FilterTestHelper.testFalsePositives(bf,
+                                            new KeyGenerator.IntGenerator(size),
+                                            new KeyGenerator.IntGenerator(size, size * 2));
+    }
+
+    @Test
+    public void testBigRandom()
+    {
+        int size = 10 * 1000 * 1000;
+        bf = LegacyBloomFilter.getFilter(size, FilterTestHelper.spec.bucketsPerElement);
+        FilterTestHelper.testFalsePositives(bf,
+                                            new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size),
+                                            new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size));
+    }
+
+    @Test
+    public void timeit()
+    {
+        int size = 300 * FilterTestHelper.ELEMENTS;
+        bf = LegacyBloomFilter.getFilter(size, FilterTestHelper.spec.bucketsPerElement);
+        for (int i = 0; i < 10; i++)
+        {
+            FilterTestHelper.testFalsePositives(bf,
+                                                new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size),
+                                                new KeyGenerator.RandomStringGenerator(new Random().nextInt(), size));
+            bf.clear();
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/AbstractSerializationsTester.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/AbstractSerializationsTester.java
index e69de29b..21518cff 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/AbstractSerializationsTester.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/AbstractSerializationsTester.java
@@ -0,0 +1,50 @@
+package org.apache.cassandra;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+
+public class AbstractSerializationsTester extends SchemaLoader
+{
+    protected static final String CUR_VER = System.getProperty("cassandra.version", "0.7");
+    
+    protected static final boolean EXECUTE_WRITES = new Boolean(System.getProperty("cassandra.test-serialization-writes", "False")).booleanValue();
+    
+    protected static DataInputStream getInput(String name) throws IOException
+    {
+        File f = new File("test/data/serialization/" + CUR_VER + "/" + name);
+        assert f.exists();
+        return new DataInputStream(new FileInputStream(f));
+    }
+    
+    protected static DataOutputStream getOutput(String name) throws IOException
+    {
+        File f = new File("test/data/serialization/" + CUR_VER + "/" + name);
+        f.getParentFile().mkdirs();
+        return new DataOutputStream(new FileOutputStream(f));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/CleanupHelper.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/CleanupHelper.java
index e69de29b..dd4cc797 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/CleanupHelper.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/CleanupHelper.java
@@ -0,0 +1,111 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.junit.BeforeClass;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.db.commitlog.CommitLog;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class CleanupHelper extends SchemaLoader
+{
+    private static Logger logger = LoggerFactory.getLogger(CleanupHelper.class);
+
+    @BeforeClass
+    public static void cleanupAndLeaveDirs() throws IOException
+    {
+        mkdirs();
+        cleanup();
+        mkdirs();
+        CommitLog.instance.resetUnsafe(); // cleanup screws w/ CommitLog, this brings it back to safe state
+    }
+
+    public static void cleanup() throws IOException
+    {
+        // clean up commitlog
+        String[] directoryNames = { DatabaseDescriptor.getCommitLogLocation(), };
+        for (String dirName : directoryNames)
+        {
+            File dir = new File(dirName);
+            if (!dir.exists())
+                throw new RuntimeException("No such directory: " + dir.getAbsolutePath());
+            FileUtils.deleteRecursive(dir);
+        }
+
+        // clean up data directory which are stored as data directory/table/data files
+        for (String dirName : DatabaseDescriptor.getAllDataFileLocations())
+        {
+            File dir = new File(dirName);
+            if (!dir.exists())
+                throw new RuntimeException("No such directory: " + dir.getAbsolutePath());
+            FileUtils.deleteRecursive(dir);
+        }
+    }
+
+    public static void mkdirs()
+    {
+        try
+        {
+            DatabaseDescriptor.createAllDirectories();
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    protected void insertData(String keyspace, String columnFamily, int offset, int numberOfRows) throws IOException
+    {
+        for (int i = offset; i < offset + numberOfRows; i++)
+        {
+            ByteBuffer key = ByteBufferUtil.bytes("key" + i);
+            RowMutation rowMutation = new RowMutation(keyspace, key);
+            QueryPath path = new QueryPath(columnFamily, null, ByteBufferUtil.bytes("col" + i));
+
+            rowMutation.add(path, ByteBufferUtil.bytes("val" + i), System.currentTimeMillis());
+            rowMutation.applyUnsafe();
+        }
+    }
+
+    /* usually used to populate the cache */
+    protected void readData(String keyspace, String columnFamily, int offset, int numberOfRows) throws IOException
+    {
+        ColumnFamilyStore store = Table.open(keyspace).getColumnFamilyStore(columnFamily);
+        for (int i = offset; i < offset + numberOfRows; i++)
+        {
+            DecoratedKey key = Util.dk("key" + i);
+            QueryPath path = new QueryPath(columnFamily, null, ByteBufferUtil.bytes("col" + i));
+
+            store.getColumnFamily(key, path, ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1);
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/EmbeddedServer.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/EmbeddedServer.java
index e69de29b..5b830eb8 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/EmbeddedServer.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/EmbeddedServer.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.cassandra;
+
+import java.io.IOException;
+import java.lang.reflect.InvocationTargetException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.cassandra.service.CassandraDaemon;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+public class EmbeddedServer extends CleanupHelper
+{
+    protected static CassandraDaemon daemon = null;
+    
+    enum GatewayService
+    {
+        Thrift, Avro
+    }
+    
+    public static GatewayService getDaemonGatewayService()
+    {
+        return GatewayService.Thrift;
+    }
+    
+    static ExecutorService executor = Executors.newSingleThreadExecutor();
+    
+    @BeforeClass
+    public static void startCassandra() throws IOException
+
+    {
+        executor.execute(new Runnable()
+        {
+            public void run()
+            {
+                switch (getDaemonGatewayService())
+                {
+                    case Avro:
+                        daemon = new org.apache.cassandra.avro.CassandraDaemon();
+                        break;
+                    case Thrift:
+                    default:
+                        daemon = new org.apache.cassandra.thrift.CassandraDaemon();
+                }
+                daemon.activate();
+            }
+        });
+        try
+        {
+            TimeUnit.SECONDS.sleep(3);
+        }
+        catch (InterruptedException e)
+        {
+            throw new AssertionError(e);
+        }
+    }
+    
+    @AfterClass
+    public static void stopCassandra() throws Exception
+    {
+        if (daemon != null)
+        {
+            daemon.deactivate();
+        }
+        executor.shutdown();
+        executor.shutdownNow();
+    }
+    
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/SchemaLoader.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/SchemaLoader.java
index e69de29b..efe67397 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/SchemaLoader.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/SchemaLoader.java
@@ -0,0 +1,48 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.config.KSMetaData;
+import org.junit.BeforeClass;
+
+public class SchemaLoader
+{
+    // todo: when xml is fully deprecated, this method should be changed to manually load a few table definitions into
+    // the definitions keyspace.
+    @BeforeClass
+    public static void loadSchemaFromYaml()
+    {
+        try
+        {
+            for (KSMetaData ksm : DatabaseDescriptor.readTablesFromYaml())
+            {
+                for (CFMetaData cfm : ksm.cfMetaData().values())
+                    CFMetaData.map(cfm);
+                DatabaseDescriptor.setTableDefinition(ksm, DatabaseDescriptor.getDefsVersion());
+            }
+        }
+        catch (ConfigurationException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/Util.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/Util.java
index e69de29b..518c045d 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/Util.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/Util.java
@@ -0,0 +1,155 @@
+package org.apache.cassandra;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.columniterator.IdentityQueryFilter;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.*;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.VersionedValue;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+import static org.junit.Assert.assertTrue;
+
+public class Util
+{
+    public static DecoratedKey dk(String key)
+    {
+        return StorageService.getPartitioner().decorateKey(ByteBufferUtil.bytes(key));
+    }
+
+    public static Column column(String name, String value, long timestamp)
+    {
+        return new Column(ByteBufferUtil.bytes(name), ByteBufferUtil.bytes(value), timestamp);
+    }
+
+    public static Token token(String key)
+    {
+        return StorageService.getPartitioner().getToken(ByteBufferUtil.bytes(key));
+    }
+
+    public static Range range(String left, String right)
+    {
+        return new Range(token(left), token(right));
+    }
+
+    public static Range range(IPartitioner p, String left, String right)
+    {
+        return new Range(p.getToken(ByteBufferUtil.bytes(left)), p.getToken(ByteBufferUtil.bytes(right)));
+    }
+
+    public static Bounds bounds(String left, String right)
+    {
+        return new Bounds(token(left), token(right));
+    }
+
+    public static void addMutation(RowMutation rm, String columnFamilyName, String superColumnName, long columnName, String value, long timestamp)
+    {
+        rm.add(new QueryPath(columnFamilyName, ByteBufferUtil.bytes(superColumnName), getBytes(columnName)), ByteBufferUtil.bytes(value), timestamp);
+    }
+
+    public static ByteBuffer getBytes(long v)
+    {
+        byte[] bytes = new byte[8];
+        ByteBuffer bb = ByteBuffer.wrap(bytes);
+        bb.putLong(v);
+        bb.rewind();
+        return bb;
+    }
+    
+    public static List<Row> getRangeSlice(ColumnFamilyStore cfs) throws IOException, ExecutionException, InterruptedException
+    {
+        Token min = StorageService.getPartitioner().getMinimumToken();
+        return cfs.getRangeSlice(null,
+                                 new Bounds(min, min),
+                                 10000,
+                                 new IdentityQueryFilter());
+    }
+
+    /**
+     * Writes out a bunch of rows for a single column family.
+     *
+     * @param rows A group of RowMutations for the same table and column family.
+     * @return The ColumnFamilyStore that was used.
+     */
+    public static ColumnFamilyStore writeColumnFamily(List<RowMutation> rms) throws IOException, ExecutionException, InterruptedException
+    {
+        RowMutation first = rms.get(0);
+        String tablename = first.getTable();
+        String cfname = first.getColumnFamilies().iterator().next().metadata().cfName;
+
+        Table table = Table.open(tablename);
+        ColumnFamilyStore store = table.getColumnFamilyStore(cfname);
+
+        for (RowMutation rm : rms)
+            rm.apply();
+
+        store.forceBlockingFlush();
+        return store;
+    }
+
+    public static ColumnFamily getColumnFamily(Table table, DecoratedKey key, String cfName) throws IOException
+    {
+        ColumnFamilyStore cfStore = table.getColumnFamilyStore(cfName);
+        assert cfStore != null : "Column family " + cfName + " has not been defined";
+        return cfStore.getColumnFamily(QueryFilter.getIdentityFilter(key, new QueryPath(cfName)));
+    }
+
+    public static ColumnFamily cloneAndRemoveDeleted(ColumnFamily cf, int gcBefore)
+    {
+        return ColumnFamilyStore.removeDeleted(cf.cloneMe(), gcBefore);
+    }
+
+    /**
+     * Creates initial set of nodes and tokens. Nodes are added to StorageService as 'normal'
+     */
+    public static void createInitialRing(StorageService ss, IPartitioner partitioner, List<Token> endpointTokens,
+                                   List<Token> keyTokens, List<InetAddress> hosts, int howMany)
+        throws UnknownHostException
+    {
+        for (int i=0; i<howMany; i++)
+        {
+            endpointTokens.add(new BigIntegerToken(String.valueOf(10 * i)));
+            keyTokens.add(new BigIntegerToken(String.valueOf(10 * i + 5)));
+        }
+
+        for (int i=0; i<endpointTokens.size(); i++)
+        {
+            InetAddress ep = InetAddress.getByName("127.0.0." + String.valueOf(i + 1));
+            ss.onChange(ep, ApplicationState.STATUS, new VersionedValue.VersionedValueFactory(partitioner).normal(endpointTokens.get(i)));
+            hosts.add(ep);
+        }
+
+        // check that all nodes are in token metadata
+        for (int i=0; i<endpointTokens.size(); ++i)
+            assertTrue(ss.getTokenMetadata().isMember(hosts.get(i)));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/auth/SimpleAuthorityTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/auth/SimpleAuthorityTest.java
index e69de29b..cc1384c5 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/auth/SimpleAuthorityTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/auth/SimpleAuthorityTest.java
@@ -0,0 +1,84 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.auth;
+
+import java.util.Arrays;
+import java.util.EnumSet;
+import java.util.List;
+
+import org.junit.Test;
+import static org.junit.Assert.assertEquals;
+
+public class SimpleAuthorityTest
+{
+    private final SimpleAuthority authority = new SimpleAuthority();
+
+    private final AuthenticatedUser USER1 = new AuthenticatedUser("user1");
+    private final AuthenticatedUser USER2 = new AuthenticatedUser("user2");
+    private final AuthenticatedUser USER3 = new AuthenticatedUser("user3");
+
+    private final List<Object> KEYSPACES_RESOURCE = Arrays.<Object>asList(Resources.ROOT, Resources.KEYSPACES);
+    private final List<Object> KEYSPACE1_RESOURCE = Arrays.<Object>asList(Resources.ROOT, Resources.KEYSPACES, "Keyspace1");
+    private final List<Object> KEYSPACE2_RESOURCE = Arrays.<Object>asList(Resources.ROOT, Resources.KEYSPACES, "Keyspace2");
+    private final List<Object> STANDARD1_RESOURCE = Arrays.<Object>asList(Resources.ROOT,
+                                                                          Resources.KEYSPACES,
+                                                                          "Keyspace1",
+                                                                          "Standard1");
+
+    @Test
+    public void testValidateConfiguration() throws Exception
+    {
+        authority.validateConfiguration();
+    }
+
+    @Test
+    public void testAuthorizeKeyspace() throws Exception
+    {
+        assertEquals(Permission.ALL, authority.authorize(USER1, KEYSPACE1_RESOURCE));
+        assertEquals(Permission.ALL, authority.authorize(USER2, KEYSPACE1_RESOURCE));
+        assertEquals(Permission.NONE, authority.authorize(USER3, KEYSPACE1_RESOURCE));
+
+        assertEquals("a keyspace not listed in the access file should be inaccessible",
+                     Permission.NONE,
+                     authority.authorize(USER1, KEYSPACE2_RESOURCE));
+    }
+
+    @Test
+    public void testAuthorizeKeyspaceList() throws Exception
+    {
+        assertEquals("user1 should be able to modify the keyspace list",
+                     Permission.ALL,
+                     authority.authorize(USER1, KEYSPACES_RESOURCE));
+        assertEquals("user2 should only be able to read the keyspace list",
+                     EnumSet.of(Permission.READ),
+                     authority.authorize(USER2, KEYSPACES_RESOURCE));
+        assertEquals("user3 should only be able to read the keyspace list",
+                     EnumSet.of(Permission.READ),
+                     authority.authorize(USER3, KEYSPACES_RESOURCE));
+    }
+    
+    @Test
+    public void testAuthorizeColumnFamily() throws Exception
+    {
+        assertEquals(Permission.ALL, authority.authorize(USER1, STANDARD1_RESOURCE));
+        assertEquals(EnumSet.of(Permission.READ), authority.authorize(USER2, STANDARD1_RESOURCE));
+        assertEquals(Permission.NONE, authority.authorize(USER3, STANDARD1_RESOURCE));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/cli/CliTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/cli/CliTest.java
index e69de29b..9c2388b7 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/cli/CliTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/cli/CliTest.java
@@ -0,0 +1,236 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cli;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.service.EmbeddedCassandraService;
+import org.apache.thrift.transport.TTransportException;
+import org.junit.Test;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.regex.Pattern;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+public class CliTest extends CleanupHelper
+{
+    // please add new statements here so they could be auto-runned by this test.
+    private String[] statements = {
+        "use TestKeySpace;",
+        "create column family CF1 with comparator=UTF8Type and column_metadata=[{ column_name:world, validation_class:IntegerType, index_type:0, index_name:IdxName }, { column_name:world2, validation_class:LongType, index_type:KEYS, index_name:LongIdxName}];",
+        "set CF1[hello][world] = 123848374878933948398384;",
+        "set CF1[hello][test_quote] = 'value\\'';",
+        "set CF1['k\\'ey'][VALUE] = 'VAL';",
+        "set CF1['k\\'ey'][VALUE] = 'VAL\\'';",
+        "set CF1[hello][-31337] = 'some string value';",
+        "get CF1[hello][-31337];",
+        "get CF1[hello][world];",
+        "get CF1[hello][test_quote];",
+        "get CF1['k\\'ey'][VALUE]",
+        "set CF1[hello][-31337] = -23876;",
+        "set CF1[hello][-31337] = long(-23876);",
+        "set CF1[hello][world2] = 15;",
+        "get CF1 where world2 = long(15);",
+        "get cF1 where world2 = long(15);",
+        "get Cf1 where world2 = long(15);",
+        "set CF1['hello'][time_spent_uuid] = timeuuid(a8098c1a-f86e-11da-bd1a-00112444be1e);",
+        "create column family CF2 with comparator=IntegerType;",
+        "set CF2['key'][98349387493847748398334] = 'some text';",
+        "get CF2['key'][98349387493847748398334];",
+        "set CF2['key'][98349387493] = 'some text other';",
+        "get CF2['key'][98349387493];",
+        "create column family CF3 with comparator=UTF8Type and column_metadata=[{column_name:'big world', validation_class:LongType}];",
+        "set CF3['hello']['big world'] = 3748;",
+        "get CF3['hello']['big world'];",
+        "list CF3;",
+        "list CF3[:];",
+        "list CF3[h:];",
+        "list CF3 limit 10;",
+        "list CF3[h:] limit 10;",
+        "create column family CF4 with comparator=IntegerType and column_metadata=[{column_name:9999, validation_class:LongType}];",
+        "set CF4['hello'][9999] = 1234;",
+        "get CF4['hello'][9999];",
+        "get CF4['hello'][9999] as Long;",
+        "get CF4['hello'][9999] as Bytes;",
+        "set CF4['hello'][9999] = Long(1234);",
+        "get CF4['hello'][9999];",
+        "get CF4['hello'][9999] as Long;",
+        "del CF4['hello'][9999];",
+        "get CF4['hello'][9999];",
+        "create column family SCF1 with column_type=Super and comparator=IntegerType and subcomparator=LongType and column_metadata=[{column_name:9999, validation_class:LongType}];",
+        "set SCF1['hello'][1][9999] = 1234;",
+        "get SCF1['hello'][1][9999];",
+        "get SCF1['hello'][1][9999] as Long;",
+        "get SCF1['hello'][1][9999] as Bytes;",
+        "set SCF1['hello'][1][9999] = Long(1234);",
+        "get SCF1['hello'][1][9999];",
+        "get SCF1['hello'][1][9999] as Long;",
+        "del SCF1['hello'][1][9999];",
+        "get SCF1['hello'][1][9999];",
+        "set SCF1['hello'][1][9999] = Long(1234);",
+        "set SCF1['hello'][-1][-12] = Long(5678);",
+        "get SCF1['hello'][-1][-12];",
+        "set SCF1['hello'][-1][-12] = -340897;",
+        "set SCF1['hello'][-1][-12] = integer(-340897);",
+        "del SCF1['hello'][9999];",
+        "get SCF1['hello'][1][9999];",
+        "truncate CF1;",
+        "update keyspace TestKeySpace with placement_strategy='org.apache.cassandra.locator.LocalStrategy';",
+        "update keyspace TestKeySpace with replication_factor=1 and strategy_options=[{DC1:3, DC2:4, DC5:1}];",
+        "assume CF1 comparator as utf8;",
+        "assume CF1 sub_comparator as integer;",
+        "assume CF1 validator as lexicaluuid;",
+        "assume CF1 keys as timeuuid;",
+        "create column family CF7;",
+        "set CF7[1][timeuuid()] = utf8(test1);",
+        "set CF7[2][lexicaluuid()] = utf8('hello world!');",
+        "set CF7[3][lexicaluuid(550e8400-e29b-41d4-a716-446655440000)] = utf8(test2);",
+        "set CF7[key2][timeuuid()] = utf8(test3);",
+        "assume CF7 comparator as lexicaluuid;",
+        "assume CF7 keys as utf8;",
+        "list CF7;",
+        "get CF7[3];",
+        "get CF7[3][lexicaluuid(550e8400-e29b-41d4-a716-446655440000)];",
+        "get sCf1['hello'][1][9999];",
+        "set sCf1['hello'][1][9999] = 938;",
+        "set sCf1['hello'][1][9999] = 938 with ttl = 30;",
+        "set sCf1['hello'][1][9999] = 938 with ttl = 560;",
+        "list sCf1;",
+        "del SCF1['hello'][1][9999];",
+        "assume sCf1 comparator as utf8;",
+        "create column family CF8;",
+        "drop column family cF8;",
+        "create keyspace TESTIN;",
+        "drop keyspace tesTIN;",
+        "create column family myCF with column_type='Super' and comparator='UTF8Type' AND subcomparator='UTF8Type';",
+        "create column family Countries with comparator=UTF8Type and column_metadata=[ {column_name: name, validation_class: UTF8Type} ];",
+        "set Countries[1][name] = USA;",
+        "get Countries[1][name];",
+        "set myCF['key']['scName']['firstname'] = 'John';",
+        "get myCF['key']['scName']",
+        "use TestKEYSpace;",
+        "describe cluster;",
+        "help describe cluster;",
+        "show cluster name",
+        "show api version",
+        "help help",
+        "help connect",
+        "help use",
+        "help describe KEYSPACE",
+        "HELP exit",
+        "help QUIT",
+        "help show cluster name",
+        "help show keyspaces",
+        "help show api version",
+        "help create keyspace",
+        "HELP update KEYSPACE",
+        "HELP CREATE column FAMILY",
+        "HELP UPDATE COLUMN family",
+        "HELP drop keyspace",
+        "help drop column family",
+        "HELP GET",
+        "HELP set",
+        "HELP DEL",
+        "HELP count",
+        "HELP list",
+        "HELP TRUNCATE",
+        "help assume",
+        "HELP",
+        "?"
+    };
+   
+    @Test
+    public void testCli() throws IOException, TTransportException, ConfigurationException
+    {
+        new EmbeddedCassandraService().start();
+
+        // new error/output streams for CliSessionState
+        ByteArrayOutputStream errStream = new ByteArrayOutputStream();
+        ByteArrayOutputStream outStream = new ByteArrayOutputStream();
+
+        // checking if we can connect to the running cassandra node on localhost
+        CliMain.connect("127.0.0.1", 9170);
+
+        // setting new output stream
+        CliMain.sessionState.setOut(new PrintStream(outStream));
+        CliMain.sessionState.setErr(new PrintStream(errStream));
+
+        // re-creating keyspace for tests
+        // dropping in case it exists e.g. could be left from previous run
+        CliMain.processStatement("drop keyspace TestKeySpace;");
+        CliMain.processStatement("create keyspace TestKeySpace;");
+
+        for (String statement : statements)
+        {
+            errStream.reset();
+            // System.out.println("Executing statement: " + statement);
+            CliMain.processStatement(statement);
+            String result = outStream.toString();
+            // System.out.println("Result:\n" + result);
+            assertEquals(errStream.toString() + " processing " + statement, "", errStream.toString());
+            if (statement.startsWith("drop ") || statement.startsWith("create ") || statement.startsWith("update "))
+            {
+                assert Pattern.compile("(.{8})-(.{4})-(.{4})-(.{4})-(.{12}).*", Pattern.DOTALL).matcher(result).matches() : result;
+            }
+            else if (statement.startsWith("set "))
+            {
+                assertEquals(result, "Value inserted." + System.getProperty("line.separator"));
+            }
+            else if (statement.startsWith("get "))
+            {
+                if (statement.contains("where"))
+                {
+                    assertTrue(result.startsWith("-------------------" + System.getProperty("line.separator") + "RowKey:"));
+                }
+                else
+                {
+                    assertTrue(result.startsWith("=> (column=") || result.startsWith("Value was not found"));
+                }
+            }
+            else if (statement.startsWith("truncate "))
+            {
+                assertTrue(result.contains(" truncated."));
+            }
+            else if (statement.startsWith("assume "))
+            {
+                assertTrue(result.contains("successfully."));
+            }
+
+            outStream.reset(); // reset stream so we have only output from next statement all the time
+            errStream.reset(); // no errors to the end user.
+        }
+    }
+
+    @Test
+    public void testEscape()
+    {
+        //escaped is the string read from the cli.
+        String escaped = "backspace \\b tab \\t linefeed \\n form feed \\f carriage return \\r duble quote \\\" " +
+                "single quote \\' backslash \\\\";
+        String unescaped = "backspace \b tab \t linefeed \n form feed \f carriage return \r duble quote \" " +
+                "single quote ' backslash \\";
+        // when read from the cli may have single quotes around it
+        assertEquals(unescaped, CliUtils.unescapeSQLString("'" + escaped + "'"));
+        assertEquals(escaped, CliUtils.escapeSQLString(unescaped));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/client/TestRingCache.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/client/TestRingCache.java
index e69de29b..84792075 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/client/TestRingCache.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/client/TestRingCache.java
@@ -0,0 +1,113 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.client;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.nio.ByteBuffer;
+import java.util.Collection;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.thrift.Cassandra;
+import org.apache.cassandra.thrift.Column;
+import org.apache.cassandra.thrift.ColumnParent;
+import org.apache.cassandra.thrift.ColumnPath;
+import org.apache.cassandra.thrift.ConsistencyLevel;
+import org.apache.commons.lang.StringUtils;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.transport.TFramedTransport;
+import org.apache.thrift.transport.TSocket;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+/**
+ *  Sample code that uses RingCache in the client.
+ */
+public class TestRingCache
+{
+    private RingCache ringCache;
+    private Cassandra.Client thriftClient;
+
+    public TestRingCache(String keyspace) throws IOException
+    {
+        String seed = DatabaseDescriptor.getSeeds().iterator().next().getHostAddress();
+    	ringCache = new RingCache(keyspace, DatabaseDescriptor.getPartitioner(), seed, DatabaseDescriptor.getRpcPort());
+    }
+    
+    private void setup(String server, int port) throws Exception
+    {
+        /* Establish a thrift connection to the cassandra instance */
+        TSocket socket = new TSocket(server, port);
+        System.out.println(" connected to " + server + ":" + port + ".");
+        TBinaryProtocol binaryProtocol = new TBinaryProtocol(new TFramedTransport(socket));
+        Cassandra.Client cassandraClient = new Cassandra.Client(binaryProtocol);
+        socket.open();
+        thriftClient = cassandraClient;
+    }
+
+    /**
+     * usage: java -cp <configpath> org.apache.cassandra.client.TestRingCache [keyspace row-id-prefix row-id-int]
+     * to test a single keyspace/row, use the parameters. row-id-prefix and row-id-int are appended together to form a
+     * single row id.  If you supply now parameters, 'Keyspace1' is assumed and will check 9 rows ('row1' through 'row9').
+     * @param args
+     * @throws Exception
+     */
+    public static void main(String[] args) throws Throwable
+    {
+        int minRow;
+        int maxRow;
+        String rowPrefix, keyspace = "Keyspace1";
+        
+        if (args.length > 0)
+        {
+            keyspace = args[0];
+            rowPrefix = args[1];
+            minRow = Integer.parseInt(args[2]);
+            maxRow = minRow + 1;
+        }
+        else
+        {
+            minRow = 1;
+            maxRow = 10;
+            rowPrefix = "row";
+        }
+        
+        TestRingCache tester = new TestRingCache(keyspace);
+
+        for (int nRows = minRow; nRows < maxRow; nRows++)
+        {
+            ByteBuffer row = ByteBufferUtil.bytes((rowPrefix + nRows));
+            ColumnPath col = new ColumnPath("Standard1").setSuper_column((ByteBuffer)null).setColumn("col1".getBytes());
+            ColumnParent parent = new ColumnParent("Standard1").setSuper_column((ByteBuffer)null);
+
+            Collection<InetAddress> endpoints = tester.ringCache.getEndpoint(row);
+            InetAddress firstEndpoint = endpoints.iterator().next();
+            System.out.printf("hosts with key %s : %s; choose %s%n",
+                              new String(row.array()), StringUtils.join(endpoints, ","), firstEndpoint);
+
+            // now, read the row back directly from the host owning the row locally
+            tester.setup(firstEndpoint.getHostAddress(), DatabaseDescriptor.getRpcPort());
+            tester.thriftClient.set_keyspace(keyspace);
+            tester.thriftClient.insert(row, parent, new Column(ByteBufferUtil.bytes("col1"), ByteBufferUtil.bytes("val1"), 1), ConsistencyLevel.ONE);
+            Column column = tester.thriftClient.get(row, col, ConsistencyLevel.ONE).column;
+            System.out.println("read row " + new String(row.array()) + " " + new String(column.name.array()) + ":" + new String(column.value.array()) + ":" + column.timestamp);
+        }
+
+        System.exit(1);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutorTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutorTest.java
index e69de29b..54f0c2ed 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutorTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutorTest.java
@@ -0,0 +1,60 @@
+package org.apache.cassandra.concurrent;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+
+import org.junit.Test;
+
+import org.apache.cassandra.utils.WrappedRunnable;
+
+public class DebuggableThreadPoolExecutorTest
+{
+    @Test
+    public void testSerialization() throws InterruptedException
+    {
+        LinkedBlockingQueue<Runnable> q = new LinkedBlockingQueue<Runnable>(1);
+        DebuggableThreadPoolExecutor executor = new DebuggableThreadPoolExecutor(1,
+                                                                                 Integer.MAX_VALUE,
+                                                                                 TimeUnit.MILLISECONDS,
+                                                                                 q,
+                                                                                 new NamedThreadFactory("TEST"));
+        WrappedRunnable runnable = new WrappedRunnable()
+        {
+            public void runMayThrow() throws InterruptedException
+            {
+                Thread.sleep(50);
+            }
+        };
+        long start = System.currentTimeMillis();
+        for (int i = 0; i < 10; i++)
+        {
+            executor.execute(runnable);
+        }
+        assert q.size() > 0 : q.size();
+        while (executor.getCompletedTaskCount() < 10)
+            continue;
+        long delta = System.currentTimeMillis() - start;
+        assert delta >= 9 * 50 : delta;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/ColumnDefinitionTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/ColumnDefinitionTest.java
index 3f24d583..ff730e35 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/ColumnDefinitionTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/ColumnDefinitionTest.java
@@ -1 +1,56 @@
   + native
+package org.apache.cassandra.config;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.nio.ByteBuffer;
+import org.junit.Test;
+import org.apache.cassandra.thrift.IndexType;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class ColumnDefinitionTest
+{
+    @Test
+    public void testSerializeDeserialize() throws Exception
+    {
+        ColumnDefinition cd0 = new ColumnDefinition(ByteBufferUtil.bytes("TestColumnDefinitionName0"),
+                                                    "BytesType",
+                                                    IndexType.KEYS,
+                                                    "random index name 0");
+
+        ColumnDefinition cd1 = new ColumnDefinition(ByteBufferUtil.bytes("TestColumnDefinition1"),
+                                                    "LongType",
+                                                    null,
+                                                    null);
+
+        testSerializeDeserialize(cd0);
+        testSerializeDeserialize(cd1);
+    }
+
+    protected void testSerializeDeserialize(ColumnDefinition cd) throws Exception
+    {
+        ColumnDefinition newCd = ColumnDefinition.inflate(cd.deflate());
+        assert cd != newCd;
+        assert cd.hashCode() == newCd.hashCode();
+        assert cd.equals(newCd);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java
index e69de29b..bd6f11c8 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java
@@ -0,0 +1,100 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.config;
+
+import static org.junit.Assert.assertNotNull;
+
+import org.apache.avro.specific.SpecificRecord;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.migration.AddKeyspace;
+import org.apache.cassandra.locator.SimpleStrategy;
+import org.apache.cassandra.io.SerDeUtils;
+
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.UUID;
+
+public class DatabaseDescriptorTest
+{
+    protected <D extends SpecificRecord> D serDe(D record, D newInstance) throws IOException
+    {
+        D actual = SerDeUtils.deserialize(record.getSchema(),
+                                              SerDeUtils.serialize(record),
+                                              newInstance);
+        assert actual.equals(record) : actual + " != " + record;
+        return actual;
+    }
+    
+    @Test
+    public void testCFMetaDataSerialization() throws IOException, ConfigurationException
+    {
+        // test serialization of all defined test CFs.
+        for (String table : DatabaseDescriptor.getNonSystemTables())
+        {
+            for (CFMetaData cfm : DatabaseDescriptor.getTableMetaData(table).values())
+            {
+                CFMetaData cfmDupe = CFMetaData.inflate(serDe(cfm.deflate(), new org.apache.cassandra.avro.CfDef()));
+                assert cfmDupe != null;
+                assert cfmDupe.equals(cfm);
+            }
+        }
+    }
+
+    @Test
+    public void testKSMetaDataSerialization() throws IOException, ConfigurationException
+    {
+        for (KSMetaData ksm : DatabaseDescriptor.tables.values())
+        {
+            KSMetaData ksmDupe = KSMetaData.inflate(serDe(ksm.deflate(), new org.apache.cassandra.avro.KsDef()));
+            assert ksmDupe != null;
+            assert ksmDupe.equals(ksm);
+        }
+    }
+    
+    // this came as a result of CASSANDRA-995
+    @Test
+    public void testTransKsMigration() throws IOException, ConfigurationException
+    {
+        CleanupHelper.cleanupAndLeaveDirs();
+        DatabaseDescriptor.loadSchemas();
+        assert DatabaseDescriptor.getNonSystemTables().size() == 0;
+        
+        // add a few.
+        AddKeyspace ks0 = new AddKeyspace(new KSMetaData("ks0", SimpleStrategy.class, null, 3));
+        ks0.apply();
+        AddKeyspace ks1 = new AddKeyspace(new KSMetaData("ks1", SimpleStrategy.class, null, 3));
+        ks1.apply();
+        
+        assert DatabaseDescriptor.getTableDefinition("ks0") != null;
+        assert DatabaseDescriptor.getTableDefinition("ks1") != null;
+        
+        DatabaseDescriptor.clearTableDefinition(DatabaseDescriptor.getTableDefinition("ks0"), new UUID(4096, 0));
+        DatabaseDescriptor.clearTableDefinition(DatabaseDescriptor.getTableDefinition("ks1"), new UUID(4096, 0));
+        
+        assert DatabaseDescriptor.getTableDefinition("ks0") == null;
+        assert DatabaseDescriptor.getTableDefinition("ks1") == null;
+        
+        DatabaseDescriptor.loadSchemas();
+        
+        assert DatabaseDescriptor.getTableDefinition("ks0") != null;
+        assert DatabaseDescriptor.getTableDefinition("ks1") != null;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/DefsTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/DefsTest.java
index e69de29b..b724a6f7 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/DefsTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/config/DefsTest.java
@@ -0,0 +1,820 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.config;
+
+import static org.junit.Assert.assertEquals;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.*;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.avro.util.Utf8;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.config.ColumnDefinition;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.config.KSMetaData;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.marshal.BytesType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.db.migration.AddColumnFamily;
+import org.apache.cassandra.db.migration.AddKeyspace;
+import org.apache.cassandra.db.migration.DropColumnFamily;
+import org.apache.cassandra.db.migration.DropKeyspace;
+import org.apache.cassandra.db.migration.Migration;
+import org.apache.cassandra.db.migration.RenameColumnFamily;
+import org.apache.cassandra.db.migration.RenameKeyspace;
+import org.apache.cassandra.db.migration.UpdateColumnFamily;
+import org.apache.cassandra.db.migration.UpdateKeyspace;
+import org.apache.cassandra.io.SerDeUtils;
+import org.apache.cassandra.io.sstable.Component;
+import org.apache.cassandra.io.sstable.Descriptor;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.locator.OldNetworkTopologyStrategy;
+import org.apache.cassandra.locator.SimpleStrategy;
+import org.apache.cassandra.thrift.IndexType;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.UUIDGen;
+import org.junit.Test;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class DefsTest extends CleanupHelper
+{   
+    @Test
+    public void testZeroInjection() throws IOException
+    {
+        org.apache.cassandra.avro.CfDef cd = new org.apache.cassandra.avro.CfDef();
+        // populate only fields that must be non-null.
+        cd.keyspace = new Utf8("Lest Ks");
+        cd.name = new Utf8("Mest Cf");
+        
+        org.apache.cassandra.avro.CfDef cd2 = SerDeUtils.deserializeWithSchema(SerDeUtils.serializeWithSchema(cd), new org.apache.cassandra.avro.CfDef());
+        assert cd.equals(cd2);
+        // make sure some of the fields didn't get unexpected zeros put in during [de]serialize operations.
+        assert cd.min_compaction_threshold == null;
+        assert cd2.min_compaction_threshold == null;
+        assert cd.row_cache_save_period_in_seconds == null;
+        assert cd2.row_cache_save_period_in_seconds == null;
+        
+    }
+    
+    @Test
+    public void ensureStaticCFMIdsAreLessThan1000()
+    {
+        assert CFMetaData.StatusCf.cfId == 0;    
+        assert CFMetaData.HintsCf.cfId == 1;    
+        assert CFMetaData.MigrationsCf.cfId == 2;    
+        assert CFMetaData.SchemaCf.cfId == 3;    
+    }
+    
+    @Test
+    public void testCFMetaDataApply() throws ConfigurationException
+    {
+        Map<ByteBuffer, ColumnDefinition> indexes = new HashMap<ByteBuffer, ColumnDefinition>();
+        for (int i = 0; i < 5; i++) 
+        {
+            ByteBuffer name = ByteBuffer.wrap(new byte[] { (byte)i });
+            indexes.put(name, new ColumnDefinition(name, null, IndexType.KEYS, Integer.toString(i)));
+        }
+        CFMetaData cfm = new CFMetaData("Keyspace1",
+                "TestApplyCFM_CF",
+                ColumnFamilyType.Standard,
+                BytesType.instance,
+                null,
+                "No comment",
+                1.0,
+                1.0,
+                0.5,
+                100000,
+                null,
+                500,
+                500,
+                500,
+                500,
+                500,
+                500,
+                500.0,
+                indexes);
+        
+        // we'll be adding this one later. make sure it's not already there.
+        assert cfm.getColumn_metadata().get(ByteBuffer.wrap(new byte[] { 5 })) == null;
+        org.apache.cassandra.avro.CfDef cfDef = CFMetaData.convertToAvro(cfm);
+        
+        // add one.
+        org.apache.cassandra.avro.ColumnDef addIndexDef = new org.apache.cassandra.avro.ColumnDef();
+        addIndexDef.index_name = "5";
+        addIndexDef.index_type = org.apache.cassandra.avro.IndexType.KEYS;
+        addIndexDef.name = ByteBuffer.wrap(new byte[] { 5 });
+        addIndexDef.validation_class = BytesType.class.getName();
+        cfDef.column_metadata.add(addIndexDef);
+        
+        // remove one.
+        org.apache.cassandra.avro.ColumnDef removeIndexDef = new org.apache.cassandra.avro.ColumnDef();
+        removeIndexDef.index_name = "0";
+        removeIndexDef.index_type = org.apache.cassandra.avro.IndexType.KEYS;
+        removeIndexDef.name = ByteBuffer.wrap(new byte[] { 0 });
+        removeIndexDef.validation_class = BytesType.class.getName();
+        assert cfDef.column_metadata.remove(removeIndexDef);
+        
+        cfm.apply(cfDef);
+        
+        for (int i = 1; i < indexes.size(); i++)
+            assert cfm.getColumn_metadata().get(ByteBuffer.wrap(new byte[] { 1 })) != null;
+        assert cfm.getColumn_metadata().get(ByteBuffer.wrap(new byte[] { 0 })) == null;
+        assert cfm.getColumn_metadata().get(ByteBuffer.wrap(new byte[] { 5 })) != null;
+    }
+    
+    @Test
+    public void testInvalidNames() throws IOException
+    {
+        String[] valid = {"1", "a", "_1", "b_", "__", "1_a"};
+        for (String s : valid)
+            assert Migration.isLegalName(s);
+        
+        String[] invalid = {"b@t", "dash-y", "", " ", "dot.s", ".hidden"};
+        for (String s : invalid)
+            assert !Migration.isLegalName(s);
+    }
+    
+    @Test
+    public void saveAndRestore() throws IOException
+    {
+        // verify dump and reload.
+        UUID first = UUIDGen.makeType1UUIDFromHost(FBUtilities.getLocalAddress());
+        DefsTable.dumpToStorage(first);
+        List<KSMetaData> defs = new ArrayList<KSMetaData>(DefsTable.loadFromStorage(first));
+
+        assert defs.size() > 0;
+        assert defs.size() == DatabaseDescriptor.getNonSystemTables().size();
+        for (KSMetaData loaded : defs)
+        {
+            KSMetaData defined = DatabaseDescriptor.getTableDefinition(loaded.name);
+            assert defined.equals(loaded);
+        }
+    }
+    
+    @Test
+    public void addNewCfToBogusTable() throws InterruptedException
+    {
+        CFMetaData newCf = addTestCF("MadeUpKeyspace", "NewCF", "new cf");
+        try
+        {
+            new AddColumnFamily(newCf).apply();
+            throw new AssertionError("You shouldn't be able to do anything to a keyspace that doesn't exist.");
+        }
+        catch (ConfigurationException expected)
+        {
+        }
+        catch (IOException unexpected)
+        {
+            throw new AssertionError("Unexpected exception.");
+        }
+    }
+
+    @Test
+    public void testMigrations() throws IOException, ConfigurationException
+    {
+        // do a save. make sure it doesn't mess with the defs version.
+        UUID prior = DatabaseDescriptor.getDefsVersion();
+        UUID ver0 = UUIDGen.makeType1UUIDFromHost(FBUtilities.getLocalAddress());
+        DefsTable.dumpToStorage(ver0);
+        assert DatabaseDescriptor.getDefsVersion().equals(prior);
+
+        // add a cf.
+        CFMetaData newCf1 = addTestCF("Keyspace1", "MigrationCf_1", "Migration CF");
+
+        Migration m1 = new AddColumnFamily(newCf1);
+        m1.apply();
+        UUID ver1 = m1.getVersion();
+        assert DatabaseDescriptor.getDefsVersion().equals(ver1);
+        
+        // rename it.
+        Migration m2 = new RenameColumnFamily("Keyspace1", "MigrationCf_1", "MigrationCf_2");
+        m2.apply();
+        UUID ver2 = m2.getVersion();
+        assert DatabaseDescriptor.getDefsVersion().equals(ver2);
+        
+        // drop it.
+        Migration m3 = new DropColumnFamily("Keyspace1", "MigrationCf_2");
+        m3.apply();
+        UUID ver3 = m3.getVersion();
+        assert DatabaseDescriptor.getDefsVersion().equals(ver3);
+        
+        // now lets load the older migrations to see if that code works.
+        Collection<IColumn> serializedMigrations = Migration.getLocalMigrations(ver1, ver3);
+        assert serializedMigrations.size() == 3;
+        
+        // test deserialization of the migrations.
+        Migration[] reconstituded = new Migration[3];
+        int i = 0;
+        for (IColumn col : serializedMigrations)
+        {
+            UUID version = UUIDGen.getUUID(col.name());
+            reconstituded[i] = Migration.deserialize(col.value());
+            assert version.equals(reconstituded[i].getVersion());
+            i++;
+        }
+        
+        assert m1.getClass().equals(reconstituded[0].getClass());
+        assert m2.getClass().equals(reconstituded[1].getClass());
+        assert m3.getClass().equals(reconstituded[2].getClass());
+        
+        // verify that the row mutations are the same. rather than exposing the private fields, serialize and verify.
+        assert m1.serialize().equals(reconstituded[0].serialize());
+        assert m2.serialize().equals(reconstituded[1].serialize());
+        assert m3.serialize().equals(reconstituded[2].serialize());
+    }
+
+    @Test
+    public void addNewCF() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        final String ks = "Keyspace1";
+        final String cf = "BrandNewCf";
+        KSMetaData original = DatabaseDescriptor.getTableDefinition(ks);
+
+        CFMetaData newCf = addTestCF(original.name, cf, "A New Column Family");
+
+        assert !DatabaseDescriptor.getTableDefinition(ks).cfMetaData().containsKey(newCf.cfName);
+        new AddColumnFamily(newCf).apply();
+
+        assert DatabaseDescriptor.getTableDefinition(ks).cfMetaData().containsKey(newCf.cfName);
+        assert DatabaseDescriptor.getTableDefinition(ks).cfMetaData().get(newCf.cfName).equals(newCf);
+
+        // now read and write to it.
+        DecoratedKey dk = Util.dk("key0");
+        RowMutation rm = new RowMutation(ks, dk.key);
+        rm.add(new QueryPath(cf, null, ByteBufferUtil.bytes("col0")), ByteBufferUtil.bytes("value0"), 1L);
+        rm.apply();
+        ColumnFamilyStore store = Table.open(ks).getColumnFamilyStore(cf);
+        assert store != null;
+        store.forceBlockingFlush();
+        
+        ColumnFamily cfam = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath(cf), ByteBufferUtil.bytes("col0")));
+        assert cfam.getColumn(ByteBufferUtil.bytes("col0")) != null;
+        IColumn col = cfam.getColumn(ByteBufferUtil.bytes("col0"));
+        assert ByteBufferUtil.bytes("value0").equals(col.value());
+    }
+
+    @Test
+    public void dropCf() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        DecoratedKey dk = Util.dk("dropCf");
+        // sanity
+        final KSMetaData ks = DatabaseDescriptor.getTableDefinition("Keyspace1");
+        assert ks != null;
+        final CFMetaData cfm = ks.cfMetaData().get("Standard1");
+        assert cfm != null;
+        
+        // write some data, force a flush, then verify that files exist on disk.
+        RowMutation rm = new RowMutation(ks.name, dk.key);
+        for (int i = 0; i < 100; i++)
+            rm.add(new QueryPath(cfm.cfName, null, ByteBufferUtil.bytes(("col" + i))), ByteBufferUtil.bytes("anyvalue"), 1L);
+        rm.apply();
+        ColumnFamilyStore store = Table.open(cfm.tableName).getColumnFamilyStore(cfm.cfName);
+        assert store != null;
+        store.forceBlockingFlush();
+        assert DefsTable.getFiles(cfm.tableName, cfm.cfName).size() > 0;
+        
+        new DropColumnFamily(ks.name, cfm.cfName).apply();
+        
+        assert !DatabaseDescriptor.getTableDefinition(ks.name).cfMetaData().containsKey(cfm.cfName);
+        
+        // any write should fail.
+        rm = new RowMutation(ks.name, dk.key);
+        boolean success = true;
+        try
+        {
+            rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("col0")), ByteBufferUtil.bytes("value0"), 1L);
+            rm.apply();
+        }
+        catch (Throwable th)
+        {
+            success = false;
+        }
+        assert !success : "This mutation should have failed since the CF no longer exists.";
+
+        // verify that the files are gone.
+        for (File file : DefsTable.getFiles(cfm.tableName, cfm.cfName))
+        {
+            if (file.getPath().endsWith("Data.db") && !new File(file.getPath().replace("Data.db", "Compacted")).exists())
+                throw new AssertionError("undeleted file " + file);
+        }
+    }
+    
+    @Test
+    public void renameCf() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        DecoratedKey dk = Util.dk("key0");
+        final KSMetaData ks = DatabaseDescriptor.getTableDefinition("Keyspace2");
+        assert ks != null;
+        final CFMetaData oldCfm = ks.cfMetaData().get("Standard1");
+        assert oldCfm != null;
+        
+        // write some data, force a flush, then verify that files exist on disk.
+        RowMutation rm = new RowMutation(ks.name, dk.key);
+        for (int i = 0; i < 100; i++)
+            rm.add(new QueryPath(oldCfm.cfName, null, ByteBufferUtil.bytes(("col" + i))), ByteBufferUtil.bytes("anyvalue"), 1L);
+        rm.apply();
+        ColumnFamilyStore store = Table.open(oldCfm.tableName).getColumnFamilyStore(oldCfm.cfName);
+        assert store != null;
+        store.forceBlockingFlush();
+        int fileCount = DefsTable.getFiles(oldCfm.tableName, oldCfm.cfName).size();
+        assert fileCount > 0;
+        
+        final String cfName = "St4ndard1Replacement";
+        new RenameColumnFamily(oldCfm.tableName, oldCfm.cfName, cfName).apply();
+        
+        assert !DatabaseDescriptor.getTableDefinition(ks.name).cfMetaData().containsKey(oldCfm.cfName);
+        assert DatabaseDescriptor.getTableDefinition(ks.name).cfMetaData().containsKey(cfName);
+        
+        // verify that new files are there.
+        assert DefsTable.getFiles(oldCfm.tableName, cfName).size() == fileCount;
+        
+        // do some reads.
+        store = Table.open(oldCfm.tableName).getColumnFamilyStore(cfName);
+        assert store != null;
+        ColumnFamily cfam = store.getColumnFamily(QueryFilter.getSliceFilter(dk, new QueryPath(cfName), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1000));
+        assert cfam.getSortedColumns().size() == 100; // should be good enough?
+        
+        // do some writes
+        rm = new RowMutation(ks.name, dk.key);
+        rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes("col5")), ByteBufferUtil.bytes("updated"), 2L);
+        rm.apply();
+        store.forceBlockingFlush();
+        
+        cfam = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath(cfName), ByteBufferUtil.bytes("col5")));
+        assert cfam.getColumnsMap().size() == 1;
+        assert cfam.getColumn(ByteBufferUtil.bytes("col5")).value().equals( ByteBufferUtil.bytes("updated"));
+    }
+    
+    @Test
+    public void addNewKS() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        DecoratedKey dk = Util.dk("key0");
+        CFMetaData newCf = addTestCF("NewKeyspace1", "AddedStandard1", "A new cf for a new ks");
+
+        KSMetaData newKs = new KSMetaData(newCf.tableName, SimpleStrategy.class, null, 5, newCf);
+        
+        new AddKeyspace(newKs).apply();
+        
+        assert DatabaseDescriptor.getTableDefinition(newCf.tableName) != null;
+        assert DatabaseDescriptor.getTableDefinition(newCf.tableName) == newKs;
+
+        // test reads and writes.
+        RowMutation rm = new RowMutation(newCf.tableName, dk.key);
+        rm.add(new QueryPath(newCf.cfName, null, ByteBufferUtil.bytes("col0")), ByteBufferUtil.bytes("value0"), 1L);
+        rm.apply();
+        ColumnFamilyStore store = Table.open(newCf.tableName).getColumnFamilyStore(newCf.cfName);
+        assert store != null;
+        store.forceBlockingFlush();
+        
+        ColumnFamily cfam = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath(newCf.cfName), ByteBufferUtil.bytes("col0")));
+        assert cfam.getColumn(ByteBufferUtil.bytes("col0")) != null;
+        IColumn col = cfam.getColumn(ByteBufferUtil.bytes("col0"));
+        assert ByteBufferUtil.bytes("value0").equals(col.value());
+    }
+    
+    @Test
+    public void dropKS() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        DecoratedKey dk = Util.dk("dropKs");
+        // sanity
+        final KSMetaData ks = DatabaseDescriptor.getTableDefinition("Keyspace1");
+        assert ks != null;
+        final CFMetaData cfm = ks.cfMetaData().get("Standard2");
+        assert cfm != null;
+
+        // write some data, force a flush, then verify that files exist on disk.
+        RowMutation rm = new RowMutation(ks.name, dk.key);
+        for (int i = 0; i < 100; i++)
+            rm.add(new QueryPath(cfm.cfName, null, ByteBufferUtil.bytes(("col" + i))), ByteBufferUtil.bytes("anyvalue"), 1L);
+        rm.apply();
+        ColumnFamilyStore store = Table.open(cfm.tableName).getColumnFamilyStore(cfm.cfName);
+        assert store != null;
+        store.forceBlockingFlush();
+        assert DefsTable.getFiles(cfm.tableName, cfm.cfName).size() > 0;
+        
+        new DropKeyspace(ks.name).apply();
+        
+        assert DatabaseDescriptor.getTableDefinition(ks.name) == null;
+        
+        // write should fail.
+        rm = new RowMutation(ks.name, dk.key);
+        boolean success = true;
+        try
+        {
+            rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("col0")), ByteBufferUtil.bytes("value0"), 1L);
+            rm.apply();
+        }
+        catch (Throwable th)
+        {
+            success = false;
+        }
+        assert !success : "This mutation should have failed since the CF no longer exists.";
+
+        // reads should fail too.
+        try
+        {
+            Table.open(ks.name);
+        }
+        catch (Throwable th)
+        {
+            // this is what has historically happened when you try to open a table that doesn't exist.
+            assert th instanceof NullPointerException;
+        }
+    }
+
+    @Test
+    public void dropKSUnflushed() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        DecoratedKey dk = Util.dk("dropKs");
+        // sanity
+        final KSMetaData ks = DatabaseDescriptor.getTableDefinition("Keyspace3");
+        assert ks != null;
+        final CFMetaData cfm = ks.cfMetaData().get("Standard1");
+        assert cfm != null;
+
+        // write some data
+        RowMutation rm = new RowMutation(ks.name, dk.key);
+        for (int i = 0; i < 100; i++)
+            rm.add(new QueryPath(cfm.cfName, null, ByteBufferUtil.bytes(("col" + i))), ByteBufferUtil.bytes("anyvalue"), 1L);
+        rm.apply();
+
+        new DropKeyspace(ks.name).apply();
+
+        assert DatabaseDescriptor.getTableDefinition(ks.name) == null;
+    }
+
+    @Test
+    public void renameKs() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        DecoratedKey dk = Util.dk("renameKs");
+        final KSMetaData oldKs = DatabaseDescriptor.getTableDefinition("Keyspace2");
+        assert oldKs != null;
+        final String cfName = "Standard3";
+        assert oldKs.cfMetaData().containsKey(cfName);
+        assert oldKs.cfMetaData().get(cfName).tableName.equals(oldKs.name);
+        
+        // write some data that we hope to read back later.
+        RowMutation rm = new RowMutation(oldKs.name, dk.key);
+        for (int i = 0; i < 10; i++)
+            rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes(("col" + i))), ByteBufferUtil.bytes("value"), 1L);
+        rm.apply();
+        ColumnFamilyStore store = Table.open(oldKs.name).getColumnFamilyStore(cfName);
+        assert store != null;
+        store.forceBlockingFlush();
+        assert DefsTable.getFiles(oldKs.name, cfName).size() > 0;
+        
+        final String newKsName = "RenamedKeyspace2";
+        new RenameKeyspace(oldKs.name, newKsName).apply();
+        KSMetaData newKs = DatabaseDescriptor.getTableDefinition(newKsName);
+        
+        assert DatabaseDescriptor.getTableDefinition(oldKs.name) == null;
+        assert newKs != null;
+        assert newKs.name.equals(newKsName);
+        assert newKs.cfMetaData().containsKey(cfName);
+        assert newKs.cfMetaData().get(cfName).tableName.equals(newKsName);
+        assert DefsTable.getFiles(newKs.name, cfName).size() > 0;
+        
+        // read on old should fail.
+        try
+        {
+            Table.open(oldKs.name);
+        }
+        catch (Throwable th)
+        {
+            assert th instanceof NullPointerException;
+        }
+        
+        // write on old should fail.
+        rm = new RowMutation(oldKs.name, ByteBufferUtil.bytes("any key will do"));
+        boolean success = true;
+        try
+        {
+            rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes("col0")), ByteBufferUtil.bytes("value0"), 1L);
+            rm.apply();
+        }
+        catch (Throwable th)
+        {
+            success = false;
+        }
+        assert !success : "This mutation should have failed since the CF/Table no longer exists.";
+        
+        // write on new should work.
+        rm = new RowMutation(newKsName, dk.key);
+        rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes("col0")), ByteBufferUtil.bytes("newvalue"), 2L);
+        rm.apply();
+        store = Table.open(newKs.name).getColumnFamilyStore(cfName);
+        assert store != null;
+        store.forceBlockingFlush();
+        
+        // read on new should work.
+        SortedSet<ByteBuffer> cols = new TreeSet<ByteBuffer>(BytesType.instance);
+        cols.add(ByteBufferUtil.bytes("col0"));
+        cols.add(ByteBufferUtil.bytes("col1"));
+        ColumnFamily cfam = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath(cfName), cols));
+        assert cfam.getColumnsMap().size() == cols.size();
+        // tests new write.
+        
+        ByteBuffer val = cfam.getColumn(ByteBufferUtil.bytes("col0")).value();
+        assertEquals(ByteBufferUtil.string(val), "newvalue");
+        // tests old write.
+         val = cfam.getColumn(ByteBufferUtil.bytes("col1")).value();
+        assertEquals(ByteBufferUtil.string(val), "value");
+    }
+
+    @Test
+    public void createEmptyKsAddNewCf() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        assert DatabaseDescriptor.getTableDefinition("EmptyKeyspace") == null;
+        
+        KSMetaData newKs = new KSMetaData("EmptyKeyspace", SimpleStrategy.class, null, 5);
+
+        new AddKeyspace(newKs).apply();
+        assert DatabaseDescriptor.getTableDefinition("EmptyKeyspace") != null;
+
+        CFMetaData newCf = addTestCF("EmptyKeyspace", "AddedLater", "A new CF to add to an empty KS");
+
+        //should not exist until apply
+        assert !DatabaseDescriptor.getTableDefinition(newKs.name).cfMetaData().containsKey(newCf.cfName);
+
+        //add the new CF to the empty space
+        new AddColumnFamily(newCf).apply();
+
+        assert DatabaseDescriptor.getTableDefinition(newKs.name).cfMetaData().containsKey(newCf.cfName);
+        assert DatabaseDescriptor.getTableDefinition(newKs.name).cfMetaData().get(newCf.cfName).equals(newCf);
+
+        // now read and write to it.
+        DecoratedKey dk = Util.dk("key0");
+        RowMutation rm = new RowMutation(newKs.name, dk.key);
+        rm.add(new QueryPath(newCf.cfName, null, ByteBufferUtil.bytes("col0")), ByteBufferUtil.bytes("value0"), 1L);
+        rm.apply();
+        ColumnFamilyStore store = Table.open(newKs.name).getColumnFamilyStore(newCf.cfName);
+        assert store != null;
+        store.forceBlockingFlush();
+
+        ColumnFamily cfam = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath(newCf.cfName), ByteBufferUtil.bytes("col0")));
+        assert cfam.getColumn(ByteBufferUtil.bytes("col0")) != null;
+        IColumn col = cfam.getColumn(ByteBufferUtil.bytes("col0"));
+        assert ByteBufferUtil.bytes("value0").equals(col.value());
+    }
+    
+    @Test
+    public void testUpdateKeyspace() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        // create a keyspace to serve as existing.
+        CFMetaData cf = addTestCF("UpdatedKeyspace", "AddedStandard1", "A new cf for a new ks");
+        KSMetaData oldKs = new KSMetaData(cf.tableName, SimpleStrategy.class, null, 5, cf);
+        
+        new AddKeyspace(oldKs).apply();
+        
+        assert DatabaseDescriptor.getTableDefinition(cf.tableName) != null;
+        assert DatabaseDescriptor.getTableDefinition(cf.tableName) == oldKs;
+        
+        // anything with cf defs should fail.
+        CFMetaData cf2 = addTestCF(cf.tableName, "AddedStandard2", "A new cf for a new ks");
+        KSMetaData newBadKs = new KSMetaData(cf.tableName, SimpleStrategy.class, null, 4, cf2);
+        try
+        {
+            new UpdateKeyspace(newBadKs).apply();
+            throw new AssertionError("Should not have been able to update a KS with a KS that described column families.");
+        }
+        catch (ConfigurationException ex)
+        {
+            // expected.
+        }
+        
+        // names should match.
+        KSMetaData newBadKs2 = new KSMetaData(cf.tableName + "trash", SimpleStrategy.class, null, 4);
+        try
+        {
+            new UpdateKeyspace(newBadKs2).apply();
+            throw new AssertionError("Should not have been able to update a KS with an invalid KS name.");
+        }
+        catch (ConfigurationException ex)
+        {
+            // expected.
+        }
+        
+        KSMetaData newKs = new KSMetaData(cf.tableName, OldNetworkTopologyStrategy.class, null, 1);
+        new UpdateKeyspace(newKs).apply();
+        
+        KSMetaData newFetchedKs = DatabaseDescriptor.getKSMetaData(newKs.name);
+        assert newFetchedKs.replicationFactor == newKs.replicationFactor;
+        assert newFetchedKs.replicationFactor != oldKs.replicationFactor;
+        assert newFetchedKs.strategyClass.equals(newKs.strategyClass);
+        assert !newFetchedKs.strategyClass.equals(oldKs.strategyClass);
+    }
+
+    @Test
+    public void testUpdateColumnFamilyNoIndexes() throws ConfigurationException, IOException, ExecutionException, InterruptedException
+    {
+        // create a keyspace with a cf to update.
+        CFMetaData cf = addTestCF("UpdatedCfKs", "Standard1added", "A new cf that will be updated");
+        KSMetaData ksm = new KSMetaData(cf.tableName, SimpleStrategy.class, null, 1, cf);
+        new AddKeyspace(ksm).apply();
+        
+        assert DatabaseDescriptor.getTableDefinition(cf.tableName) != null;
+        assert DatabaseDescriptor.getTableDefinition(cf.tableName) == ksm;
+        assert DatabaseDescriptor.getCFMetaData(cf.tableName, cf.cfName) != null;
+        
+        // updating certain fields should fail.
+        org.apache.cassandra.avro.CfDef cf_def = CFMetaData.convertToAvro(cf);
+        cf_def.row_cache_size = 43.3;
+        cf_def.column_metadata = new ArrayList<org.apache.cassandra.avro.ColumnDef>();
+        cf_def.default_validation_class ="BytesType";
+        cf_def.min_compaction_threshold = 5;
+        cf_def.max_compaction_threshold = 31;
+        
+        // test valid operations.
+        cf_def.comment = "Modified comment";
+        new UpdateColumnFamily(cf_def).apply(); // doesn't get set back here.
+        
+        cf_def.row_cache_size = 2d;
+        new UpdateColumnFamily(cf_def).apply();
+        
+        cf_def.key_cache_size = 3d;
+        new UpdateColumnFamily(cf_def).apply();
+        
+        cf_def.read_repair_chance = 0.23;
+        new UpdateColumnFamily(cf_def).apply();
+        
+        cf_def.gc_grace_seconds = 12;
+        new UpdateColumnFamily(cf_def).apply();
+        
+        cf_def.default_validation_class = "UTF8Type";
+        new UpdateColumnFamily(cf_def).apply();
+
+        cf_def.min_compaction_threshold = 3;
+        new UpdateColumnFamily(cf_def).apply();
+
+        cf_def.max_compaction_threshold = 33;
+        new UpdateColumnFamily(cf_def).apply();
+
+        // can't test changing the reconciler because there is only one impl.
+        
+        // check the cumulative affect.
+        assert DatabaseDescriptor.getCFMetaData(cf.tableName, cf.cfName).getComment().equals(cf_def.comment);
+        assert DatabaseDescriptor.getCFMetaData(cf.tableName, cf.cfName).getRowCacheSize() == cf_def.row_cache_size;
+        assert DatabaseDescriptor.getCFMetaData(cf.tableName, cf.cfName).getKeyCacheSize() == cf_def.key_cache_size;
+        assert DatabaseDescriptor.getCFMetaData(cf.tableName, cf.cfName).getReadRepairChance() == cf_def.read_repair_chance;
+        assert DatabaseDescriptor.getCFMetaData(cf.tableName, cf.cfName).getGcGraceSeconds() == cf_def.gc_grace_seconds;
+        assert DatabaseDescriptor.getCFMetaData(cf.tableName, cf.cfName).getDefaultValidator() == UTF8Type.instance;
+        
+        // todo: we probably don't need to reset old values in the catches anymore.
+        // make sure some invalid operations fail.
+        int oldId = cf_def.id;
+        try
+        {
+            cf_def.id++;
+            cf.apply(cf_def);
+            throw new AssertionError("Should have blown up when you used a different id.");
+        }
+        catch (ConfigurationException expected) 
+        {
+            cf_def.id = oldId;    
+        }
+        
+        CharSequence oldStr = cf_def.name;
+        try
+        {
+            cf_def.name = cf_def.name + "_renamed";
+            cf.apply(cf_def);
+            throw new AssertionError("Should have blown up when you used a different name.");
+        }
+        catch (ConfigurationException expected)
+        {
+            cf_def.name = oldStr;
+        }
+        
+        oldStr = cf_def.keyspace;
+        try
+        {
+            cf_def.keyspace = oldStr + "_renamed";
+            cf.apply(cf_def);
+            throw new AssertionError("Should have blown up when you used a different keyspace.");
+        }
+        catch (ConfigurationException expected)
+        {
+            cf_def.keyspace = oldStr;
+        }
+        
+        try
+        {
+            cf_def.column_type = ColumnFamilyType.Super.name();
+            cf.apply(cf_def);
+            throw new AssertionError("Should have blwon up when you used a different cf type.");
+        }
+        catch (ConfigurationException expected)
+        {
+            cf_def.column_type = ColumnFamilyType.Standard.name();
+        }
+        
+        oldStr = cf_def.comparator_type;
+        try 
+        {
+            cf_def.comparator_type = BytesType.class.getSimpleName();
+            cf.apply(cf_def);
+            throw new AssertionError("Should have blown up when you used a different comparator.");
+        }
+        catch (ConfigurationException expected)
+        {
+            cf_def.comparator_type = UTF8Type.class.getSimpleName();
+        }
+
+        try
+        {
+            cf_def.min_compaction_threshold = 34;
+            cf.apply(cf_def);
+            throw new AssertionError("Should have blown up when min > max.");
+        }
+        catch (ConfigurationException expected)
+        {
+            cf_def.min_compaction_threshold = 3;
+        }
+
+        try
+        {
+            cf_def.max_compaction_threshold = 2;
+            cf.apply(cf_def);
+            throw new AssertionError("Should have blown up when max > min.");
+        }
+        catch (ConfigurationException expected)
+        {
+            cf_def.max_compaction_threshold = 33;
+        }
+    }
+
+    @Test
+    public void testDropIndex() throws IOException, ExecutionException, InterruptedException, ConfigurationException
+    {
+        // insert some data.  save the sstable descriptor so we can make sure it's marked for delete after the drop
+        RowMutation rm = new RowMutation("Keyspace6", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.apply();
+        ColumnFamilyStore cfs = Table.open("Keyspace6").getColumnFamilyStore("Indexed1");
+        cfs.forceBlockingFlush();
+        ColumnFamilyStore indexedCfs = cfs.getIndexedColumnFamilyStore(cfs.getIndexedColumns().iterator().next());
+        Descriptor desc = indexedCfs.getSSTables().iterator().next().descriptor;
+
+        // drop the index
+        CFMetaData meta = CFMetaData.rename(cfs.metadata, cfs.metadata.cfName); // abusing rename to clone
+        ColumnDefinition cdOld = meta.column_metadata.values().iterator().next();
+        ColumnDefinition cdNew = new ColumnDefinition(cdOld.name, cdOld.getValidator().getClass().getName(), null, null);
+        meta.column_metadata.put(cdOld.name, cdNew);
+        UpdateColumnFamily update = new UpdateColumnFamily(CFMetaData.convertToAvro(meta));
+        update.apply();
+
+        // check
+        assert cfs.getIndexedColumns().isEmpty();
+        ColumnFamilyStore.scrubDataDirectories("Keyspace6", "Indexed1");
+        assert !new File(desc.filenameFor(Component.DATA)).exists();
+    }
+
+    private CFMetaData addTestCF(String ks, String cf, String comment)
+    {
+        return new CFMetaData(ks,
+                              cf,
+                              ColumnFamilyType.Standard,
+                              UTF8Type.instance,
+                              null,
+                              comment,
+                              0,
+                              1.0,
+                              0,
+                              CFMetaData.DEFAULT_GC_GRACE_SECONDS,
+                              BytesType.instance,
+                              CFMetaData.DEFAULT_MIN_COMPACTION_THRESHOLD,
+                              CFMetaData.DEFAULT_MAX_COMPACTION_THRESHOLD,
+                              CFMetaData.DEFAULT_ROW_CACHE_SAVE_PERIOD_IN_SECONDS,
+                              CFMetaData.DEFAULT_KEY_CACHE_SAVE_PERIOD_IN_SECONDS,
+                              CFMetaData.DEFAULT_MEMTABLE_LIFETIME_IN_MINS,
+                              CFMetaData.DEFAULT_MEMTABLE_THROUGHPUT_IN_MB,
+                              CFMetaData.DEFAULT_MEMTABLE_OPERATIONS_IN_MILLIONS,
+                              Collections.<ByteBuffer, ColumnDefinition>emptyMap());
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CleanupTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CleanupTest.java
index e69de29b..cd26538b 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CleanupTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CleanupTest.java
@@ -0,0 +1,152 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.net.InetAddress;
+import java.util.Arrays;
+import java.util.List;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.db.columniterator.IdentityQueryFilter;
+import org.apache.cassandra.db.filter.IFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.BytesToken;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.locator.TokenMetadata;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.thrift.IndexClause;
+import org.apache.cassandra.thrift.IndexExpression;
+import org.apache.cassandra.thrift.IndexOperator;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class CleanupTest extends CleanupHelper
+{
+    public static final int LOOPS = 200;
+    public static final String TABLE1 = "Keyspace1";
+    public static final String CF1 = "Indexed1";
+    public static final String CF2 = "Standard1";
+    public static final ByteBuffer COLUMN = ByteBufferUtil.bytes("birthdate");
+    public static final ByteBuffer VALUE = ByteBuffer.allocate(8);
+    static
+    {
+        VALUE.putLong(20101229);
+        VALUE.flip();
+    }
+
+    @Test
+    public void testCleanup() throws IOException, ExecutionException, InterruptedException, ConfigurationException
+    {
+        StorageService.instance.initServer();
+
+        Table table = Table.open(TABLE1);
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(CF2);
+
+        List<Row> rows;
+
+        // insert data and verify we get it back w/ range query
+        fillCF(cfs, LOOPS);
+        rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter());
+        assertEquals(LOOPS, rows.size());
+
+        // with one token in the ring, owned by the local node, cleanup should be a no-op
+        CompactionManager.instance.performCleanup(cfs);
+
+        // check data is still there
+        rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter());
+        assertEquals(LOOPS, rows.size());
+    }
+
+    @Test
+    public void testCleanupWithIndexes() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open(TABLE1);
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(CF1);
+        assertEquals(cfs.getIndexedColumns().iterator().next(), COLUMN);
+
+        List<Row> rows;
+
+        // insert data and verify we get it back w/ range query
+        fillCF(cfs, LOOPS);
+        rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter());
+        assertEquals(LOOPS, rows.size());
+
+        ColumnFamilyStore cfi = cfs.getIndexedColumnFamilyStore(COLUMN);
+        assertTrue(cfi.isIndexBuilt());
+
+        // verify we get it back w/ index query too
+        IndexExpression expr = new IndexExpression(COLUMN, IndexOperator.EQ, VALUE);
+        IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, Integer.MAX_VALUE);
+        IFilter filter = new IdentityQueryFilter();
+        IPartitioner p = StorageService.getPartitioner();
+        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+        rows = table.getColumnFamilyStore(CF1).scan(clause, range, filter);
+        assertEquals(LOOPS, rows.size());
+
+        // we don't allow cleanup when the local host has no range to avoid wipping up all data when a node has not join the ring.
+        // So to make sure cleanup erase everything here, we give the localhost the tiniest possible range.
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+        byte[] tk1 = new byte[1], tk2 = new byte[1];
+        tk1[0] = 2;
+        tk2[0] = 1;
+        tmd.updateNormalToken(new BytesToken(tk1), InetAddress.getByName("127.0.0.1"));
+        tmd.updateNormalToken(new BytesToken(tk2), InetAddress.getByName("127.0.0.2"));
+
+        CompactionManager.instance.performCleanup(cfs);
+
+        // row data should be gone
+        rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter());
+        assertEquals(0, rows.size());
+
+        // not only should it be gone but there should be no data on disk, not even tombstones
+        assert cfs.getSSTables().isEmpty();
+
+        // 2ary indexes should result in no results, too (although tombstones won't be gone until compacted)
+        rows = cfs.scan(clause, range, filter);
+        assertEquals(0, rows.size());
+    }
+
+    protected void fillCF(ColumnFamilyStore cfs, int rowsPerSSTable) throws ExecutionException, InterruptedException, IOException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        for (int i = 0; i < rowsPerSSTable; i++)
+        {
+            String key = String.valueOf(i);
+            // create a row and update the birthdate value, test that the index query fetches the new version
+            RowMutation rm;
+            rm = new RowMutation(TABLE1, ByteBufferUtil.bytes(key));
+            rm.add(new QueryPath(cfs.getColumnFamilyName(), null, COLUMN), VALUE, System.currentTimeMillis());
+            rm.applyUnsafe();
+        }
+
+        cfs.forceBlockingFlush();
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
index 3f24d583..e8cd093b 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
@@ -1 +1,633 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.charset.CharacterCodingException;
+import java.util.*;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+
+import org.apache.commons.lang.ArrayUtils;
+import org.apache.commons.lang.StringUtils;
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.config.ColumnDefinition;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.columniterator.IdentityQueryFilter;
+import org.apache.cassandra.db.filter.*;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.thrift.*;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.WrappedRunnable;
+
+import static junit.framework.Assert.assertEquals;
+import static junit.framework.Assert.assertTrue;
+import static org.apache.cassandra.Util.column;
+import static org.apache.cassandra.Util.getBytes;
+import static org.junit.Assert.assertNull;
+
+public class ColumnFamilyStoreTest extends CleanupHelper
+{
+    static byte[] bytes1, bytes2;
+
+    static
+    {
+        Random random = new Random();
+        bytes1 = new byte[1024];
+        bytes2 = new byte[128];
+        random.nextBytes(bytes1);
+        random.nextBytes(bytes2);
+    }
+
+    @Test
+    public void testGetColumnWithWrongBF() throws IOException, ExecutionException, InterruptedException
+    {
+        List<RowMutation> rms = new LinkedList<RowMutation>();
+        RowMutation rm;
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("key1"));
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1")), ByteBufferUtil.bytes("asdf"), 0);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column2")), ByteBufferUtil.bytes("asdf"), 0);
+        rms.add(rm);
+        ColumnFamilyStore store = Util.writeColumnFamily(rms);
+
+        Table table = Table.open("Keyspace1");
+        List<SSTableReader> ssTables = table.getAllSSTables();
+        assertEquals(1, ssTables.size());
+        ssTables.get(0).forceFilterFailures();
+        ColumnFamily cf = store.getColumnFamily(QueryFilter.getIdentityFilter(Util.dk("key2"), new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1"))));
+        assertNull(cf);
+    }
+
+    @Test
+    public void testEmptyRow() throws Exception
+    {
+        Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore store = table.getColumnFamilyStore("Standard2");
+        RowMutation rm;
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("key1"));
+        rm.delete(new QueryPath("Standard2", null, null), System.currentTimeMillis());
+        rm.apply();
+
+        Runnable r = new WrappedRunnable()
+        {
+            public void runMayThrow() throws IOException
+            {
+                QueryFilter sliceFilter = QueryFilter.getSliceFilter(Util.dk("key1"), new QueryPath("Standard2", null, null), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1);
+                ColumnFamily cf = store.getColumnFamily(sliceFilter);
+                assert cf.isMarkedForDelete();
+                assert cf.getColumnsMap().isEmpty();
+
+                QueryFilter namesFilter = QueryFilter.getNamesFilter(Util.dk("key1"), new QueryPath("Standard2", null, null), ByteBufferUtil.bytes("a"));
+                cf = store.getColumnFamily(namesFilter);
+                assert cf.isMarkedForDelete();
+                assert cf.getColumnsMap().isEmpty();
+            }
+        };
+
+        TableTest.reTest(store, r);
+    }
+
+    @Test
+    public void testSkipStartKey() throws IOException, ExecutionException, InterruptedException
+    {
+        ColumnFamilyStore cfs = insertKey1Key2();
+
+        IPartitioner p = StorageService.getPartitioner();
+        List<Row> result = cfs.getRangeSlice(ByteBufferUtil.EMPTY_BYTE_BUFFER,
+                                             Util.range(p, "key1", "key2"),
+                                             10,
+                                             new NamesQueryFilter(ByteBufferUtil.bytes("asdf")));
+        assertEquals(1, result.size());
+        assert result.get(0).key.key.equals(ByteBufferUtil.bytes("key2"));
+    }
+
+    @Test
+    public void testIndexScan() throws IOException
+    {
+        RowMutation rm;
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("k2"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes(2L), 0);
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(2L), 0);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("k3"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes(2L), 0);
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("k4aaaa"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes(2L), 0);
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(3L), 0);
+        rm.apply();
+
+        // basic single-expression query
+        IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L));
+        IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        IFilter filter = new IdentityQueryFilter();
+        IPartitioner p = StorageService.getPartitioner();
+        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+        List<Row> rows = Table.open("Keyspace1").getColumnFamilyStore("Indexed1").scan(clause, range, filter);
+
+        assert rows != null;
+        assert rows.size() == 2 : StringUtils.join(rows, ",");
+        
+        String key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining()); 
+        assert "k1".equals( key );
+    
+        key = new String(rows.get(1).key.key.array(),rows.get(1).key.key.position(),rows.get(1).key.key.remaining()); 
+        assert "k3".equals(key);
+        
+        assert ByteBufferUtil.bytes(1L).equals( rows.get(0).cf.getColumn(ByteBufferUtil.bytes("birthdate")).value());
+        assert ByteBufferUtil.bytes(1L).equals( rows.get(1).cf.getColumn(ByteBufferUtil.bytes("birthdate")).value());
+
+        // add a second expression
+        IndexExpression expr2 = new IndexExpression(ByteBufferUtil.bytes("notbirthdate"), IndexOperator.GTE, ByteBufferUtil.bytes(2L));
+        clause = new IndexClause(Arrays.asList(expr, expr2), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        rows = Table.open("Keyspace1").getColumnFamilyStore("Indexed1").scan(clause, range, filter);
+
+        assert rows.size() == 1 : StringUtils.join(rows, ",");
+        key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining()); 
+        assert "k3".equals( key );
+    
+        // same query again, but with resultset not including the subordinate expression
+        rows = Table.open("Keyspace1").getColumnFamilyStore("Indexed1").scan(clause, range, new NamesQueryFilter(ByteBufferUtil.bytes("birthdate")));
+
+        assert rows.size() == 1 : StringUtils.join(rows, ",");
+        key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining()); 
+        assert "k3".equals( key );
+    
+        assert rows.get(0).cf.getColumnCount() == 1 : rows.get(0).cf;
+
+        // once more, this time with a slice rowset that needs to be expanded
+        SliceQueryFilter emptyFilter = new SliceQueryFilter(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 0);
+        rows = Table.open("Keyspace1").getColumnFamilyStore("Indexed1").scan(clause, range, emptyFilter);
+      
+        assert rows.size() == 1 : StringUtils.join(rows, ",");
+        key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining()); 
+        assert "k3".equals( key );
+    
+        assert rows.get(0).cf.getColumnCount() == 0;
+
+        // query with index hit but rejected by secondary clause, with a small enough count that just checking count
+        // doesn't tell the scan loop that it's done
+        IndexExpression expr3 = new IndexExpression(ByteBufferUtil.bytes("notbirthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(-1L));
+        clause = new IndexClause(Arrays.asList(expr, expr3), ByteBufferUtil.EMPTY_BYTE_BUFFER, 1);
+        rows = Table.open("Keyspace1").getColumnFamilyStore("Indexed1").scan(clause, range, filter);
+
+        assert rows.isEmpty();
+    }
+
+    @Test
+    public void testLargeScan() throws IOException
+    {
+        RowMutation rm;
+        for (int i = 0; i < 100; i++)
+        {
+            rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("key" + i));
+            rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(34L), 0);
+            rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes((long) (i % 2)), 0);
+            rm.applyUnsafe();
+        }
+
+        IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(34L));
+        IndexExpression expr2 = new IndexExpression(ByteBufferUtil.bytes("notbirthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L));
+        IndexClause clause = new IndexClause(Arrays.asList(expr, expr2), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        IFilter filter = new IdentityQueryFilter();
+        IPartitioner p = StorageService.getPartitioner();
+        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+        List<Row> rows = Table.open("Keyspace1").getColumnFamilyStore("Indexed1").scan(clause, range, filter);
+
+        assert rows != null;
+        assert rows.size() == 50 : rows.size();
+        Set<DecoratedKey> keys = new HashSet<DecoratedKey>();
+        // extra check that there are no duplicate results -- see https://issues.apache.org/jira/browse/CASSANDRA-2406
+        for (Row row : rows)
+            keys.add(row.key);
+        assert rows.size() == keys.size();
+    }
+
+    @Test
+    public void testIndexDeletions() throws IOException
+    {
+        ColumnFamilyStore cfs = Table.open("Keyspace3").getColumnFamilyStore("Indexed1");
+        RowMutation rm;
+
+        rm = new RowMutation("Keyspace3", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.apply();
+
+        IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L));
+        IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        IFilter filter = new IdentityQueryFilter();
+        IPartitioner p = StorageService.getPartitioner();
+        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+        List<Row> rows = cfs.scan(clause, range, filter);
+        assert rows.size() == 1 : StringUtils.join(rows, ",");
+        String key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining()); 
+        assert "k1".equals( key );
+
+        // delete the column directly
+        rm = new RowMutation("Keyspace3", ByteBufferUtil.bytes("k1"));
+        rm.delete(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), 1);
+        rm.apply();
+        rows = cfs.scan(clause, range, filter);
+        assert rows.isEmpty();
+
+        // verify that it's not being indexed under the deletion column value either
+        IColumn deletion = rm.getColumnFamilies().iterator().next().iterator().next();
+        ByteBuffer deletionLong = ByteBufferUtil.bytes((long) ByteBufferUtil.toInt(deletion.value()));
+        IndexExpression expr0 = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, deletionLong);
+        IndexClause clause0 = new IndexClause(Arrays.asList(expr0), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        rows = cfs.scan(clause0, range, filter);
+        assert rows.isEmpty();
+
+        // resurrect w/ a newer timestamp
+        rm = new RowMutation("Keyspace3", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 2);
+        rm.apply();
+        rows = cfs.scan(clause, range, filter);
+        assert rows.size() == 1 : StringUtils.join(rows, ",");
+        key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining());
+        assert "k1".equals( key );
+
+        // verify that row and delete w/ older timestamp does nothing
+        rm = new RowMutation("Keyspace3", ByteBufferUtil.bytes("k1"));
+        rm.delete(new QueryPath("Indexed1"), 1);
+        rm.apply();
+        rows = cfs.scan(clause, range, filter);
+        assert rows.size() == 1 : StringUtils.join(rows, ",");
+        key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining());
+        assert "k1".equals( key );
+
+        // similarly, column delete w/ older timestamp should do nothing
+        rm = new RowMutation("Keyspace3", ByteBufferUtil.bytes("k1"));
+        rm.delete(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), 1);
+        rm.apply();
+        rows = cfs.scan(clause, range, filter);
+        assert rows.size() == 1 : StringUtils.join(rows, ",");
+        key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining());
+        assert "k1".equals( key );
+
+        // delete the entire row (w/ newer timestamp this time)
+        rm = new RowMutation("Keyspace3", ByteBufferUtil.bytes("k1"));
+        rm.delete(new QueryPath("Indexed1"), 3);
+        rm.apply();
+        rows = cfs.scan(clause, range, filter);
+        assert rows.isEmpty() : StringUtils.join(rows, ",");
+
+        // make sure obsolete mutations don't generate an index entry
+        rm = new RowMutation("Keyspace3", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 3);
+        rm.apply();
+        rows = cfs.scan(clause, range, filter);
+        assert rows.isEmpty() : StringUtils.join(rows, ",");
+    }
+
+    @Test
+    public void testIndexUpdate() throws IOException
+    {
+        Table table = Table.open("Keyspace2");
+
+        // create a row and update the birthdate value, test that the index query fetches the new version
+        RowMutation rm;
+        rm = new RowMutation("Keyspace2", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 1);
+        rm.apply();
+        rm = new RowMutation("Keyspace2", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(2L), 2);
+        rm.apply();
+
+        IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L));
+        IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        IFilter filter = new IdentityQueryFilter();
+        IPartitioner p = StorageService.getPartitioner();
+        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+        List<Row> rows = table.getColumnFamilyStore("Indexed1").scan(clause, range, filter);
+        assert rows.size() == 0;
+
+        expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(2L));
+        clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        rows = table.getColumnFamilyStore("Indexed1").scan(clause, range, filter);
+        String key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining()); 
+        assert "k1".equals( key );
+        
+        // update the birthdate value with an OLDER timestamp, and test that the index ignores this
+        rm = new RowMutation("Keyspace2", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(3L), 0);
+        rm.apply();
+
+        rows = table.getColumnFamilyStore("Indexed1").scan(clause, range, filter);
+        key = new String(rows.get(0).key.key.array(),rows.get(0).key.key.position(),rows.get(0).key.key.remaining()); 
+        assert "k1".equals( key );
+    
+    }
+
+    // See CASSANDRA-2628
+    @Test
+    public void testIndexScanWithLimitOne() throws IOException
+    {
+        RowMutation rm;
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("kk1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("kk2"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes(2L), 0);
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("kk3"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes(2L), 0);
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("kk4"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("notbirthdate")), ByteBufferUtil.bytes(2L), 0);
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.apply();
+
+        // basic single-expression query
+        IndexExpression expr1 = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L));
+        IndexExpression expr2 = new IndexExpression(ByteBufferUtil.bytes("notbirthdate"), IndexOperator.GT, ByteBufferUtil.bytes(1L));
+        IndexClause clause = new IndexClause(Arrays.asList(new IndexExpression[]{ expr1, expr2 }), ByteBufferUtil.EMPTY_BYTE_BUFFER, 1);
+        IFilter filter = new IdentityQueryFilter();
+        IPartitioner p = StorageService.getPartitioner();
+        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+        List<Row> rows = Table.open("Keyspace1").getColumnFamilyStore("Indexed1").scan(clause, range, filter);
+
+        assert rows != null;
+        assert rows.size() == 1 : StringUtils.join(rows, ",");
+    }
+
+    @Test
+    public void testIndexCreate() throws IOException, ConfigurationException, InterruptedException, ExecutionException
+    {
+        Table table = Table.open("Keyspace1");
+
+        // create a row and update the birthdate value, test that the index query fetches the new version
+        RowMutation rm;
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed2", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 1);
+        rm.apply();
+
+        ColumnFamilyStore cfs = table.getColumnFamilyStore("Indexed2");
+        ColumnDefinition old = cfs.metadata.getColumn_metadata().get(ByteBufferUtil.bytes("birthdate"));
+        ColumnDefinition cd = new ColumnDefinition(old.name, old.getValidator().getClass().getName(), IndexType.KEYS, "birthdate_index");
+        Future<?> future = cfs.addIndex(cd);
+        future.get();
+        // we had a bug (CASSANDRA-2244) where index would get created but not flushed -- check for that
+        assert cfs.getIndexedColumnFamilyStore(cd.name).getSSTables().size() > 0;
+
+        queryBirthdate(table);
+
+        // validate that drop clears it out & rebuild works (CASSANDRA-2320)
+        ColumnFamilyStore indexedCfs = cfs.getIndexedColumnFamilyStore(ByteBufferUtil.bytes("birthdate"));
+        cfs.removeIndex(ByteBufferUtil.bytes("birthdate"));
+        assert !indexedCfs.isIndexBuilt();
+
+        // rebuild & re-query
+        future = cfs.addIndex(cd);
+        future.get();
+        queryBirthdate(table);
+    }
+
+    private void queryBirthdate(Table table) throws CharacterCodingException
+    {
+        IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L));
+        IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        IFilter filter = new IdentityQueryFilter();
+        IPartitioner p = StorageService.getPartitioner();
+        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+        List<Row> rows = table.getColumnFamilyStore("Indexed2").scan(clause, range, filter);
+        assert rows.size() == 1 : StringUtils.join(rows, ",");
+        assertEquals("k1", ByteBufferUtil.string(rows.get(0).key.key));
+    }
+
+    @Test
+    public void testDeleteSuperRowSticksAfterFlush() throws Throwable
+    {
+        String tableName = "Keyspace1";
+        String cfName= "Super1";
+        ByteBuffer scfName = ByteBufferUtil.bytes("SuperDuper");
+        Table table = Table.open(tableName);
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(cfName);
+        DecoratedKey key = Util.dk("flush-resurrection");
+        
+        // create an isolated sstable.
+        putColsSuper(cfs, key, scfName, 
+                new Column(getBytes(1), ByteBufferUtil.bytes("val1"), 1),
+                new Column(getBytes(2), ByteBufferUtil.bytes("val2"), 1),
+                new Column(getBytes(3), ByteBufferUtil.bytes("val3"), 1));
+        cfs.forceBlockingFlush();
+        
+        // insert, don't flush.
+        putColsSuper(cfs, key, scfName, 
+                new Column(getBytes(4), ByteBufferUtil.bytes("val4"), 1),
+                new Column(getBytes(5), ByteBufferUtil.bytes("val5"), 1),
+                new Column(getBytes(6), ByteBufferUtil.bytes("val6"), 1));
+        
+        // verify insert.
+        final SlicePredicate sp = new SlicePredicate();
+        sp.setSlice_range(new SliceRange());
+        sp.getSlice_range().setCount(100);
+        sp.getSlice_range().setStart(ArrayUtils.EMPTY_BYTE_ARRAY);
+        sp.getSlice_range().setFinish(ArrayUtils.EMPTY_BYTE_ARRAY);
+        
+        assertRowAndColCount(1, 6, scfName, false, cfs.getRangeSlice(scfName, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // deeleet.
+        RowMutation rm = new RowMutation(table.name, key.key);
+        rm.delete(new QueryPath(cfName, scfName), 2);
+        rm.apply();
+        
+        // verify delete.
+        assertRowAndColCount(1, 0, scfName, false, cfs.getRangeSlice(scfName, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // flush
+        cfs.forceBlockingFlush();
+        
+        // re-verify delete.
+        assertRowAndColCount(1, 0, scfName, false, cfs.getRangeSlice(scfName, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // late insert.
+        putColsSuper(cfs, key, scfName, 
+                new Column(getBytes(4), ByteBufferUtil.bytes("val4"), 1L),
+                new Column(getBytes(7), ByteBufferUtil.bytes("val7"), 1L));
+        
+        // re-verify delete.
+        assertRowAndColCount(1, 0, scfName, false, cfs.getRangeSlice(scfName, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // make sure new writes are recognized.
+        putColsSuper(cfs, key, scfName, 
+                new Column(getBytes(3), ByteBufferUtil.bytes("val3"), 3),
+                new Column(getBytes(8), ByteBufferUtil.bytes("val8"), 3),
+                new Column(getBytes(9), ByteBufferUtil.bytes("val9"), 3));
+        assertRowAndColCount(1, 3, scfName, false, cfs.getRangeSlice(scfName, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+    }
+    
+    private static void assertRowAndColCount(int rowCount, int colCount, ByteBuffer sc, boolean isDeleted, Collection<Row> rows)
+    {
+        assert rows.size() == rowCount : "rowcount " + rows.size();
+        for (Row row : rows)
+        {
+            assert row.cf != null : "cf was null";
+            if (sc != null)
+                assert row.cf.getColumn(sc).getSubColumns().size() == colCount : row.cf.getColumn(sc).getSubColumns().size();
+            else
+                assert row.cf.getColumnCount() == colCount : "colcount " + row.cf.getColumnCount() + "|" + str(row.cf);
+            if (isDeleted)
+                assert row.cf.isMarkedForDelete() : "cf not marked for delete";
+        }
+    }
+    
+    private static String str(ColumnFamily cf)
+    {
+        StringBuilder sb = new StringBuilder();
+        for (IColumn col : cf.getSortedColumns())
+            sb.append(String.format("(%s,%s,%d),", new String(col.name().array()), new String(col.value().array()), col.timestamp()));
+        return sb.toString();
+    }
+    
+    private static void putColsSuper(ColumnFamilyStore cfs, DecoratedKey key, ByteBuffer scfName, Column... cols) throws Throwable
+    {
+        RowMutation rm = new RowMutation(cfs.table.name, key.key);
+        ColumnFamily cf = ColumnFamily.create(cfs.table.name, cfs.getColumnFamilyName());
+        SuperColumn sc = new SuperColumn(scfName, LongType.instance);
+        for (Column col : cols)
+            sc.addColumn(col);
+        cf.addColumn(sc);
+        rm.add(cf);
+        rm.apply();
+    }
+    
+    private static void putColsStandard(ColumnFamilyStore cfs, DecoratedKey key, Column... cols) throws Throwable
+    {
+        RowMutation rm = new RowMutation(cfs.table.name, key.key);
+        ColumnFamily cf = ColumnFamily.create(cfs.table.name, cfs.getColumnFamilyName());
+        for (Column col : cols)
+            cf.addColumn(col);
+        rm.add(cf);
+        rm.apply();
+    }
+    
+    @Test
+    public void testDeleteStandardRowSticksAfterFlush() throws Throwable
+    {
+        // test to make sure flushing after a delete doesn't resurrect delted cols.
+        String tableName = "Keyspace1";
+        String cfName = "Standard1";
+        Table table = Table.open(tableName);
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(cfName);
+        DecoratedKey key = Util.dk("f-flush-resurrection");
+        
+        SlicePredicate sp = new SlicePredicate();
+        sp.setSlice_range(new SliceRange());
+        sp.getSlice_range().setCount(100);
+        sp.getSlice_range().setStart(ArrayUtils.EMPTY_BYTE_ARRAY);
+        sp.getSlice_range().setFinish(ArrayUtils.EMPTY_BYTE_ARRAY);
+        
+        // insert
+        putColsStandard(cfs, key, column("col1", "val1", 1), column("col2", "val2", 1));
+        assertRowAndColCount(1, 2, null, false, cfs.getRangeSlice(null, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // flush.
+        cfs.forceBlockingFlush();
+        
+        // insert, don't flush
+        putColsStandard(cfs, key, column("col3", "val3", 1), column("col4", "val4", 1));
+        assertRowAndColCount(1, 4, null, false, cfs.getRangeSlice(null, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // delete (from sstable and memtable)
+        RowMutation rm = new RowMutation(table.name, key.key);
+        rm.delete(new QueryPath(cfs.columnFamily, null, null), 2);
+        rm.apply();
+        
+        // verify delete
+        assertRowAndColCount(1, 0, null, true, cfs.getRangeSlice(null, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // flush
+        cfs.forceBlockingFlush();
+        
+        // re-verify delete. // first breakage is right here because of CASSANDRA-1837.
+        assertRowAndColCount(1, 0, null, true, cfs.getRangeSlice(null, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // simulate a 'late' insertion that gets put in after the deletion. should get inserted, but fail on read.
+        putColsStandard(cfs, key, column("col5", "val5", 1), column("col2", "val2", 1));
+        
+        // should still be nothing there because we deleted this row. 2nd breakage, but was undetected because of 1837.
+        assertRowAndColCount(1, 0, null, true, cfs.getRangeSlice(null, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // make sure that new writes are recognized.
+        putColsStandard(cfs, key, column("col6", "val6", 3), column("col7", "val7", 3));
+        assertRowAndColCount(1, 2, null, true, cfs.getRangeSlice(null, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+        
+        // and it remains so after flush. (this wasn't failing before, but it's good to check.)
+        cfs.forceBlockingFlush();
+        assertRowAndColCount(1, 2, null, true, cfs.getRangeSlice(null, Util.range("f", "g"), 100, QueryFilter.getFilter(sp, cfs.getComparator())));
+    }
+        
+
+    private ColumnFamilyStore insertKey1Key2() throws IOException, ExecutionException, InterruptedException
+    {
+        List<RowMutation> rms = new LinkedList<RowMutation>();
+        RowMutation rm;
+        rm = new RowMutation("Keyspace2", ByteBufferUtil.bytes("key1"));
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1")), ByteBufferUtil.bytes("asdf"), 0);
+        rms.add(rm);
+        Util.writeColumnFamily(rms);
+
+        rm = new RowMutation("Keyspace2", ByteBufferUtil.bytes("key2"));
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1")), ByteBufferUtil.bytes("asdf"), 0);
+        rms.add(rm);
+        return Util.writeColumnFamily(rms);
+    }
+    
+    @Test
+    public void testBackupAfterFlush() throws Throwable
+    {
+        insertKey1Key2();
+
+        File backupDir = new File(DatabaseDescriptor.getDataFileLocationForTable("Keyspace2", 0), "backups");
+        for (String f : Arrays.asList("Standard1-f-1-Data.db", "Standard1-f-1-Index.db", "Standard1-f-2-Data.db", "Standard1-f-2-Index.db",
+                                      "Standard1-f-1-Filter.db", "Standard1-f-1-Statistics.db", "Standard1-f-2-Filter.db", "Standard1-f-2-Statistics.db"))
+        {
+            assertTrue("can not find backedup file:" + f, new File(backupDir, f).exists());
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ColumnFamilyTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ColumnFamilyTest.java
index e69de29b..983d2dba 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ColumnFamilyTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ColumnFamilyTest.java
@@ -0,0 +1,149 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.TreeMap;
+
+import org.apache.cassandra.SchemaLoader;
+import org.junit.Test;
+
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.db.filter.QueryPath;
+import static org.apache.cassandra.Util.column;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class ColumnFamilyTest extends SchemaLoader
+{
+    // TODO test SuperColumns
+
+    @Test
+    public void testSingleColumn() throws IOException
+    {
+        ColumnFamily cf;
+
+        cf = ColumnFamily.create("Keyspace1", "Standard1");
+        cf.addColumn(column("C", "v", 1));
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        ColumnFamily.serializer().serialize(cf, bufOut);
+
+        ByteArrayInputStream bufIn = new ByteArrayInputStream(bufOut.getData(), 0, bufOut.getLength());
+        cf = ColumnFamily.serializer().deserialize(new DataInputStream(bufIn));
+        assert cf != null;
+        assert cf.metadata().cfName.equals("Standard1");
+        assert cf.getSortedColumns().size() == 1;
+    }
+
+    @Test
+    public void testManyColumns() throws IOException
+    {
+        ColumnFamily cf;
+
+        TreeMap<String, String> map = new TreeMap<String, String>();
+        for (int i = 100; i < 1000; ++i)
+        {
+            map.put(Integer.toString(i), "Avinash Lakshman is a good man: " + i);
+        }
+
+        // write
+        cf = ColumnFamily.create("Keyspace1", "Standard1");
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        for (String cName : map.navigableKeySet())
+        {
+            cf.addColumn(column(cName, map.get(cName), 314));
+        }
+        ColumnFamily.serializer().serialize(cf, bufOut);
+
+        // verify
+        ByteArrayInputStream bufIn = new ByteArrayInputStream(bufOut.getData(), 0, bufOut.getLength());
+        cf = ColumnFamily.serializer().deserialize(new DataInputStream(bufIn));
+        for (String cName : map.navigableKeySet())
+        {
+            ByteBuffer val = cf.getColumn(ByteBufferUtil.bytes(cName)).value();
+            assert new String(val.array(),val.position(),val.remaining()).equals(map.get(cName));
+        }
+        assert cf.getColumnNames().size() == map.size();
+    }
+
+    @Test
+    public void testGetColumnCount()
+    {
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+
+        cf.addColumn(column("col1", "", 1));
+        cf.addColumn(column("col2", "", 2));
+        cf.addColumn(column("col1", "", 3));
+
+        assert 2 == cf.getColumnCount();
+        assert 2 == cf.getSortedColumns().size();
+    }
+
+    @Test
+    public void testTimestamp()
+    {
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+
+        cf.addColumn(column("col1", "val1", 2));
+        cf.addColumn(column("col1", "val2", 2)); // same timestamp, new value
+        cf.addColumn(column("col1", "val3", 1)); // older timestamp -- should be ignored
+
+        assert ByteBufferUtil.bytes("val2").equals(cf.getColumn(ByteBufferUtil.bytes("col1")).value());
+    }
+
+    @Test
+    public void testMergeAndAdd()
+    {
+        ColumnFamily cf_new = ColumnFamily.create("Keyspace1", "Standard1");
+        ColumnFamily cf_old = ColumnFamily.create("Keyspace1", "Standard1");
+        ColumnFamily cf_result = ColumnFamily.create("Keyspace1", "Standard1");
+        ByteBuffer val = ByteBufferUtil.bytes("sample value");
+        ByteBuffer val2 = ByteBufferUtil.bytes("x value ");
+
+        // exercise addColumn(QueryPath, ...)
+        cf_new.addColumn(QueryPath.column(ByteBufferUtil.bytes("col1")), val, 3);
+        cf_new.addColumn(QueryPath.column(ByteBufferUtil.bytes("col2")), val, 4);
+
+        cf_old.addColumn(QueryPath.column(ByteBufferUtil.bytes("col2")), val2, 1);
+        cf_old.addColumn(QueryPath.column(ByteBufferUtil.bytes("col3")), val2, 2);
+
+        cf_result.addAll(cf_new);
+        cf_result.addAll(cf_old);
+
+        assert 3 == cf_result.getColumnCount() : "Count is " + cf_new.getColumnCount();
+        //addcolumns will only add if timestamp >= old timestamp
+        assert val.equals(cf_result.getColumn(ByteBufferUtil.bytes("col2")).value());
+
+        // check that tombstone wins timestamp ties
+        cf_result.addTombstone(ByteBufferUtil.bytes("col1"), 0, 3);
+        assert cf_result.getColumn(ByteBufferUtil.bytes("col1")).isMarkedForDelete();
+        cf_result.addColumn(QueryPath.column(ByteBufferUtil.bytes("col1")), val2, 3);
+        assert cf_result.getColumn(ByteBufferUtil.bytes("col1")).isMarkedForDelete();
+
+        // check that column value wins timestamp ties in absence of tombstone
+        cf_result.addColumn(QueryPath.column(ByteBufferUtil.bytes("col3")), val, 2);
+        assert cf_result.getColumn(ByteBufferUtil.bytes("col3")).value().equals(val2);
+        cf_result.addColumn(QueryPath.column(ByteBufferUtil.bytes("col3")), ByteBufferUtil.bytes("z"), 2);
+        assert cf_result.getColumn(ByteBufferUtil.bytes("col3")).value().equals(ByteBufferUtil.bytes("z"));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CommitLogTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CommitLogTest.java
index 3f24d583..ae4ce165 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CommitLogTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CommitLogTest.java
@@ -1 +1,177 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.db;
+
+import java.io.*;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.commitlog.CommitLog;
+import org.apache.cassandra.db.commitlog.CommitLogHeader;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.utils.Pair;
+
+public class CommitLogTest extends CleanupHelper
+{
+    @Test
+    public void testRecoveryWithEmptyHeader() throws Exception
+    {
+        testRecovery(new byte[0], new byte[10]);
+    }
+
+    @Test
+    public void testRecoveryWithShortHeader() throws Exception
+    {
+        testRecovery(new byte[2], new byte[10]);
+    }
+
+    @Test
+    public void testRecoveryWithGarbageHeader() throws Exception
+    {
+        byte[] garbage = new byte[100];
+        (new java.util.Random()).nextBytes(garbage);
+        testRecovery(garbage, garbage);
+    }
+
+    @Test
+    public void testRecoveryWithEmptyLog() throws Exception
+    {
+        CommitLog.recover(new File[] {tmpFiles().right});
+    }
+
+    @Test
+    public void testRecoveryWithShortLog() throws Exception
+    {
+        // force EOF while reading log
+        testRecoveryWithBadSizeArgument(100, 10);
+    }
+
+    @Test
+    public void testRecoveryWithShortSize() throws Exception
+    {
+        testRecovery(new byte[0], new byte[2]);
+    }
+
+    @Test
+    public void testRecoveryWithShortCheckSum() throws Exception
+    {
+        testRecovery(new byte[0], new byte[6]);
+    }
+
+    @Test
+    public void testRecoveryWithGarbageLog() throws Exception
+    {
+        byte[] garbage = new byte[100];
+        (new java.util.Random()).nextBytes(garbage);
+        testRecovery(new byte[0], garbage);
+    }
+
+    @Test
+    public void testRecoveryWithBadSizeChecksum() throws Exception
+    {
+        Checksum checksum = new CRC32();
+        checksum.update(100);
+        testRecoveryWithBadSizeArgument(100, 100, ~checksum.getValue());
+    }
+    
+    @Test
+    public void testRecoveryWithZeroSegmentSizeArgument() throws Exception
+    {
+        // many different combinations of 4 bytes (garbage) will be read as zero by readInt()
+        testRecoveryWithBadSizeArgument(0, 10); // zero size, but no EOF
+    }
+
+    @Test
+    public void testRecoveryWithNegativeSizeArgument() throws Exception
+    {
+        // garbage from a partial/bad flush could be read as a negative size even if there is no EOF
+        testRecoveryWithBadSizeArgument(-10, 10); // negative size, but no EOF
+    }
+
+    @Test
+    public void testRecoveryWithHeaderPositionGreaterThanLogLength() throws Exception
+    {
+        // Note: this can actually happen (in periodic mode) when data is flushed
+        // before it had time to hit the commitlog (since the header is flushed by the system)
+        // see https://issues.apache.org/jira/browse/CASSANDRA-2285
+        ByteArrayOutputStream out = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(out);
+        Checksum checksum = new CRC32();
+
+        // write the first checksum after the fixed-size part, so we won't read garbage lastFlushedAt data.
+        dos.writeInt(1);
+        checksum.update(1);
+        dos.writeLong(checksum.getValue());
+        dos.writeInt(0);
+        checksum.update(0);
+        dos.writeInt(200);
+        checksum.update(200);
+        dos.writeLong(checksum.getValue());
+        dos.close();
+
+        testRecovery(out.toByteArray(), new byte[0]);
+    }
+
+    protected void testRecoveryWithBadSizeArgument(int size, int dataSize) throws Exception
+    {
+        Checksum checksum = new CRC32();
+        checksum.update(size);
+        testRecoveryWithBadSizeArgument(size, dataSize, checksum.getValue());
+    }
+
+    protected void testRecoveryWithBadSizeArgument(int size, int dataSize, long checksum) throws Exception
+    {
+        ByteArrayOutputStream out = new ByteArrayOutputStream();
+        DataOutputStream dout = new DataOutputStream(out);
+        dout.writeInt(size);
+        dout.writeLong(checksum);
+        dout.write(new byte[dataSize]);
+        dout.close();
+        testRecovery(new byte[0], out.toByteArray());
+    }
+
+    protected Pair<File, File> tmpFiles() throws IOException
+    {
+        File logFile = File.createTempFile("testRecoveryWithPartiallyWrittenHeaderTestFile", null);
+        File headerFile = new File(CommitLogHeader.getHeaderPathFromSegmentPath(logFile.getAbsolutePath()));
+        logFile.deleteOnExit();
+        headerFile.deleteOnExit();
+        assert logFile.length() == 0;
+        assert headerFile.length() == 0;
+        return new Pair<File, File>(headerFile, logFile);
+    }
+
+    protected void testRecovery(byte[] headerData, byte[] logData) throws Exception
+    {
+        Pair<File, File> tmpFiles = tmpFiles();
+        File logFile = tmpFiles.right;
+        File headerFile = tmpFiles.left;
+        OutputStream lout = new FileOutputStream(logFile);
+        OutputStream hout = new FileOutputStream(headerFile);
+        lout.write(logData);
+        hout.write(headerData);
+        //statics make it annoying to test things correctly
+        CommitLog.recover(new File[] {logFile}); //CASSANDRA-1119 / CASSANDRA-1179 throw on failure*/
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java
index 3f24d583..f7b013dc 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java
@@ -1 +1,231 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collection;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.Util;
+
+import static junit.framework.Assert.assertEquals;
+import static org.apache.cassandra.db.TableTest.assertColumns;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class CompactionsPurgeTest extends CleanupHelper
+{
+    public static final String TABLE1 = "Keyspace1";
+    public static final String TABLE2 = "Keyspace2";
+
+    @Test
+    public void testMajorCompactionPurge() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open(TABLE1);
+        String cfName = "Standard1";
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(cfName);
+
+        DecoratedKey key = Util.dk("key1");
+        RowMutation rm;
+
+        // inserts
+        rm = new RowMutation(TABLE1, key.key);
+        for (int i = 0; i < 10; i++)
+        {
+            rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(i))), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        }
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        // deletes
+        for (int i = 0; i < 10; i++)
+        {
+            rm = new RowMutation(TABLE1, key.key);
+            rm.delete(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(i))), 1);
+            rm.apply();
+        }
+        cfs.forceBlockingFlush();
+
+        // resurrect one column
+        rm = new RowMutation(TABLE1, key.key);
+        rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(5))), ByteBufferUtil.EMPTY_BYTE_BUFFER, 2);
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        // major compact and test that all columns but the resurrected one is completely gone
+        CompactionManager.instance.submitMajor(cfs, 0, Integer.MAX_VALUE).get();
+        cfs.invalidateCachedRow(key);
+        ColumnFamily cf = cfs.getColumnFamily(QueryFilter.getIdentityFilter(key, new QueryPath(cfName)));
+        assertColumns(cf, "5");
+        assert cf.getColumn(ByteBufferUtil.bytes(String.valueOf(5))) != null;
+    }
+
+    @Test
+    public void testMinorCompactionPurge() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open(TABLE2);
+        String cfName = "Standard1";
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(cfName);
+
+        RowMutation rm;
+        for (int k = 1; k <= 2; ++k) {
+            DecoratedKey key = Util.dk("key" + k);
+
+            // inserts
+            rm = new RowMutation(TABLE2, key.key);
+            for (int i = 0; i < 10; i++)
+            {
+                rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(i))), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+            }
+            rm.apply();
+            cfs.forceBlockingFlush();
+
+            // deletes
+            for (int i = 0; i < 10; i++)
+            {
+                rm = new RowMutation(TABLE2, key.key);
+                rm.delete(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(i))), 1);
+                rm.apply();
+            }
+            cfs.forceBlockingFlush();
+        }
+
+        DecoratedKey key1 = Util.dk("key1");
+        DecoratedKey key2 = Util.dk("key2");
+
+        // flush, remember the current sstable and then resurrect one column
+        // for first key. Then submit minor compaction on remembered sstables.
+        cfs.forceBlockingFlush();
+        Collection<SSTableReader> sstablesIncomplete = cfs.getSSTables();
+        rm = new RowMutation(TABLE2, key1.key);
+        rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(5))), ByteBufferUtil.EMPTY_BYTE_BUFFER, 2);
+        rm.apply();
+        cfs.forceBlockingFlush();
+        CompactionManager.instance.doCompaction(cfs, sstablesIncomplete, Integer.MAX_VALUE);
+
+        // verify that minor compaction does not GC when key is present
+        // in a non-compacted sstable
+        ColumnFamily cf = cfs.getColumnFamily(QueryFilter.getIdentityFilter(key1, new QueryPath(cfName)));
+        assert cf.getColumnCount() == 10;
+
+        // verify that minor compaction does GC when key is provably not
+        // present in a non-compacted sstable
+        cf = cfs.getColumnFamily(QueryFilter.getIdentityFilter(key2, new QueryPath(cfName)));
+        assert cf == null;
+    }
+
+    @Test
+    public void testCompactionPurgeOneFile() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open(TABLE1);
+        String cfName = "Standard2";
+        ColumnFamilyStore store = table.getColumnFamilyStore(cfName);
+
+        DecoratedKey key = Util.dk("key1");
+        RowMutation rm;
+
+        // inserts
+        rm = new RowMutation(TABLE1, key.key);
+        for (int i = 0; i < 5; i++)
+        {
+            rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(i))), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        }
+        rm.apply();
+
+        // deletes
+        for (int i = 0; i < 5; i++)
+        {
+            rm = new RowMutation(TABLE1, key.key);
+            rm.delete(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(i))), 1);
+            rm.apply();
+        }
+        store.forceBlockingFlush();
+
+        assert store.getSSTables().size() == 1 : store.getSSTables(); // inserts & deletes were in the same memtable -> only deletes in sstable
+
+        // compact and test that the row is completely gone
+        CompactionManager.instance.submitMajor(store, 0, Integer.MAX_VALUE).get();
+        assert store.getSSTables().isEmpty();
+        ColumnFamily cf = table.getColumnFamilyStore(cfName).getColumnFamily(QueryFilter.getIdentityFilter(key, new QueryPath(cfName)));
+        assert cf == null : cf;
+    }
+
+    @Test
+    public void testCompactionPurgeTombstonedRow() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        String tableName = "RowCacheSpace";
+        String cfName = "CachedCF";
+        Table table = Table.open(tableName);
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(cfName);
+
+        DecoratedKey key = Util.dk("key3");
+        RowMutation rm;
+
+        // inserts
+        rm = new RowMutation(tableName, key.key);
+        for (int i = 0; i < 10; i++)
+        {
+            rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(i))), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        }
+        rm.apply();
+
+        // move the key up in row cache
+        cfs.getColumnFamily(QueryFilter.getIdentityFilter(key, new QueryPath(cfName)));
+
+        // deletes row
+        rm = new RowMutation(tableName, key.key);
+        rm.delete(new QueryPath(cfName, null, null), 1);
+        rm.apply();
+
+        // flush and major compact
+        cfs.forceBlockingFlush();
+        CompactionManager.instance.submitMajor(cfs, 0, Integer.MAX_VALUE).get();
+        //cfs.invalidateCachedRow(key);
+
+        // re-inserts with timestamp lower than delete
+        rm = new RowMutation(tableName, key.key);
+        for (int i = 0; i < 10; i++)
+        {
+            rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes(String.valueOf(i))), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        }
+        rm.apply();
+
+        // Check that the second insert did went in
+        ColumnFamily cf = cfs.getColumnFamily(QueryFilter.getIdentityFilter(key, new QueryPath(cfName)));
+        assert cf.getColumnCount() == 10;
+        for (IColumn c : cf)
+            assert !c.isMarkedForDelete();
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CompactionsTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CompactionsTest.java
index e69de29b..06f85367 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CompactionsTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/CompactionsTest.java
@@ -0,0 +1,140 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Set;
+import java.util.HashSet;
+
+import org.apache.cassandra.Util;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.Pair;
+import static junit.framework.Assert.assertEquals;
+
+public class CompactionsTest extends CleanupHelper
+{
+    public static final String TABLE1 = "Keyspace1";
+    public static final String TABLE2 = "Keyspace2";
+    public static final InetAddress LOCAL = FBUtilities.getLocalAddress();
+
+    @Test
+    public void testCompactions() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        // this test does enough rows to force multiple block indexes to be used
+        Table table = Table.open(TABLE1);
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard1");
+
+        final int ROWS_PER_SSTABLE = 10;
+        Set<DecoratedKey> inserted = new HashSet<DecoratedKey>();
+        for (int j = 0; j < (DatabaseDescriptor.getIndexInterval() * 3) / ROWS_PER_SSTABLE; j++) {
+            for (int i = 0; i < ROWS_PER_SSTABLE; i++) {
+                DecoratedKey key = Util.dk(String.valueOf(i % 2));
+                RowMutation rm = new RowMutation(TABLE1, key.key);
+                rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes(String.valueOf(i / 2))), ByteBufferUtil.EMPTY_BYTE_BUFFER, j * ROWS_PER_SSTABLE + i);
+                rm.apply();
+                inserted.add(key);
+            }
+            store.forceBlockingFlush();
+            assertEquals(inserted.toString(), inserted.size(), Util.getRangeSlice(store).size());
+        }
+        while (true)
+        {
+            Future<Integer> ft = CompactionManager.instance.submitMinorIfNeeded(store);
+            if (ft.get() == 0)
+                break;
+        }
+        if (store.getSSTables().size() > 1)
+        {
+            CompactionManager.instance.performMajor(store);
+        }
+        assertEquals(inserted.size(), Util.getRangeSlice(store).size());
+    }
+
+    @Test
+    public void testGetBuckets()
+    {
+        List<Pair<String, Long>> pairs = new ArrayList<Pair<String, Long>>();
+        String[] strings = { "a", "bbbb", "cccccccc", "cccccccc", "bbbb", "a" };
+        for (String st : strings)
+        {
+            Pair<String, Long> pair = new Pair<String, Long>(st, new Long(st.length()));
+            pairs.add(pair);
+        }
+
+        Set<List<String>> buckets = CompactionManager.getBuckets(pairs, 2);
+        assertEquals(3, buckets.size());
+
+        for (List<String> bucket : buckets)
+        {
+            assertEquals(2, bucket.size());
+            assertEquals(bucket.get(0).length(), bucket.get(1).length());
+            assertEquals(bucket.get(0).charAt(0), bucket.get(1).charAt(0));
+        }
+
+        pairs.clear();
+        buckets.clear();
+
+        String[] strings2 = { "aaa", "bbbbbbbb", "aaa", "bbbbbbbb", "bbbbbbbb", "aaa" };
+        for (String st : strings2)
+        {
+            Pair<String, Long> pair = new Pair<String, Long>(st, new Long(st.length()));
+            pairs.add(pair);
+        }
+
+        buckets = CompactionManager.getBuckets(pairs, 2);
+        assertEquals(2, buckets.size());
+
+        for (List<String> bucket : buckets)
+        {
+            assertEquals(3, bucket.size());
+            assertEquals(bucket.get(0).charAt(0), bucket.get(1).charAt(0));
+            assertEquals(bucket.get(1).charAt(0), bucket.get(2).charAt(0));
+        }
+
+        // Test the "min" functionality
+        pairs.clear();
+        buckets.clear();
+
+        String[] strings3 = { "aaa", "bbbbbbbb", "aaa", "bbbbbbbb", "bbbbbbbb", "aaa" };
+        for (String st : strings3)
+        {
+            Pair<String, Long> pair = new Pair<String, Long>(st, new Long(st.length()));
+            pairs.add(pair);
+        }
+
+        buckets = CompactionManager.getBuckets(pairs, 10); // notice the min is 10
+        assertEquals(1, buckets.size());
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/KeyCacheTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/KeyCacheTest.java
index 3f24d583..ab9aa97b 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/KeyCacheTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/KeyCacheTest.java
@@ -1 +1,143 @@
   + native
+package org.apache.cassandra.db;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.io.sstable.Descriptor;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.Pair;
+
+public class KeyCacheTest extends CleanupHelper
+{
+    private static final String TABLE1 = "KeyCacheSpace";
+    private static final String COLUMN_FAMILY1 = "Standard1";
+    private static final String COLUMN_FAMILY2 = "Standard2";
+
+    @Test
+    public void testKeyCache50() throws IOException, ExecutionException, InterruptedException
+    {
+        testKeyCache(COLUMN_FAMILY1, 64);
+    }
+
+    @Test
+    public void testKeyCache100() throws IOException, ExecutionException, InterruptedException
+    {
+        testKeyCache(COLUMN_FAMILY2, 128);
+    }
+
+    @Test
+    public void testKeyCacheLoad() throws Exception
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        ColumnFamilyStore store = Table.open(TABLE1).getColumnFamilyStore(COLUMN_FAMILY2);
+
+        // empty the cache
+        store.invalidateKeyCache();
+        assert store.getKeyCacheSize() == 0;
+
+        // insert data and force to disk
+        insertData(TABLE1, COLUMN_FAMILY2, 0, 100);
+        store.forceBlockingFlush();
+
+        // populate the cache
+        readData(TABLE1, COLUMN_FAMILY2, 0, 100);
+        assert store.getKeyCacheSize() == 100;
+
+        // really? our caches don't implement the map interface? (hence no .addAll)
+        Map<Pair<Descriptor, DecoratedKey>, Long> savedMap = new HashMap<Pair<Descriptor, DecoratedKey>, Long>();
+        for (Map.Entry<Pair<Descriptor, DecoratedKey>, Long> entry : store.getKeyCache().getEntrySet())
+        {
+            savedMap.put(entry.getKey(), entry.getValue());
+        }
+
+        // force the cache to disk
+        store.submitKeyCacheWrite().get();
+
+        // empty the cache again to make sure values came from disk
+        store.invalidateKeyCache();
+        assert store.getKeyCacheSize() == 0;
+
+        // load the cache from disk
+        store.unregisterMBean(); // unregistering old MBean to test how key cache will be loaded
+        ColumnFamilyStore newStore = ColumnFamilyStore.createColumnFamilyStore(Table.open(TABLE1), COLUMN_FAMILY2);
+        assert newStore.getKeyCacheSize() == 100;
+
+        assert savedMap.size() == 100;
+        for (Map.Entry<Pair<Descriptor, DecoratedKey>, Long> entry : savedMap.entrySet())
+        {
+            assert newStore.getKeyCache().get(entry.getKey()).equals(entry.getValue());
+        }
+    }
+
+    public void testKeyCache(String cfName, int expectedCacheSize) throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open(TABLE1);
+        ColumnFamilyStore store = table.getColumnFamilyStore(cfName);
+
+        // KeyCache should start at size 1 if we're caching X% of zero data.
+        int keyCacheSize = store.getKeyCacheCapacity();
+        assert keyCacheSize == 1 : keyCacheSize;
+
+        DecoratedKey key1 = Util.dk("key1");
+        DecoratedKey key2 = Util.dk("key2");
+        RowMutation rm;
+
+        // inserts
+        rm = new RowMutation(TABLE1, key1.key);
+        rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes("1")), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        rm.apply();
+        rm = new RowMutation(TABLE1, key2.key);
+        rm.add(new QueryPath(cfName, null, ByteBufferUtil.bytes("2")), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        rm.apply();
+
+        // deletes
+        rm = new RowMutation(TABLE1, key1.key);
+        rm.delete(new QueryPath(cfName, null, ByteBufferUtil.bytes("1")), 1);
+        rm.apply();
+        rm = new RowMutation(TABLE1, key2.key);
+        rm.delete(new QueryPath(cfName, null, ByteBufferUtil.bytes("2")), 1);
+        rm.apply();
+
+        // After a flush, the cache should expand to be X% of indices * INDEX_INTERVAL.
+        store.forceBlockingFlush();
+        keyCacheSize = store.getKeyCacheCapacity();
+        assert keyCacheSize == expectedCacheSize : keyCacheSize;
+
+        // After a compaction, the cache should expand to be X% of zero data.
+        CompactionManager.instance.submitMajor(store, 0, Integer.MAX_VALUE).get();
+        keyCacheSize = store.getKeyCacheCapacity();
+        assert keyCacheSize == 1 : keyCacheSize;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/MultitableTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/MultitableTest.java
index e69de29b..40bba6e9 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/MultitableTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/MultitableTest.java
@@ -0,0 +1,65 @@
+package org.apache.cassandra.db;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.io.IOException;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.cassandra.Util;
+import org.junit.Test;
+
+import static org.apache.cassandra.db.TableTest.assertColumns;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import static org.apache.cassandra.Util.column;
+
+public class MultitableTest extends CleanupHelper
+{
+    @Test
+    public void testSameCFs() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table1 = Table.open("Keyspace1");
+        Table table2 = Table.open("Keyspace2");
+
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("keymulti");
+        ColumnFamily cf;
+
+        rm = new RowMutation("Keyspace1", dk.key);
+        cf = ColumnFamily.create("Keyspace1", "Standard1");
+        cf.addColumn(column("col1", "val1", 1L));
+        rm.add(cf);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace2", dk.key);
+        cf = ColumnFamily.create("Keyspace2", "Standard1");
+        cf.addColumn(column("col2", "val2", 1L));
+        rm.add(cf);
+        rm.apply();
+
+        table1.getColumnFamilyStore("Standard1").forceBlockingFlush();
+        table2.getColumnFamilyStore("Standard1").forceBlockingFlush();
+
+        assertColumns(Util.getColumnFamily(table1, dk, "Standard1"), "col1");
+        assertColumns(Util.getColumnFamily(table2, dk, "Standard1"), "col2");
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/NameSortTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/NameSortTest.java
index 3f24d583..f82ff155 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/NameSortTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/NameSortTest.java
@@ -1 +1,136 @@
   + native
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.cassandra.db;
+
+import static junit.framework.Assert.assertEquals;
+import static org.apache.cassandra.Util.addMutation;
+import static org.apache.cassandra.Util.column;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collection;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.commitlog.CommitLog;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+import org.junit.Test;
+
+public class NameSortTest extends CleanupHelper
+{
+    @Test
+    public void testNameSort1() throws IOException, ExecutionException, InterruptedException
+    {
+        // single key
+        testNameSort(1);
+    }
+
+    @Test
+    public void testNameSort10() throws IOException, ExecutionException, InterruptedException
+    {
+        // multiple keys, flushing concurrently w/ inserts
+        testNameSort(10);
+    }
+
+    @Test
+    public void testNameSort100() throws IOException, ExecutionException, InterruptedException
+    {
+        // enough keys to force compaction concurrently w/ inserts
+        testNameSort(100);
+    }
+
+    private void testNameSort(int N) throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+
+        for (int i = 0; i < N; ++i)
+        {
+            ByteBuffer key = ByteBufferUtil.bytes(Integer.toString(i));
+            RowMutation rm;
+
+            // standard
+            for (int j = 0; j < 8; ++j)
+            {
+                ByteBuffer bytes = j % 2 == 0 ? ByteBufferUtil.bytes("a") : ByteBufferUtil.bytes("b");
+                rm = new RowMutation("Keyspace1", key);
+                rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes(("Column-" + j))), bytes, j);
+                rm.applyUnsafe();
+            }
+
+            // super
+            for (int j = 0; j < 8; ++j)
+            {
+                rm = new RowMutation("Keyspace1", key);
+                for (int k = 0; k < 4; ++k)
+                {
+                    String value = (j + k) % 2 == 0 ? "a" : "b";
+                    addMutation(rm, "Super1", "SuperColumn-" + j, k, value, k);
+                }
+                rm.applyUnsafe();
+            }
+        }
+
+        validateNameSort(table, N);
+
+        table.getColumnFamilyStore("Standard1").forceBlockingFlush();
+        table.getColumnFamilyStore("Super1").forceBlockingFlush();
+        validateNameSort(table, N);
+    }
+
+    private void validateNameSort(Table table, int N) throws IOException
+    {
+        for (int i = 0; i < N; ++i)
+        {
+            DecoratedKey key = Util.dk(Integer.toString(i));
+            ColumnFamily cf;
+
+            cf = Util.getColumnFamily(table, key, "Standard1");
+            Collection<IColumn> columns = cf.getSortedColumns();
+            for (IColumn column : columns)
+            {
+                String name = ByteBufferUtil.string(column.name());
+                int j = Integer.valueOf(name.substring(name.length() - 1));
+                byte[] bytes = j % 2 == 0 ? "a".getBytes() : "b".getBytes();
+                assertEquals(new String(bytes), ByteBufferUtil.string(column.value()));
+            }
+
+            cf = Util.getColumnFamily(table, key, "Super1");
+            assert cf != null : "key " + key + " is missing!";
+            Collection<IColumn> superColumns = cf.getSortedColumns();
+            assert superColumns.size() == 8 : cf;
+            for (IColumn superColumn : superColumns)
+            {
+                int j = Integer.valueOf(ByteBufferUtil.string(superColumn.name()).split("-")[1]);
+                Collection<IColumn> subColumns = superColumn.getSubColumns();
+                assert subColumns.size() == 4;
+                for (IColumn subColumn : subColumns)
+                {
+                    long k = subColumn.name().getLong(subColumn.name().position());
+                    byte[] bytes = (j + k) % 2 == 0 ? "a".getBytes() : "b".getBytes();
+                    assertEquals(new String(bytes), ByteBufferUtil.string(subColumn.value()));
+                }
+            }
+        }
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/OneCompactionTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/OneCompactionTest.java
index 3f24d583..4c9e96ed 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/OneCompactionTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/OneCompactionTest.java
@@ -1 +1,71 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.concurrent.ExecutionException;
+import java.util.Set;
+import java.util.HashSet;
+
+import org.apache.cassandra.Util;
+
+import org.junit.Test;
+
+import static junit.framework.Assert.assertEquals;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class OneCompactionTest extends CleanupHelper
+{
+    private void testCompaction(String columnFamilyName, int insertsPerTable) throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore(columnFamilyName);
+
+        Set<DecoratedKey> inserted = new HashSet<DecoratedKey>();
+        for (int j = 0; j < insertsPerTable; j++) {
+            DecoratedKey key = Util.dk(String.valueOf(j));
+            RowMutation rm = new RowMutation("Keyspace1", key.key);
+            rm.add(new QueryPath(columnFamilyName, null, ByteBufferUtil.bytes("0")), ByteBufferUtil.EMPTY_BYTE_BUFFER, j);
+            rm.apply();
+            inserted.add(key);
+            store.forceBlockingFlush();
+            assertEquals(inserted.size(), Util.getRangeSlice(store).size());
+        }
+        CompactionManager.instance.performMajor(store);
+        assertEquals(1, store.getSSTables().size());
+    }
+
+    @Test
+    public void testCompaction1() throws IOException, ExecutionException, InterruptedException
+    {
+        testCompaction("Standard1", 1);
+    }
+
+    @Test
+    public void testCompaction2() throws IOException, ExecutionException, InterruptedException
+    {
+        testCompaction("Standard2", 2);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ReadMessageTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ReadMessageTest.java
index 3f24d583..9f6388d5 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ReadMessageTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ReadMessageTest.java
@@ -1 +1,101 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import static org.junit.Assert.assertEquals;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Arrays;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+
+import org.junit.Test;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class ReadMessageTest extends SchemaLoader
+{
+    @Test
+    public void testMakeReadMessage() throws IOException
+    {
+        ArrayList<ByteBuffer> colList = new ArrayList<ByteBuffer>();
+        colList.add(ByteBufferUtil.bytes("col1"));
+        colList.add(ByteBufferUtil.bytes("col2"));
+        
+        ReadCommand rm, rm2;
+        DecoratedKey dk = Util.dk("row1");
+        
+        rm = new SliceByNamesReadCommand("Keyspace1", dk.key, new QueryPath("Standard1"), colList);
+        rm2 = serializeAndDeserializeReadMessage(rm);
+        assert rm2.toString().equals(rm.toString());
+
+        rm = new SliceFromReadCommand("Keyspace1", dk.key, new QueryPath("Standard1"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 2);
+        rm2 = serializeAndDeserializeReadMessage(rm);
+        assert rm2.toString().equals(rm.toString());
+        
+        rm = new SliceFromReadCommand("Keyspace1", dk.key, new QueryPath("Standard1"), ByteBufferUtil.bytes("a"), ByteBufferUtil.bytes("z"), true, 5);
+        rm2 = serializeAndDeserializeReadMessage(rm);
+        assertEquals(rm2.toString(), rm.toString());
+
+        rm = new SliceFromReadCommand("Keyspace1", dk.key, new QueryPath("Standard1"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 2);
+        rm2 = serializeAndDeserializeReadMessage(rm);
+        assert rm2.toString().equals(rm.toString());
+
+        rm = new SliceFromReadCommand("Keyspace1", dk.key, new QueryPath("Standard1"), ByteBufferUtil.bytes("a"), ByteBufferUtil.bytes("z"), true, 5);
+        rm2 = serializeAndDeserializeReadMessage(rm);
+        assertEquals(rm2.toString(), rm.toString());
+    }
+
+    private ReadCommand serializeAndDeserializeReadMessage(ReadCommand rm) throws IOException
+    {
+        ReadCommandSerializer rms = ReadCommand.serializer();
+        DataOutputBuffer dos = new DataOutputBuffer();
+        ByteArrayInputStream bis;
+
+        rms.serialize(rm, dos);
+        bis = new ByteArrayInputStream(dos.getData(), 0, dos.getLength());
+        return rms.deserialize(new DataInputStream(bis));
+    }
+    
+    @Test
+    public void testGetColumn() throws IOException, ColumnFamilyNotDefinedException
+    {
+        Table table = Table.open("Keyspace1");
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("key1");
+
+        // add data
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1")), ByteBufferUtil.bytes("abcd"), 0);
+        rm.apply();
+
+        ReadCommand command = new SliceByNamesReadCommand("Keyspace1", dk.key, new QueryPath("Standard1"), Arrays.asList(ByteBufferUtil.bytes("Column1")));
+        Row row = command.getRow(table);
+        IColumn col = row.cf.getColumn(ByteBufferUtil.bytes("Column1"));
+        assert Arrays.equals(col.value().array(), "abcd".getBytes());  
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManager2Test.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManager2Test.java
index e69de29b..55632299 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManager2Test.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManager2Test.java
@@ -0,0 +1,83 @@
+package org.apache.cassandra.db;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.junit.Test;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import static org.apache.cassandra.Util.column;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.commitlog.CommitLog;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class RecoveryManager2Test extends CleanupHelper
+{
+    private static Logger logger = LoggerFactory.getLogger(RecoveryManager2Test.class);
+
+    @Test
+    /* test that commit logs do not replay flushed data */
+    public void testWithFlush() throws Exception
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        // add a row to another CF so we test skipping mutations within a not-entirely-flushed CF
+        insertRow("Standard2", "key");
+
+        for (int i = 0; i < 100; i++)
+        {
+            String key = "key" + i;
+            insertRow("Standard1", key);
+        }
+
+        Table table1 = Table.open("Keyspace1");
+        ColumnFamilyStore cfs = table1.getColumnFamilyStore("Standard1");
+        logger.debug("forcing flush");
+        cfs.forceBlockingFlush();
+
+        // remove Standard1 SSTable/MemTables
+        cfs.clearUnsafe();
+
+        logger.debug("begin manual replay");
+        // replay the commit log (nothing should be replayed since everything was flushed)
+        CommitLog.instance.resetUnsafe();
+        CommitLog.recover();
+
+        // since everything that was flushed was removed (i.e. clearUnsafe)
+        // and the commit shouldn't have replayed anything, there should be no data
+        assert Util.getRangeSlice(cfs).isEmpty();
+    }
+
+    private void insertRow(String cfname, String key) throws IOException
+    {
+        RowMutation rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes(key));
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", cfname);
+        cf.addColumn(column("col1", "val1", 1L));
+        rm.add(cf);
+        rm.apply();
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManager3Test.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManager3Test.java
index 3f24d583..390e4f87 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManager3Test.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManager3Test.java
@@ -1 +1,80 @@
   + native
+package org.apache.cassandra.db;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.io.File;
+import java.io.IOException;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.commitlog.CommitLog;
+import org.apache.cassandra.io.util.FileUtils;
+
+import static org.apache.cassandra.Util.column;
+import static org.apache.cassandra.db.TableTest.assertColumns;
+
+public class RecoveryManager3Test extends CleanupHelper
+{
+    @Test
+    public void testMissingHeader() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table1 = Table.open("Keyspace1");
+        Table table2 = Table.open("Keyspace2");
+
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("keymulti");
+        ColumnFamily cf;
+
+        rm = new RowMutation("Keyspace1", dk.key);
+        cf = ColumnFamily.create("Keyspace1", "Standard1");
+        cf.addColumn(column("col1", "val1", 1L));
+        rm.add(cf);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace2", dk.key);
+        cf = ColumnFamily.create("Keyspace2", "Standard3");
+        cf.addColumn(column("col2", "val2", 1L));
+        rm.add(cf);
+        rm.apply();
+
+        table1.getColumnFamilyStore("Standard1").clearUnsafe();
+        table2.getColumnFamilyStore("Standard3").clearUnsafe();
+
+        // nuke the header
+        for (File file : new File(DatabaseDescriptor.getCommitLogLocation()).listFiles())
+        {
+            if (file.getName().endsWith(".header"))
+                FileUtils.deleteWithConfirm(file);
+        }
+
+        CommitLog.instance.resetUnsafe(); // disassociate segments from live CL
+        CommitLog.recover();
+
+        assertColumns(Util.getColumnFamily(table1, dk, "Standard1"), "col1");
+        assertColumns(Util.getColumnFamily(table2, dk, "Standard3"), "col2");
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManagerTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManagerTest.java
index 3f24d583..41719e24 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManagerTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManagerTest.java
@@ -1 +1,73 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.cassandra.Util;
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.commitlog.CommitLog;
+
+import org.apache.cassandra.Util;
+import static org.apache.cassandra.Util.column;
+import static org.apache.cassandra.db.TableTest.assertColumns;
+
+public class RecoveryManagerTest extends CleanupHelper
+{
+    @Test
+    public void testNothingToRecover() throws IOException {
+        CommitLog.recover();
+    }
+
+    @Test
+    public void testOne() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table1 = Table.open("Keyspace1");
+        Table table2 = Table.open("Keyspace2");
+
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("keymulti");
+        ColumnFamily cf;
+
+        rm = new RowMutation("Keyspace1", dk.key);
+        cf = ColumnFamily.create("Keyspace1", "Standard1");
+        cf.addColumn(column("col1", "val1", 1L));
+        rm.add(cf);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace2", dk.key);
+        cf = ColumnFamily.create("Keyspace2", "Standard3");
+        cf.addColumn(column("col2", "val2", 1L));
+        rm.add(cf);
+        rm.apply();
+
+        table1.getColumnFamilyStore("Standard1").clearUnsafe();
+        table2.getColumnFamilyStore("Standard3").clearUnsafe();
+
+        CommitLog.instance.resetUnsafe(); // disassociate segments from live CL
+        CommitLog.recover();
+
+        assertColumns(Util.getColumnFamily(table1, dk, "Standard1"), "col1");
+        assertColumns(Util.getColumnFamily(table2, dk, "Standard3"), "col2");
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
index 3f24d583..0d6d5a4c 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
@@ -1 +1,100 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import static org.apache.cassandra.Util.column;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertNull;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.commitlog.CommitLog;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.junit.Test;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+/**
+ * Test for the truncate operation.
+ *
+ * @author Ran Tavory (rantav@gmail.com)
+ *
+ */
+public class RecoveryManagerTruncateTest extends CleanupHelper
+{
+	@Test
+	public void testTruncate() throws IOException, ExecutionException, InterruptedException
+	{
+		Table table = Table.open("Keyspace1");
+		ColumnFamilyStore cfs = table.getColumnFamilyStore("Standard1");
+
+		RowMutation rm;
+		ColumnFamily cf;
+
+		// trucate clears memtable
+		rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("keymulti"));
+		cf = ColumnFamily.create("Keyspace1", "Standard1");
+		cf.addColumn(column("col1", "val1", 1L));
+		rm.add(cf);
+		rm.apply();
+
+		// Make sure data was written
+		assertNotNull(getFromTable(table, "Standard1", "keymulti", "col1"));
+
+		// and now truncate it
+		cfs.truncate().get();
+		CommitLog.recover();
+
+		// and validate truncation.
+		assertNull(getFromTable(table, "Standard1", "keymulti", "col1"));
+
+		// truncate clears sstable
+		rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("keymulti"));
+		cf = ColumnFamily.create("Keyspace1", "Standard1");
+		cf.addColumn(column("col1", "val1", 1L));
+		rm.add(cf);
+		rm.apply();
+		cfs.forceBlockingFlush();
+		cfs.truncate().get();
+		CommitLog.recover();
+		assertNull(getFromTable(table, "Standard1", "keymulti", "col1"));
+	}
+
+	private IColumn getFromTable(Table table, String cfName, String keyName, String columnName)
+	{
+		ColumnFamily cf;
+		ColumnFamilyStore cfStore = table.getColumnFamilyStore(cfName);
+		if (cfStore == null)
+		{
+			return null;
+		}
+		cf = cfStore.getColumnFamily(QueryFilter.getNamesFilter(
+		        Util.dk(keyName), new QueryPath(cfName), ByteBufferUtil.bytes(columnName)));
+		if (cf == null)
+		{
+			return null;
+		}
+		return cf.getColumn(ByteBufferUtil.bytes(columnName));
+	}
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyTest.java
index e69de29b..3c27d12e 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyTest.java
@@ -0,0 +1,61 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import static junit.framework.Assert.assertNull;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class RemoveColumnFamilyTest extends CleanupHelper
+{
+    @Test
+    public void testRemoveColumnFamily() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard1");
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("key1");
+
+        // add data
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1")), ByteBufferUtil.bytes("asdf"), 0);
+        rm.apply();
+
+        // remove
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.delete(new QueryPath("Standard1"), 1);
+        rm.apply();
+
+        ColumnFamily retrieved = store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1"))));
+        assert retrieved.isMarkedForDelete();
+        assertNull(retrieved.getColumn(ByteBufferUtil.bytes("Column1")));
+        assertNull(Util.cloneAndRemoveDeleted(retrieved, Integer.MAX_VALUE));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyWithFlush1Test.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyWithFlush1Test.java
index 3f24d583..3e02103c 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyWithFlush1Test.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyWithFlush1Test.java
@@ -1 +1,64 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import static junit.framework.Assert.assertNull;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class RemoveColumnFamilyWithFlush1Test extends CleanupHelper
+{
+    @Test
+    public void testRemoveColumnFamilyWithFlush1() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard1");
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("key1");
+
+        // add data
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1")), ByteBufferUtil.bytes("asdf"), 0);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column2")), ByteBufferUtil.bytes("asdf"), 0);
+        rm.apply();
+        store.forceBlockingFlush();
+
+        // remove
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.delete(new QueryPath("Standard1"), 1);
+        rm.apply();
+
+        ColumnFamily retrieved = store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Standard1")));
+        assert retrieved.isMarkedForDelete();
+        assertNull(retrieved.getColumn(ByteBufferUtil.bytes("Column1")));
+        assertNull(Util.cloneAndRemoveDeleted(retrieved, Integer.MAX_VALUE));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyWithFlush2Test.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyWithFlush2Test.java
index e69de29b..3bcd2423 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyWithFlush2Test.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnFamilyWithFlush2Test.java
@@ -0,0 +1,61 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import static junit.framework.Assert.assertNull;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class RemoveColumnFamilyWithFlush2Test extends CleanupHelper
+{
+    @Test
+    public void testRemoveColumnFamilyWithFlush2() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard1");
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("key1");
+
+        // add data
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1")), ByteBufferUtil.bytes("asdf"), 0);
+        rm.apply();
+        // remove
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.delete(new QueryPath("Standard1"), 1);
+        rm.apply();
+        store.forceBlockingFlush();
+
+        ColumnFamily retrieved = store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1"))));
+        assert retrieved.isMarkedForDelete();
+        assertNull(retrieved.getColumn(ByteBufferUtil.bytes("Column1")));
+        assertNull(Util.cloneAndRemoveDeleted(retrieved, Integer.MAX_VALUE));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnTest.java
index e69de29b..d65a9996 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveColumnTest.java
@@ -0,0 +1,63 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import static junit.framework.Assert.assertNull;
+
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class RemoveColumnTest extends CleanupHelper
+{
+    @Test
+    public void testRemoveColumn() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard1");
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("key1");
+
+        // add data
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1")), ByteBufferUtil.bytes("asdf"), 0);
+        rm.apply();
+        store.forceBlockingFlush();
+
+        // remove
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.delete(new QueryPath("Standard1", null, ByteBufferUtil.bytes("Column1")), 1);
+        rm.apply();
+
+        ColumnFamily retrieved = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath("Standard1"), ByteBufferUtil.bytes("Column1")));
+        assert retrieved.getColumn(ByteBufferUtil.bytes("Column1")).isMarkedForDelete();
+        assertNull(Util.cloneAndRemoveDeleted(retrieved, Integer.MAX_VALUE));
+        assertNull(Util.cloneAndRemoveDeleted(store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Standard1"))), Integer.MAX_VALUE));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveSubColumnTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveSubColumnTest.java
index 3f24d583..5c452e1e 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveSubColumnTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveSubColumnTest.java
@@ -1 +1,62 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import static junit.framework.Assert.assertNull;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import static org.apache.cassandra.Util.getBytes;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class RemoveSubColumnTest extends CleanupHelper
+{
+    @Test
+    public void testRemoveSubColumn() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore("Super1");
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("key1");
+
+        // add data
+        rm = new RowMutation("Keyspace1", dk.key);
+        Util.addMutation(rm, "Super1", "SC1", 1, "asdf", 0);
+        rm.apply();
+        store.forceBlockingFlush();
+
+        // remove
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.delete(new QueryPath("Super1", ByteBufferUtil.bytes("SC1"), getBytes(1)), 1);
+        rm.apply();
+
+        ColumnFamily retrieved = store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Super1", ByteBufferUtil.bytes("SC1"))));
+        assert retrieved.getColumn(ByteBufferUtil.bytes("SC1")).getSubColumn(getBytes(1)).isMarkedForDelete();
+        assertNull(Util.cloneAndRemoveDeleted(retrieved, Integer.MAX_VALUE));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveSuperColumnTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveSuperColumnTest.java
index 3f24d583..c7ce896c 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveSuperColumnTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RemoveSuperColumnTest.java
@@ -1 +1,201 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+import java.util.Collection;
+
+import org.junit.Test;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertEquals;
+
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import static org.apache.cassandra.Util.addMutation;
+import static org.apache.cassandra.Util.getBytes;
+
+import org.apache.cassandra.CleanupHelper;
+import static junit.framework.Assert.assertNotNull;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class RemoveSuperColumnTest extends CleanupHelper
+{
+    @Test
+    public void testRemoveSuperColumn() throws IOException, ExecutionException, InterruptedException
+    {
+        ColumnFamilyStore store = Table.open("Keyspace1").getColumnFamilyStore("Super1");
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("key1");
+
+        // add data
+        rm = new RowMutation("Keyspace1", dk.key);
+        addMutation(rm, "Super1", "SC1", 1, "val1", 0);
+        rm.apply();
+        store.forceBlockingFlush();
+
+        // remove
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.delete(new QueryPath("Super1", ByteBufferUtil.bytes("SC1")), 1);
+        rm.apply();
+
+        validateRemoveTwoSources(dk);
+
+        store.forceBlockingFlush();
+        validateRemoveTwoSources(dk);
+
+        CompactionManager.instance.performMajor(store);
+        assertEquals(1, store.getSSTables().size());
+        validateRemoveCompacted(dk);
+    }
+
+    @Test
+    public void testRemoveDeletedSubColumn() throws IOException, ExecutionException, InterruptedException
+    {
+        ColumnFamilyStore store = Table.open("Keyspace1").getColumnFamilyStore("Super3");
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("key1");
+
+        // add data
+        rm = new RowMutation("Keyspace1", dk.key);
+        addMutation(rm, "Super3", "SC1", 1, "val1", 0);
+        addMutation(rm, "Super3", "SC1", 2, "val1", 0);
+        rm.apply();
+        store.forceBlockingFlush();
+
+        // remove
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.delete(new QueryPath("Super3", ByteBufferUtil.bytes("SC1"), Util.getBytes(1)), 1);
+        rm.apply();
+
+        validateRemoveSubColumn(dk);
+
+        store.forceBlockingFlush();
+        validateRemoveSubColumn(dk);
+    }
+
+    private void validateRemoveSubColumn(DecoratedKey dk) throws IOException
+    {
+        ColumnFamilyStore store = Table.open("Keyspace1").getColumnFamilyStore("Super3");
+        ColumnFamily cf = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath("Super3", ByteBufferUtil.bytes("SC1")), Util.getBytes(1)));
+        assertNull(Util.cloneAndRemoveDeleted(cf, Integer.MAX_VALUE));
+        cf = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath("Super3", ByteBufferUtil.bytes("SC1")), Util.getBytes(2)));
+        assertNotNull(Util.cloneAndRemoveDeleted(cf, Integer.MAX_VALUE));
+    }
+
+    private void validateRemoveTwoSources(DecoratedKey dk) throws IOException
+    {
+        ColumnFamilyStore store = Table.open("Keyspace1").getColumnFamilyStore("Super1");
+        ColumnFamily cf = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath("Super1"), ByteBufferUtil.bytes("SC1")));
+        assert cf.getSortedColumns().iterator().next().getMarkedForDeleteAt() == 1 : cf;
+        assert cf.getSortedColumns().iterator().next().getSubColumns().size() == 0 : cf;
+        assertNull(Util.cloneAndRemoveDeleted(cf, Integer.MAX_VALUE));
+        cf = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath("Super1"), ByteBufferUtil.bytes("SC1")));
+        assertNull(Util.cloneAndRemoveDeleted(cf, Integer.MAX_VALUE));
+        cf = store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Super1")));
+        assertNull(Util.cloneAndRemoveDeleted(cf, Integer.MAX_VALUE));
+        assertNull(Util.cloneAndRemoveDeleted(store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Super1"))), Integer.MAX_VALUE));
+    }
+
+    private void validateRemoveCompacted(DecoratedKey dk) throws IOException
+    {
+        ColumnFamilyStore store = Table.open("Keyspace1").getColumnFamilyStore("Super1");
+        ColumnFamily resolved = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath("Super1"), ByteBufferUtil.bytes("SC1")));
+        assert resolved.getSortedColumns().iterator().next().getMarkedForDeleteAt() == 1;
+        Collection<IColumn> subColumns = resolved.getSortedColumns().iterator().next().getSubColumns();
+        assert subColumns.size() == 0;
+    }
+
+    @Test
+    public void testRemoveSuperColumnWithNewData() throws IOException, ExecutionException, InterruptedException
+    {
+        ColumnFamilyStore store = Table.open("Keyspace1").getColumnFamilyStore("Super2");
+        RowMutation rm;
+        DecoratedKey dk = Util.dk("key1");
+
+        // add data
+        rm = new RowMutation("Keyspace1", dk.key);
+        addMutation(rm, "Super2", "SC1", 1, "val1", 0);
+        rm.apply();
+        store.forceBlockingFlush();
+
+        // remove
+        rm = new RowMutation("Keyspace1", dk.key);
+        rm.delete(new QueryPath("Super2", ByteBufferUtil.bytes("SC1")), 1);
+        rm.apply();
+
+        // new data
+        rm = new RowMutation("Keyspace1", dk.key);
+        addMutation(rm, "Super2", "SC1", 2, "val2", 2);
+        rm.apply();
+
+        validateRemoveWithNewData(dk);
+
+        store.forceBlockingFlush();
+        validateRemoveWithNewData(dk);
+
+        CompactionManager.instance.performMajor(store);
+        assertEquals(1, store.getSSTables().size());
+        validateRemoveWithNewData(dk);
+    }
+
+    private void validateRemoveWithNewData(DecoratedKey dk) throws IOException
+    {
+        ColumnFamilyStore store = Table.open("Keyspace1").getColumnFamilyStore("Super2");
+        ColumnFamily cf = store.getColumnFamily(QueryFilter.getNamesFilter(dk, new QueryPath("Super2", ByteBufferUtil.bytes("SC1")), getBytes(2)));
+        Collection<IColumn> subColumns = cf.getSortedColumns().iterator().next().getSubColumns();
+        assert subColumns.size() == 1;
+        assert subColumns.iterator().next().timestamp() == 2;
+    }
+
+    @Test
+    public void testRemoveSuperColumnResurrection() throws IOException, ExecutionException, InterruptedException
+    {
+        ColumnFamilyStore store = Table.open("Keyspace1").getColumnFamilyStore("Super2");
+        RowMutation rm;
+        DecoratedKey key = Util.dk("keyC");
+
+        // add data
+        rm = new RowMutation("Keyspace1", key.key);
+        addMutation(rm, "Super2", "SC1", 1, "val1", 0);
+        rm.apply();
+
+        // remove
+        rm = new RowMutation("Keyspace1", key.key);
+        rm.delete(new QueryPath("Super2", ByteBufferUtil.bytes("SC1")), 1);
+        rm.apply();
+        assertNull(Util.cloneAndRemoveDeleted(store.getColumnFamily(QueryFilter.getNamesFilter(key, new QueryPath("Super2"), ByteBufferUtil.bytes("SC1"))), Integer.MAX_VALUE));
+
+        // resurrect
+        rm = new RowMutation("Keyspace1", key.key);
+        addMutation(rm, "Super2", "SC1", 1, "val2", 2);
+        rm.apply();
+
+        // validate
+        ColumnFamily cf = store.getColumnFamily(QueryFilter.getNamesFilter(key, new QueryPath("Super2"), ByteBufferUtil.bytes("SC1")));
+        cf = Util.cloneAndRemoveDeleted(cf, Integer.MAX_VALUE);
+        Collection<IColumn> subColumns = cf.getSortedColumns().iterator().next().getSubColumns();
+        assert subColumns.size() == 1;
+        assert subColumns.iterator().next().timestamp() == 2;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowCacheTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowCacheTest.java
index 3f24d583..c964a678 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowCacheTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowCacheTest.java
@@ -1 +1,146 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.Collection;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class RowCacheTest extends CleanupHelper
+{
+    private String KEYSPACE = "RowCacheSpace";
+    private String COLUMN_FAMILY_WITH_CACHE = "CachedCF";
+    private String COLUMN_FAMILY_WITHOUT_CACHE = "CFWithoutCache";
+
+    @Test
+    public void testRowCache() throws Exception
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open(KEYSPACE);
+        ColumnFamilyStore cachedStore  = table.getColumnFamilyStore(COLUMN_FAMILY_WITH_CACHE);
+        ColumnFamilyStore noCacheStore = table.getColumnFamilyStore(COLUMN_FAMILY_WITHOUT_CACHE);
+
+        // empty the row cache
+        cachedStore.invalidateRowCache();
+
+        // inserting 100 rows into both column families
+        insertData(KEYSPACE, COLUMN_FAMILY_WITH_CACHE, 0, 100);
+        insertData(KEYSPACE, COLUMN_FAMILY_WITHOUT_CACHE, 0, 100);
+
+        // now reading rows one by one and checking if row change grows
+        for (int i = 0; i < 100; i++)
+        {
+            DecoratedKey key = Util.dk("key" + i);
+            QueryPath path = new QueryPath(COLUMN_FAMILY_WITH_CACHE, null, ByteBufferUtil.bytes("col" + i));
+
+            cachedStore.getColumnFamily(key, path, ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1);
+            assert cachedStore.getRowCacheSize() == i + 1;
+            assert cachedStore.getRawCachedRow(key) != null; // current key should be stored in the cache
+
+            // checking if column is read correctly after cache
+            ColumnFamily cf = cachedStore.getColumnFamily(key, path, ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1);
+            Collection<IColumn> columns = cf.getSortedColumns();
+
+            IColumn column = columns.iterator().next();
+
+            assert columns.size() == 1;
+            assert column.name().equals(ByteBufferUtil.bytes("col" + i));
+            assert column.value().equals(ByteBufferUtil.bytes("val" + i));
+
+            path = new QueryPath(COLUMN_FAMILY_WITHOUT_CACHE, null, ByteBufferUtil.bytes("col" + i));
+
+            // row cache should not get populated for the second store
+            noCacheStore.getColumnFamily(key, path, ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1);
+            assert noCacheStore.getRowCacheSize() == 0;
+        }
+
+        // insert 10 more keys and check that row cache is still at store.getRowCacheCapacity()
+        insertData(KEYSPACE, COLUMN_FAMILY_WITH_CACHE, 100, 10);
+
+        for (int i = 100; i < 110; i++)
+        {
+            DecoratedKey key = Util.dk("key" + i);
+            QueryPath path = new QueryPath(COLUMN_FAMILY_WITH_CACHE, null, ByteBufferUtil.bytes("col" + i));
+
+            cachedStore.getColumnFamily(key, path, ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1);
+            assert cachedStore.getRowCacheSize() == cachedStore.getRowCacheCapacity();
+            assert cachedStore.getRawCachedRow(key) != null; // cache should be populated with the latest rows read (old ones should be popped)
+
+            // checking if column is read correctly after cache
+            ColumnFamily cf = cachedStore.getColumnFamily(key, path, ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1);
+            Collection<IColumn> columns = cf.getSortedColumns();
+
+            IColumn column = columns.iterator().next();
+
+            assert columns.size() == 1;
+            assert column.name().equals(ByteBufferUtil.bytes("col" + i));
+            assert column.value().equals(ByteBufferUtil.bytes("val" + i));
+        }
+
+        // clear all 100 rows from the cache
+        int keysLeft = 99;
+        for (int i = 109; i >= 10; i--)
+        {
+            cachedStore.invalidateCachedRow(Util.dk("key" + i));
+            assert cachedStore.getRowCacheSize() == keysLeft;
+            keysLeft--;
+        }
+    }
+
+    @Test
+    public void testRowCacheLoad() throws Exception
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        ColumnFamilyStore store = Table.open(KEYSPACE).getColumnFamilyStore(COLUMN_FAMILY_WITH_CACHE);
+
+        // empty the cache
+        store.invalidateRowCache();
+        assert store.getRowCacheSize() == 0;
+
+        // insert data and fill the cache
+        insertData(KEYSPACE, COLUMN_FAMILY_WITH_CACHE, 0, 100);
+        readData(KEYSPACE, COLUMN_FAMILY_WITH_CACHE, 0, 100);
+        assert store.getRowCacheSize() == 100;
+
+        // force the cache to disk
+        store.submitRowCacheWrite().get();
+
+        // empty the cache again to make sure values came from disk
+        store.invalidateRowCache();
+        assert store.getRowCacheSize() == 0;
+
+        // load the cache from disk
+        store.initRowCache();
+        assert store.getRowCacheSize() == 100;
+
+        for (int i = 0; i < 100; i++)
+        {
+            // verify the correct data was found
+            assert store.getRawCachedRow(Util.dk("key" + i)).getColumn(ByteBufferUtil.bytes("col" + i)).value().equals(ByteBufferUtil.bytes("val" + i));
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowIterationTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowIterationTest.java
index 3f24d583..0bef5785 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowIterationTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowIterationTest.java
@@ -1 +1,113 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+import java.util.ArrayList;
+import java.util.Set;
+import java.util.HashSet;
+
+import org.apache.cassandra.Util;
+
+import org.junit.Test;
+
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.utils.FBUtilities;
+import static junit.framework.Assert.assertEquals;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class RowIterationTest extends CleanupHelper
+{
+    public static final String TABLE1 = "Keyspace2";
+    public static final InetAddress LOCAL = FBUtilities.getLocalAddress();
+
+    @Test
+    public void testRowIteration() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open(TABLE1);
+        ColumnFamilyStore store = table.getColumnFamilyStore("Super3");
+
+        final int ROWS_PER_SSTABLE = 10;
+        Set<DecoratedKey> inserted = new HashSet<DecoratedKey>();
+        for (int i = 0; i < ROWS_PER_SSTABLE; i++) {
+            DecoratedKey key = Util.dk(String.valueOf(i));
+            RowMutation rm = new RowMutation(TABLE1, key.key);
+            rm.add(new QueryPath("Super3", ByteBufferUtil.bytes("sc"), ByteBufferUtil.bytes(String.valueOf(i))), ByteBuffer.wrap(new byte[ROWS_PER_SSTABLE * 10 - i * 2]), i);
+            rm.apply();
+            inserted.add(key);
+        }
+        store.forceBlockingFlush();
+        assertEquals(inserted.toString(), inserted.size(), Util.getRangeSlice(store).size());
+    }
+
+    @Test
+    public void testRowIterationDeletionTime() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open(TABLE1);
+        String CF_NAME = "Standard3";
+        ColumnFamilyStore store = table.getColumnFamilyStore(CF_NAME);
+        DecoratedKey key = Util.dk("key");
+
+        // Delete row in first sstable
+        RowMutation rm = new RowMutation(TABLE1, key.key);
+        rm.delete(new QueryPath(CF_NAME, null, null), 0);
+        rm.add(new QueryPath(CF_NAME, null, ByteBufferUtil.bytes("c")), ByteBufferUtil.bytes("values"), 0L);
+        int tstamp1 = rm.getColumnFamilies().iterator().next().getLocalDeletionTime();
+        rm.apply();
+        store.forceBlockingFlush();
+
+        // Delete row in second sstable with higher timestamp
+        rm = new RowMutation(TABLE1, key.key);
+        rm.delete(new QueryPath(CF_NAME, null, null), 1);
+        rm.add(new QueryPath(CF_NAME, null, ByteBufferUtil.bytes("c")), ByteBufferUtil.bytes("values"), 1L);
+        int tstamp2 = rm.getColumnFamilies().iterator().next().getLocalDeletionTime();
+        rm.apply();
+        store.forceBlockingFlush();
+
+        ColumnFamily cf = Util.getRangeSlice(store).iterator().next().cf;
+        assert cf.getMarkedForDeleteAt() == 1L;
+        assert cf.getLocalDeletionTime() == tstamp2;
+    }
+
+    @Test
+    public void testRowIterationDeletion() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open(TABLE1);
+        String CF_NAME = "Standard3";
+        ColumnFamilyStore store = table.getColumnFamilyStore(CF_NAME);
+        DecoratedKey key = Util.dk("key");
+
+        // Delete a row in first sstable
+        RowMutation rm = new RowMutation(TABLE1, key.key);
+        rm.delete(new QueryPath(CF_NAME, null, null), 0);
+        rm.apply();
+        store.forceBlockingFlush();
+
+        ColumnFamily cf = Util.getRangeSlice(store).iterator().next().cf;
+        assert cf != null;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowTest.java
index 3f24d583..7542bd14 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/RowTest.java
@@ -1 +1,99 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+
+import org.apache.cassandra.SchemaLoader;
+import org.junit.Test;
+
+import static junit.framework.Assert.assertEquals;
+import static junit.framework.Assert.fail;
+import org.apache.cassandra.db.marshal.AsciiType;
+import static org.apache.cassandra.Util.column;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class RowTest extends SchemaLoader
+{
+    @Test
+    public void testDiffColumnFamily()
+    {
+        ColumnFamily cf1 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf1.addColumn(column("one", "onev", 0));
+
+        ColumnFamily cf2 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf2.delete(0, 0);
+
+        ColumnFamily cfDiff = cf1.diff(cf2);
+        assertEquals(cfDiff.getColumnsMap().size(), 0);
+        assertEquals(cfDiff.getMarkedForDeleteAt(), 0);
+    }
+
+    @Test
+    public void testDiffSuperColumn()
+    {
+        SuperColumn sc1 = new SuperColumn(ByteBufferUtil.bytes("one"), AsciiType.instance);
+        sc1.addColumn(column("subcolumn", "A", 0));
+
+        SuperColumn sc2 = new SuperColumn(ByteBufferUtil.bytes("one"), AsciiType.instance);
+        sc2.markForDeleteAt(0, 0);
+
+        SuperColumn scDiff = (SuperColumn)sc1.diff(sc2);
+        assertEquals(scDiff.getSubColumns().size(), 0);
+        assertEquals(scDiff.getMarkedForDeleteAt(), 0);
+    }
+
+    @Test
+    public void testResolve()
+    {
+        ColumnFamily cf1 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf1.addColumn(column("one", "A", 0));
+
+        ColumnFamily cf2 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf2.addColumn(column("one", "B", 1));
+        cf2.addColumn(column("two", "C", 1));
+
+        cf1.resolve(cf2);
+        assert Arrays.equals(cf1.getColumn(ByteBufferUtil.bytes("one")).value().array(), "B".getBytes());
+        assert Arrays.equals(cf1.getColumn(ByteBufferUtil.bytes("two")).value().array(), "C".getBytes());
+    }
+
+    @Test
+    public void testExpiringColumnExpiration()
+    {
+        Column c = new ExpiringColumn(ByteBufferUtil.bytes("one"), ByteBufferUtil.bytes("A"), 0, 1);
+        assert !c.isMarkedForDelete();
+
+        try
+        {
+            // Because we keep the local deletion time with a precision of a
+            // second, we could have to wait 2 seconds in worst case scenario.
+            Thread.sleep(2000);
+        }
+        catch (InterruptedException e)
+        {
+            fail("Cannot test column expiration if you wake me up too early");
+        }
+
+        assert c.isMarkedForDelete() && c.getMarkedForDeleteAt() == 0;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ScrubTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ScrubTest.java
index 3f24d583..29698b33 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ScrubTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/ScrubTest.java
@@ -1 +1,194 @@
   + native
+package org.apache.cassandra.db;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.io.File;
+import java.io.IOException;
+import java.util.List;
+import java.util.concurrent.ExecutionException;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.columniterator.IdentityQueryFilter;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.CLibrary;
+
+import static org.apache.cassandra.Util.column;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
+public class ScrubTest extends CleanupHelper
+{
+    public String TABLE = "Keyspace1";
+    public String CF = "Standard1";
+    public String CF2 = "Super5";
+    public String CF3 = "Standard2";
+    public String  corruptSSTableName;
+
+    
+    public void copySSTables() throws IOException 
+    {
+        String root = System.getProperty("corrupt-sstable-root");
+        assert root != null;
+        File rootDir = new File(root);
+        assert rootDir.isDirectory();
+        
+        String[] destDirs = DatabaseDescriptor.getAllDataFileLocationsForTable(TABLE);
+        assert destDirs != null;
+        assert destDirs.length > 0;
+       
+        FileUtils.createDirectory(destDirs[0]);
+        for (File srcFile : rootDir.listFiles())
+        {
+            if (srcFile.getName().equals(".svn"))
+                continue;
+            File destFile = new File(destDirs[0]+File.separator+srcFile.getName());
+            CLibrary.createHardLinkWithExec(srcFile, destFile);
+                        
+            destFile = new File(destDirs[0]+File.separator+srcFile.getName());
+                        
+            assert destFile.exists() : destFile.getAbsoluteFile();
+            
+            if(destFile.getName().endsWith("Data.db"))
+                corruptSSTableName = destFile.getCanonicalPath();
+        }   
+
+        assert corruptSSTableName != null;
+    }
+   
+    @Test
+    public void testScrubFile() throws Exception
+    {        
+        copySSTables();
+
+        Table table = Table.open(TABLE);
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(CF2);
+        assert cfs.getSSTables().size() > 0;
+      
+        List<Row> rows;
+        boolean caught = false;
+        try
+        {
+             rows = cfs.getRangeSlice(ByteBufferUtil.bytes("1"), Util.range("", ""), 1000, new IdentityQueryFilter());
+             fail("This slice should fail");
+        }
+        catch (NegativeArraySizeException e)
+        {
+            caught = true;
+        }
+        assert caught : "'corrupt' test file actually was not";
+        
+        CompactionManager.instance.performScrub(cfs);
+        rows = cfs.getRangeSlice(ByteBufferUtil.bytes("1"), Util.range("", ""), 1000, new IdentityQueryFilter());
+        assertEquals(100, rows.size());
+    }
+    
+    
+    @Test
+    public void testScrubOneRow() throws IOException, ExecutionException, InterruptedException, ConfigurationException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+        Table table = Table.open(TABLE);
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(CF);
+
+        List<Row> rows;
+
+        // insert data and verify we get it back w/ range query
+        fillCF(cfs, 1);
+        rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter());
+        assertEquals(1, rows.size());
+
+        CompactionManager.instance.performScrub(cfs);
+
+        // check data is still there
+        rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter());
+        assertEquals(1, rows.size());
+    }
+
+    @Test
+    public void testScrubDeletedRow() throws IOException, ExecutionException, InterruptedException, ConfigurationException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+        Table table = Table.open(TABLE);
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(CF3);
+
+        RowMutation rm;
+        rm = new RowMutation(TABLE, ByteBufferUtil.bytes(1));
+        ColumnFamily cf = ColumnFamily.create(TABLE, CF3);
+        cf.delete(0, 1); // expired tombstone
+        rm.add(cf);
+        rm.applyUnsafe();
+        cfs.forceBlockingFlush();
+
+        CompactionManager.instance.performScrub(cfs);
+        assert cfs.getSSTables().isEmpty();
+    }
+
+    @Test
+    public void testScrubMultiRow() throws IOException, ExecutionException, InterruptedException, ConfigurationException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+        Table table = Table.open(TABLE);
+        ColumnFamilyStore cfs = table.getColumnFamilyStore(CF);
+
+        List<Row> rows;
+
+        // insert data and verify we get it back w/ range query
+        fillCF(cfs, 10);
+        rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter());
+        assertEquals(10, rows.size());
+
+        CompactionManager.instance.performScrub(cfs);
+
+        // check data is still there
+        rows = cfs.getRangeSlice(null, Util.range("", ""), 1000, new IdentityQueryFilter());
+        assertEquals(10, rows.size());
+    }
+      
+    protected void fillCF(ColumnFamilyStore cfs, int rowsPerSSTable) throws ExecutionException, InterruptedException, IOException
+    {
+        for (int i = 0; i < rowsPerSSTable; i++)
+        {
+            String key = String.valueOf(i);
+            // create a row and update the birthdate value, test that the index query fetches the new version
+            RowMutation rm;
+            rm = new RowMutation(TABLE, ByteBufferUtil.bytes(key));
+            ColumnFamily cf = ColumnFamily.create(TABLE, CF);
+            cf.addColumn(column("c1", "1", 1L));
+            cf.addColumn(column("c2", "2", 1L));
+            rm.add(cf);
+            rm.applyUnsafe();
+        }
+
+        cfs.forceBlockingFlush();
+    }
+
+    
+    
+    
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/SerializationsTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/SerializationsTest.java
index 3f24d583..31ac739a 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/SerializationsTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/SerializationsTest.java
@@ -1 +1,339 @@
   + native
+package org.apache.cassandra.db;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import org.apache.cassandra.AbstractSerializationsTester;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.AbstractBounds;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.thrift.SlicePredicate;
+import org.apache.cassandra.thrift.SliceRange;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+import org.junit.Test;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.HashMap;
+
+public class SerializationsTest extends AbstractSerializationsTester
+{
+    private void testRangeSliceCommandWrite() throws IOException
+    {
+        ByteBuffer startCol = ByteBufferUtil.bytes("Start");
+        ByteBuffer stopCol = ByteBufferUtil.bytes("Stop");
+        ByteBuffer emptyCol = ByteBufferUtil.bytes("");
+        SlicePredicate namesPred = new SlicePredicate();
+        namesPred.column_names = Statics.NamedCols;
+        SliceRange emptySliceRange = new SliceRange(emptyCol, emptyCol, false, 100); 
+        SliceRange nonEmptySliceRange = new SliceRange(startCol, stopCol, true, 100);
+        SlicePredicate emptyRangePred = new SlicePredicate();
+        emptyRangePred.slice_range = emptySliceRange;
+        SlicePredicate nonEmptyRangePred = new SlicePredicate();
+        nonEmptyRangePred.slice_range = nonEmptySliceRange;
+        IPartitioner part = StorageService.getPartitioner();
+        AbstractBounds bounds = new Range(part.getRandomToken(), part.getRandomToken());
+        
+        Message namesCmd = new RangeSliceCommand(Statics.KS, "Standard1", null, namesPred, bounds, 100).getMessage();
+        Message emptyRangeCmd = new RangeSliceCommand(Statics.KS, "Standard1", null, emptyRangePred, bounds, 100).getMessage();
+        Message regRangeCmd = new RangeSliceCommand(Statics.KS, "Standard1", null,  nonEmptyRangePred, bounds, 100).getMessage();
+        Message namesCmdSup = new RangeSliceCommand(Statics.KS, "Super1", Statics.SC, namesPred, bounds, 100).getMessage();
+        Message emptyRangeCmdSup = new RangeSliceCommand(Statics.KS, "Super1", Statics.SC, emptyRangePred, bounds, 100).getMessage();
+        Message regRangeCmdSup = new RangeSliceCommand(Statics.KS, "Super1", Statics.SC,  nonEmptyRangePred, bounds, 100).getMessage();
+        
+        DataOutputStream dout = getOutput("db.RangeSliceCommand.bin");
+        
+        Message.serializer().serialize(namesCmd, dout);
+        Message.serializer().serialize(emptyRangeCmd, dout);
+        Message.serializer().serialize(regRangeCmd, dout);
+        Message.serializer().serialize(namesCmdSup, dout);
+        Message.serializer().serialize(emptyRangeCmdSup, dout);
+        Message.serializer().serialize(regRangeCmdSup, dout);
+        dout.close();
+    }
+    
+    @Test
+    public void testRangeSliceCommandRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testRangeSliceCommandWrite();
+        
+        DataInputStream in = getInput("db.RangeSliceCommand.bin");
+        for (int i = 0; i < 6; i++)
+        {
+            Message msg = Message.serializer().deserialize(in);
+            RangeSliceCommand cmd = RangeSliceCommand.read(msg);
+        }
+        in.close();
+    }
+    
+    private void testSliceByNamesReadCommandWrite() throws IOException
+    {
+        SliceByNamesReadCommand standardCmd = new SliceByNamesReadCommand(Statics.KS, Statics.Key, Statics.StandardPath, Statics.NamedCols);
+        SliceByNamesReadCommand superCmd = new SliceByNamesReadCommand(Statics.KS, Statics.Key, Statics.SuperPath, Statics.NamedCols);
+        
+        DataOutputStream out = getOutput("db.SliceByNamesReadCommand.bin");
+        SliceByNamesReadCommand.serializer().serialize(standardCmd, out);
+        SliceByNamesReadCommand.serializer().serialize(superCmd, out);
+        ReadCommand.serializer().serialize(standardCmd, out);
+        ReadCommand.serializer().serialize(superCmd, out);
+        Message.serializer().serialize(standardCmd.makeReadMessage(), out);
+        Message.serializer().serialize(superCmd.makeReadMessage(), out);
+        out.close();
+    }
+    
+    @Test 
+    public void testSliceByNamesReadCommandRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testSliceByNamesReadCommandWrite();
+        
+        DataInputStream in = getInput("db.SliceByNamesReadCommand.bin");
+        assert SliceByNamesReadCommand.serializer().deserialize(in) != null;
+        assert SliceByNamesReadCommand.serializer().deserialize(in) != null;
+        assert ReadCommand.serializer().deserialize(in) != null;
+        assert ReadCommand.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private void testSliceFromReadCommandWrite() throws IOException
+    {
+        SliceFromReadCommand standardCmd = new SliceFromReadCommand(Statics.KS, Statics.Key, Statics.StandardPath, Statics.Start, Statics.Stop, true, 100);
+        SliceFromReadCommand superCmd = new SliceFromReadCommand(Statics.KS, Statics.Key, Statics.SuperPath, Statics.Start, Statics.Stop, true, 100);
+        DataOutputStream out = getOutput("db.SliceFromReadCommand.bin");
+        SliceFromReadCommand.serializer().serialize(standardCmd, out);
+        SliceFromReadCommand.serializer().serialize(superCmd, out);
+        ReadCommand.serializer().serialize(standardCmd, out);
+        ReadCommand.serializer().serialize(superCmd, out);
+        Message.serializer().serialize(standardCmd.makeReadMessage(), out);
+        Message.serializer().serialize(superCmd.makeReadMessage(), out);
+        out.close();
+    }
+    
+    @Test
+    public void testSliceFromReadCommandRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testSliceFromReadCommandWrite();
+        
+        DataInputStream in = getInput("db.SliceFromReadCommand.bin");
+        assert SliceFromReadCommand.serializer().deserialize(in) != null;
+        assert SliceFromReadCommand.serializer().deserialize(in) != null;
+        assert ReadCommand.serializer().deserialize(in) != null;
+        assert ReadCommand.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private void testRowWrite() throws IOException
+    {
+        DataOutputStream out = getOutput("db.Row.bin");
+        Row.serializer().serialize(Statics.StandardRow, out);
+        Row.serializer().serialize(Statics.SuperRow, out);
+        Row.serializer().serialize(Statics.NullRow, out);
+        out.close();
+    }
+    
+    @Test
+    public void testRowRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testRowWrite();
+        
+        DataInputStream in = getInput("db.Row.bin");
+        assert Row.serializer().deserialize(in) != null;
+        assert Row.serializer().deserialize(in) != null;
+        assert Row.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private void restRowMutationWrite() throws IOException
+    {
+        RowMutation emptyRm = new RowMutation(Statics.KS, Statics.Key);
+        RowMutation standardRowRm = new RowMutation(Statics.KS, Statics.StandardRow);
+        RowMutation superRowRm = new RowMutation(Statics.KS, Statics.SuperRow);
+        RowMutation standardRm = new RowMutation(Statics.KS, Statics.Key);
+        standardRm.add(Statics.StandardCf);
+        RowMutation superRm = new RowMutation(Statics.KS, Statics.Key);
+        superRm.add(Statics.SuperCf);
+        Map<Integer, ColumnFamily> mods = new HashMap<Integer, ColumnFamily>();
+        mods.put(Statics.StandardCf.metadata().cfId, Statics.StandardCf);
+        mods.put(Statics.SuperCf.metadata().cfId, Statics.SuperCf);
+        RowMutation mixedRm = new RowMutation(Statics.KS, Statics.Key, mods);
+        
+        DataOutputStream out = getOutput("db.RowMutation.bin");
+        RowMutation.serializer().serialize(emptyRm, out);
+        RowMutation.serializer().serialize(standardRowRm, out);
+        RowMutation.serializer().serialize(superRowRm, out);
+        RowMutation.serializer().serialize(standardRm, out);
+        RowMutation.serializer().serialize(superRm, out);
+        RowMutation.serializer().serialize(mixedRm, out);
+        Message.serializer().serialize(emptyRm.makeRowMutationMessage(), out);
+        Message.serializer().serialize(standardRowRm.makeRowMutationMessage(), out);
+        Message.serializer().serialize(superRowRm.makeRowMutationMessage(), out);
+        Message.serializer().serialize(standardRm.makeRowMutationMessage(), out);
+        Message.serializer().serialize(superRm.makeRowMutationMessage(), out);
+        Message.serializer().serialize(mixedRm.makeRowMutationMessage(), out);
+        out.close(); 
+    }
+    
+    @Test
+    public void testRowMutationRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            restRowMutationWrite();
+        
+        DataInputStream in = getInput("db.RowMutation.bin");
+        assert RowMutation.serializer().deserialize(in) != null;
+        assert RowMutation.serializer().deserialize(in) != null;
+        assert RowMutation.serializer().deserialize(in) != null;
+        assert RowMutation.serializer().deserialize(in) != null;
+        assert RowMutation.serializer().deserialize(in) != null;
+        assert RowMutation.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    public void testTruncateWrite() throws IOException
+    {
+        Truncation tr = new Truncation(Statics.KS, "Doesn't Really Matter");
+        TruncateResponse aff = new TruncateResponse(Statics.KS, "Doesn't Matter Either", true);
+        TruncateResponse neg = new TruncateResponse(Statics.KS, "Still Doesn't Matter", false);
+        DataOutputStream out = getOutput("db.Truncation.bin");
+        Truncation.serializer().serialize(tr, out);
+        TruncateResponse.serializer().serialize(aff, out);
+        TruncateResponse.serializer().serialize(neg, out);
+        Message.serializer().serialize(tr.makeTruncationMessage(), out);
+        Message.serializer().serialize(TruncateResponse.makeTruncateResponseMessage(tr.makeTruncationMessage(), aff), out);
+        Message.serializer().serialize(TruncateResponse.makeTruncateResponseMessage(tr.makeTruncationMessage(), neg), out);
+        // todo: notice how CF names weren't validated.
+        out.close();
+    }
+    
+    @Test
+    public void testTruncateRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testTruncateWrite();
+        
+        DataInputStream in = getInput("db.Truncation.bin");
+        assert Truncation.serializer().deserialize(in) != null;
+        assert TruncateResponse.serializer().deserialize(in) != null;
+        assert TruncateResponse.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private void testWriteResponseWrite() throws IOException
+    {
+        WriteResponse aff = new WriteResponse(Statics.KS, Statics.Key, true);
+        WriteResponse neg = new WriteResponse(Statics.KS, Statics.Key, false);
+        DataOutputStream out = getOutput("db.WriteResponse.bin");
+        WriteResponse.serializer().serialize(aff, out);
+        WriteResponse.serializer().serialize(neg, out);
+        out.close();
+    }
+    
+    @Test
+    public void testWriteResponseRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testWriteResponseWrite();
+        
+        DataInputStream in = getInput("db.WriteResponse.bin");
+        assert WriteResponse.serializer().deserialize(in) != null;
+        assert WriteResponse.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private static ByteBuffer bb(String s) {
+        return ByteBufferUtil.bytes(s);
+    }
+    
+    private static class Statics 
+    {
+        private static final String KS = "Keyspace1";
+        private static final ByteBuffer Key = ByteBufferUtil.bytes("Key01");
+        private static final List<ByteBuffer> NamedCols = new ArrayList<ByteBuffer>() 
+        {{
+            add(ByteBufferUtil.bytes("AAA"));
+            add(ByteBufferUtil.bytes("BBB"));
+            add(ByteBufferUtil.bytes("CCC"));
+        }};
+        private static final ByteBuffer SC = ByteBufferUtil.bytes("SCName");
+        private static final QueryPath StandardPath = new QueryPath("Standard1");
+        private static final QueryPath SuperPath = new QueryPath("Super1", SC);
+        private static final ByteBuffer Start = ByteBufferUtil.bytes("Start");
+        private static final ByteBuffer Stop = ByteBufferUtil.bytes("Stop");
+        
+        private static final ColumnFamily StandardCf = ColumnFamily.create(Statics.KS, "Standard1");
+        private static final ColumnFamily SuperCf = ColumnFamily.create(Statics.KS, "Super1");
+        
+        private static final SuperColumn SuperCol = new SuperColumn(Statics.SC, DatabaseDescriptor.getComparator(Statics.KS, "Super1"))
+        {{
+            addColumn(new Column(bb("aaaa")));
+            addColumn(new Column(bb("bbbb"), bb("bbbbb-value")));
+            addColumn(new Column(bb("cccc"), bb("ccccc-value"), 1000L));
+            addColumn(new DeletedColumn(bb("dddd"), 500, 1000));
+            addColumn(new DeletedColumn(bb("eeee"), bb("eeee-value"), 1001));
+            addColumn(new ExpiringColumn(bb("ffff"), bb("ffff-value"), 2000, 1000));
+            addColumn(new ExpiringColumn(bb("gggg"), bb("gggg-value"), 2001, 1000, 2002));
+        }};
+        
+        private static final Row StandardRow = new Row(Util.dk("key0"), Statics.StandardCf);
+        private static final Row SuperRow = new Row(Util.dk("key1"), Statics.SuperCf);
+        private static final Row NullRow = new Row(Util.dk("key2"), null);
+        
+        static {
+            StandardCf.addColumn(new Column(bb("aaaa")));
+            StandardCf.addColumn(new Column(bb("bbbb"), bb("bbbbb-value")));
+            StandardCf.addColumn(new Column(bb("cccc"), bb("ccccc-value"), 1000L));
+            StandardCf.addColumn(new DeletedColumn(bb("dddd"), 500, 1000));
+            StandardCf.addColumn(new DeletedColumn(bb("eeee"), bb("eeee-value"), 1001));
+            StandardCf.addColumn(new ExpiringColumn(bb("ffff"), bb("ffff-value"), 2000, 1000));
+            StandardCf.addColumn(new ExpiringColumn(bb("gggg"), bb("gggg-value"), 2001, 1000, 2002));
+            
+            SuperCf.addColumn(Statics.SuperCol);
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/SuperColumnTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/SuperColumnTest.java
index 3f24d583..93346244 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/SuperColumnTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/SuperColumnTest.java
@@ -1 +1,41 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.nio.ByteBuffer;
+
+import org.junit.Test;
+
+import static junit.framework.Assert.assertNotNull;
+import static junit.framework.Assert.assertNull;
+import static org.apache.cassandra.Util.getBytes;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class SuperColumnTest
+{   
+    @Test
+    public void testMissingSubcolumn() {
+    	SuperColumn sc = new SuperColumn(ByteBufferUtil.bytes("sc1"), LongType.instance);
+    	sc.addColumn(new Column(getBytes(1), ByteBufferUtil.bytes("value"), 1));
+    	assertNotNull(sc.getSubColumn(getBytes(1)));
+    	assertNull(sc.getSubColumn(getBytes(2)));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/TableTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/TableTest.java
index e69de29b..227a1b6d 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/TableTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/TableTest.java
@@ -0,0 +1,547 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.nio.ByteBuffer;
+import java.nio.charset.CharacterCodingException;
+import java.text.DecimalFormat;
+import java.text.NumberFormat;
+import java.util.*;
+import java.io.IOException;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+import org.apache.commons.lang.StringUtils;
+import org.junit.Test;
+
+import static junit.framework.Assert.*;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.utils.WrappedRunnable;
+import static org.apache.cassandra.Util.column;
+import static org.apache.cassandra.Util.getBytes;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.io.sstable.IndexHelper;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.util.BufferedRandomAccessFile;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class TableTest extends CleanupHelper
+{
+    private static final DecoratedKey TEST_KEY = Util.dk("key1");
+    private static final DecoratedKey TEST_SLICE_KEY = Util.dk("key1-slicerange");
+
+    public static void reTest(ColumnFamilyStore cfs, Runnable verify) throws Exception
+    {
+        verify.run();
+        cfs.forceBlockingFlush();
+        verify.run();
+    }
+
+    @Test
+    public void testGetRowNoColumns() throws Throwable
+    {
+        final Table table = Table.open("Keyspace2");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard3");
+
+        RowMutation rm = new RowMutation("Keyspace2", TEST_KEY.key);
+        ColumnFamily cf = ColumnFamily.create("Keyspace2", "Standard3");
+        cf.addColumn(column("col1","val1", 1L));
+        rm.add(cf);
+        rm.apply();
+
+        Runnable verify = new WrappedRunnable()
+        {
+            public void runMayThrow() throws Exception
+            {
+                ColumnFamily cf;
+
+                cf = cfStore.getColumnFamily(QueryFilter.getNamesFilter(TEST_KEY, new QueryPath("Standard3"), new TreeSet<ByteBuffer>()));
+                assertColumns(cf);
+
+                cf = cfStore.getColumnFamily(QueryFilter.getSliceFilter(TEST_KEY, new QueryPath("Standard3"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 0));
+                assertColumns(cf);
+
+                cf = cfStore.getColumnFamily(QueryFilter.getNamesFilter(TEST_KEY, new QueryPath("Standard3"), ByteBufferUtil.bytes("col99")));
+                assertColumns(cf);
+            }
+        };
+        reTest(table.getColumnFamilyStore("Standard3"), verify);
+    }
+
+    @Test
+    public void testGetRowSingleColumn() throws Throwable
+    {
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+
+        RowMutation rm = new RowMutation("Keyspace1", TEST_KEY.key);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+        cf.addColumn(column("col1","val1", 1L));
+        cf.addColumn(column("col2","val2", 1L));
+        cf.addColumn(column("col3","val3", 1L));
+        rm.add(cf);
+        rm.apply();
+
+        Runnable verify = new WrappedRunnable()
+        {
+            public void runMayThrow() throws Exception
+            {
+                ColumnFamily cf;
+
+                cf = cfStore.getColumnFamily(QueryFilter.getNamesFilter(TEST_KEY, new QueryPath("Standard1"), ByteBufferUtil.bytes("col1")));
+                assertColumns(cf, "col1");
+
+                cf = cfStore.getColumnFamily(QueryFilter.getNamesFilter(TEST_KEY, new QueryPath("Standard1"), ByteBufferUtil.bytes("col3")));
+                assertColumns(cf, "col3");
+            }
+        };
+        reTest(table.getColumnFamilyStore("Standard1"), verify);
+    }
+
+    @Test
+    public void testGetRowSliceByRange() throws Throwable
+    {
+    	DecoratedKey key = TEST_SLICE_KEY;
+    	Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+    	RowMutation rm = new RowMutation("Keyspace1", key.key);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+        // First write "a", "b", "c"
+        cf.addColumn(column("a", "val1", 1L));
+        cf.addColumn(column("b", "val2", 1L));
+        cf.addColumn(column("c", "val3", 1L));
+        rm.add(cf);
+        rm.apply();
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("b"), ByteBufferUtil.bytes("c"), false, 100);
+        assertEquals(2, cf.getColumnCount());
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("b"), ByteBufferUtil.bytes("b"), false, 100);
+        assertEquals(1, cf.getColumnCount());
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("b"), ByteBufferUtil.bytes("c"), false, 1);
+        assertEquals(1, cf.getColumnCount());
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("c"), ByteBufferUtil.bytes("b"), false, 1);
+        assertNull(cf);
+    }
+
+    @Test
+    public void testGetSliceNoMatch() throws Throwable
+    {
+        Table table = Table.open("Keyspace1");
+        RowMutation rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("row1000"));
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard2");
+        cf.addColumn(column("col1", "val1", 1));
+        rm.add(cf);
+        rm.apply();
+
+        validateGetSliceNoMatch(table);
+        table.getColumnFamilyStore("Standard2").forceBlockingFlush();
+        validateGetSliceNoMatch(table);
+
+        Collection<SSTableReader> ssTables = table.getColumnFamilyStore("Standard2").getSSTables();
+        assertEquals(1, ssTables.size());
+        ssTables.iterator().next().forceFilterFailures();
+        validateGetSliceNoMatch(table);
+    }
+
+    @Test
+    public void testGetSliceWithCutoff() throws Throwable
+    {
+        // tests slicing against data from one row in a memtable and then flushed to an sstable
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+        final DecoratedKey ROW = Util.dk("row4");
+        final NumberFormat fmt = new DecimalFormat("000");
+
+        RowMutation rm = new RowMutation("Keyspace1", ROW.key);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+        // at this rate, we're getting 78-79 cos/block, assuming the blocks are set to be about 4k.
+        // so if we go to 300, we'll get at least 4 blocks, which is plenty for testing.
+        for (int i = 0; i < 300; i++)
+            cf.addColumn(column("col" + fmt.format(i), "omg!thisisthevalue!"+i, 1L));
+        rm.add(cf);
+        rm.apply();
+
+        Runnable verify = new WrappedRunnable()
+        {
+            public void runMayThrow() throws Exception
+            {
+                ColumnFamily cf;
+
+                // blocks are partitioned like this: 000-097, 098-193, 194-289, 290-299, assuming a 4k column index size.
+                assert DatabaseDescriptor.getColumnIndexSize() == 4096 : "Unexpected column index size, block boundaries won't be where tests expect them.";
+
+                // test forward, spanning a segment.
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col096"), ByteBufferUtil.bytes("col099"), false, 4);
+                assertColumns(cf, "col096", "col097", "col098", "col099");
+
+                // test reversed, spanning a segment.
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col099"), ByteBufferUtil.bytes("col096"), true, 4);
+                assertColumns(cf, "col096", "col097", "col098", "col099");
+
+                // test forward, within a segment.
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col100"), ByteBufferUtil.bytes("col103"), false, 4);
+                assertColumns(cf, "col100", "col101", "col102", "col103");
+
+                // test reversed, within a segment.
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col103"), ByteBufferUtil.bytes("col100"), true, 4);
+                assertColumns(cf, "col100", "col101", "col102", "col103");
+
+                // test forward from beginning, spanning a segment.
+                String[] strCols = new String[100]; // col000-col099
+                for (int i = 0; i < 100; i++)
+                    strCols[i] = "col" + fmt.format(i);
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.bytes("col099"), false, 100);
+                assertColumns(cf, strCols);
+
+                // test reversed, from end, spanning a segment.
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.bytes("col288"), true, 12);
+                assertColumns(cf, "col288", "col289", "col290", "col291", "col292", "col293", "col294", "col295", "col296", "col297", "col298", "col299");
+            }
+        };
+
+        reTest(table.getColumnFamilyStore("Standard1"), verify);
+    }
+
+    @Test
+    public void testReversedWithFlushing() throws IOException, ExecutionException, InterruptedException
+    {
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfs = table.getColumnFamilyStore("StandardLong1");
+        final DecoratedKey ROW = Util.dk("row4");
+
+        for (int i = 0; i < 10; i++)
+        {
+            RowMutation rm = new RowMutation("Keyspace1", ROW.key);
+            ColumnFamily cf = ColumnFamily.create("Keyspace1", "StandardLong1");
+            cf.addColumn(new Column(ByteBufferUtil.bytes((long)i), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0));
+            rm.add(cf);
+            rm.apply();
+        }
+
+        cfs.forceBlockingFlush();
+
+        for (int i = 10; i < 20; i++)
+        {
+            RowMutation rm = new RowMutation("Keyspace1", ROW.key);
+            ColumnFamily cf = ColumnFamily.create("Keyspace1", "StandardLong1");
+            cf.addColumn(new Column(ByteBufferUtil.bytes((long)i), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0));
+            rm.add(cf);
+            rm.apply();
+
+            cf = cfs.getColumnFamily(ROW, new QueryPath("StandardLong1"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 1);
+            assertEquals(1, cf.getColumnNames().size());
+            assertEquals(i, cf.getColumnNames().iterator().next().getLong());
+        }
+    }
+
+    private void validateGetSliceNoMatch(Table table) throws IOException
+    {
+        ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard2");
+        ColumnFamily cf;
+
+        // key before the rows that exists
+        cf = cfStore.getColumnFamily(Util.dk("a"), new QueryPath("Standard2"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1);
+        assertColumns(cf);
+
+        // key after the rows that exist
+        cf = cfStore.getColumnFamily(Util.dk("z"), new QueryPath("Standard2"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1);
+        assertColumns(cf);
+    }
+
+    @Test
+    public void testGetSliceFromBasic() throws Throwable
+    {
+        // tests slicing against data from one row in a memtable and then flushed to an sstable
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+        final DecoratedKey ROW = Util.dk("row1");
+
+        RowMutation rm = new RowMutation("Keyspace1", ROW.key);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+        cf.addColumn(column("col1", "val1", 1L));
+        cf.addColumn(column("col3", "val3", 1L));
+        cf.addColumn(column("col4", "val4", 1L));
+        cf.addColumn(column("col5", "val5", 1L));
+        cf.addColumn(column("col7", "val7", 1L));
+        cf.addColumn(column("col9", "val9", 1L));
+        rm.add(cf);
+        rm.apply();
+
+        rm = new RowMutation("Keyspace1", ROW.key);
+        rm.delete(new QueryPath("Standard1", null, ByteBufferUtil.bytes("col4")), 2L);
+        rm.apply();
+
+        Runnable verify = new WrappedRunnable()
+        {
+            public void runMayThrow() throws Exception
+            {
+                ColumnFamily cf;
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col5"), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 2);
+                assertColumns(cf, "col5", "col7");
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col4"), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 2);
+                assertColumns(cf, "col4", "col5", "col7");
+                assertColumns(ColumnFamilyStore.removeDeleted(cf, Integer.MAX_VALUE), "col5", "col7");
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col5"), ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 2);
+                assertColumns(cf, "col3", "col4", "col5");
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col6"), ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 2);
+                assertColumns(cf, "col3", "col4", "col5");
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 2);
+                assertColumns(cf, "col7", "col9");
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col95"), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 2);
+                assertColumns(cf);
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col0"), ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 2);
+                assertColumns(cf);
+            }
+        };
+
+        reTest(table.getColumnFamilyStore("Standard1"), verify);
+    }
+
+    @Test
+    public void testGetSliceFromAdvanced() throws Throwable
+    {
+        // tests slicing against data from one row spread across two sstables
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+        final DecoratedKey ROW = Util.dk("row2");
+
+        RowMutation rm = new RowMutation("Keyspace1", ROW.key);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+        cf.addColumn(column("col1", "val1", 1L));
+        cf.addColumn(column("col2", "val2", 1L));
+        cf.addColumn(column("col3", "val3", 1L));
+        cf.addColumn(column("col4", "val4", 1L));
+        cf.addColumn(column("col5", "val5", 1L));
+        cf.addColumn(column("col6", "val6", 1L));
+        rm.add(cf);
+        rm.apply();
+        cfStore.forceBlockingFlush();
+
+        rm = new RowMutation("Keyspace1", ROW.key);
+        cf = ColumnFamily.create("Keyspace1", "Standard1");
+        cf.addColumn(column("col1", "valx", 2L));
+        cf.addColumn(column("col2", "valx", 2L));
+        cf.addColumn(column("col3", "valx", 2L));
+        rm.add(cf);
+        rm.apply();
+
+        Runnable verify = new WrappedRunnable()
+        {
+            public void runMayThrow() throws Exception
+            {
+                ColumnFamily cf;
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), ByteBufferUtil.bytes("col2"), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 3);
+                assertColumns(cf, "col2", "col3", "col4");
+                
+                ByteBuffer col = cf.getColumn(ByteBufferUtil.bytes("col2")).value();
+                assertEquals(ByteBufferUtil.string(col), "valx");
+                
+                col = cf.getColumn(ByteBufferUtil.bytes("col3")).value();
+                assertEquals(ByteBufferUtil.string(col), "valx");
+                
+                col = cf.getColumn(ByteBufferUtil.bytes("col4")).value();
+                assertEquals(ByteBufferUtil.string(col), "val4");
+            }
+        };
+
+        reTest(table.getColumnFamilyStore("Standard1"), verify);
+    }
+
+    @Test
+    public void testGetSliceFromLarge() throws Throwable
+    {
+        // tests slicing against 1000 columns in an sstable
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+        DecoratedKey key = Util.dk("row3");
+        RowMutation rm = new RowMutation("Keyspace1", key.key);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+        for (int i = 1000; i < 2000; i++)
+            cf.addColumn(column("col" + i, ("v" + i), 1L));
+        rm.add(cf);
+        rm.apply();
+        cfStore.forceBlockingFlush();
+
+        validateSliceLarge(cfStore);
+
+        // compact so we have a big row with more than the minimum index count
+        if (cfStore.getSSTables().size() > 1)
+        {
+            CompactionManager.instance.performMajor(cfStore);
+        }
+        // verify that we do indeed have multiple index entries
+        SSTableReader sstable = cfStore.getSSTables().iterator().next();
+        long position = sstable.getPosition(key, SSTableReader.Operator.EQ);
+        BufferedRandomAccessFile file = new BufferedRandomAccessFile(sstable.getFilename(), "r");
+        file.seek(position);
+        assert ByteBufferUtil.readWithShortLength(file).equals(key.key);
+        SSTableReader.readRowSize(file, sstable.descriptor);
+        IndexHelper.skipBloomFilter(file);
+        ArrayList<IndexHelper.IndexInfo> indexes = IndexHelper.deserializeIndex(file);
+        assert indexes.size() > 2;
+
+        validateSliceLarge(cfStore);
+    }
+
+    private void validateSliceLarge(ColumnFamilyStore cfStore) throws IOException
+    {
+        DecoratedKey key = Util.dk("row3");
+        ColumnFamily cf;
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("col1000"), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 3);
+        assertColumns(cf, "col1000", "col1001", "col1002");
+        
+        ByteBuffer col; 
+        col = cf.getColumn(ByteBufferUtil.bytes("col1000")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1000");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1001")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1001");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1002")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1002");
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("col1195"), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 3);
+        assertColumns(cf, "col1195", "col1196", "col1197");
+        
+        col = cf.getColumn(ByteBufferUtil.bytes("col1195")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1195");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1196")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1196");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1197")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1197");
+        
+       
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("col1996"), ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 1000);
+        IColumn[] columns = cf.getSortedColumns().toArray(new IColumn[0]);
+        for (int i = 1000; i < 1996; i++)
+        {
+            String expectedName = "col" + i;
+            IColumn column = columns[i - 1000];
+            assertEquals(ByteBufferUtil.string(column.name()), expectedName);
+            assertEquals(ByteBufferUtil.string(column.value()), ("v" + i));
+        }
+
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("col1990"), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 3);
+        assertColumns(cf, "col1990", "col1991", "col1992");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1990")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1990");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1991")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1991");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1992")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1992");
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 3);
+        assertColumns(cf, "col1997", "col1998", "col1999");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1997")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1997");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1998")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1998");
+        col = cf.getColumn(ByteBufferUtil.bytes("col1999")).value();
+        assertEquals(ByteBufferUtil.string(col), "v1999");
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("col9000"), ByteBufferUtil.EMPTY_BYTE_BUFFER, true, 3);
+        assertColumns(cf, "col1997", "col1998", "col1999");
+
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), ByteBufferUtil.bytes("col9000"), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 3);
+        assertColumns(cf);
+    }
+
+    @Test
+    public void testGetSliceFromSuperBasic() throws Throwable
+    {
+        // tests slicing against data from one row spread across two sstables
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Super1");
+        final DecoratedKey ROW = Util.dk("row2");
+
+        RowMutation rm = new RowMutation("Keyspace1", ROW.key);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Super1");
+        SuperColumn sc = new SuperColumn(ByteBufferUtil.bytes("sc1"), LongType.instance);
+        sc.addColumn(new Column(getBytes(1), ByteBufferUtil.bytes("val1"), 1L));
+        cf.addColumn(sc);
+        rm.add(cf);
+        rm.apply();
+
+        Runnable verify = new WrappedRunnable()
+        {
+            public void runMayThrow() throws Exception
+            {
+                ColumnFamily cf = cfStore.getColumnFamily(ROW, new QueryPath("Super1"), ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 10);
+                assertColumns(cf, "sc1");
+                
+                ByteBuffer val = cf.getColumn(ByteBufferUtil.bytes("sc1")).getSubColumn(getBytes(1)).value();
+                
+                assertEquals(ByteBufferUtil.string(val), "val1");
+            }
+        };
+
+        reTest(table.getColumnFamilyStore("Standard1"), verify);
+    }
+
+    public static void assertColumns(ColumnFamily cf, String... columnNames)
+    {
+        Collection<IColumn> columns = cf == null ? new TreeSet<IColumn>() : cf.getSortedColumns();
+        List<String> L = new ArrayList<String>();
+        for (IColumn column : columns)
+        {
+            try
+            {
+                L.add(ByteBufferUtil.string(column.name()));
+            }
+            catch (CharacterCodingException e)
+            {
+                throw new AssertionError(e);
+            }
+        }
+
+        List<String> names = new ArrayList<String>(columnNames.length);
+
+        names.addAll(Arrays.asList(columnNames));
+
+        String[] columnNames1 = names.toArray(new String[0]);
+        String[] la = L.toArray(new String[columns.size()]);
+        StringBuffer lasb = new StringBuffer();
+        for (String l: la)
+        {
+            lasb.append(l);
+            lasb.append(", ");
+        }
+
+        assert Arrays.equals(la, columnNames1)
+                : String.format("Columns [%s(as string: %s)])] is not expected [%s]",
+                                ((cf == null) ? "" : cf.getComparator().getColumnsString(columns)),
+                                lasb.toString(),
+                                StringUtils.join(columnNames1, ","));
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/TimeSortTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/TimeSortTest.java
index 3f24d583..b583893f 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/TimeSortTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/TimeSortTest.java
@@ -1 +1,137 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+import java.util.*;
+
+import org.junit.Test;
+import static org.junit.Assert.assertEquals;
+
+import org.apache.cassandra.CleanupHelper;
+import static org.apache.cassandra.Util.getBytes;
+import org.apache.cassandra.Util;
+
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+
+public class TimeSortTest extends CleanupHelper
+{
+    @Test
+    public void testMixedSources() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfStore = table.getColumnFamilyStore("StandardLong1");
+        RowMutation rm;
+        DecoratedKey key = Util.dk("key0");
+
+        rm = new RowMutation("Keyspace1", key.key);
+        rm.add(new QueryPath("StandardLong1", null, getBytes(100)), ByteBufferUtil.bytes("a"), 100);
+        rm.apply();
+        cfStore.forceBlockingFlush();
+
+        rm = new RowMutation("Keyspace1", key.key);
+        rm.add(new QueryPath("StandardLong1", null, getBytes(0)), ByteBufferUtil.bytes("b"), 0);
+        rm.apply();
+
+        ColumnFamily cf = cfStore.getColumnFamily(key, new QueryPath("StandardLong1"), getBytes(10), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1000);
+        Collection<IColumn> columns = cf.getSortedColumns();
+        assert columns.size() == 1;
+    }
+
+    @Test
+    public void testTimeSort() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfStore = table.getColumnFamilyStore("StandardLong1");
+
+        for (int i = 900; i < 1000; ++i)
+        {
+            RowMutation rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes(Integer.toString(i)));
+            for (int j = 0; j < 8; ++j)
+            {
+                rm.add(new QueryPath("StandardLong1", null, getBytes(j * 2)), ByteBufferUtil.bytes("a"), j * 2);
+            }
+            rm.apply();
+        }
+
+        validateTimeSort(table);
+
+        cfStore.forceBlockingFlush();
+        validateTimeSort(table);
+
+        // interleave some new data to test memtable + sstable
+        DecoratedKey key = Util.dk("900");
+        RowMutation rm = new RowMutation("Keyspace1", key.key);
+        for (int j = 0; j < 4; ++j)
+        {
+            rm.add(new QueryPath("StandardLong1", null, getBytes(j * 2 + 1)), ByteBufferUtil.bytes("b"), j * 2 + 1);
+        }
+        rm.apply();
+        // and some overwrites
+        rm = new RowMutation("Keyspace1", key.key);
+        rm.add(new QueryPath("StandardLong1", null, getBytes(0)), ByteBufferUtil.bytes("c"), 100);
+        rm.add(new QueryPath("StandardLong1", null, getBytes(10)), ByteBufferUtil.bytes("c"), 100);
+        rm.apply();
+
+        // verify
+        ColumnFamily cf = cfStore.getColumnFamily(key, new QueryPath("StandardLong1"), getBytes(0), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1000);
+        Collection<IColumn> columns = cf.getSortedColumns();
+        assertEquals(12, columns.size());
+        Iterator<IColumn> iter = columns.iterator();
+        IColumn column;
+        for (int j = 0; j < 8; j++)
+        {
+            column = iter.next();
+            assert column.name().equals(getBytes(j));
+        }
+        TreeSet<ByteBuffer> columnNames = new TreeSet<ByteBuffer>(LongType.instance);
+        columnNames.add(getBytes(10));
+        columnNames.add(getBytes(0));
+        cf = cfStore.getColumnFamily(QueryFilter.getNamesFilter(Util.dk("900"), new QueryPath("StandardLong1"), columnNames));
+        assert "c".equals(new String(cf.getColumn(getBytes(0)).value().array()));
+        assert "c".equals(new String(cf.getColumn(getBytes(10)).value().array()));
+    }
+
+    private void validateTimeSort(Table table) throws IOException
+    {
+        for (int i = 900; i < 1000; ++i)
+        {
+            DecoratedKey key = Util.dk(Integer.toString(i));
+            for (int j = 0; j < 8; j += 3)
+            {
+                ColumnFamily cf = table.getColumnFamilyStore("StandardLong1").getColumnFamily(key, new QueryPath("StandardLong1"), getBytes(j * 2), ByteBufferUtil.EMPTY_BYTE_BUFFER, false, 1000);
+                Collection<IColumn> columns = cf.getSortedColumns();
+                assert columns.size() == 8 - j;
+                int k = j;
+                for (IColumn c : columns)
+                {
+                    assertEquals((k++) * 2, c.timestamp());
+
+                }
+            }
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/commitlog/CommitLogHeaderTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/commitlog/CommitLogHeaderTest.java
index 3f24d583..dd7ed6c5 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/commitlog/CommitLogHeaderTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/commitlog/CommitLogHeaderTest.java
@@ -1 +1,51 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db.commitlog;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.junit.Test;
+
+import org.apache.cassandra.SchemaLoader;
+
+public class CommitLogHeaderTest extends SchemaLoader
+{
+    
+    @Test
+    public void testEmptyHeader()
+    {
+        CommitLogHeader clh = new CommitLogHeader();
+        assert clh.getReplayPosition() < 0;
+    }
+    
+    @Test
+    public void lowestPositionWithZero()
+    {
+        CommitLogHeader clh = new CommitLogHeader();
+        clh.turnOn(2, 34);
+        assert clh.getReplayPosition() == 34;
+        clh.turnOn(100, 0);
+        assert clh.getReplayPosition() == 0;
+        clh.turnOn(65, 2);
+        assert clh.getReplayPosition() == 0;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/IntegerTypeTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/IntegerTypeTest.java
index e69de29b..0eafc4ad 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/IntegerTypeTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/IntegerTypeTest.java
@@ -0,0 +1,196 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.cassandra.db.marshal;
+
+import static org.junit.Assert.assertArrayEquals;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Random;
+
+import org.junit.ComparisonFailure;
+import org.junit.Test;
+
+public class IntegerTypeTest
+{
+    private static void assertSignum(String message, int expected, double value)
+    {
+        int signum = (int)Math.signum(value);
+        if (signum != expected)
+            throw new ComparisonFailure(message, Integer.toString(expected), Integer.toString(signum));
+    }
+
+    private final IntegerType comparator = IntegerType.instance;
+
+    @Test
+    public void testTrimming()
+    {
+        ByteBuffer n1, n2;
+        n1 = ByteBuffer.wrap(new byte[] {0});
+        n2 = ByteBuffer.wrap(new byte[] {0, 0, 0, 0});
+        assertEquals(0, comparator.compare(n1, n2));
+        n1 = ByteBuffer.wrap(new byte[] {1, 0, 0, 1});
+        n2 = ByteBuffer.wrap(new byte[] {0, 0, 0, 1, 0, 0, 1});
+        assertEquals(0, comparator.compare(n1, n2));
+        n1 = ByteBuffer.wrap(new byte[] {-1, 0, 0, -1 });
+        n2 = ByteBuffer.wrap(new byte[] {-1, -1, -1, -1, 0, 0, -1});
+        assertEquals(0, comparator.compare(n1, n2));
+        n1 = ByteBuffer.wrap(new byte[] {-1, 0});
+        n2 = ByteBuffer.wrap(new byte[] {0, -1, 0});
+        assertSignum("", -1, comparator.compare(n1, n2));
+        n1 = ByteBuffer.wrap(new byte[] {1, 0});
+        n2 = ByteBuffer.wrap(new byte[] {0, -1, 0});
+        assertSignum("", -1, comparator.compare(n1, n2));
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void testNullLeft()
+    {
+        comparator.compare(null, ByteBuffer.wrap(new byte[1]));
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void testNullRight()
+    {
+        comparator.compare(ByteBuffer.wrap(new byte[1]), null);
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void testNullBoth()
+    {
+        comparator.compare(null, null);
+    }
+
+    @Test
+    public void testZeroLengthArray()
+    {
+        assertSignum("0-1", -1, comparator.compare(ByteBuffer.wrap(new byte[0]), ByteBuffer.wrap(new byte[1])));
+        assertSignum("1-0", 1, comparator.compare(ByteBuffer.wrap(new byte[1]), ByteBuffer.wrap(new byte[0])));
+        assertSignum("0-0", 0, comparator.compare(ByteBuffer.wrap(new byte[0]), ByteBuffer.wrap(new byte[0])));
+    }
+
+    @Test
+    public void testSanity()
+    {
+        ByteBuffer nN = ByteBuffer.wrap(new byte[] {-1});
+        ByteBuffer nZ = ByteBuffer.wrap(new byte[] {0});
+        ByteBuffer nP = ByteBuffer.wrap(new byte[] {1});
+        assertSignum("ZN", 1, comparator.compare(nZ, nN));
+        assertSignum("NZ", -1, comparator.compare(nN, nZ));
+        assertSignum("ZP", -1, comparator.compare(nZ, nP));
+        assertSignum("PZ", 1, comparator.compare(nP, nZ));
+        assertSignum("PN", 1, comparator.compare(nP, nN));
+        assertSignum("NP", -1, comparator.compare(nN, nP));
+    }
+
+    @Test
+    public void testSameLength()
+    {
+        ByteBuffer n1 = ByteBuffer.wrap(new byte[] {-2, 2, -4, -5});
+        ByteBuffer n2 = ByteBuffer.wrap(new byte[] {-2, 3, -5, -4});
+        ByteBuffer p1 = ByteBuffer.wrap(new byte[] {2, 3, -4, -5});
+        ByteBuffer p2 = ByteBuffer.wrap(new byte[] {2, -2, -5, -4});
+
+        assertSignum("n1n2", -1, comparator.compare(n1, n2));
+        assertSignum("n2n1", 1, comparator.compare(n2, n1));
+
+        assertSignum("p1p2", -1, comparator.compare(p1, p2));
+        assertSignum("p2p1", 1, comparator.compare(p2, p1));
+
+        assertSignum("p1n1", 1, comparator.compare(p1, n1));
+        assertSignum("p1n2", 1, comparator.compare(p1, n2));
+        assertSignum("n1p1", -1, comparator.compare(n1, p1));
+        assertSignum("n2p1", -1, comparator.compare(n2, p1));
+    }
+
+    @Test
+    public void testCommonPrefix()
+    {
+        ByteBuffer[] data = {
+                ByteBuffer.wrap(new byte[]{1, 0, 0, 1}),
+                ByteBuffer.wrap(new byte[]{1, 0, 0, 1, 0}),
+                ByteBuffer.wrap(new byte[]{1, 0, 0, 1}),
+                ByteBuffer.wrap(new byte[]{1, 0, 0, 1, 0}),
+                ByteBuffer.wrap(new byte[]{-1, 0, 0, 1}),
+                ByteBuffer.wrap(new byte[]{-1, 0, 0, 1, 0}),
+                ByteBuffer.wrap(new byte[]{-1, 0, 0, 1}),
+                ByteBuffer.wrap(new byte[]{-1, 0, 0, 1, 0})
+        };
+
+        Arrays.sort(data, comparator);
+        assertArrayEquals(new byte[]{-1, 0, 0, 1, 0}, data[0].array());
+        assertArrayEquals(new byte[]{-1, 0, 0, 1, 0},data[1].array());
+        assertArrayEquals(new byte[]{-1, 0, 0, 1},data[2].array());
+        assertArrayEquals(new byte[]{-1, 0, 0, 1},data[3].array());
+        assertArrayEquals(new byte[]{1, 0, 0, 1},data[4].array());
+        assertArrayEquals(new byte[]{1, 0, 0, 1},data[5].array());
+        assertArrayEquals(new byte[]{1, 0, 0, 1, 0},data[6].array());
+        assertArrayEquals(new byte[]{1, 0, 0, 1, 0},data[7].array());
+    }
+
+    @Test
+    public void testSorting()
+    {
+        ByteBuffer[] data = {
+                ByteBuffer.wrap(new byte[]{ 1, 0, 0, 0}),
+                ByteBuffer.wrap(new byte[]{-2, 0, 0}),
+                ByteBuffer.wrap(new byte[]{ 3, 0}),
+                ByteBuffer.wrap(new byte[]{-4}),
+                ByteBuffer.wrap(new byte[]{ 4}),
+                ByteBuffer.wrap(new byte[]{-3, 0}),
+                ByteBuffer.wrap(new byte[]{ 2, 0, 0}),
+                ByteBuffer.wrap(new byte[]{-1, 0, 0, 0})
+        };
+
+        Arrays.sort(data, comparator);
+        assertArrayEquals("-1", new byte[] {-1, 0, 0, 0}, data[0].array());
+        assertArrayEquals("-2", new byte[] {-2, 0, 0}, data[1].array());
+        assertArrayEquals("-3", new byte[] {-3, 0}, data[2].array());
+        assertArrayEquals("-4", new byte[] {-4}, data[3].array());
+        assertArrayEquals(" 4", new byte[] { 4}, data[4].array());
+        assertArrayEquals(" 3", new byte[] { 3, 0}, data[5].array());
+        assertArrayEquals(" 2", new byte[] { 2, 0, 0}, data[6].array());
+        assertArrayEquals(" 1", new byte[] { 1, 0, 0, 0}, data[7].array());
+    }
+
+    @Test
+    public void testSortingSpecialExtendedVersion()
+    {
+        Random rng = new Random(-9078270684023566599L);
+
+        ByteBuffer[] data = new ByteBuffer[10000];
+        for (int i = 0; i < data.length; i++)
+        {
+            data[i] = ByteBuffer.allocate(rng.nextInt(32) + 1);
+            rng.nextBytes(data[i].array());
+        }
+
+        Arrays.sort(data, comparator);
+
+        for (int i = 1; i < data.length; i++)
+        {
+            BigInteger i0 = new BigInteger(data[i - 1].array());
+            BigInteger i1 = new BigInteger(data[i].array());
+            assertTrue("#" + i, i0.compareTo(i1) <= 0);
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TimeUUIDTypeTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TimeUUIDTypeTest.java
index 3f24d583..522c880b 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TimeUUIDTypeTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TimeUUIDTypeTest.java
@@ -1 +1,120 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db.marshal;
+
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Random;
+
+import org.junit.Test;
+import static org.junit.Assert.assertEquals;
+
+import org.apache.cassandra.utils.UUIDGen;
+import org.safehaus.uuid.UUID;
+import org.safehaus.uuid.UUIDGenerator;
+
+public class TimeUUIDTypeTest
+{
+    TimeUUIDType timeUUIDType = new TimeUUIDType();
+    UUIDGenerator generator = UUIDGenerator.getInstance();
+
+    @Test
+    public void testEquality()
+    {
+        UUID a = generator.generateTimeBasedUUID();
+        UUID b = new UUID(a.asByteArray());
+
+        timeUUIDType.validate(ByteBuffer.wrap(a.asByteArray()));
+        timeUUIDType.validate(ByteBuffer.wrap(b.asByteArray()));
+        assertEquals(0, timeUUIDType.compare(ByteBuffer.wrap(a.asByteArray()), ByteBuffer.wrap(b.asByteArray())));
+    }
+
+    @Test
+    public void testSmaller()
+    {
+        UUID a = generator.generateTimeBasedUUID();
+        UUID b = generator.generateTimeBasedUUID();
+        UUID c = generator.generateTimeBasedUUID();
+
+        timeUUIDType.validate(ByteBuffer.wrap(a.asByteArray()));
+        timeUUIDType.validate(ByteBuffer.wrap(b.asByteArray()));
+        timeUUIDType.validate(ByteBuffer.wrap(c.asByteArray()));
+        
+        assert timeUUIDType.compare(ByteBuffer.wrap(a.asByteArray()), ByteBuffer.wrap(b.asByteArray())) < 0;
+        assert timeUUIDType.compare(ByteBuffer.wrap(b.asByteArray()), ByteBuffer.wrap(c.asByteArray())) < 0;
+        assert timeUUIDType.compare(ByteBuffer.wrap(a.asByteArray()), ByteBuffer.wrap(c.asByteArray())) < 0;
+    }
+
+    @Test
+    public void testBigger()
+    {
+        UUID a = generator.generateTimeBasedUUID();
+        UUID b = generator.generateTimeBasedUUID();
+        UUID c = generator.generateTimeBasedUUID();
+        
+        timeUUIDType.validate(ByteBuffer.wrap(a.asByteArray()));
+        timeUUIDType.validate(ByteBuffer.wrap(b.asByteArray()));
+        timeUUIDType.validate(ByteBuffer.wrap(c.asByteArray()));
+
+        assert timeUUIDType.compare(ByteBuffer.wrap(c.asByteArray()), ByteBuffer.wrap(b.asByteArray())) > 0;
+        assert timeUUIDType.compare(ByteBuffer.wrap(b.asByteArray()), ByteBuffer.wrap(a.asByteArray())) > 0;
+        assert timeUUIDType.compare(ByteBuffer.wrap(c.asByteArray()), ByteBuffer.wrap(a.asByteArray())) > 0;
+    }
+
+    @Test
+    public void testTimestampComparison()
+    {
+        Random rng = new Random();
+        ByteBuffer[] uuids = new ByteBuffer[100];
+        for (int i = 0; i < uuids.length; i++)
+        {
+            uuids[i] = ByteBuffer.allocate(16);
+            rng.nextBytes(uuids[i].array());
+            // set version to 1
+            uuids[i].array()[6] &= 0x0F;
+            uuids[i].array()[6] |= 0x10;
+        }
+        Arrays.sort(uuids, timeUUIDType);
+        for (int i = 1; i < uuids.length; i++)
+        {
+            long i0 = UUIDGen.getUUID(uuids[i - 1]).timestamp();
+            long i1 = UUIDGen.getUUID(uuids[i]).timestamp();
+            assert i0 <= i1;
+        }
+    }
+    
+    @Test
+    public void testValidTimeVersion()
+    {
+        java.util.UUID uuid1 = java.util.UUID.fromString("00000000-0000-1000-0000-000000000000");
+        assert uuid1.version() == 1;
+        timeUUIDType.validate(ByteBuffer.wrap(UUIDGen.decompose(uuid1)));
+    }
+    
+    @Test(expected = MarshalException.class)
+    public void testInvalidTimeVersion()
+    {
+        java.util.UUID uuid2 = java.util.UUID.fromString("00000000-0000-2100-0000-000000000000");
+        assert uuid2.version() == 2;
+        timeUUIDType.validate(ByteBuffer.wrap(UUIDGen.decompose(uuid2)));
+    }
+    
+    
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TypeCompareTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TypeCompareTest.java
index e69de29b..96502d2b 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TypeCompareTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TypeCompareTest.java
@@ -0,0 +1,111 @@
+package org.apache.cassandra.db.marshal;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Random;
+import java.util.UUID;
+
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+import org.junit.Test;
+
+public class TypeCompareTest
+{
+    @Test
+    public void testAscii()
+    {
+        AsciiType comparator = new AsciiType();
+        assert comparator.compare(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.bytes("asdf")) < 0;
+        assert comparator.compare(ByteBufferUtil.bytes("asdf"), ByteBufferUtil.EMPTY_BYTE_BUFFER) > 0;
+        assert comparator.compare(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER) == 0;
+        assert comparator.compare(ByteBufferUtil.bytes("z"), ByteBufferUtil.bytes("a")) > 0;
+        assert comparator.compare(ByteBufferUtil.bytes("a"), ByteBufferUtil.bytes("z")) < 0;
+        assert comparator.compare(ByteBufferUtil.bytes("asdf"), ByteBufferUtil.bytes("asdf")) == 0;
+        assert comparator.compare(ByteBufferUtil.bytes("asdz"), ByteBufferUtil.bytes("asdf")) > 0;
+    }
+
+    @Test
+    public void testBytes()
+    {
+        BytesType comparator = new BytesType();
+        assert comparator.compare(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.bytes("asdf")) < 0;
+        assert comparator.compare(ByteBufferUtil.bytes("asdf"), ByteBufferUtil.EMPTY_BYTE_BUFFER) > 0;
+        assert comparator.compare(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER) == 0;
+        assert comparator.compare(ByteBufferUtil.bytes("z"), ByteBufferUtil.bytes("a")) > 0;
+        assert comparator.compare(ByteBufferUtil.bytes("a"), ByteBufferUtil.bytes("z")) < 0;
+        assert comparator.compare(ByteBufferUtil.bytes("asdf"), ByteBufferUtil.bytes("asdf")) == 0;
+        assert comparator.compare(ByteBufferUtil.bytes("asdz"), ByteBufferUtil.bytes("asdf")) > 0;
+    }
+
+    @Test
+    public void testUTF8()
+    {
+        UTF8Type comparator = new UTF8Type();
+        assert comparator.compare(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.bytes("asdf")) < 0;
+        assert comparator.compare(ByteBufferUtil.bytes("asdf"), ByteBufferUtil.EMPTY_BYTE_BUFFER) > 0;
+        assert comparator.compare(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER) == 0;
+        assert comparator.compare(ByteBufferUtil.bytes("z"), ByteBufferUtil.bytes("a")) > 0;
+        assert comparator.compare(ByteBufferUtil.bytes("z"), ByteBufferUtil.bytes("z")) == 0;
+        assert comparator.compare(ByteBufferUtil.bytes("a"), ByteBufferUtil.bytes("z")) < 0;
+    }
+
+    @Test
+    public void testLong()
+    {
+        Random rng = new Random();
+        ByteBuffer[] data = new ByteBuffer[1000];
+        for (int i = 0; i < data.length; i++)
+        {
+            data[i] = ByteBuffer.allocate(8);
+            rng.nextBytes(data[i].array());
+        }
+
+        Arrays.sort(data, LongType.instance);
+
+        for (int i = 1; i < data.length; i++)
+        {
+        	
+            long l0 = data[i - 1].getLong(data[i - 1].position());
+            long l1 = data[i].getLong(data[i].position());
+            assert l0 <= l1;
+        }
+    }
+
+    @Test
+    public void testTimeUUID()
+    {
+        // two different UUIDs w/ the same timestamp
+        UUID uuid1 = UUID.fromString("1077e700-c7f2-11de-86d5-f5bcc793a028");
+        byte[] bytes1 = new byte[16];
+        ByteBuffer bb1 = ByteBuffer.wrap(bytes1);
+        bb1.putLong(uuid1.getMostSignificantBits());  bb1.putLong(uuid1.getLeastSignificantBits());
+
+        UUID uuid2 = UUID.fromString("1077e700-c7f2-11de-982e-6fad363d5f29");
+        byte[] bytes2 = new byte[16];
+        ByteBuffer bb2 = ByteBuffer.wrap(bytes2);
+        bb2.putLong(uuid2.getMostSignificantBits());  bb2.putLong(uuid2.getLeastSignificantBits());
+
+        assert new TimeUUIDType().compare(ByteBuffer.wrap(bytes1), ByteBuffer.wrap(bytes2)) != 0;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TypeValidationTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TypeValidationTest.java
index 3f24d583..b9018775 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TypeValidationTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/marshal/TypeValidationTest.java
@@ -1 +1,124 @@
   + native
+package org.apache.cassandra.db.marshal;
+
+import com.google.common.base.Charsets;
+import org.apache.cassandra.Util;
+import org.junit.Test;
+import org.safehaus.uuid.UUIDGenerator;
+
+import java.io.UnsupportedEncodingException;
+import java.nio.ByteBuffer;
+import java.nio.CharBuffer;
+import java.util.Random;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+public class TypeValidationTest
+{
+    @Test(expected = MarshalException.class)
+    public void testInvalidAscii()
+    {
+        AsciiType.instance.validate(ByteBuffer.wrap(new byte[]{ (byte)0x80 }));
+    }
+    
+    @Test(expected = MarshalException.class)
+    public void testInvalidTimeUUID()
+    {
+        org.safehaus.uuid.UUID uuid = UUIDGenerator.getInstance().generateRandomBasedUUID();
+        TimeUUIDType.instance.validate(ByteBuffer.wrap(uuid.toByteArray()));
+    }
+    
+    @Test 
+    public void testValidTimeUUID()
+    {
+        org.safehaus.uuid.UUID uuid = UUIDGenerator.getInstance().generateTimeBasedUUID();
+        TimeUUIDType.instance.validate(ByteBuffer.wrap(uuid.toByteArray()));
+    }
+    
+    @Test
+    public void testLong()
+    {
+        LongType.instance.validate(Util.getBytes(5));
+        LongType.instance.validate(Util.getBytes(5555555555555555555L));
+    }
+    
+    @Test
+    public void testValidUtf8() throws UnsupportedEncodingException
+    {
+        assert Character.MAX_CODE_POINT == 0x0010ffff;
+        CharBuffer cb = CharBuffer.allocate(2837314);
+        // let's test all of the unicode space.
+        for (int i = 0; i < Character.MAX_CODE_POINT; i++)
+        {
+            // skip U+D800..U+DFFF. those CPs are invalid in utf8. java tolerates them, but doesn't convert them to
+            // valid byte sequences (gives us '?' instead), so there is no point testing them.
+            if (i >= 55296 && i <= 57343)
+                continue;
+            char[] ch = Character.toChars(i);
+            for (char c : ch)
+                cb.append(c);
+        }
+        String s = new String(cb.array());
+        byte[] arr = s.getBytes("UTF8");
+        ByteBuffer buf = ByteBuffer.wrap(arr);
+        UTF8Type.instance.validate(buf);
+        
+        // some you might not expect.
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {}));
+        // valid Utf8, unspecified in modified utf8.
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {0}));
+        
+        // modified utf8 null.
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {99, (byte)0xc0, (byte)0x80, 112}));
+        
+        // edges, for my sanity.
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {(byte)0xc2, (byte)0x81}));
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {(byte)0xe0, (byte)0xa0, (byte)0x81}));
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {(byte)0xf0, (byte)0x90, (byte)0x81, (byte)0x81}));
+    }
+    
+    // now test for bogies.
+    
+    @Test(expected = MarshalException.class)
+    public void testFloatingc0()
+    {
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {99, (byte)0xc0, 112}));
+    }
+    
+    @Test(expected = MarshalException.class)
+    public void testInvalid2nd()
+    {
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {(byte)0xc2, (byte)0xff}));
+    }
+    
+    @Test(expected = MarshalException.class)
+    public void testInvalid3rd()
+    {
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {(byte)0xe0, (byte)0xa0, (byte)0xff}));
+    }
+    
+    @Test(expected = MarshalException.class)
+    public void testInvalid4th()
+    {
+        UTF8Type.instance.validate(ByteBuffer.wrap(new byte[] {(byte)0xf0, (byte)0x90, (byte)0x81, (byte)0xff}));
+    }
+    
+    // todo: for completeness, should test invalid two byte pairs.
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/migration/SerializationsTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/migration/SerializationsTest.java
index e69de29b..a0ffe350 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/migration/SerializationsTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/db/migration/SerializationsTest.java
@@ -0,0 +1,80 @@
+package org.apache.cassandra.db.migration;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import org.apache.cassandra.AbstractSerializationsTester;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.config.KSMetaData;
+import org.apache.cassandra.io.SerDeUtils;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.UUIDGen;
+import org.apache.commons.codec.binary.Base64;
+import org.junit.Test;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.nio.ByteBuffer;
+import java.util.UUID;
+
+public class SerializationsTest extends AbstractSerializationsTester
+{
+    private static final int ksCount = 5;
+    
+    private void testWrite() throws IOException, ConfigurationException
+    {
+        for (int i = 0; i < ksCount; i++)
+        {
+            String tableName = "Keyspace" + (i + 1);
+            KSMetaData ksm = DatabaseDescriptor.getKSMetaData(tableName);
+            UUID uuid = UUIDGen.makeType1UUIDFromHost(FBUtilities.getLocalAddress());
+            DatabaseDescriptor.clearTableDefinition(ksm, uuid);
+            Migration m = new AddKeyspace(ksm);
+            ByteBuffer bytes = m.serialize();
+            
+            DataOutputStream out = getOutput("db.migration." + tableName + ".bin");
+            out.writeUTF(new String(Base64.encodeBase64(bytes.array())));
+            out.close();
+        }
+    }
+    
+    @Test
+    public void testRead() throws IOException, ConfigurationException
+    {
+        if (AbstractSerializationsTester.EXECUTE_WRITES)
+            testWrite();
+        
+        for (int i = 0; i < ksCount; i++)
+        {
+            String tableName = "Keyspace" + (i + 1);
+            DataInputStream in = getInput("db.migration." + tableName + ".bin");
+            byte[] raw = Base64.decodeBase64(in.readUTF().getBytes());
+            org.apache.cassandra.db.migration.avro.Migration obj = new org.apache.cassandra.db.migration.avro.Migration();
+            SerDeUtils.deserializeWithSchema(ByteBuffer.wrap(raw), obj);
+            in.close();
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/BootStrapperTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/BootStrapperTest.java
index e69de29b..756d8498 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/BootStrapperTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/BootStrapperTest.java
@@ -0,0 +1,211 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.dht;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.commons.lang.StringUtils;
+import static org.junit.Assert.assertEquals;
+import org.junit.Test;
+
+import com.google.common.collect.Multimap;
+
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.IFailureDetectionEventListener;
+import org.apache.cassandra.gms.IFailureDetector;
+import org.apache.cassandra.locator.TokenMetadata;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class BootStrapperTest extends CleanupHelper
+{
+    @Test
+    public void testTokenRoundtrip() throws Exception
+    {
+        StorageService.instance.initServer();
+        // fetch a bootstrap token from the local node
+        assert BootStrapper.getBootstrapTokenFrom(FBUtilities.getLocalAddress()) != null;
+    }
+    
+    @Test
+    public void testMulitipleAutomaticBootstraps() throws IOException
+    {
+        StorageService ss = StorageService.instance;
+        generateFakeEndpoints(5);
+        InetAddress[] addrs = new InetAddress[] 
+        {
+            InetAddress.getByName("127.0.0.2"),  
+            InetAddress.getByName("127.0.0.3"),  
+            InetAddress.getByName("127.0.0.4"),  
+            InetAddress.getByName("127.0.0.5"),  
+        };
+        InetAddress[] bootstrapAddrs = new InetAddress[]
+        {
+            InetAddress.getByName("127.0.0.12"),  
+            InetAddress.getByName("127.0.0.13"),  
+            InetAddress.getByName("127.0.0.14"),  
+            InetAddress.getByName("127.0.0.15"),  
+        };
+        Map<InetAddress, Double> load = new HashMap<InetAddress, Double>();
+        for (int i = 0; i < addrs.length; i++)
+            load.put(addrs[i], (double)i+2);
+        
+        // give every node a bootstrap source.
+        for (int i = 3; i >=0; i--)
+        {
+            InetAddress bootstrapSource = BootStrapper.getBootstrapSource(ss.getTokenMetadata(), load);
+            assert bootstrapSource != null;
+            assert bootstrapSource.equals(addrs[i]) : String.format("expected %s but got %s for %d", addrs[i], bootstrapSource, i);
+            assert !ss.getTokenMetadata().getBootstrapTokens().containsValue(bootstrapSource);
+            
+            Range range = ss.getPrimaryRangeForEndpoint(bootstrapSource);
+            Token token = StorageService.getPartitioner().midpoint(range.left, range.right);
+            assert range.contains(token);
+            ss.onChange(bootstrapAddrs[i], ApplicationState.STATUS, StorageService.instance.valueFactory.bootstrapping(token));
+        }
+        
+        // any further attempt to bootsrtap should fail since every node in the cluster is splitting.
+        try
+        {
+            BootStrapper.getBootstrapSource(ss.getTokenMetadata(), load);
+            throw new AssertionError("This bootstrap should have failed.");
+        }
+        catch (RuntimeException ex) 
+        {
+            // success!
+        }
+        
+        // indicate that one of the nodes is done. see if the node it was bootstrapping from is still available.
+        Range range = ss.getPrimaryRangeForEndpoint(addrs[2]);
+        Token token = StorageService.getPartitioner().midpoint(range.left, range.right);
+        ss.onChange(bootstrapAddrs[2], ApplicationState.STATUS, StorageService.instance.valueFactory.normal(token));
+        load.put(bootstrapAddrs[2], 0d);
+        InetAddress addr = BootStrapper.getBootstrapSource(ss.getTokenMetadata(), load);
+        assert addr != null && addr.equals(addrs[2]);
+    }
+
+    @Test
+    public void testGuessToken() throws IOException
+    {
+        StorageService ss = StorageService.instance;
+
+        generateFakeEndpoints(5);
+
+        InetAddress two = InetAddress.getByName("127.0.0.2");
+        InetAddress three = InetAddress.getByName("127.0.0.3");
+        InetAddress four = InetAddress.getByName("127.0.0.4");
+        InetAddress five = InetAddress.getByName("127.0.0.5");
+
+        Map<InetAddress, Double> load = new HashMap<InetAddress, Double>();
+        load.put(two, 2.0);
+        load.put(three, 3.0);
+        load.put(four, 4.0);
+        load.put(five, 5.0);
+
+        TokenMetadata tmd = ss.getTokenMetadata();
+        InetAddress source = BootStrapper.getBootstrapSource(tmd, load);
+        assert five.equals(source) : five + " != " + source;
+
+        InetAddress myEndpoint = InetAddress.getByName("127.0.0.1");
+        Range range5 = ss.getPrimaryRangeForEndpoint(five);
+        Token fakeToken = StorageService.getPartitioner().midpoint(range5.left, range5.right);
+        assert range5.contains(fakeToken);
+        ss.onChange(myEndpoint, ApplicationState.STATUS, StorageService.instance.valueFactory.bootstrapping(fakeToken));
+        tmd = ss.getTokenMetadata();
+
+        InetAddress source4 = BootStrapper.getBootstrapSource(tmd, load);
+        assert four.equals(source4) : four + " != " + source4;
+    }
+
+    @Test
+    public void testSourceTargetComputation() throws UnknownHostException
+    {
+        final int[] clusterSizes = new int[] { 1, 3, 5, 10, 100};
+        for (String table : DatabaseDescriptor.getNonSystemTables())
+        {
+            int replicationFactor = Table.open(table).getReplicationStrategy().getReplicationFactor();
+            for (int clusterSize : clusterSizes)
+                if (clusterSize >= replicationFactor)
+                    testSourceTargetComputation(table, clusterSize, replicationFactor);
+        }
+    }
+
+    private void testSourceTargetComputation(String table, int numOldNodes, int replicationFactor) throws UnknownHostException
+    {
+        StorageService ss = StorageService.instance;
+
+        generateFakeEndpoints(numOldNodes);
+        Token myToken = StorageService.getPartitioner().getRandomToken();
+        InetAddress myEndpoint = InetAddress.getByName("127.0.0.1");
+
+        TokenMetadata tmd = ss.getTokenMetadata();
+        assertEquals(numOldNodes, tmd.sortedTokens().size());
+        BootStrapper b = new BootStrapper(myEndpoint, myToken, tmd);
+        Multimap<Range, InetAddress> res = b.getRangesWithSources(table);
+        
+        int transferCount = 0;
+        for (Map.Entry<Range, Collection<InetAddress>> e : res.asMap().entrySet())
+        {
+            assert e.getValue() != null && e.getValue().size() > 0 : StringUtils.join(e.getValue(), ", ");
+            transferCount++;
+        }
+
+        assertEquals(replicationFactor, transferCount);
+        IFailureDetector mockFailureDetector = new IFailureDetector()
+        {
+            public boolean isAlive(InetAddress ep)
+            {
+                return true;
+            }
+
+            public void interpret(InetAddress ep) { throw new UnsupportedOperationException(); }
+            public void report(InetAddress ep) { throw new UnsupportedOperationException(); }
+            public void registerFailureDetectionEventListener(IFailureDetectionEventListener listener) { throw new UnsupportedOperationException(); }
+            public void unregisterFailureDetectionEventListener(IFailureDetectionEventListener listener) { throw new UnsupportedOperationException(); }
+            public void remove(InetAddress ep) { throw new UnsupportedOperationException(); }
+        };
+        Multimap<InetAddress, Range> temp = BootStrapper.getWorkMap(res, mockFailureDetector);
+        // there isn't any point in testing the size of these collections for any specific size.  When a random partitioner
+        // is used, they will vary.
+        assert temp.keySet().size() > 0;
+        assert temp.asMap().values().iterator().next().size() > 0;
+        assert !temp.keySet().iterator().next().equals(myEndpoint);
+    }
+
+    private void generateFakeEndpoints(int numOldNodes) throws UnknownHostException
+    {
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+        tmd.clearUnsafe();
+        IPartitioner<?> p = StorageService.getPartitioner();
+
+        for (int i = 1; i <= numOldNodes; i++)
+        {
+            // leave .1 for myEndpoint
+            tmd.updateNormalToken(p.getRandomToken(), InetAddress.getByName("127.0.0." + (i + 1)));
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/ByteOrderedPartitionerTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/ByteOrderedPartitionerTest.java
index 3f24d583..eaf5cae7 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/ByteOrderedPartitionerTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/ByteOrderedPartitionerTest.java
@@ -1 +1,33 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.dht;
+
+import org.junit.Test;
+
+import org.apache.cassandra.utils.FBUtilities;
+
+public class ByteOrderedPartitionerTest extends PartitionerTestCase<BytesToken>
+{
+    @Override
+    public void initPartitioner()
+    {
+        partitioner = new ByteOrderedPartitioner();
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/CollatingOrderPreservingPartitionerTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/CollatingOrderPreservingPartitionerTest.java
index 3f24d583..78231288 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/CollatingOrderPreservingPartitionerTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/CollatingOrderPreservingPartitionerTest.java
@@ -1 +1,58 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.dht;
+
+import java.nio.ByteBuffer;
+
+import org.junit.Test;
+
+import org.apache.cassandra.utils.FBUtilities;
+
+public class CollatingOrderPreservingPartitionerTest extends PartitionerTestCase<BytesToken>
+{
+    @Override
+    public void initPartitioner()
+    {
+        partitioner = new CollatingOrderPreservingPartitioner();
+    }
+
+    /**
+     * Test that a non-UTF-8 byte array can still be encoded.
+     */
+    @Test
+    public void testTokenFactoryStringsNonUTF()
+    {
+        Token.TokenFactory factory = this.partitioner.getTokenFactory();
+        BytesToken tok = new BytesToken(new byte[]{(byte)0xFF, (byte)0xFF});
+        assert tok.compareTo(factory.fromString(factory.toString(tok))) == 0;
+    }
+
+    @Test
+    public void testCompare()
+    {
+        assert tok("").compareTo(tok("asdf")) < 0;
+        assert tok("asdf").compareTo(tok("")) > 0;
+        assert tok("").compareTo(tok("")) == 0;
+        assert tok("z").compareTo(tok("a")) > 0;
+        assert tok("a").compareTo(tok("z")) < 0;
+        assert tok("asdf").compareTo(tok("asdf")) == 0;
+        assert tok("asdz").compareTo(tok("asdf")) > 0;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/OrderPreservingPartitionerTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/OrderPreservingPartitionerTest.java
index 3f24d583..829f6b99 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/OrderPreservingPartitionerTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/OrderPreservingPartitionerTest.java
@@ -1 +1,47 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.dht;
+
+import java.math.BigInteger;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import org.apache.cassandra.utils.FBUtilities;
+
+public class OrderPreservingPartitionerTest extends PartitionerTestCase<StringToken> {
+    @Override
+    public void initPartitioner()
+    {
+        partitioner = new OrderPreservingPartitioner();
+    }
+
+    @Test
+    public void testCompare()
+    {
+        assert tok("").compareTo(tok("asdf")) < 0;
+        assert tok("asdf").compareTo(tok("")) > 0;
+        assert tok("").compareTo(tok("")) == 0;
+        assert tok("z").compareTo(tok("a")) > 0;
+        assert tok("a").compareTo(tok("z")) < 0;
+        assert tok("asdf").compareTo(tok("asdf")) == 0;
+        assert tok("asdz").compareTo(tok("asdf")) > 0;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/PartitionerTestCase.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/PartitionerTestCase.java
index 3f24d583..79ea91a1 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/PartitionerTestCase.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/PartitionerTestCase.java
@@ -1 +1,118 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.dht;
+
+import java.nio.ByteBuffer;
+import java.util.Random;
+
+import static org.junit.Assert.assertEquals;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import org.apache.cassandra.db.DecoratedKey;
+
+public abstract class PartitionerTestCase<T extends Token>
+{
+    protected IPartitioner<T> partitioner;
+
+    public abstract void initPartitioner();
+
+    @Before
+    public void clean()
+    {
+        initPartitioner();
+    }
+
+    public T tok(byte[] key)
+    {
+        return partitioner.getToken(ByteBuffer.wrap(key));
+    }
+
+    public T tok(String key)
+    {
+        return tok(key.getBytes());
+    }
+
+    /**
+     * Recurses randomly to the given depth a few times.
+     */
+    public void assertMidpoint(T left, T right, int depth)
+    {
+        Random rand = new Random();
+        for (int i = 0; i < 1000; i++)
+        {
+            assertMidpoint(left, right, rand, depth);
+        }
+    }
+
+    private void assertMidpoint(Token left, Token right, Random rand, int depth)
+    {
+        Token mid = partitioner.midpoint(left, right);
+        assert new Range(left, right).contains(mid)
+                : "For " + left + "," + right + ": range did not contain mid:" + mid;
+        if (depth < 1)
+            return;
+
+        if (rand.nextBoolean())
+            assertMidpoint(left, mid, rand, depth-1);
+        else
+            assertMidpoint(mid, right, rand, depth-1);
+    }
+
+    @Test
+    public void testMidpoint()
+    {
+        assertMidpoint(tok("a"), tok("b"), 16);
+        assertMidpoint(tok("a"), tok("bbb"), 16);
+    }
+
+    @Test
+    public void testMidpointMinimum()
+    {
+        T mintoken = partitioner.getMinimumToken(); 
+        assert mintoken.compareTo(partitioner.midpoint(mintoken, mintoken)) != 0;
+        assertMidpoint(mintoken, tok("a"), 16);
+        assertMidpoint(mintoken, tok("aaa"), 16);
+        assertMidpoint(mintoken, mintoken, 126);
+        assertMidpoint(tok("a"), mintoken, 16);
+    }
+
+    @Test
+    public void testMidpointWrapping()
+    {
+        assertMidpoint(tok("b"), tok("a"), 16);
+        assertMidpoint(tok("bbb"), tok("a"), 16);
+    }
+    
+    @Test
+    public void testTokenFactoryBytes()
+    {
+        Token.TokenFactory factory = partitioner.getTokenFactory();
+        assert tok("a").compareTo(factory.fromByteArray(factory.toByteArray(tok("a")))) == 0;
+    }
+    
+    @Test
+    public void testTokenFactoryStrings()
+    {
+        Token.TokenFactory factory = partitioner.getTokenFactory();
+        assert tok("a").compareTo(factory.fromString(factory.toString(tok("a")))) == 0;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/RandomPartitionerTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/RandomPartitionerTest.java
index 3f24d583..9caff43a 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/RandomPartitionerTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/RandomPartitionerTest.java
@@ -1 +1,36 @@
   + native
+package org.apache.cassandra.dht;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import static org.junit.Assert.assertEquals;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.junit.Test;
+
+public class RandomPartitionerTest extends PartitionerTestCase<BigIntegerToken>
+{
+    public void initPartitioner()
+    {
+        partitioner = new RandomPartitioner();
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/RangeTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/RangeTest.java
index 3f24d583..30678e1a 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/RangeTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/dht/RangeTest.java
@@ -1 +1,318 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.dht;
+
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.commons.lang.StringUtils;
+
+import org.junit.Test;
+
+public class RangeTest
+{
+    @Test
+    public void testContains()
+    {
+        Range left = new Range(new BigIntegerToken("0"), new BigIntegerToken("100"));
+        assert !left.contains(new BigIntegerToken("0"));
+        assert left.contains(new BigIntegerToken("10"));
+        assert left.contains(new BigIntegerToken("100"));
+        assert !left.contains(new BigIntegerToken("101"));
+    }
+
+    @Test
+    public void testContainsWrapping()
+    {
+        Range range = new Range(new BigIntegerToken("0"), new BigIntegerToken("0"));
+        assert range.contains(new BigIntegerToken("0"));
+        assert range.contains(new BigIntegerToken("10"));
+        assert range.contains(new BigIntegerToken("100"));
+        assert range.contains(new BigIntegerToken("101"));
+
+        range = new Range(new BigIntegerToken("100"), new BigIntegerToken("0"));
+        assert range.contains(new BigIntegerToken("0"));
+        assert !range.contains(new BigIntegerToken("1"));
+        assert !range.contains(new BigIntegerToken("100"));
+        assert range.contains(new BigIntegerToken("200"));
+    }
+
+    @Test
+    public void testContainsRange()
+    {
+        Range one = new Range(new BigIntegerToken("2"), new BigIntegerToken("10"));
+        Range two = new Range(new BigIntegerToken("2"), new BigIntegerToken("5"));
+        Range thr = new Range(new BigIntegerToken("5"), new BigIntegerToken("10"));
+        Range fou = new Range(new BigIntegerToken("10"), new BigIntegerToken("12"));
+
+        assert one.contains(two);
+        assert one.contains(thr);
+        assert !one.contains(fou);
+
+        assert !two.contains(one);
+        assert !two.contains(thr);
+        assert !two.contains(fou);
+
+        assert !thr.contains(one);
+        assert !thr.contains(two);
+        assert !thr.contains(fou);
+
+        assert !fou.contains(one);
+        assert !fou.contains(two);
+        assert !fou.contains(thr);
+    }
+
+    @Test
+    public void testContainsRangeWrapping()
+    {
+        Range one = new Range(new BigIntegerToken("10"), new BigIntegerToken("2"));
+        Range two = new Range(new BigIntegerToken("5"), new BigIntegerToken("3"));
+        Range thr = new Range(new BigIntegerToken("10"), new BigIntegerToken("12"));
+        Range fou = new Range(new BigIntegerToken("2"), new BigIntegerToken("6"));
+        Range fiv = new Range(new BigIntegerToken("0"), new BigIntegerToken("0"));
+
+        assert !one.contains(two);
+        assert one.contains(thr);
+        assert !one.contains(fou);
+
+        assert two.contains(one);
+        assert two.contains(thr);
+        assert !two.contains(fou);
+
+        assert !thr.contains(one);
+        assert !thr.contains(two);
+        assert !thr.contains(fou);
+
+        assert !fou.contains(one);
+        assert !fou.contains(two);
+        assert !fou.contains(thr);
+
+        assert fiv.contains(one);
+        assert fiv.contains(two);
+        assert fiv.contains(thr);
+        assert fiv.contains(fou);
+    }
+
+    @Test
+    public void testContainsRangeOneWrapping()
+    {
+        Range wrap1 = new Range(new BigIntegerToken("0"), new BigIntegerToken("0"));
+        Range wrap2 = new Range(new BigIntegerToken("10"), new BigIntegerToken("2"));
+
+        Range nowrap1 = new Range(new BigIntegerToken("0"), new BigIntegerToken("2"));
+        Range nowrap2 = new Range(new BigIntegerToken("2"), new BigIntegerToken("10"));
+        Range nowrap3 = new Range(new BigIntegerToken("10"), new BigIntegerToken("100"));
+
+        assert wrap1.contains(nowrap1);
+        assert wrap1.contains(nowrap2);
+        assert wrap1.contains(nowrap3);
+
+        assert wrap2.contains(nowrap1);
+        assert !wrap2.contains(nowrap2);
+        assert wrap2.contains(nowrap3);
+    }
+
+    @Test
+    public void testIntersects()
+    {
+        Range all = new Range(new BigIntegerToken("0"), new BigIntegerToken("0")); // technically, this is a wrapping range
+        Range one = new Range(new BigIntegerToken("2"), new BigIntegerToken("10"));
+        Range two = new Range(new BigIntegerToken("0"), new BigIntegerToken("8"));
+        Range not = new Range(new BigIntegerToken("10"), new BigIntegerToken("12"));
+
+        assert all.intersects(one);
+        assert all.intersects(two);
+
+        assert one.intersects(two);
+        assert two.intersects(one);
+
+        assert !one.intersects(not);
+        assert !not.intersects(one);
+
+        assert !two.intersects(not);
+        assert !not.intersects(two);
+    }
+
+    @Test
+    public void testIntersectsWrapping()
+    {
+        Range onewrap = new Range(new BigIntegerToken("10"), new BigIntegerToken("2"));
+        Range onecomplement = new Range(onewrap.right, onewrap.left);
+        Range onestartswith = new Range(onewrap.left, new BigIntegerToken("12"));
+        Range oneendswith = new Range(new BigIntegerToken("1"), onewrap.right);
+        Range twowrap = new Range(new BigIntegerToken("5"), new BigIntegerToken("3"));
+        Range not = new Range(new BigIntegerToken("2"), new BigIntegerToken("6"));
+
+        assert !onewrap.intersects(onecomplement);
+        assert onewrap.intersects(onestartswith);
+        assert onewrap.intersects(oneendswith);
+
+        assert onewrap.intersects(twowrap);
+        assert twowrap.intersects(onewrap);
+
+        assert !onewrap.intersects(not);
+        assert !not.intersects(onewrap);
+
+        assert twowrap.intersects(not);
+        assert not.intersects(twowrap);
+    }
+
+    static void assertIntersection(Range one, Range two, Range ... ranges)
+    {
+        Set<Range> correct = Range.rangeSet(ranges);
+        Set<Range> result1 = one.intersectionWith(two);
+        assert result1.equals(correct) : String.format("%s != %s",
+                                                       StringUtils.join(result1, ","),
+                                                       StringUtils.join(correct, ","));
+        Set<Range> result2 = two.intersectionWith(one);
+        assert result2.equals(correct) : String.format("%s != %s",
+                                                       StringUtils.join(result2, ","),
+                                                       StringUtils.join(correct, ","));
+    }
+
+    private void assertNoIntersection(Range wraps1, Range nowrap3)
+    {
+        assertIntersection(wraps1, nowrap3);
+    }
+
+    @Test
+    public void testIntersectionWithAll()
+    {
+        Range all0 = new Range(new BigIntegerToken("0"), new BigIntegerToken("0"));
+        Range all10 = new Range(new BigIntegerToken("10"), new BigIntegerToken("10"));
+        Range all100 = new Range(new BigIntegerToken("100"), new BigIntegerToken("100"));
+        Range all1000 = new Range(new BigIntegerToken("1000"), new BigIntegerToken("1000"));
+        Range wraps = new Range(new BigIntegerToken("100"), new BigIntegerToken("10"));
+
+        assertIntersection(all0, wraps, wraps);
+        assertIntersection(all10, wraps, wraps);
+        assertIntersection(all100, wraps, wraps);
+        assertIntersection(all1000, wraps, wraps);
+    }
+
+    @Test
+    public void testIntersectionContains()
+    {
+        Range wraps1 = new Range(new BigIntegerToken("100"), new BigIntegerToken("10"));
+        Range wraps2 = new Range(new BigIntegerToken("90"), new BigIntegerToken("20"));
+        Range wraps3 = new Range(new BigIntegerToken("90"), new BigIntegerToken("0"));
+        Range nowrap1 = new Range(new BigIntegerToken("100"), new BigIntegerToken("110"));
+        Range nowrap2 = new Range(new BigIntegerToken("0"), new BigIntegerToken("10"));
+        Range nowrap3 = new Range(new BigIntegerToken("0"), new BigIntegerToken("9"));
+
+        assertIntersection(wraps1, wraps2, wraps1);
+        assertIntersection(wraps3, wraps2, wraps3);
+
+        assertIntersection(wraps1, nowrap1, nowrap1);
+        assertIntersection(wraps1, nowrap2, nowrap2);
+        assertIntersection(nowrap2, nowrap3, nowrap3);
+
+        assertIntersection(wraps1, wraps1, wraps1);
+        assertIntersection(nowrap1, nowrap1, nowrap1);
+        assertIntersection(nowrap2, nowrap2, nowrap2);
+        assertIntersection(wraps3, wraps3, wraps3);
+    }
+
+    @Test
+    public void testNoIntersection()
+    {
+        Range wraps1 = new Range(new BigIntegerToken("100"), new BigIntegerToken("10"));
+        Range wraps2 = new Range(new BigIntegerToken("100"), new BigIntegerToken("0"));
+        Range nowrap1 = new Range(new BigIntegerToken("0"), new BigIntegerToken("100"));
+        Range nowrap2 = new Range(new BigIntegerToken("100"), new BigIntegerToken("200"));
+        Range nowrap3 = new Range(new BigIntegerToken("10"), new BigIntegerToken("100"));
+
+        assertNoIntersection(wraps1, nowrap3);
+        assertNoIntersection(wraps2, nowrap1);
+        assertNoIntersection(nowrap1, nowrap2);
+    }
+
+    @Test
+    public void testIntersectionOneWraps()
+    {
+        Range wraps1 = new Range(new BigIntegerToken("100"), new BigIntegerToken("10"));
+        Range wraps2 = new Range(new BigIntegerToken("100"), new BigIntegerToken("0"));
+        Range nowrap1 = new Range(new BigIntegerToken("0"), new BigIntegerToken("200"));
+        Range nowrap2 = new Range(new BigIntegerToken("0"), new BigIntegerToken("100"));
+
+        assertIntersection(wraps1,
+                           nowrap1,
+                           new Range(new BigIntegerToken("0"), new BigIntegerToken("10")),
+                           new Range(new BigIntegerToken("100"), new BigIntegerToken("200")));
+        assertIntersection(wraps2,
+                           nowrap1,
+                           new Range(new BigIntegerToken("100"), new BigIntegerToken("200")));
+        assertIntersection(wraps1,
+                           nowrap2,
+                           new Range(new BigIntegerToken("0"), new BigIntegerToken("10")));
+    }
+
+    @Test
+    public void testIntersectionTwoWraps()
+    {
+        Range wraps1 = new Range(new BigIntegerToken("100"), new BigIntegerToken("20"));
+        Range wraps2 = new Range(new BigIntegerToken("120"), new BigIntegerToken("90"));
+        Range wraps3 = new Range(new BigIntegerToken("120"), new BigIntegerToken("110"));
+        Range wraps4 = new Range(new BigIntegerToken("10"), new BigIntegerToken("0"));
+        Range wraps5 = new Range(new BigIntegerToken("10"), new BigIntegerToken("1"));
+        Range wraps6 = new Range(new BigIntegerToken("30"), new BigIntegerToken("10"));
+
+        assertIntersection(wraps1,
+                           wraps2,
+                           new Range(new BigIntegerToken("120"), new BigIntegerToken("20")));
+        assertIntersection(wraps1,
+                           wraps3,
+                           new Range(new BigIntegerToken("120"), new BigIntegerToken("20")),
+                           new Range(new BigIntegerToken("100"), new BigIntegerToken("110")));
+        assertIntersection(wraps1,
+                           wraps4,
+                           new Range(new BigIntegerToken("10"), new BigIntegerToken("20")),
+                           new Range(new BigIntegerToken("100"), new BigIntegerToken("0")));
+        assertIntersection(wraps1,
+                           wraps5,
+                           new Range(new BigIntegerToken("10"), new BigIntegerToken("20")),
+                           new Range(new BigIntegerToken("100"), new BigIntegerToken("1")));
+        assertIntersection(wraps1,
+                           wraps6,
+                           new Range(new BigIntegerToken("100"), new BigIntegerToken("10")));
+    }
+
+    @Test
+    public void testByteTokensCompare()
+    {
+        Token t1 = new BytesToken(ByteBuffer.wrap(new byte[] { 1,2,3 }));
+        Token t2 = new BytesToken(ByteBuffer.wrap(new byte[] { 1,2,3 }));
+        Token t3 = new BytesToken(ByteBuffer.wrap(new byte[]{1, 2, 3, 4}));
+
+        assert Range.compare(t1, t2) == 0;
+        assert Range.compare(t1, t3) == -1;
+        assert Range.compare(t3, t1) == 1;
+        assert Range.compare(t1, t1) == 0;
+
+        Token t4 = new BytesToken(new byte[] { 1,2,3 });
+        Token t5 = new BytesToken(new byte[] { 4,5,6,7 });
+
+        assert Range.compare(t4, t5) == -1;
+        assert Range.compare(t5, t4) == 1;
+        assert Range.compare(t1, t4) == 0;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/ArrivalWindowTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/ArrivalWindowTest.java
index 3f24d583..c066ac3f 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/ArrivalWindowTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/ArrivalWindowTest.java
@@ -1 +1,51 @@
   + native
+package org.apache.cassandra.gms;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import static org.junit.Assert.*;
+
+import org.junit.Test;
+
+public class ArrivalWindowTest
+{
+    
+    @Test
+    public void test()
+    {
+        ArrivalWindow window = new ArrivalWindow(4);
+        //base readings
+        window.add(111);
+        window.add(222);
+        window.add(333);
+        window.add(444);
+        window.add(555);
+
+        //all good
+        assertEquals(0.4342, window.phi(666), 0.01);
+        
+        //oh noes, a much higher timestamp, something went wrong!
+        assertEquals(9.566, window.phi(3000), 0.01);
+    }
+
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/GossipDigestTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/GossipDigestTest.java
index e69de29b..1353ac9a 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/GossipDigestTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/GossipDigestTest.java
@@ -0,0 +1,58 @@
+package org.apache.cassandra.gms;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import static org.junit.Assert.*;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import java.net.InetAddress;
+import org.junit.Test;
+
+public class GossipDigestTest
+{
+
+    @Test
+    public void test() throws IOException
+    {
+        InetAddress endpoint = InetAddress.getByName("127.0.0.1");
+        int generation = 0;
+        int maxVersion = 123;
+        GossipDigest expected = new GossipDigest(endpoint, generation, maxVersion);
+        //make sure we get the same values out
+        assertEquals(endpoint, expected.getEndpoint());
+        assertEquals(generation, expected.getGeneration());
+        assertEquals(maxVersion, expected.getMaxVersion());
+        
+        //test the serialization and equals
+        DataOutputBuffer output = new DataOutputBuffer();
+        GossipDigest.serializer().serialize(expected, output);
+        
+        ByteArrayInputStream input = new ByteArrayInputStream(output.getData(), 0, output.getLength());
+        GossipDigest actual = GossipDigest.serializer().deserialize(new DataInputStream(input));
+        assertEquals(0, expected.compareTo(actual));
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/SerializationsTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/SerializationsTest.java
index 3f24d583..1a587897 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/SerializationsTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/gms/SerializationsTest.java
@@ -1 +1,116 @@
   + native
+package org.apache.cassandra.gms;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import org.apache.cassandra.AbstractSerializationsTester;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+import org.junit.Test;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+public class SerializationsTest extends AbstractSerializationsTester
+{
+    private void testEndpointStateWrite() throws IOException 
+    {
+        DataOutputStream out = getOutput("gms.EndpointState.bin");
+        HeartBeatState.serializer().serialize(Statics.HeartbeatSt, out);
+        EndpointState.serializer().serialize(Statics.EndpointSt, out);
+        VersionedValue.serializer.serialize(Statics.vv0, out);
+        VersionedValue.serializer.serialize(Statics.vv1, out);
+        out.close();
+    }
+    
+    @Test
+    public void testEndpointStateRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testEndpointStateWrite();
+        
+        DataInputStream in = getInput("gms.EndpointState.bin");
+        assert HeartBeatState.serializer().deserialize(in) != null;
+        assert EndpointState.serializer().deserialize(in) != null;
+        assert VersionedValue.serializer.deserialize(in) != null;
+        assert VersionedValue.serializer.deserialize(in) != null;
+        in.close();
+    }
+     
+    private void testGossipDigestWrite() throws IOException
+    {
+        Map<InetAddress, EndpointState> states = new HashMap<InetAddress, EndpointState>();
+        states.put(InetAddress.getByName("127.0.0.1"), Statics.EndpointSt);
+        states.put(InetAddress.getByName("127.0.0.2"), Statics.EndpointSt);
+        GossipDigestAckMessage ack = new GossipDigestAckMessage(Statics.Digests, states);
+        GossipDigestAck2Message ack2 = new GossipDigestAck2Message(states);
+        GossipDigestSynMessage syn = new GossipDigestSynMessage("Not a real cluster name", Statics.Digests);
+        
+        DataOutputStream out = getOutput("gms.Gossip.bin");
+        for (GossipDigest gd : Statics.Digests)
+            GossipDigest.serializer().serialize(gd, out);
+        GossipDigestAckMessage.serializer().serialize(ack, out);
+        GossipDigestAck2Message.serializer().serialize(ack2, out);
+        GossipDigestSynMessage.serializer().serialize(syn, out);
+        out.close();
+    }
+    
+    @Test
+    public void testGossipDigestRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testGossipDigestWrite();
+        
+        int count = 0;
+        DataInputStream in = getInput("gms.Gossip.bin");
+        while (count < Statics.Digests.size())
+            assert GossipDigestAck2Message.serializer().deserialize(in) != null;
+        assert GossipDigestAckMessage.serializer().deserialize(in) != null;
+        assert GossipDigestAck2Message.serializer().deserialize(in) != null;
+        assert GossipDigestSynMessage.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private static class Statics
+    {
+        private static HeartBeatState HeartbeatSt = new HeartBeatState(101, 201);
+        private static EndpointState EndpointSt = new EndpointState(HeartbeatSt);
+        private static VersionedValue.VersionedValueFactory vvFact = new VersionedValue.VersionedValueFactory(StorageService.getPartitioner());
+        private static VersionedValue vv0 = vvFact.load(23d);
+        private static VersionedValue vv1 = vvFact.bootstrapping(StorageService.getPartitioner().getRandomToken());
+        private static List<GossipDigest> Digests = new ArrayList<GossipDigest>();
+        
+        {
+            HeartbeatSt.updateHeartBeat();
+            EndpointSt.addApplicationState(ApplicationState.LOAD, vv0);
+            EndpointSt.addApplicationState(ApplicationState.STATUS, vv1);
+            for (int i = 0; i < 100; i++)
+                Digests.add(new GossipDigest(FBUtilities.getLocalAddress(), 100 + i, 1000 + 2 * i));
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/hadoop/ColumnFamilyInputFormatTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/hadoop/ColumnFamilyInputFormatTest.java
index 3f24d583..760d616c 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/hadoop/ColumnFamilyInputFormatTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/hadoop/ColumnFamilyInputFormatTest.java
@@ -1 +1,53 @@
   + native
+package org.apache.cassandra.hadoop;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.cassandra.thrift.SlicePredicate;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.hadoop.conf.Configuration;
+import org.junit.Test;
+
+public class ColumnFamilyInputFormatTest
+{
+    @Test
+    public void testSlicePredicate()
+    {
+        long columnValue = 1271253600000l;
+        ByteBuffer columnBytes = ByteBufferUtil.bytes(columnValue);
+
+        List<ByteBuffer> columnNames = new ArrayList<ByteBuffer>();
+        columnNames.add(columnBytes);
+        SlicePredicate originalPredicate = new SlicePredicate().setColumn_names(columnNames);
+
+        Configuration conf = new Configuration();
+        ConfigHelper.setInputSlicePredicate(conf, originalPredicate);
+
+        SlicePredicate rtPredicate = ConfigHelper.getInputSlicePredicate(conf);
+        assert rtPredicate.column_names.size() == 1;
+        assert originalPredicate.column_names.get(0).equals(rtPredicate.column_names.get(0));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/BloomFilterTrackerTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/BloomFilterTrackerTest.java
index e69de29b..29a1588b 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/BloomFilterTrackerTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/BloomFilterTrackerTest.java
@@ -0,0 +1,72 @@
+package org.apache.cassandra.io;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.io.sstable.BloomFilterTracker;
+
+import static org.junit.Assert.assertEquals;
+
+public class BloomFilterTrackerTest extends CleanupHelper
+{
+    @Test
+    public void testAddingFalsePositives()
+    {
+        BloomFilterTracker bft = new BloomFilterTracker();
+        assertEquals(0L, bft.getFalsePositiveCount());
+        assertEquals(0L, bft.getRecentFalsePositiveCount());
+        bft.addFalsePositive();
+        bft.addFalsePositive();
+        assertEquals(2L, bft.getFalsePositiveCount());
+        assertEquals(2L, bft.getRecentFalsePositiveCount());
+        assertEquals(0L, bft.getRecentFalsePositiveCount());
+        assertEquals(2L, bft.getFalsePositiveCount()); // sanity check
+    }
+
+    @Test
+    public void testAddingTruePositives()
+    {
+        BloomFilterTracker bft = new BloomFilterTracker();
+        assertEquals(0L, bft.getTruePositiveCount());
+        assertEquals(0L, bft.getRecentTruePositiveCount());
+        bft.addTruePositive();
+        bft.addTruePositive();
+        assertEquals(2L, bft.getTruePositiveCount());
+        assertEquals(2L, bft.getRecentTruePositiveCount());
+        assertEquals(0L, bft.getRecentTruePositiveCount());
+        assertEquals(2L, bft.getTruePositiveCount()); // sanity check
+    }
+
+    @Test
+    public void testAddingToOneLeavesTheOtherAlone()
+    {
+        BloomFilterTracker bft = new BloomFilterTracker();
+        bft.addFalsePositive();
+        assertEquals(0L, bft.getTruePositiveCount());
+        assertEquals(0L, bft.getRecentTruePositiveCount());
+        bft.addTruePositive();
+        assertEquals(1L, bft.getFalsePositiveCount());
+        assertEquals(1L, bft.getRecentFalsePositiveCount());
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/CompactSerializerTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/CompactSerializerTest.java
index e69de29b..302b17b3 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/CompactSerializerTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/CompactSerializerTest.java
@@ -0,0 +1,148 @@
+package org.apache.cassandra.io;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import org.apache.cassandra.CleanupHelper;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import java.io.File;
+import java.lang.reflect.ParameterizedType;
+import java.lang.reflect.Type;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+public class CompactSerializerTest extends CleanupHelper
+{
+    private static Set<String> expectedClassNames;
+    private static List<String> discoveredClassNames;
+    
+    @BeforeClass
+    public static void scanClasspath()
+    {
+        expectedClassNames = new HashSet<String>();
+        expectedClassNames.add("RangeSliceCommandSerializer");
+        expectedClassNames.add("ReadCommandSerializer");
+        expectedClassNames.add("ReadResponseSerializer");
+        expectedClassNames.add("RowSerializer");
+        expectedClassNames.add("RowMutationSerializer");
+        expectedClassNames.add("SliceByNamesReadCommandSerializer");
+        expectedClassNames.add("SliceFromReadCommandSerializer");
+        expectedClassNames.add("TruncateResponseSerializer");
+        expectedClassNames.add("TruncationSerializer");
+        expectedClassNames.add("WriteResponseSerializer");
+        expectedClassNames.add("EndpointStateSerializer");
+        expectedClassNames.add("GossipDigestSerializer");
+        expectedClassNames.add("GossipDigestAck2MessageSerializer");
+        expectedClassNames.add("GossipDigestAckMessageSerializer");
+        expectedClassNames.add("GossipDigestSynMessageSerializer");
+        expectedClassNames.add("HeartBeatStateSerializer");
+        expectedClassNames.add("VersionedValueSerializer");
+        expectedClassNames.add("HeaderSerializer");
+        expectedClassNames.add("MessageSerializer");
+        expectedClassNames.add("TreeRequestVerbHandler");
+        expectedClassNames.add("TreeResponseVerbHandler");
+        expectedClassNames.add("PendingFileSerializer");
+        expectedClassNames.add("StreamHeaderSerializer");
+        expectedClassNames.add("FileStatusSerializer");
+        expectedClassNames.add("StreamRequestMessageSerializer");
+        expectedClassNames.add("BloomFilterSerializer");
+        expectedClassNames.add("LegacyBloomFilterSerializer");
+        
+        discoveredClassNames = new ArrayList<String>();
+        String cp = System.getProperty("java.class.path");
+        assert cp != null;
+        String[] parts = cp.split(File.pathSeparator, -1);
+        class DirScanner 
+        {
+            void scan(File f, String ctx) 
+            {
+                String newCtx = ctx == null ? f.getName().equals("org") ? f.getName() : null : ctx + "." + f.getName();
+                if (f.isDirectory())
+                {
+                    for (File child : f.listFiles())
+                    {
+                        scan(child, newCtx);
+                    }
+                }
+                else if (f.getName().endsWith(".class"))
+                {
+                    String fName = f.getName();
+                    String className = ctx + "." + fName.substring(0, fName.lastIndexOf('.'));
+                    try
+                    {
+                        Class cls = Class.forName(className);
+                        String simpleName = cls.getSimpleName();
+                        classTraversal: while (cls != null)
+                        {
+                            Type[] interfaces = cls.getGenericInterfaces();
+                            for (Type t : interfaces)
+                            {
+                                if(t instanceof ParameterizedType)
+                                {
+                                    ParameterizedType pt = (ParameterizedType)t;
+                                    if (((Class)pt.getRawType()).getSimpleName().equals("ICompactSerializer"))
+                                    {
+                                        discoveredClassNames.add(simpleName);
+                                        break classTraversal;
+                                    }
+                                }
+                            }
+                            cls = cls.getSuperclass();
+                        }
+                    }
+                    catch (ClassNotFoundException ex) 
+                    {
+                        throw new RuntimeException(ex);
+                    }
+                }
+            }
+        }
+        
+        DirScanner dirScanner = new DirScanner();
+        for (String cpItem : parts)
+        {
+            File f = new File(cpItem);
+            if (f.exists() && f.isDirectory())
+                dirScanner.scan(f, null);
+        }
+    }
+    
+    /** look for classes I expect to find. */
+    @Test
+    public void verifyAllSimpleNamesTest()
+    {
+        for (String clsName : expectedClassNames)
+            assert discoveredClassNames.contains(clsName) : clsName + " was not discovered";
+    }
+    
+    /** look for classes I do not expect to find. */
+    @Test
+    public void noOthersTest()
+    {
+        for (String clsName : discoveredClassNames)
+            assert expectedClassNames.contains(clsName) : clsName + " was discovered";
+        assert true;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/LazilyCompactedRowTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/LazilyCompactedRowTest.java
index e69de29b..d742c0fd 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/LazilyCompactedRowTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/LazilyCompactedRowTest.java
@@ -0,0 +1,264 @@
+package org.apache.cassandra.io;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import static junit.framework.Assert.assertEquals;
+
+import java.io.*;
+import java.nio.ByteBuffer;
+import java.util.Collection;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.CompactionManager;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.io.sstable.IndexHelper;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.io.util.MappedFileDataInput;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.UUIDGen;
+
+import org.junit.Test;
+
+
+public class LazilyCompactedRowTest extends CleanupHelper
+{
+    private void assertBytes(ColumnFamilyStore cfs, int gcBefore, boolean major) throws IOException
+    {
+        Collection<SSTableReader> sstables = cfs.getSSTables();
+        CompactionIterator ci1 = new CompactionIterator(cfs, sstables, gcBefore, major);
+        LazyCompactionIterator ci2 = new LazyCompactionIterator(cfs, sstables, gcBefore, major);
+
+        while (true)
+        {
+            if (!ci1.hasNext())
+            {
+                assert !ci2.hasNext();
+                break;
+            }
+
+            AbstractCompactedRow row1 = ci1.next();
+            AbstractCompactedRow row2 = ci2.next();
+            DataOutputBuffer out1 = new DataOutputBuffer();
+            DataOutputBuffer out2 = new DataOutputBuffer();
+            row1.write(out1);
+            row2.write(out2);
+
+            File tmpFile1 = File.createTempFile("lcrt1", null);
+            File tmpFile2 = File.createTempFile("lcrt2", null);
+
+            tmpFile1.deleteOnExit();
+            tmpFile2.deleteOnExit();
+
+            new FileOutputStream(tmpFile1).write(out1.getData()); // writing data from row1
+            new FileOutputStream(tmpFile2).write(out2.getData()); // writing data from row2
+
+            MappedFileDataInput in1 = new MappedFileDataInput(new FileInputStream(tmpFile1), tmpFile1.getAbsolutePath(), 0);
+            MappedFileDataInput in2 = new MappedFileDataInput(new FileInputStream(tmpFile2), tmpFile2.getAbsolutePath(), 0);
+
+            // key isn't part of what CompactedRow writes, that's done by SSTW.append
+
+            // row size can differ b/c of bloom filter counts being different
+            long rowSize1 = SSTableReader.readRowSize(in1, sstables.iterator().next().descriptor);
+            long rowSize2 = SSTableReader.readRowSize(in2, sstables.iterator().next().descriptor);
+            assertEquals(out1.getLength(), rowSize1 + 8);
+            assertEquals(out2.getLength(), rowSize2 + 8);
+            // bloom filter
+            IndexHelper.defreezeBloomFilter(in1, rowSize1, false);
+            IndexHelper.defreezeBloomFilter(in2, rowSize2, false);
+            // index
+            int indexSize1 = in1.readInt();
+            int indexSize2 = in2.readInt();
+            assertEquals(indexSize1, indexSize2);
+
+            ByteBuffer bytes1 = in1.readBytes(indexSize1);
+            ByteBuffer bytes2 = in2.readBytes(indexSize2);
+
+            assert bytes1.equals(bytes2);
+
+            // cf metadata
+            ColumnFamily cf1 = ColumnFamily.create("Keyspace1", "Standard1");
+            ColumnFamily cf2 = ColumnFamily.create("Keyspace1", "Standard1");
+            ColumnFamily.serializer().deserializeFromSSTableNoColumns(cf1, in1);
+            ColumnFamily.serializer().deserializeFromSSTableNoColumns(cf2, in2);
+            assert cf1.getLocalDeletionTime() == cf2.getLocalDeletionTime();
+            assert cf1.getMarkedForDeleteAt() == cf2.getMarkedForDeleteAt();   
+            // columns
+            int columns = in1.readInt();
+            assert columns == in2.readInt();
+            for (int i = 0; i < columns; i++)
+            {
+                IColumn c1 = cf1.getColumnSerializer().deserialize(in1);
+                IColumn c2 = cf2.getColumnSerializer().deserialize(in2);
+                assert c1.equals(c2);
+            }
+            // that should be everything
+            assert in1.available() == 0;
+            assert in2.available() == 0;
+        }
+    }
+
+    @Test
+    public void testOneRow() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfs = table.getColumnFamilyStore("Standard1");
+
+        ByteBuffer key = ByteBufferUtil.bytes("k");
+        RowMutation rm = new RowMutation("Keyspace1", key);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("c")), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        assertBytes(cfs, Integer.MAX_VALUE, true);
+    }
+
+    @Test
+    public void testOneRowTwoColumns() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfs = table.getColumnFamilyStore("Standard1");
+
+        ByteBuffer key = ByteBufferUtil.bytes("k");
+        RowMutation rm = new RowMutation("Keyspace1", key);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("c")), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("d")), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        assertBytes(cfs, Integer.MAX_VALUE, true);
+    }
+
+    @Test
+    public void testTwoRows() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfs = table.getColumnFamilyStore("Standard1");
+
+        ByteBuffer key = ByteBufferUtil.bytes("k");
+        RowMutation rm = new RowMutation("Keyspace1", key);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("c")), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        assertBytes(cfs, Integer.MAX_VALUE, true);
+    }
+
+    @Test
+    public void testTwoRowsTwoColumns() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfs = table.getColumnFamilyStore("Standard1");
+
+        ByteBuffer key = ByteBufferUtil.bytes("k");
+        RowMutation rm = new RowMutation("Keyspace1", key);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("c")), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("d")), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        assertBytes(cfs, Integer.MAX_VALUE, true);
+    }
+
+    @Test
+    public void testManyRows() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfs = table.getColumnFamilyStore("Standard1");
+
+        final int ROWS_PER_SSTABLE = 10;
+        for (int j = 0; j < (DatabaseDescriptor.getIndexInterval() * 3) / ROWS_PER_SSTABLE; j++) {
+            for (int i = 0; i < ROWS_PER_SSTABLE; i++) {
+                ByteBuffer key = ByteBufferUtil.bytes(String.valueOf(i % 2));
+                RowMutation rm = new RowMutation("Keyspace1", key);
+                rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes(String.valueOf(i / 2))), ByteBufferUtil.EMPTY_BYTE_BUFFER, j * ROWS_PER_SSTABLE + i);
+                rm.apply();
+            }
+            cfs.forceBlockingFlush();
+        }
+
+        assertBytes(cfs, Integer.MAX_VALUE, true);
+    }
+
+    @Test
+    public void testTwoRowSuperColumn() throws IOException, ExecutionException, InterruptedException
+    {
+        CompactionManager.instance.disableAutoCompaction();
+
+        Table table = Table.open("Keyspace4");
+        ColumnFamilyStore cfs = table.getColumnFamilyStore("Super5");
+
+        ByteBuffer key = ByteBufferUtil.bytes("k");
+        RowMutation rm = new RowMutation("Keyspace4", key);
+        ByteBuffer scKey = ByteBuffer.wrap(UUIDGen.decompose(UUIDGen.makeType1UUIDFromHost(FBUtilities.getLocalAddress())));
+        rm.add(new QueryPath("Super5", scKey , ByteBufferUtil.bytes("c")), ByteBufferUtil.EMPTY_BYTE_BUFFER, 0);
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        rm.apply();
+        cfs.forceBlockingFlush();
+
+        assertBytes(cfs, Integer.MAX_VALUE, true);
+    }
+
+
+    private static class LazyCompactionIterator extends CompactionIterator
+    {
+        private final ColumnFamilyStore cfStore;
+
+        public LazyCompactionIterator(ColumnFamilyStore cfStore, Iterable<SSTableReader> sstables, int gcBefore, boolean major) throws IOException
+        {
+            super(cfStore, sstables, gcBefore, major);
+            this.cfStore = cfStore;
+        }
+
+        @Override
+        protected AbstractCompactedRow getCompactedRow()
+        {
+            return new LazilyCompactedRow(cfStore, rows, true, Integer.MAX_VALUE, true);
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/DescriptorTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/DescriptorTest.java
index 3f24d583..2b465dcf 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/DescriptorTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/DescriptorTest.java
@@ -1 +1,38 @@
   + native
+package org.apache.cassandra.io.sstable;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.io.File;
+
+import org.junit.Test;
+
+public class DescriptorTest
+{
+    @Test
+    public void testLegacy()
+    {
+        Descriptor descriptor = Descriptor.fromFilename(new File("Keyspace1"), "userActionUtilsKey-9-Data.db").left;
+        assert descriptor.version.equals(Descriptor.LEGACY_VERSION);
+        assert descriptor.usesOldBloomFilter;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/IndexHelperTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/IndexHelperTest.java
index e69de29b..96e4c9a4 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/IndexHelperTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/IndexHelperTest.java
@@ -0,0 +1,56 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.io.sstable;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.nio.ByteBuffer;
+
+import org.junit.Test;
+
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.db.marshal.IntegerType;
+import static org.apache.cassandra.io.sstable.IndexHelper.IndexInfo;
+import static org.apache.cassandra.utils.ByteBufferUtil.bytes;
+
+public class IndexHelperTest
+{
+    @Test
+    public void testIndexHelper()
+    {
+        List<IndexInfo> indexes = new ArrayList<IndexInfo>();
+        indexes.add(new IndexInfo(bytes(0L), bytes(5L), 0, 0));
+        indexes.add(new IndexInfo(bytes(10L), bytes(15L), 0, 0));
+        indexes.add(new IndexInfo(bytes(20L), bytes(25L), 0, 0));
+
+        AbstractType comp = IntegerType.instance;
+
+        assert 0 == IndexHelper.indexFor(bytes(-1L), indexes, comp, false);
+        assert 0 == IndexHelper.indexFor(bytes(5L), indexes, comp, false);
+        assert 1 == IndexHelper.indexFor(bytes(12L), indexes, comp, false);
+        assert 2 == IndexHelper.indexFor(bytes(17L), indexes, comp, false);
+        assert 3 == IndexHelper.indexFor(bytes(100L), indexes, comp, false);
+
+        assert -1 == IndexHelper.indexFor(bytes(-1L), indexes, comp, true);
+        assert 0 == IndexHelper.indexFor(bytes(5L), indexes, comp, true);
+        assert 1 == IndexHelper.indexFor(bytes(12L), indexes, comp, true);
+        assert 1 == IndexHelper.indexFor(bytes(17L), indexes, comp, true);
+        assert 2 == IndexHelper.indexFor(bytes(100L), indexes, comp, true);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java
index e69de29b..01d547a3 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java
@@ -0,0 +1,116 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.*;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.columniterator.SSTableNamesIterator;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ * Tests backwards compatibility for SSTables. Requires that older SSTables match up with the existing config file,
+ * and currently only tests specific cases for specific upgrades.
+ */
+public class LegacySSTableTest extends CleanupHelper
+{
+    public static final String LEGACY_SSTABLE_PROP = "legacy-sstable-root";
+    public static final String KSNAME = "Keyspace1";
+    public static final String CFNAME = "Standard1";
+
+    public static Set<String> TEST_DATA;
+    public static File LEGACY_SSTABLE_ROOT;
+
+    @BeforeClass
+    public static void beforeClass()
+    {
+        String scp = System.getProperty(LEGACY_SSTABLE_PROP);
+        assert scp != null;
+        LEGACY_SSTABLE_ROOT = new File(scp).getAbsoluteFile();
+        assert LEGACY_SSTABLE_ROOT.isDirectory();
+
+        TEST_DATA = new HashSet<String>();
+        for (int i = 100; i < 1000; ++i)
+            TEST_DATA.add(Integer.toString(i));
+    }
+
+    /**
+     * Get a descriptor for the legacy sstable at the given version.
+     */
+    protected Descriptor getDescriptor(String ver) throws IOException
+    {
+        File directory = new File(LEGACY_SSTABLE_ROOT + File.separator + ver + File.separator + KSNAME);
+        return new Descriptor(ver, directory, KSNAME, CFNAME, 0, false);
+    }
+
+    /**
+     * Generates a test SSTable for use in this classes' tests. Uncomment and run against an older build
+     * and the output will be copied to a version subdirectory in 'LEGACY_SSTABLE_ROOT'
+     *
+    @Test
+    public void buildTestSSTable() throws IOException
+    {
+        // write the output in a version specific directory
+        Descriptor dest = getDescriptor(Descriptor.CURRENT_VERSION);
+        assert dest.directory.mkdirs() : "Could not create " + dest.directory + ". Might it already exist?";
+
+        SSTableReader ssTable = SSTableUtils.prepare().ks(KSNAME).cf(CFNAME).dest(dest).write(TEST_DATA);
+        assert ssTable.descriptor.generation == 0 :
+            "In order to create a generation 0 sstable, please run this test alone.";
+        System.out.println(">>> Wrote " + dest);
+    }
+    */
+
+    @Test
+    public void testVersions() throws Throwable
+    {
+        for (File version : LEGACY_SSTABLE_ROOT.listFiles())
+            if (Descriptor.versionValidate(version.getName()))
+                testVersion(version.getName());
+    }
+
+    public void testVersion(String version) throws Throwable
+    {
+        try
+        {
+            SSTableReader reader = SSTableReader.open(getDescriptor(version));
+            for (String keystring : TEST_DATA)
+            {
+                ByteBuffer key = ByteBufferUtil.bytes(keystring);
+                // confirm that the bloom filter does not reject any keys/names
+                DecoratedKey dk = reader.partitioner.decorateKey(key);
+                SSTableNamesIterator iter = new SSTableNamesIterator(reader, dk, FBUtilities.singleton(key));
+                assert iter.next().name().equals(key);
+            }
+        }
+        catch (Throwable e)
+        {
+            System.err.println("Failed to read " + version);
+            throw e;
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java
index e69de29b..ff899245 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java
@@ -0,0 +1,196 @@
+package org.apache.cassandra.io.sstable;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.concurrent.ExecutionException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.util.FileDataInput;
+import org.apache.cassandra.io.util.MmappedSegmentedFile;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.Pair;
+
+import org.apache.cassandra.Util;
+
+import static org.junit.Assert.assertEquals;
+
+public class SSTableReaderTest extends CleanupHelper
+{
+    static Token t(int i)
+    {
+        return StorageService.getPartitioner().getToken(ByteBufferUtil.bytes(String.valueOf(i)));
+    }
+
+    @Test
+    public void testGetPositionsForRanges() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard2");
+
+        // insert data and compact to a single sstable
+        CompactionManager.instance.disableAutoCompaction();
+        for (int j = 0; j < 10; j++)
+        {
+            ByteBuffer key = ByteBufferUtil.bytes(String.valueOf(j));
+            RowMutation rm = new RowMutation("Keyspace1", key);
+            rm.add(new QueryPath("Standard2", null, ByteBufferUtil.bytes("0")), ByteBufferUtil.EMPTY_BYTE_BUFFER, j);
+            rm.apply();
+        }
+        store.forceBlockingFlush();
+        CompactionManager.instance.performMajor(store);
+
+        List<Range> ranges = new ArrayList<Range>();
+        // 1 key
+        ranges.add(new Range(t(0), t(1)));
+        // 2 keys
+        ranges.add(new Range(t(2), t(4)));
+        // wrapping range from key to end
+        ranges.add(new Range(t(6), StorageService.getPartitioner().getMinimumToken()));
+        // empty range (should be ignored)
+        ranges.add(new Range(t(9), t(91)));
+
+        // confirm that positions increase continuously
+        SSTableReader sstable = store.getSSTables().iterator().next();
+        long previous = -1;
+        for (Pair<Long,Long> section : sstable.getPositionsForRanges(ranges))
+        {
+            assert previous <= section.left : previous + " ! < " + section.left;
+            assert section.left < section.right : section.left + " ! < " + section.right;
+            previous = section.right;
+        }
+    }
+
+    @Test
+    public void testSpannedIndexPositions() throws IOException, ExecutionException, InterruptedException
+    {
+        MmappedSegmentedFile.MAX_SEGMENT_SIZE = 40; // each index entry is ~11 bytes, so this will generate lots of segments
+
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard1");
+
+        // insert a bunch of data and compact to a single sstable
+        CompactionManager.instance.disableAutoCompaction();
+        for (int j = 0; j < 100; j += 2)
+        {
+            ByteBuffer key = ByteBufferUtil.bytes(String.valueOf(j));
+            RowMutation rm = new RowMutation("Keyspace1", key);
+            rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("0")), ByteBufferUtil.EMPTY_BYTE_BUFFER, j);
+            rm.apply();
+        }
+        store.forceBlockingFlush();
+        CompactionManager.instance.performMajor(store);
+
+        // check that all our keys are found correctly
+        SSTableReader sstable = store.getSSTables().iterator().next();
+        for (int j = 0; j < 100; j += 2)
+        {
+            DecoratedKey dk = Util.dk(String.valueOf(j));
+            FileDataInput file = sstable.getFileDataInput(dk, DatabaseDescriptor.getIndexedReadBufferSizeInKB() * 1024);
+            DecoratedKey keyInDisk = SSTableReader.decodeKey(sstable.partitioner,
+                                                             sstable.descriptor,
+                                                             ByteBufferUtil.readWithShortLength(file));
+            assert keyInDisk.equals(dk) : String.format("%s != %s in %s", keyInDisk, dk, file.getPath());
+        }
+
+        // check no false positives
+        for (int j = 1; j < 110; j += 2)
+        {
+            DecoratedKey dk = Util.dk(String.valueOf(j));
+            assert sstable.getPosition(dk, SSTableReader.Operator.EQ) == -1;
+        }
+    }
+
+    @Test
+    public void testPersistentStatistics() throws IOException, ExecutionException, InterruptedException
+    {
+
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard1");
+
+        for (int j = 0; j < 100; j += 2)
+        {
+            ByteBuffer key = ByteBufferUtil.bytes(String.valueOf(j));
+            RowMutation rm = new RowMutation("Keyspace1", key);
+            rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("0")), ByteBufferUtil.EMPTY_BYTE_BUFFER, j);
+            rm.apply();
+        }
+        store.forceBlockingFlush();
+        assert store.getMaxRowSize() != 0;
+    }
+
+    @Test
+    public void testGetPositionsForRangesWithKeyCache() throws IOException, ExecutionException, InterruptedException
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore store = table.getColumnFamilyStore("Standard2");
+        store.getKeyCache().setCapacity(100);
+
+        // insert data and compact to a single sstable
+        CompactionManager.instance.disableAutoCompaction();
+        for (int j = 0; j < 10; j++)
+        {
+            ByteBuffer key = ByteBufferUtil.bytes(String.valueOf(j));
+            RowMutation rm = new RowMutation("Keyspace1", key);
+            rm.add(new QueryPath("Standard2", null, ByteBufferUtil.bytes("0")), ByteBufferUtil.EMPTY_BYTE_BUFFER, j);
+            rm.apply();
+        }
+        store.forceBlockingFlush();
+        CompactionManager.instance.performMajor(store);
+
+        SSTableReader sstable = store.getSSTables().iterator().next();
+        long p2 = sstable.getPosition(k(2), SSTableReader.Operator.EQ);
+        long p3 = sstable.getPosition(k(3), SSTableReader.Operator.EQ);
+        long p6 = sstable.getPosition(k(6), SSTableReader.Operator.EQ);
+        long p7 = sstable.getPosition(k(7), SSTableReader.Operator.EQ);
+
+        Pair<Long, Long> p = sstable.getPositionsForRanges(makeRanges(t(2), t(6))).iterator().next();
+
+        // range are start exclusive so we should start at 3
+        assert p.left == p3;
+
+        // to capture 6 we have to stop at the start of 7
+        assert p.right == p7;
+    }
+
+    private List<Range> makeRanges(Token left, Token right)
+    {
+        return Arrays.asList(new Range[]{ new Range(left, right) });
+    }
+
+    private DecoratedKey k(int i)
+    {
+        return new DecoratedKey(t(i), ByteBufferUtil.bytes(String.valueOf(i)));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java
index e69de29b..003f247f 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java
@@ -0,0 +1,101 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.*;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.io.util.BufferedRandomAccessFile;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class SSTableTest extends CleanupHelper
+{
+    @Test
+    public void testSingleWrite() throws IOException {
+        // write test data
+        ByteBuffer key = ByteBufferUtil.bytes(Integer.toString(1));
+        ByteBuffer bytes = ByteBuffer.wrap(new byte[1024]);
+        new Random().nextBytes(bytes.array());
+
+        Map<ByteBuffer, ByteBuffer> map = new HashMap<ByteBuffer,ByteBuffer>();
+        map.put(key, bytes);
+        SSTableReader ssTable = SSTableUtils.prepare().cf("Standard1").writeRaw(map);
+
+        // verify
+        verifySingle(ssTable, bytes, key);
+        ssTable = SSTableReader.open(ssTable.descriptor); // read the index from disk
+        verifySingle(ssTable, bytes, key);
+    }
+
+    private void verifySingle(SSTableReader sstable, ByteBuffer bytes, ByteBuffer key) throws IOException
+    {
+        BufferedRandomAccessFile file = new BufferedRandomAccessFile(sstable.getFilename(), "r");
+        file.seek(sstable.getPosition(sstable.partitioner.decorateKey(key), SSTableReader.Operator.EQ));
+        assert key.equals(ByteBufferUtil.readWithShortLength(file));
+        int size = (int)SSTableReader.readRowSize(file, sstable.descriptor);
+        byte[] bytes2 = new byte[size];
+        file.readFully(bytes2);
+        assert ByteBuffer.wrap(bytes2).equals(bytes);
+    }
+
+    @Test
+    public void testManyWrites() throws IOException {
+        Map<ByteBuffer, ByteBuffer> map = new HashMap<ByteBuffer,ByteBuffer>();
+        for (int i = 100; i < 1000; ++i)
+        {
+            map.put(ByteBufferUtil.bytes(Integer.toString(i)), ByteBufferUtil.bytes(("Avinash Lakshman is a good man: " + i)));
+        }
+
+        // write
+        SSTableReader ssTable = SSTableUtils.prepare().cf("Standard2").writeRaw(map);
+
+        // verify
+        verifyMany(ssTable, map);
+        ssTable = SSTableReader.open(ssTable.descriptor); // read the index from disk
+        verifyMany(ssTable, map);
+
+        Set<Component> live = SSTable.componentsFor(ssTable.descriptor, true);
+        assert !live.isEmpty() : "SSTable has live components";
+        Set<Component> all = SSTable.componentsFor(ssTable.descriptor, false);
+        assert live.equals(all) : "live components same as all components";
+        all.removeAll(live);
+        assert all.isEmpty() : "SSTable has no temp components";
+    }
+
+    private void verifyMany(SSTableReader sstable, Map<ByteBuffer, ByteBuffer> map) throws IOException
+    {
+        List<ByteBuffer> keys = new ArrayList<ByteBuffer>(map.keySet());
+        Collections.shuffle(keys);
+        BufferedRandomAccessFile file = new BufferedRandomAccessFile(sstable.getFilename(), "r");
+        for (ByteBuffer key : keys)
+        {
+            file.seek(sstable.getPosition(sstable.partitioner.decorateKey(key), SSTableReader.Operator.EQ));
+            assert key.equals( ByteBufferUtil.readWithShortLength(file));
+            int size = (int)SSTableReader.readRowSize(file, sstable.descriptor);
+            byte[] bytes2 = new byte[size];
+            file.readFully(bytes2);
+            assert Arrays.equals(bytes2, map.get(key).array());
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java
index e69de29b..aa639763 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java
@@ -0,0 +1,163 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.*;
+
+import org.apache.cassandra.db.Column;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class SSTableUtils
+{
+    // first configured table and cf
+    public static String TABLENAME = "Keyspace1";
+    public static String CFNAME = "Standard1";
+
+    public static ColumnFamily createCF(long mfda, int ldt, IColumn... cols)
+    {
+        ColumnFamily cf = ColumnFamily.create(TABLENAME, CFNAME);
+        cf.delete(ldt, mfda);
+        for (IColumn col : cols)
+            cf.addColumn(col);
+        return cf;
+    }
+
+    public static File tempSSTableFile(String tablename, String cfname) throws IOException
+    {
+        return tempSSTableFile(tablename, cfname, 0);
+    }
+
+    public static File tempSSTableFile(String tablename, String cfname, int generation) throws IOException
+    {
+        File tempdir = File.createTempFile(tablename, cfname);
+        if(!tempdir.delete() || !tempdir.mkdir())
+            throw new IOException("Temporary directory creation failed.");
+        tempdir.deleteOnExit();
+        File tabledir = new File(tempdir, tablename);
+        tabledir.mkdir();
+        tabledir.deleteOnExit();
+        File datafile = new File(new Descriptor(tabledir, tablename, cfname, generation, false).filenameFor("Data.db"));
+        if (!datafile.createNewFile())
+            throw new IOException("unable to create file " + datafile);
+        datafile.deleteOnExit();
+        return datafile;
+    }
+
+    /**
+     * @return A Context with chainable methods to configure and write a SSTable.
+     */
+    public static Context prepare()
+    {
+        return new Context();
+    }
+
+    public static class Context
+    {
+        private String ksname = TABLENAME;
+        private String cfname = CFNAME;
+        private Descriptor dest = null;
+        private boolean cleanup = true;
+        private int generation = 0;
+
+        Context() {}
+
+        public Context ks(String ksname)
+        {
+            this.ksname = ksname;
+            return this;
+        }
+
+        public Context cf(String cfname)
+        {
+            this.cfname = cfname;
+            return this;
+        }
+
+        /**
+         * Set an alternate path for the written SSTable: if unset, the SSTable will
+         * be cleaned up on JVM exit.
+         */
+        public Context dest(Descriptor dest)
+        {
+            this.dest = dest;
+            this.cleanup = false;
+            return this;
+        }
+
+        /**
+         * Sets the generation number for the generated SSTable. Ignored if "dest()" is set.
+         */
+        public Context generation(int generation)
+        {
+            this.generation = generation;
+            return this;
+        }
+
+        public SSTableReader write(Set<String> keys) throws IOException
+        {
+            Map<String, ColumnFamily> map = new HashMap<String, ColumnFamily>();
+            for (String key : keys)
+            {
+                ColumnFamily cf = ColumnFamily.create(ksname, cfname);
+                cf.addColumn(new Column(ByteBufferUtil.bytes(key), ByteBufferUtil.bytes(key), 0));
+                map.put(key, cf);
+            }
+            return write(map);
+        }
+
+        public SSTableReader write(Map<String, ColumnFamily> entries) throws IOException
+        {
+            Map<ByteBuffer, ByteBuffer> map = new HashMap<ByteBuffer, ByteBuffer>();
+            for (Map.Entry<String, ColumnFamily> entry : entries.entrySet())
+            {
+                DataOutputBuffer buffer = new DataOutputBuffer();
+                ColumnFamily.serializer().serializeWithIndexes(entry.getValue(), buffer);
+                map.put(ByteBufferUtil.bytes(entry.getKey()), ByteBuffer.wrap(buffer.asByteArray()));
+            }
+            return writeRaw(map);
+        }
+
+        /**
+         * @Deprecated: Writes the binary content of a row, which should be encapsulated.
+         */
+        public SSTableReader writeRaw(Map<ByteBuffer, ByteBuffer> entries) throws IOException
+        {
+            File datafile = (dest == null) ? tempSSTableFile(ksname, cfname, generation) : new File(dest.filenameFor(Component.DATA));
+            SSTableWriter writer = new SSTableWriter(datafile.getAbsolutePath(), entries.size());
+            SortedMap<DecoratedKey, ByteBuffer> sortedEntries = new TreeMap<DecoratedKey, ByteBuffer>();
+            for (Map.Entry<ByteBuffer, ByteBuffer> entry : entries.entrySet())
+                sortedEntries.put(writer.partitioner.decorateKey(entry.getKey()), entry.getValue());
+            for (Map.Entry<DecoratedKey, ByteBuffer> entry : sortedEntries.entrySet())
+                writer.append(entry.getKey(), entry.getValue());
+            SSTableReader reader = writer.closeAndOpenReader();
+            if (cleanup)
+                for (Component comp : reader.components)
+                    new File(reader.descriptor.filenameFor(comp)).deleteOnExit();
+            return reader;
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableWriterTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableWriterTest.java
index e69de29b..3010cee2 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableWriterTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/sstable/SSTableWriterTest.java
@@ -0,0 +1,98 @@
+package org.apache.cassandra.io.sstable;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import static org.junit.Assert.*;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.filter.IFilter;
+import org.apache.cassandra.db.columniterator.IdentityQueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.thrift.IndexClause;
+import org.apache.cassandra.thrift.IndexExpression;
+import org.apache.cassandra.thrift.IndexOperator;
+import org.junit.Test;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class SSTableWriterTest extends CleanupHelper {
+
+    @Test
+    public void testRecoverAndOpen() throws IOException, ExecutionException, InterruptedException
+    {
+        RowMutation rm;
+
+        rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes("k1"));
+        rm.add(new QueryPath("Indexed1", null, ByteBufferUtil.bytes("birthdate")), ByteBufferUtil.bytes(1L), 0);
+        rm.apply();
+        
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Indexed1");        
+        cf.addColumn(new Column(ByteBufferUtil.bytes("birthdate"), ByteBufferUtil.bytes(1L), 0));
+        cf.addColumn(new Column(ByteBufferUtil.bytes("anydate"), ByteBufferUtil.bytes(1L), 0));
+        
+        Map<ByteBuffer, ByteBuffer> entries = new HashMap<ByteBuffer, ByteBuffer>();
+        
+        DataOutputBuffer buffer = new DataOutputBuffer();
+        ColumnFamily.serializer().serializeWithIndexes(cf, buffer);
+        entries.put(ByteBufferUtil.bytes("k2"), ByteBuffer.wrap(Arrays.copyOf(buffer.getData(), buffer.getLength())));        
+        cf.clear();
+        
+        cf.addColumn(new Column(ByteBufferUtil.bytes("anydate"), ByteBufferUtil.bytes(1L), 0));
+        buffer = new DataOutputBuffer();
+        ColumnFamily.serializer().serializeWithIndexes(cf, buffer);               
+        entries.put(ByteBufferUtil.bytes("k3"), ByteBuffer.wrap(Arrays.copyOf(buffer.getData(), buffer.getLength())));
+        
+        SSTableReader orig = SSTableUtils.prepare().cf("Indexed1").writeRaw(entries);        
+        // whack the index to trigger the recover
+        FileUtils.deleteWithConfirm(orig.descriptor.filenameFor(Component.PRIMARY_INDEX));
+        FileUtils.deleteWithConfirm(orig.descriptor.filenameFor(Component.FILTER));
+
+        SSTableReader sstr = CompactionManager.instance.submitSSTableBuild(orig.descriptor).get();
+        assert sstr != null;
+        ColumnFamilyStore cfs = Table.open("Keyspace1").getColumnFamilyStore("Indexed1");
+        cfs.addSSTable(sstr);
+        cfs.buildSecondaryIndexes(cfs.getSSTables(), cfs.getIndexedColumns());
+        
+        IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L));
+        IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        IFilter filter = new IdentityQueryFilter();
+        IPartitioner p = StorageService.getPartitioner();
+        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+        List<Row> rows = cfs.scan(clause, range, filter);
+        
+        assertEquals("IndexExpression should return two rows on recoverAndOpen", 2, rows.size());
+        assertTrue("First result should be 'k1'",ByteBufferUtil.bytes("k1").equals(rows.get(0).key.key));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/util/BufferedRandomAccessFileTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/util/BufferedRandomAccessFileTest.java
index 3f24d583..f20fc374 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/util/BufferedRandomAccessFileTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/io/util/BufferedRandomAccessFileTest.java
@@ -1 +1,654 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+package org.apache.cassandra.io.util;
+
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+import java.io.EOFException;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.ClosedChannelException;
+import java.util.Arrays;
+import java.util.concurrent.Callable;
+
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.assertEquals;
+
+import org.junit.Test;
+
+public class BufferedRandomAccessFileTest
+{
+    @Test
+    public void testReadAndWrite() throws Exception
+    {
+        BufferedRandomAccessFile file = createTempFile("braf");
+
+        // writting string of data to the file
+        byte[] data = "Hello".getBytes();
+        file.write(data);
+        assertEquals(file.length(), data.length);
+        assertEquals(file.getFilePointer(), data.length);
+
+        // reading small amount of data from file, this is handled by initial buffer
+        file.seek(0);
+        byte[] buffer = new byte[data.length];
+        assertEquals(file.read(buffer), data.length);
+        assertTrue(Arrays.equals(buffer, data)); // we read exactly what we wrote
+        assertEquals(file.read(), -1); // nothing more to read EOF
+        assert file.bytesRemaining() == 0 && file.isEOF();
+
+        // writing buffer bigger than page size, which will trigger reBuffer()
+        byte[] bigData = new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE + 10];
+
+        for (int i = 0; i < bigData.length; i++)
+            bigData[i] = 'd';
+
+        long initialPosition = file.getFilePointer();
+        file.write(bigData); // writing data
+        assertEquals(file.getFilePointer(), initialPosition + bigData.length);
+        assertEquals(file.length(), initialPosition + bigData.length); // file size should equals to last position
+
+        // reading written buffer
+        file.seek(initialPosition); // back to initial (before write) position
+        data = new byte[bigData.length];
+        long sizeRead = 0;
+        for (int i = 0; i < data.length; i++)
+        {
+            data[i] = (byte) file.read(); // this will trigger reBuffer()
+            sizeRead++;
+        }
+
+        assertEquals(sizeRead, data.length); // read exactly data.length bytes
+        assertEquals(file.getFilePointer(), initialPosition + data.length);
+        assertEquals(file.length(), initialPosition + bigData.length);
+        assertTrue(Arrays.equals(bigData, data));
+        assert file.bytesRemaining() == 0 && file.isEOF(); // we are at the of the file
+
+        // test readBytes(int) method
+        file.seek(0);
+        ByteBuffer fileContent = file.readBytes((int) file.length());
+        assertEquals(fileContent.limit(), file.length());
+        assert ByteBufferUtil.string(fileContent).equals("Hello" + new String(bigData));
+
+        // read the same buffer but using readFully(int)
+        data = new byte[bigData.length];
+        file.seek(initialPosition);
+        file.readFully(data);
+        assert file.bytesRemaining() == 0 && file.isEOF(); // we should be at EOF
+        assertTrue(Arrays.equals(bigData, data));
+
+        // try to read past mark (all methods should return -1)
+        data = new byte[10];
+        assertEquals(file.read(), -1);
+        assertEquals(file.read(data), -1);
+        assertEquals(file.read(data, 0, data.length), -1);
+
+        // test read(byte[], int, int)
+        file.seek(0);
+        data = new byte[20];
+        assertEquals(file.read(data, 0, 15), 15);
+        assertTrue(new String(data).contains("Hellodddddddddd"));
+        for (int i = 16; i < data.length; i++)
+        {
+            assert data[i] == 0;
+        }
+
+        // try to seek past EOF
+        file.seek(file.length() + 10); // should not throw an exception
+        assert file.bytesRemaining() == 0 && file.isEOF();
+
+        file.close();
+    }
+
+     @Test
+    public void testReadsAndWriteOnCapacity() throws IOException
+    {
+        File tmpFile = File.createTempFile("readtest", "bin");
+        BufferedRandomAccessFile rw = new BufferedRandomAccessFile(tmpFile, "rw");
+
+        // Fully write the file and sync..
+        byte[] in = new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE];
+        rw.write(in);
+
+        // Read it into a same size array.
+        byte[] out = new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE];
+        rw.read(out);
+
+        // We're really at the end.
+        long rem = rw.bytesRemaining();
+        assert rw.isEOF();
+        assert rem == 0 : "BytesRemaining should be 0 but it's " + rem;
+
+        // Cannot read any more.
+        int negone = rw.read();
+        assert negone == -1 : "We read past the end of the file, should have gotten EOF -1. Instead, " + negone;
+
+        // Writing will succeed
+        rw.write(new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE]);
+        // Forcing a rebuffer here
+        rw.write(42);
+    }
+
+    @Test
+    public void testLength() throws IOException
+    {
+        File tmpFile = File.createTempFile("lengthtest", "bin");
+        BufferedRandomAccessFile rw = new BufferedRandomAccessFile(tmpFile, "rw");
+        assertEquals(0, rw.length());
+
+        // write a chunk smaller then our buffer, so will not be flushed
+        // to disk
+        byte[] lessThenBuffer = new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE / 2];
+        rw.write(lessThenBuffer);
+        assertEquals(lessThenBuffer.length, rw.length());
+
+        // sync the data and check length
+        rw.sync();
+        assertEquals(lessThenBuffer.length, rw.length());
+
+        // write more then the buffer can hold and check length
+        byte[] biggerThenBuffer = new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE * 2];
+        rw.write(biggerThenBuffer);
+        assertEquals(biggerThenBuffer.length + lessThenBuffer.length, rw.length());
+
+        // checking that reading doesn't interfere
+        rw.seek(0);
+        rw.read();
+        assertEquals(biggerThenBuffer.length + lessThenBuffer.length, rw.length());
+
+        rw.close();
+
+        // will use cachedlength
+        BufferedRandomAccessFile r = new BufferedRandomAccessFile(tmpFile, "r");
+        assertEquals(lessThenBuffer.length + biggerThenBuffer.length, r.length());
+        r.close();
+    }
+
+    @Test
+    public void testReadBytes() throws IOException
+    {
+        final BufferedRandomAccessFile file = createTempFile("brafReadBytes");
+
+        byte[] data = new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE + 10];
+
+        for (int i = 0; i < data.length; i++)
+        {
+            data[i] = 'c';
+        }
+
+        file.write(data);
+
+        file.seek(0);
+        ByteBuffer content = file.readBytes((int) file.length());
+
+        // after reading whole file we should be at EOF
+        assertEquals(ByteBufferUtil.compare(content, data), 0);
+        assert file.bytesRemaining() == 0 && file.isEOF();
+
+        file.seek(0);
+        content = file.readBytes(10); // reading first 10 bytes
+        assertEquals(ByteBufferUtil.compare(content, "cccccccccc".getBytes()), 0);
+        assertEquals(file.bytesRemaining(), file.length() - content.limit());
+
+        // trying to read more than file has right now
+        expectEOF(new Callable<Object>()
+        {
+            public Object call() throws IOException
+            {
+                return file.readBytes((int) file.length() + 10);
+            }
+        });
+
+        file.close();
+    }
+
+    @Test
+    public void testSeek() throws Exception
+    {
+        final BufferedRandomAccessFile file = createTempFile("brafSeek");
+        byte[] data = new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE + 20];
+        for (int i = 0; i < data.length; i++)
+        {
+            data[i] = 'c';
+        }
+
+        file.write(data);
+        assert file.bytesRemaining() == 0 && file.isEOF();
+
+        file.seek(0);
+        assertEquals(file.getFilePointer(), 0);
+        assertEquals(file.bytesRemaining(), file.length());
+
+        file.seek(20);
+        assertEquals(file.getFilePointer(), 20);
+        assertEquals(file.bytesRemaining(), file.length() - 20);
+
+        // trying to seek past the end of the file
+        file.seek(file.length() + 30);
+        assertEquals(file.getFilePointer(), data.length + 30);
+        assertEquals(file.getFilePointer(), file.length()); // length should be at seek position
+        assert file.bytesRemaining() == 0 && file.isEOF();
+
+        expectException(new Callable<Object>()
+        {
+            public Object call() throws IOException
+            {
+                file.seek(-1);
+                return null;
+            }
+        }, IllegalArgumentException.class); // throws IllegalArgumentException
+
+        file.close();
+    }
+
+    @Test
+    public void testSkipBytes() throws IOException
+    {
+        BufferedRandomAccessFile file = createTempFile("brafSkipBytes");
+        byte[] data = new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE * 2];
+
+        file.write(data);
+        assert file.bytesRemaining() == 0 && file.isEOF();
+
+        file.seek(0); // back to the beginning of the file
+        assertEquals(file.skipBytes(10), 10);
+        assertEquals(file.bytesRemaining(), file.length() - 10);
+
+        int initialPosition = (int) file.getFilePointer();
+        // can't skip more than file size
+        assertEquals(file.skipBytes((int) file.length() + 10), file.length() - initialPosition);
+        assertEquals(file.getFilePointer(), file.length());
+        assert file.bytesRemaining() == 0 && file.isEOF();
+
+        file.seek(0);
+
+        // skipping negative amount should return 0
+        assertEquals(file.skipBytes(-1000), 0);
+        assertEquals(file.getFilePointer(), 0);
+        assertEquals(file.bytesRemaining(), file.length());
+
+        file.close();
+    }
+
+    @Test
+    public void testGetFilePointer() throws IOException
+    {
+        final BufferedRandomAccessFile file = createTempFile("brafGetFilePointer");
+
+        assertEquals(file.getFilePointer(), 0); // initial position should be 0
+
+        file.write(new byte[20]);
+        assertEquals(file.getFilePointer(), 20); // position 20 after writing 20 bytes
+
+        file.seek(10);
+        assertEquals(file.getFilePointer(), 10); // after seek to 10 should be 10
+
+        expectException(new Callable<Object>()
+        {
+            public Object call() throws IOException
+            {
+                file.seek(-1);
+                return null;
+            }
+        }, IllegalArgumentException.class);
+
+        assertEquals(file.getFilePointer(), 10);
+
+        file.seek(30); // past previous end file
+        assertEquals(file.getFilePointer(), 30);
+
+        // position should change after skip bytes
+        file.seek(0);
+        file.skipBytes(15);
+        assertEquals(file.getFilePointer(), 15);
+
+        file.read();
+        assertEquals(file.getFilePointer(), 16);
+        file.read(new byte[4]);
+        assertEquals(file.getFilePointer(), 20);
+
+        file.close();
+    }
+
+    @Test
+    public void testGetPath() throws IOException
+    {
+        BufferedRandomAccessFile file = createTempFile("brafGetPath");
+        assert file.getPath().contains("brafGetPath");
+    }
+
+     @Test
+    public void testIsEOF() throws IOException
+    {
+        for (String mode : Arrays.asList("r", "rw")) // read, read+write
+        {
+            for (int bufferSize : Arrays.asList(1, 2, 3, 5, 8, 64))  // smaller, equal, bigger buffer sizes
+            {
+                final byte[] target = new byte[32];
+
+                // single too-large read
+                for (final int offset : Arrays.asList(0, 8))
+                {
+                    final BufferedRandomAccessFile file = new BufferedRandomAccessFile(writeTemporaryFile(new byte[16]), mode, bufferSize);
+                    expectEOF(new Callable<Object>()
+                    {
+                        public Object call() throws IOException
+                        {
+                            file.readFully(target, offset, 17);
+                            return null;
+                        }
+                    });
+                }
+
+                // first read is ok but eventually EOFs
+                for (final int n : Arrays.asList(1, 2, 4, 8))
+                {
+                    final BufferedRandomAccessFile file = new BufferedRandomAccessFile(writeTemporaryFile(new byte[16]), mode, bufferSize);
+                    expectEOF(new Callable<Object>()
+                    {
+                        public Object call() throws IOException
+                        {
+                            while (true)
+                                file.readFully(target, 0, n);
+                        }
+                    });
+                }
+            }
+        }
+    }
+
+    @Test
+    public void testNotEOF() throws IOException
+    {
+        assertEquals(1, new BufferedRandomAccessFile(writeTemporaryFile(new byte[1]), "rw").read(new byte[2]));
+    }
+
+    @Test
+    public void testBytesRemaining() throws IOException
+    {
+        BufferedRandomAccessFile file = createTempFile("brafBytesRemaining");
+        assertEquals(file.bytesRemaining(), 0);
+
+        int toWrite = BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE + 10;
+
+        file.write(new byte[toWrite]);
+        assertEquals(file.bytesRemaining(), 0);
+
+        file.seek(0);
+        assertEquals(file.bytesRemaining(), toWrite);
+
+        for (int i = 1; i <= file.length(); i++)
+        {
+            file.read();
+            assertEquals(file.bytesRemaining(), file.length() - i);
+        }
+
+        file.seek(0);
+        file.skipBytes(10);
+        assertEquals(file.bytesRemaining(), file.length() - 10);
+
+        file.close();
+    }
+
+    @Test
+    public void testBytesPastMark() throws IOException
+    {
+        File tmpFile = File.createTempFile("overflowtest", "bin");
+        tmpFile.deleteOnExit();
+
+        // Create the BRAF by filename instead of by file.
+        final BufferedRandomAccessFile rw = new BufferedRandomAccessFile(tmpFile.getPath(), "rw");
+        assert tmpFile.getPath().equals(rw.getPath());
+
+        // Create a mark and move the rw there.
+        final FileMark mark = rw.mark();
+        rw.reset(mark);
+
+        // Expect this call to succeed.
+        rw.bytesPastMark(mark);
+    }
+
+    @Test
+    public void testClose() throws IOException
+    {
+        final BufferedRandomAccessFile file = createTempFile("brafClose");
+
+        byte[] data = new byte[BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE + 20];
+        for (int i = 0; i < data.length; i++)
+        {
+            data[i] = 'c';
+        }
+
+        file.write(data);
+        file.close();
+
+        expectException(new Callable<Object>()
+        {
+            public Object call() throws IOException
+            {
+                return file.read();
+            }
+        }, ClosedChannelException.class);
+
+        expectException(new Callable<Object>()
+        {
+            public Object call() throws IOException
+            {
+                file.write(new byte[1]);
+                return null;
+            }
+        }, ClosedChannelException.class);
+
+        BufferedRandomAccessFile copy = new BufferedRandomAccessFile(file.getPath(), "r");
+        ByteBuffer contents = copy.readBytes((int) copy.length());
+
+        assertEquals(contents.limit(), data.length);
+        assertEquals(ByteBufferUtil.compare(contents, data), 0);
+    }
+
+    @Test
+    public void testMarkAndReset() throws IOException
+    {
+        BufferedRandomAccessFile file = createTempFile("brafTestMark");
+        file.write(new byte[30]);
+
+        file.seek(10);
+        FileMark mark = file.mark();
+
+        file.seek(file.length());
+        assertTrue(file.isEOF());
+
+        file.reset();
+        assertEquals(file.bytesRemaining(), 20);
+
+        file.seek(file.length());
+        assertTrue(file.isEOF());
+
+        file.reset(mark);
+        assertEquals(file.bytesRemaining(), 20);
+
+        file.seek(file.length());
+        assertEquals(file.bytesPastMark(), 20);
+        assertEquals(file.bytesPastMark(mark), 20);
+
+        file.reset(mark);
+        assertEquals(file.bytesPastMark(), 0);
+
+        file.close();
+    }
+
+    @Test (expected = AssertionError.class)
+    public void testAssertionErrorWhenBytesPastMarkIsNegative() throws IOException
+    {
+        BufferedRandomAccessFile file = createTempFile("brafAssertionErrorWhenBytesPastMarkIsNegative");
+        file.write(new byte[30]);
+
+        file.seek(10);
+        file.mark();
+
+        file.seek(0);
+        file.bytesPastMark();
+    }
+
+    @Test
+    public void testReadOnly() throws IOException
+    {
+        BufferedRandomAccessFile file = createTempFile("brafReadOnlyTest");
+
+        byte[] data = new byte[20];
+        for (int i = 0; i < data.length; i++)
+            data[i] = 'c';
+
+        file.write(data);
+        file.sync(); // flushing file to the disk
+
+        // read-only copy of the file, with fixed file length
+        final BufferedRandomAccessFile copy = new BufferedRandomAccessFile(file.getPath(), "r");
+
+        copy.seek(copy.length());
+        assertTrue(copy.bytesRemaining() == 0 && copy.isEOF());
+
+        // can't seek past the end of the file for read-only files
+        expectEOF(new Callable<Object>()
+        {
+            public Object call() throws IOException
+            {
+                copy.seek(copy.length() + 1);
+                return null;
+            }
+        });
+
+        /* Any write() call should fail */
+        expectException(new Callable<Object>()
+        {
+            public Object call() throws IOException
+            {
+                copy.write(1);
+                return null;
+            }
+        }, IOException.class);
+
+        expectException(new Callable<Object>()
+        {
+            public Object call() throws IOException
+            {
+                copy.write(new byte[1]);
+                return null;
+            }
+        }, IOException.class);
+
+        expectException(new Callable<Object>()
+        {
+            public Object call() throws IOException
+            {
+                copy.write(new byte[3], 0, 2);
+                return null;
+            }
+        }, IOException.class);
+
+        copy.seek(0);
+        copy.skipBytes(5);
+
+        assertEquals(copy.bytesRemaining(), 15);
+        assertEquals(copy.getFilePointer(), 5);
+        assertTrue(!copy.isEOF());
+
+        copy.seek(0);
+        ByteBuffer contents = copy.readBytes((int) copy.length());
+
+        assertEquals(contents.limit(), copy.length());
+        assertTrue(ByteBufferUtil.compare(contents, data) == 0);
+
+        copy.seek(0);
+
+        int count = 0;
+        while (!copy.isEOF())
+        {
+            assertEquals((byte) copy.read(), 'c');
+            count++;
+        }
+
+        assertEquals(count, copy.length());
+
+        copy.seek(0);
+        byte[] content = new byte[10];
+        copy.read(content);
+
+        assertEquals(new String(content), "cccccccccc");
+
+        file.close();
+        copy.close();
+    }
+
+    @Test
+    public void testSeekPastEOF() throws IOException
+    {
+        BufferedRandomAccessFile file = createTempFile("brafTestSeekPastEOF");
+        file.seek(1);
+        file.write(1);
+        file.seek(0);
+        assertEquals(0, file.read());
+        assertEquals(1, file.read());
+    }
+
+    private void expectException(Callable<?> callable, Class<?> exception)
+    {
+        boolean thrown = false;
+
+        try
+        {
+            callable.call();
+        }
+        catch (Exception e)
+        {
+            assert e.getClass().equals(exception) : e.getClass().getName() + " is not " + exception.getName();
+            thrown = true;
+        }
+
+        assert thrown : exception.getName() + " not received";
+    }
+
+    private void expectEOF(Callable<?> callable)
+    {
+        expectException(callable, EOFException.class);
+    }
+
+    private BufferedRandomAccessFile createTempFile(String name) throws IOException
+    {
+        File tempFile = File.createTempFile(name, null);
+        tempFile.deleteOnExit();
+
+        return new BufferedRandomAccessFile(tempFile, "rw");
+    }
+
+    private File writeTemporaryFile(byte[] data) throws IOException
+    {
+        File f = File.createTempFile("BRAFTestFile", null);
+        f.deleteOnExit();
+        FileOutputStream fout = new FileOutputStream(f);
+        fout.write(data);
+        fout.getFD().sync();
+        fout.close();
+        return f;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/DynamicEndpointSnitchTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/DynamicEndpointSnitchTest.java
index 3f24d583..46a7bf55 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/DynamicEndpointSnitchTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/DynamicEndpointSnitchTest.java
@@ -1 +1,109 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.ArrayList;
+
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.service.StorageService;
+import org.junit.Test;
+
+import org.apache.cassandra.utils.FBUtilities;
+
+public class DynamicEndpointSnitchTest
+{
+    @Test
+    public void testSnitch() throws InterruptedException, IOException, ConfigurationException
+    {
+        // do this because SS needs to be initialized before DES can work properly.
+        StorageService.instance.initClient();
+        int sleeptime = 150;
+        DynamicEndpointSnitch dsnitch = new DynamicEndpointSnitch(new SimpleSnitch());
+        InetAddress self = FBUtilities.getLocalAddress();
+        ArrayList<InetAddress> order = new ArrayList<InetAddress>();
+        InetAddress host1 = InetAddress.getByName("127.0.0.1");
+        InetAddress host2 = InetAddress.getByName("127.0.0.2");
+        InetAddress host3 = InetAddress.getByName("127.0.0.3");
+
+        // first, make all hosts equal
+        for (int i = 0; i < 5; i++)
+        {
+            dsnitch.receiveTiming(host1, 1.0);
+            dsnitch.receiveTiming(host2, 1.0);
+            dsnitch.receiveTiming(host3, 1.0);
+        }
+
+        Thread.sleep(sleeptime);
+
+        order.add(host1);
+        order.add(host2);
+        order.add(host3);
+        assert dsnitch.getSortedListByProximity(self, order).equals(order);
+
+        // make host1 a little worse
+        dsnitch.receiveTiming(host1, 2.0);
+        Thread.sleep(sleeptime);
+
+        order.clear();
+        order.add(host2);
+        order.add(host3);
+        order.add(host1);
+        assert dsnitch.getSortedListByProximity(self, order).equals(order);
+
+        // make host2 as bad as host1
+        dsnitch.receiveTiming(host2, 2.0);
+        Thread.sleep(sleeptime);
+
+        order.clear();
+        order.add(host3);
+        order.add(host1);
+        order.add(host2);
+        assert dsnitch.getSortedListByProximity(self, order).equals(order);
+
+        // make host3 the worst
+        for (int i = 0; i < 2; i++)
+        {
+            dsnitch.receiveTiming(host3, 2.0);
+        }
+        Thread.sleep(sleeptime);
+
+        order.clear();
+        order.add(host1);
+        order.add(host2);
+        order.add(host3);
+        assert dsnitch.getSortedListByProximity(self, order).equals(order);
+
+        // make host3 equal to the others
+        for (int i = 0; i < 2; i++)
+        {
+            dsnitch.receiveTiming(host3, 1.0);
+        }
+        Thread.sleep(sleeptime);
+
+        order.clear();
+        order.add(host1);
+        order.add(host2);
+        order.add(host3);
+        assert dsnitch.getSortedListByProximity(self, order).equals(order);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/NetworkTopologyStrategyTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/NetworkTopologyStrategyTest.java
index e69de29b..04cffbb2 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/NetworkTopologyStrategyTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/NetworkTopologyStrategyTest.java
@@ -0,0 +1,116 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.locator;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import javax.xml.parsers.ParserConfigurationException;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.dht.StringToken;
+import org.apache.cassandra.dht.Token;
+import org.xml.sax.SAXException;
+
+public class NetworkTopologyStrategyTest
+{
+    private String table = "Keyspace1";
+
+    @Test
+    public void testProperties() throws IOException, ParserConfigurationException, SAXException, ConfigurationException
+    {
+        IEndpointSnitch snitch = new PropertyFileSnitch();
+        TokenMetadata metadata = new TokenMetadata();
+        createDummyTokens(metadata, true);
+
+        Map<String, String> configOptions = new HashMap<String, String>();
+        configOptions.put("DC1", "3");
+        configOptions.put("DC2", "2");
+        configOptions.put("DC3", "1");
+
+        // Set the localhost to the tokenmetadata. Embedded cassandra way?
+        NetworkTopologyStrategy strategy = new NetworkTopologyStrategy(table, metadata, snitch, configOptions);
+        assert strategy.getReplicationFactor("DC1") == 3;
+        assert strategy.getReplicationFactor("DC2") == 2;
+        assert strategy.getReplicationFactor("DC3") == 1;
+        // Query for the natural hosts
+        ArrayList<InetAddress> endpoints = strategy.getNaturalEndpoints(new StringToken("123"));
+        assert 6 == endpoints.size();
+        assert 6 == new HashSet<InetAddress>(endpoints).size(); // ensure uniqueness
+    }
+
+    @Test
+    public void testPropertiesWithEmptyDC() throws IOException, ParserConfigurationException, SAXException, ConfigurationException
+    {
+        IEndpointSnitch snitch = new PropertyFileSnitch();
+        TokenMetadata metadata = new TokenMetadata();
+        createDummyTokens(metadata, false);
+
+        Map<String, String> configOptions = new HashMap<String, String>();
+        configOptions.put("DC1", "3");
+        configOptions.put("DC2", "3");
+        configOptions.put("DC3", "0");
+
+        // Set the localhost to the tokenmetadata. Embedded cassandra way?
+        NetworkTopologyStrategy strategy = new NetworkTopologyStrategy(table, metadata, snitch, configOptions);
+        assert strategy.getReplicationFactor("DC1") == 3;
+        assert strategy.getReplicationFactor("DC2") == 3;
+        assert strategy.getReplicationFactor("DC3") == 0;
+        // Query for the natural hosts
+        ArrayList<InetAddress> endpoints = strategy.getNaturalEndpoints(new StringToken("123"));
+        assert 6 == endpoints.size();
+        assert 6 == new HashSet<InetAddress>(endpoints).size(); // ensure uniqueness
+    }
+
+    public void createDummyTokens(TokenMetadata metadata, boolean populateDC3) throws UnknownHostException
+    {
+        // DC 1
+        tokenFactory(metadata, "123", new byte[]{ 10, 0, 0, 10 });
+        tokenFactory(metadata, "234", new byte[]{ 10, 0, 0, 11 });
+        tokenFactory(metadata, "345", new byte[]{ 10, 0, 0, 12 });
+        // Tokens for DC 2
+        tokenFactory(metadata, "789", new byte[]{ 10, 20, 114, 10 });
+        tokenFactory(metadata, "890", new byte[]{ 10, 20, 114, 11 });
+        //tokens for DC3
+        if (populateDC3)
+        {
+            tokenFactory(metadata, "456", new byte[]{ 10, 21, 119, 13 });
+            tokenFactory(metadata, "567", new byte[]{ 10, 21, 119, 10 });
+        }
+        // Extra Tokens
+        tokenFactory(metadata, "90A", new byte[]{ 10, 0, 0, 13 });
+        if (populateDC3)
+            tokenFactory(metadata, "0AB", new byte[]{ 10, 21, 119, 14 });
+        tokenFactory(metadata, "ABC", new byte[]{ 10, 20, 114, 15 });
+    }
+
+    public void tokenFactory(TokenMetadata metadata, String token, byte[] bytes) throws UnknownHostException
+    {
+        Token token1 = new StringToken(token);
+        InetAddress add1 = InetAddress.getByAddress(bytes);
+        metadata.updateNormalToken(token1, add1);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/OldNetworkTopologyStrategyTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/OldNetworkTopologyStrategyTest.java
index e69de29b..288491d3 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/OldNetworkTopologyStrategyTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/OldNetworkTopologyStrategyTest.java
@@ -0,0 +1,164 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.locator;
+
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.junit.Assert.assertEquals;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.dht.BigIntegerToken;
+import org.apache.cassandra.dht.Token;
+
+public class OldNetworkTopologyStrategyTest extends SchemaLoader
+{
+    private List<Token> endpointTokens;
+    private List<Token> keyTokens;
+    private TokenMetadata tmd;
+    private Map<String, ArrayList<InetAddress>> expectedResults;
+
+    @Before
+    public void init()
+    {
+        endpointTokens = new ArrayList<Token>();
+        keyTokens = new ArrayList<Token>();
+        tmd = new TokenMetadata();
+        expectedResults = new HashMap<String, ArrayList<InetAddress>>();
+    }
+
+    /**
+     * 4 same rack endpoints
+     *
+     * @throws UnknownHostException
+     */
+    @Test
+    public void testBigIntegerEndpointsA() throws UnknownHostException
+    {
+        RackInferringSnitch endpointSnitch = new RackInferringSnitch();
+
+        AbstractReplicationStrategy strategy = new OldNetworkTopologyStrategy("Keyspace1", tmd, endpointSnitch, null);
+        addEndpoint("0", "5", "254.0.0.1");
+        addEndpoint("10", "15", "254.0.0.2");
+        addEndpoint("20", "25", "254.0.0.3");
+        addEndpoint("30", "35", "254.0.0.4");
+
+        expectedResults.put("5", buildResult("254.0.0.2", "254.0.0.3", "254.0.0.4"));
+        expectedResults.put("15", buildResult("254.0.0.3", "254.0.0.4", "254.0.0.1"));
+        expectedResults.put("25", buildResult("254.0.0.4", "254.0.0.1", "254.0.0.2"));
+        expectedResults.put("35", buildResult("254.0.0.1", "254.0.0.2", "254.0.0.3"));
+
+        testGetEndpoints(strategy, keyTokens.toArray(new Token[0]));
+    }
+
+    /**
+     * 3 same rack endpoints
+     * 1 external datacenter
+     *
+     * @throws UnknownHostException
+     */
+    @Test
+    public void testBigIntegerEndpointsB() throws UnknownHostException
+    {
+        RackInferringSnitch endpointSnitch = new RackInferringSnitch();
+
+        AbstractReplicationStrategy strategy = new OldNetworkTopologyStrategy("Keyspace1", tmd, endpointSnitch, null);
+        addEndpoint("0", "5", "254.0.0.1");
+        addEndpoint("10", "15", "254.0.0.2");
+        addEndpoint("20", "25", "254.1.0.3");
+        addEndpoint("30", "35", "254.0.0.4");
+
+        expectedResults.put("5", buildResult("254.0.0.2", "254.1.0.3", "254.0.0.4"));
+        expectedResults.put("15", buildResult("254.1.0.3", "254.0.0.4", "254.0.0.1"));
+        expectedResults.put("25", buildResult("254.0.0.4", "254.1.0.3", "254.0.0.1"));
+        expectedResults.put("35", buildResult("254.0.0.1", "254.1.0.3", "254.0.0.2"));
+
+        testGetEndpoints(strategy, keyTokens.toArray(new Token[0]));
+    }
+
+    /**
+     * 2 same rack endpoints
+     * 1 same datacenter, different rack endpoints
+     * 1 external datacenter
+     *
+     * @throws UnknownHostException
+     */
+    @Test
+    public void testBigIntegerEndpointsC() throws UnknownHostException
+    {
+        RackInferringSnitch endpointSnitch = new RackInferringSnitch();
+
+        AbstractReplicationStrategy strategy = new OldNetworkTopologyStrategy("Keyspace1", tmd, endpointSnitch, null);
+        addEndpoint("0", "5", "254.0.0.1");
+        addEndpoint("10", "15", "254.0.0.2");
+        addEndpoint("20", "25", "254.0.1.3");
+        addEndpoint("30", "35", "254.1.0.4");
+
+        expectedResults.put("5", buildResult("254.0.0.2", "254.0.1.3", "254.1.0.4"));
+        expectedResults.put("15", buildResult("254.0.1.3", "254.1.0.4", "254.0.0.1"));
+        expectedResults.put("25", buildResult("254.1.0.4", "254.0.0.1", "254.0.0.2"));
+        expectedResults.put("35", buildResult("254.0.0.1", "254.0.1.3", "254.1.0.4"));
+
+        testGetEndpoints(strategy, keyTokens.toArray(new Token[0]));
+    }
+
+    private ArrayList<InetAddress> buildResult(String... addresses) throws UnknownHostException
+    {
+        ArrayList<InetAddress> result = new ArrayList<InetAddress>();
+        for (String address : addresses)
+        {
+            result.add(InetAddress.getByName(address));
+        }
+        return result;
+    }
+
+    private void addEndpoint(String endpointTokenID, String keyTokenID, String endpointAddress) throws UnknownHostException
+    {
+        BigIntegerToken endpointToken = new BigIntegerToken(endpointTokenID);
+        endpointTokens.add(endpointToken);
+
+        BigIntegerToken keyToken = new BigIntegerToken(keyTokenID);
+        keyTokens.add(keyToken);
+
+        InetAddress ep = InetAddress.getByName(endpointAddress);
+        tmd.updateNormalToken(endpointToken, ep);
+    }
+
+    private void testGetEndpoints(AbstractReplicationStrategy strategy, Token[] keyTokens) throws UnknownHostException
+    {
+        for (Token keyToken : keyTokens)
+        {
+            List<InetAddress> endpoints = strategy.getNaturalEndpoints(keyToken);
+            for (int j = 0; j < endpoints.size(); j++)
+            {
+                ArrayList<InetAddress> hostsExpected = expectedResults.get(keyToken.toString());
+                assertEquals(endpoints.get(j), hostsExpected.get(j));
+            }
+        }
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/ReplicationStrategyEndpointCacheTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/ReplicationStrategyEndpointCacheTest.java
index e69de29b..33dc4cc4 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/ReplicationStrategyEndpointCacheTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/ReplicationStrategyEndpointCacheTest.java
@@ -0,0 +1,174 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.locator;
+
+import java.net.InetAddress;
+import java.util.*;
+
+import org.apache.cassandra.db.Table;
+
+import org.apache.commons.lang.StringUtils;
+import org.junit.Test;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.dht.BigIntegerToken;
+import org.apache.cassandra.dht.Token;
+
+public class ReplicationStrategyEndpointCacheTest extends SchemaLoader
+{
+    private TokenMetadata tmd;
+    private Token searchToken;
+    private AbstractReplicationStrategy strategy;
+
+    public void setup(Class stratClass, Map<String, String> strategyOptions) throws Exception
+    {
+        tmd = new TokenMetadata();
+        searchToken = new BigIntegerToken(String.valueOf(15));
+
+        strategy = getStrategyWithNewTokenMetadata(Table.open("Keyspace3").getReplicationStrategy(), tmd);
+
+        tmd.updateNormalToken(new BigIntegerToken(String.valueOf(10)), InetAddress.getByName("127.0.0.1"));
+        tmd.updateNormalToken(new BigIntegerToken(String.valueOf(20)), InetAddress.getByName("127.0.0.2"));
+        tmd.updateNormalToken(new BigIntegerToken(String.valueOf(30)), InetAddress.getByName("127.0.0.3"));
+        tmd.updateNormalToken(new BigIntegerToken(String.valueOf(40)), InetAddress.getByName("127.0.0.4"));
+        //tmd.updateNormalToken(new BigIntegerToken(String.valueOf(50)), InetAddress.getByName("127.0.0.5"));
+        tmd.updateNormalToken(new BigIntegerToken(String.valueOf(60)), InetAddress.getByName("127.0.0.6"));
+        tmd.updateNormalToken(new BigIntegerToken(String.valueOf(70)), InetAddress.getByName("127.0.0.7"));
+        tmd.updateNormalToken(new BigIntegerToken(String.valueOf(80)), InetAddress.getByName("127.0.0.8"));
+    }
+
+    @Test
+    public void testEndpointsWereCached() throws Exception
+    {
+        runEndpointsWereCachedTest(FakeSimpleStrategy.class, null);
+        runEndpointsWereCachedTest(FakeOldNetworkTopologyStrategy.class, null);
+        runEndpointsWereCachedTest(FakeNetworkTopologyStrategy.class, new HashMap<String, String>());
+    }
+
+    public void runEndpointsWereCachedTest(Class stratClass, Map<String, String> configOptions) throws Exception
+    {
+        setup(stratClass, configOptions);
+        assert strategy.getNaturalEndpoints(searchToken).equals(strategy.getNaturalEndpoints(searchToken));
+    }
+
+    @Test
+    public void testCacheRespectsTokenChanges() throws Exception
+    {
+        runCacheRespectsTokenChangesTest(SimpleStrategy.class, null);
+        runCacheRespectsTokenChangesTest(OldNetworkTopologyStrategy.class, null);
+        runCacheRespectsTokenChangesTest(NetworkTopologyStrategy.class, new HashMap<String, String>());
+    }
+
+    public void runCacheRespectsTokenChangesTest(Class stratClass, Map<String, String> configOptions) throws Exception
+    {
+        setup(stratClass, configOptions);
+        ArrayList<InetAddress> initial;
+        ArrayList<InetAddress> endpoints;
+
+        endpoints = strategy.getNaturalEndpoints(searchToken);
+        assert endpoints.size() == 5 : StringUtils.join(endpoints, ",");
+
+        // test token addition, in DC2 before existing token
+        initial = strategy.getNaturalEndpoints(searchToken);
+        tmd.updateNormalToken(new BigIntegerToken(String.valueOf(35)), InetAddress.getByName("127.0.0.5"));
+        endpoints = strategy.getNaturalEndpoints(searchToken);
+        assert endpoints.size() == 5 : StringUtils.join(endpoints, ",");
+        assert !endpoints.equals(initial);
+
+        // test token removal, newly created token
+        initial = strategy.getNaturalEndpoints(searchToken);
+        tmd.removeEndpoint(InetAddress.getByName("127.0.0.5"));
+        endpoints = strategy.getNaturalEndpoints(searchToken);
+        assert endpoints.size() == 5 : StringUtils.join(endpoints, ",");
+        assert !endpoints.contains(InetAddress.getByName("127.0.0.5"));
+        assert !endpoints.equals(initial);
+
+        // test token change
+        initial = strategy.getNaturalEndpoints(searchToken);
+        //move .8 after search token but before other DC3
+        tmd.updateNormalToken(new BigIntegerToken(String.valueOf(25)), InetAddress.getByName("127.0.0.8"));
+        endpoints = strategy.getNaturalEndpoints(searchToken);
+        assert endpoints.size() == 5 : StringUtils.join(endpoints, ",");
+        assert !endpoints.equals(initial);
+    }
+
+    protected static class FakeSimpleStrategy extends SimpleStrategy
+    {
+        private boolean called = false;
+
+        public FakeSimpleStrategy(String table, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions)
+        {
+            super(table, tokenMetadata, snitch, configOptions);
+        }
+
+        public List<InetAddress> calculateNaturalEndpoints(Token token, TokenMetadata metadata)
+        {
+            assert !called : "calculateNaturalEndpoints was already called, result should have been cached";
+            called = true;
+            return super.calculateNaturalEndpoints(token, metadata);
+        }
+    }
+
+    protected static class FakeOldNetworkTopologyStrategy extends OldNetworkTopologyStrategy
+    {
+        private boolean called = false;
+
+        public FakeOldNetworkTopologyStrategy(String table, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions)
+        {
+            super(table, tokenMetadata, snitch, configOptions);
+        }
+
+        public List<InetAddress> calculateNaturalEndpoints(Token token, TokenMetadata metadata)
+        {
+            assert !called : "calculateNaturalEndpoints was already called, result should have been cached";
+            called = true;
+            return super.calculateNaturalEndpoints(token, metadata);
+        }
+    }
+
+    protected static class FakeNetworkTopologyStrategy extends NetworkTopologyStrategy
+    {
+        private boolean called = false;
+
+        public FakeNetworkTopologyStrategy(String table, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions) throws ConfigurationException
+        {
+            super(table, tokenMetadata, snitch, configOptions);
+        }
+
+        public List<InetAddress> calculateNaturalEndpoints(Token token, TokenMetadata metadata)
+        {
+            assert !called : "calculateNaturalEndpoints was already called, result should have been cached";
+            called = true;
+            return super.calculateNaturalEndpoints(token, metadata);
+        }
+    }
+
+    private AbstractReplicationStrategy getStrategyWithNewTokenMetadata(AbstractReplicationStrategy strategy, TokenMetadata newTmd) throws ConfigurationException
+    {
+        return AbstractReplicationStrategy.createReplicationStrategy(
+                strategy.table,
+                strategy.getClass().getName(),
+                newTmd,
+                strategy.snitch,
+                strategy.configOptions);
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/SimpleStrategyTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/SimpleStrategyTest.java
index e69de29b..42466e9f 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/SimpleStrategyTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/SimpleStrategyTest.java
@@ -0,0 +1,187 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.locator;
+
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.List;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.dht.*;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.service.StorageServiceAccessor;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+import static org.junit.Assert.*;
+
+public class SimpleStrategyTest extends CleanupHelper
+{
+    @Test
+    public void tryValidTable()
+    {
+        assert Table.open("Keyspace1").getReplicationStrategy() != null;
+    }
+
+    @Test
+    public void testBigIntegerEndpoints() throws UnknownHostException, ConfigurationException
+    {
+        List<Token> endpointTokens = new ArrayList<Token>();
+        List<Token> keyTokens = new ArrayList<Token>();
+        for (int i = 0; i < 5; i++) {
+            endpointTokens.add(new BigIntegerToken(String.valueOf(10 * i)));
+            keyTokens.add(new BigIntegerToken(String.valueOf(10 * i + 5)));
+        }
+        verifyGetNaturalEndpoints(endpointTokens.toArray(new Token[0]), keyTokens.toArray(new Token[0]));
+    }
+
+    @Test
+    public void testStringEndpoints() throws UnknownHostException, ConfigurationException
+    {
+        IPartitioner partitioner = new OrderPreservingPartitioner();
+
+        List<Token> endpointTokens = new ArrayList<Token>();
+        List<Token> keyTokens = new ArrayList<Token>();
+        for (int i = 0; i < 5; i++) {
+            endpointTokens.add(new StringToken(String.valueOf((char)('a' + i * 2))));
+            keyTokens.add(partitioner.getToken(ByteBufferUtil.bytes(String.valueOf((char)('a' + i * 2 + 1)))));
+        }
+        verifyGetNaturalEndpoints(endpointTokens.toArray(new Token[0]), keyTokens.toArray(new Token[0]));
+    }
+
+    // given a list of endpoint tokens, and a set of key tokens falling between the endpoint tokens,
+    // make sure that the Strategy picks the right endpoints for the keys.
+    private void verifyGetNaturalEndpoints(Token[] endpointTokens, Token[] keyTokens) throws UnknownHostException, ConfigurationException
+    {
+        TokenMetadata tmd;
+        AbstractReplicationStrategy strategy;
+        for (String table : DatabaseDescriptor.getNonSystemTables())
+        {
+            tmd = new TokenMetadata();
+            strategy = getStrategy(table, tmd);
+            List<InetAddress> hosts = new ArrayList<InetAddress>();
+            for (int i = 0; i < endpointTokens.length; i++)
+            {
+                InetAddress ep = InetAddress.getByName("127.0.0." + String.valueOf(i + 1));
+                tmd.updateNormalToken(endpointTokens[i], ep);
+                hosts.add(ep);
+            }
+
+            for (int i = 0; i < keyTokens.length; i++)
+            {
+                List<InetAddress> endpoints = strategy.getNaturalEndpoints(keyTokens[i]);
+                assertEquals(strategy.getReplicationFactor(), endpoints.size());
+                List<InetAddress> correctEndpoints = new ArrayList<InetAddress>();
+                for (int j = 0; j < endpoints.size(); j++)
+                    correctEndpoints.add(hosts.get((i + j + 1) % hosts.size()));
+                assertEquals(new HashSet<InetAddress>(correctEndpoints), new HashSet<InetAddress>(endpoints));
+            }
+        }
+    }
+    
+    @Test
+    public void testGetEndpointsDuringBootstrap() throws UnknownHostException, ConfigurationException
+    {
+        // the token difference will be RING_SIZE * 2.
+        final int RING_SIZE = 10;
+        TokenMetadata tmd = new TokenMetadata();
+        TokenMetadata oldTmd = StorageServiceAccessor.setTokenMetadata(tmd);
+
+        Token[] endpointTokens = new Token[RING_SIZE];
+        Token[] keyTokens = new Token[RING_SIZE];
+        
+        for (int i = 0; i < RING_SIZE; i++)
+        {
+            endpointTokens[i] = new BigIntegerToken(String.valueOf(RING_SIZE * 2 * i));
+            keyTokens[i] = new BigIntegerToken(String.valueOf(RING_SIZE * 2 * i + RING_SIZE));
+        }
+        
+        List<InetAddress> hosts = new ArrayList<InetAddress>();
+        for (int i = 0; i < endpointTokens.length; i++)
+        {
+            InetAddress ep = InetAddress.getByName("127.0.0." + String.valueOf(i + 1));
+            tmd.updateNormalToken(endpointTokens[i], ep);
+            hosts.add(ep);
+        }
+
+        // bootstrap at the end of the ring
+        Token bsToken = new BigIntegerToken(String.valueOf(210));
+        InetAddress bootstrapEndpoint = InetAddress.getByName("127.0.0.11");
+        tmd.addBootstrapToken(bsToken, bootstrapEndpoint);
+
+        AbstractReplicationStrategy strategy = null;
+        for (String table : DatabaseDescriptor.getNonSystemTables())
+        {
+            strategy = getStrategy(table, tmd);
+
+            StorageService.calculatePendingRanges(strategy, table);
+
+            int replicationFactor = strategy.getReplicationFactor();
+
+            for (int i = 0; i < keyTokens.length; i++)
+            {
+                Collection<InetAddress> endpoints = tmd.getWriteEndpoints(keyTokens[i], table, strategy.getNaturalEndpoints(keyTokens[i]));
+                assertTrue(endpoints.size() >= replicationFactor);
+
+                for (int j = 0; j < replicationFactor; j++)
+                {
+                    //Check that the old nodes are definitely included
+                    assertTrue(endpoints.contains(hosts.get((i + j + 1) % hosts.size())));
+                }
+
+                // bootstrapEndpoint should be in the endpoints for i in MAX-RF to MAX, but not in any earlier ep.
+                if (i < RING_SIZE - replicationFactor)
+                    assertFalse(endpoints.contains(bootstrapEndpoint));
+                else
+                    assertTrue(endpoints.contains(bootstrapEndpoint));
+            }
+        }
+
+        StorageServiceAccessor.setTokenMetadata(oldTmd);
+    }
+
+    private AbstractReplicationStrategy getStrategyWithNewTokenMetadata(AbstractReplicationStrategy strategy, TokenMetadata newTmd) throws ConfigurationException
+    {
+        return AbstractReplicationStrategy.createReplicationStrategy(
+                strategy.table,
+                strategy.getClass().getName(),
+                newTmd,
+                strategy.snitch,
+                strategy.configOptions);
+    }
+
+    private AbstractReplicationStrategy getStrategy(String table, TokenMetadata tmd) throws ConfigurationException
+    {
+        return AbstractReplicationStrategy.createReplicationStrategy(
+                table,
+                "org.apache.cassandra.locator.SimpleStrategy",
+                tmd,
+                new SimpleSnitch(),
+                null);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/TokenMetadataTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/TokenMetadataTest.java
index e69de29b..0a92c535 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/TokenMetadataTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/locator/TokenMetadataTest.java
@@ -0,0 +1,87 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.locator;
+
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.List;
+
+import com.google.common.collect.Iterators;
+
+import org.junit.BeforeClass;
+import org.junit.Test;
+import static org.junit.Assert.assertEquals;
+
+import org.apache.cassandra.CleanupHelper;
+import static org.apache.cassandra.Util.token;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.TokenMetadata;
+import org.apache.cassandra.service.StorageService;
+
+public class TokenMetadataTest
+{
+    public final static String ONE = "1";
+    public final static String SIX = "6";
+
+    public static ArrayList<Token> RING;
+
+    @BeforeClass
+    public static void beforeClass() throws Throwable
+    {
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+        tmd.updateNormalToken(token(ONE), InetAddress.getByName("127.0.0.1"));
+        tmd.updateNormalToken(token(SIX), InetAddress.getByName("127.0.0.6"));
+        RING = tmd.sortedTokens();
+    }
+
+    private void testRingIterator(String start, boolean includeMin, String... expected)
+    {
+        ArrayList<Token> actual = new ArrayList<Token>();
+        Iterators.addAll(actual, TokenMetadata.ringIterator(RING, token(start), includeMin));
+        assertEquals(actual.toString(), expected.length, actual.size());
+        for (int i = 0; i < expected.length; i++)
+            assertEquals("Mismatch at index " + i + ": " + actual, token(expected[i]), actual.get(i));
+    }
+
+    @Test
+    public void testRingIterator()
+    {
+        testRingIterator("2", false, "6", "1");
+        testRingIterator("7", false, "1", "6");
+        testRingIterator("0", false, "1", "6");
+        testRingIterator("", false, "1", "6");
+    }
+
+    @Test
+    public void testRingIteratorIncludeMin()
+    {
+        testRingIterator("2", true, "6", "", "1");
+        testRingIterator("7", true, "", "1", "6");
+        testRingIterator("0", true, "1", "6", "");
+        testRingIterator("", true, "1", "6", "");
+    }
+
+    @Test
+    public void testRingIteratorEmptyRing()
+    {
+        RING.clear();
+        testRingIterator("2", false);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/AntiEntropyServiceTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/AntiEntropyServiceTest.java
index 3f24d583..94350db1 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/AntiEntropyServiceTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/AntiEntropyServiceTest.java
@@ -1 +1,263 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.service;
+
+import java.net.InetAddress;
+import java.util.*;
+import java.util.concurrent.Callable;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.ThreadPoolExecutor;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.concurrent.Stage;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.PrecompactedRow;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.locator.AbstractReplicationStrategy;
+import org.apache.cassandra.locator.TokenMetadata;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+
+import static org.apache.cassandra.service.AntiEntropyService.*;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class AntiEntropyServiceTest extends CleanupHelper
+{
+    // table and column family to test against
+    public static AntiEntropyService aes;
+
+    public static String tablename;
+    public static String cfname;
+    public static TreeRequest request;
+    public static ColumnFamilyStore store;
+    public static InetAddress LOCAL, REMOTE;
+
+    @BeforeClass
+    public static void prepareClass() throws Exception
+    {
+        LOCAL = FBUtilities.getLocalAddress();
+        tablename = "Keyspace5";
+        StorageService.instance.initServer();
+        // generate a fake endpoint for which we can spoof receiving/sending trees
+        REMOTE = InetAddress.getByName("127.0.0.2");
+        store = Table.open(tablename).getColumnFamilyStores().iterator().next();
+        cfname = store.columnFamily;
+    }
+
+    @Before
+    public void prepare() throws Exception
+    {
+        aes = AntiEntropyService.instance;
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+        tmd.clearUnsafe();
+        StorageService.instance.setToken(StorageService.getPartitioner().getRandomToken());
+        tmd.updateNormalToken(StorageService.getPartitioner().getMinimumToken(), REMOTE);
+        assert tmd.isMember(REMOTE);
+
+        Gossiper.instance.initializeNodeUnsafe(REMOTE, 1);
+
+        // random session id for each test
+        request = new TreeRequest(UUID.randomUUID().toString(), LOCAL, new CFPair(tablename, cfname));
+    }
+
+    @After
+    public void teardown() throws Exception
+    {
+        flushAES();
+    }
+
+    @Test
+    public void testValidatorPrepare() throws Throwable
+    {
+        Validator validator;
+
+        // write
+        List<RowMutation> rms = new LinkedList<RowMutation>();
+        RowMutation rm;
+        rm = new RowMutation(tablename, ByteBufferUtil.bytes("key1"));
+        rm.add(new QueryPath(cfname, null, ByteBufferUtil.bytes("Column1")), ByteBufferUtil.bytes("asdf"), 0);
+        rms.add(rm);
+        Util.writeColumnFamily(rms);
+
+        // sample
+        validator = new Validator(request);
+        validator.prepare(store);
+
+        // and confirm that the tree was split
+        assertTrue(validator.tree.size() > 1);
+    }
+    
+    @Test
+    public void testValidatorComplete() throws Throwable
+    {
+        Validator validator = new Validator(request);
+        validator.prepare(store);
+        validator.complete();
+
+        // confirm that the tree was validated
+        Token min = validator.tree.partitioner().getMinimumToken();
+        assert null != validator.tree.hash(new Range(min, min));
+    }
+
+    @Test
+    public void testValidatorAdd() throws Throwable
+    {
+        Validator validator = new Validator(request);
+        IPartitioner part = validator.tree.partitioner();
+        Token min = part.getMinimumToken();
+        Token mid = part.midpoint(min, min);
+        validator.prepare(store);
+
+        // add a row with the minimum token
+        validator.add(new PrecompactedRow(new DecoratedKey(min, ByteBufferUtil.bytes("nonsense!")),
+                                       new DataOutputBuffer()));
+
+        // and a row after it
+        validator.add(new PrecompactedRow(new DecoratedKey(mid, ByteBufferUtil.bytes("inconceivable!")),
+                                       new DataOutputBuffer()));
+        validator.complete();
+
+        // confirm that the tree was validated
+        assert null != validator.tree.hash(new Range(min, min));
+    }
+
+    @Test
+    public void testManualRepair() throws Throwable
+    {
+        AntiEntropyService.RepairSession sess = AntiEntropyService.instance.getRepairSession(tablename, cfname);
+        sess.start();
+        sess.blockUntilRunning();
+
+        // ensure that the session doesn't end without a response from REMOTE
+        sess.join(100);
+        assert sess.isAlive();
+
+        // deliver a fake response from REMOTE
+        AntiEntropyService.instance.completedRequest(new TreeRequest(sess.getName(), REMOTE, request.cf));
+
+        // block until the repair has completed
+        sess.join();
+    }
+
+    @Test
+    public void testGetNeighborsPlusOne() throws Throwable
+    {
+        // generate rf+1 nodes, and ensure that all nodes are returned
+        Set<InetAddress> expected = addTokens(1 + Table.open(tablename).getReplicationStrategy().getReplicationFactor());
+        expected.remove(FBUtilities.getLocalAddress());
+        assertEquals(expected, AntiEntropyService.getNeighbors(tablename));
+    }
+
+    @Test
+    public void testGetNeighborsTimesTwo() throws Throwable
+    {
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+
+        // generate rf*2 nodes, and ensure that only neighbors specified by the ARS are returned
+        addTokens(2 * Table.open(tablename).getReplicationStrategy().getReplicationFactor());
+        AbstractReplicationStrategy ars = Table.open(tablename).getReplicationStrategy();
+        Set<InetAddress> expected = new HashSet<InetAddress>();
+        for (Range replicaRange : ars.getAddressRanges().get(FBUtilities.getLocalAddress()))
+        {
+            expected.addAll(ars.getRangeAddresses(tmd).get(replicaRange));
+        }
+        expected.remove(FBUtilities.getLocalAddress());
+        assertEquals(expected, AntiEntropyService.getNeighbors(tablename));
+    }
+
+    @Test
+    public void testDifferencer() throws Throwable
+    {
+        // generate a tree
+        Validator validator = new Validator(request);
+        validator.prepare(store);
+        validator.complete();
+        MerkleTree ltree = validator.tree;
+
+        // and a clone
+        validator = new Validator(request);
+        validator.prepare(store);
+        validator.complete();
+        MerkleTree rtree = validator.tree;
+
+        // change a range we own in one of the trees
+        Token ltoken = StorageService.instance.getLocalToken();
+        ltree.invalidate(ltoken);
+        MerkleTree.TreeRange changed = ltree.invalids(StorageService.instance.getLocalPrimaryRange()).next();
+        changed.hash("non-empty hash!".getBytes());
+        // the changed range has two halves, split on our local token: both will be repaired
+        // (since this keyspace has RF > N, so every node is responsible for the entire ring)
+        Set<Range> interesting = new HashSet<Range>();
+        interesting.add(new Range(changed.left, ltoken));
+        interesting.add(new Range(ltoken, changed.right));
+
+        // difference the trees
+        Differencer diff = new Differencer(request, ltree, rtree);
+        diff.run();
+        
+        // ensure that the changed range was recorded
+        assertEquals("Wrong differing ranges", interesting, new HashSet<Range>(diff.differences));
+    }
+
+    Set<InetAddress> addTokens(int max) throws Throwable
+    {
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+        Set<InetAddress> endpoints = new HashSet<InetAddress>();
+        for (int i = 1; i <= max; i++)
+        {
+            InetAddress endpoint = InetAddress.getByName("127.0.0." + i);
+            tmd.updateNormalToken(StorageService.getPartitioner().getRandomToken(), endpoint);
+            endpoints.add(endpoint);
+        }
+        return endpoints;
+    }
+
+    void flushAES() throws Exception
+    {
+        final ThreadPoolExecutor stage = StageManager.getStage(Stage.ANTI_ENTROPY);
+        final Callable noop = new Callable<Object>()
+        {
+            public Boolean call()
+            {
+                return true;
+            }
+        };
+        
+        // send two tasks through the stage: one to follow existing tasks and a second to follow tasks created by
+        // those existing tasks: tasks won't recursively create more tasks
+        stage.submit(noop).get(5000, TimeUnit.MILLISECONDS);
+        stage.submit(noop).get(5000, TimeUnit.MILLISECONDS);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/CassandraServerTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/CassandraServerTest.java
index 3f24d583..159cd2d9 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/CassandraServerTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/CassandraServerTest.java
@@ -1 +1,61 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.service;
+
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+
+public class CassandraServerTest extends CleanupHelper
+{
+    @Test
+    public void test_get_column() throws Throwable {
+        /*
+        CassandraServer server = new CassandraServer();
+        server.start();
+
+        try {
+            Column c1 = column("c1", "0", 0L);
+            Column c2 = column("c2", "0", 0L);
+            List<Column> columns = new ArrayList<Column>();
+            columns.add(c1);
+            columns.add(c2);
+            Map<String, List<Column>> cfmap = new HashMap<String, List<Column>>();
+            cfmap.put("Standard1", columns);
+            cfmap.put("Standard2", columns);
+
+            BatchMutation m = new BatchMutation("Keyspace1", "key1", cfmap);
+            server.batch_insert(m, 1);
+
+            Column column;
+            column = server.get_column("Keyspace1", "key1", "Standard1:c2");
+            assert column.value.equals("0");
+
+            column = server.get_column("Keyspace1", "key1", "Standard2:c2");
+            assert column.value.equals("0");
+
+            ArrayList<Column> Columns = server.get_slice_strong("Keyspace1", "key1", "Standard1", -1, -1);
+            assert Columns.size() == 2;
+        } finally {
+            server.shutdown();
+        }
+        */
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/ConsistencyLevelTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/ConsistencyLevelTest.java
index e69de29b..40465808 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/ConsistencyLevelTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/ConsistencyLevelTest.java
@@ -0,0 +1,187 @@
+package org.apache.cassandra.service;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.List;
+
+import com.google.common.collect.HashMultimap;
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.RandomPartitioner;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.locator.AbstractReplicationStrategy;
+import org.apache.cassandra.locator.SimpleSnitch;
+import org.apache.cassandra.locator.TokenMetadata;
+import org.apache.cassandra.thrift.ConsistencyLevel;
+import org.apache.cassandra.thrift.UnavailableException;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class ConsistencyLevelTest extends CleanupHelper
+{
+    @Test
+    public void testReadWriteConsistencyChecks() throws Exception
+    {
+        StorageService ss = StorageService.instance;
+        final int RING_SIZE = 3;
+
+        TokenMetadata tmd = ss.getTokenMetadata();
+        tmd.clearUnsafe();
+        IPartitioner partitioner = new RandomPartitioner();
+
+        ss.setPartitionerUnsafe(partitioner);
+
+        ArrayList<Token> endpointTokens = new ArrayList<Token>();
+        ArrayList<Token> keyTokens = new ArrayList<Token>();
+        List<InetAddress> hosts = new ArrayList<InetAddress>();
+
+        Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, RING_SIZE);
+
+        HashMultimap<InetAddress, InetAddress> hintedNodes = HashMultimap.create();
+
+
+        AbstractReplicationStrategy strategy;
+
+        for (final String table : DatabaseDescriptor.getNonSystemTables())
+        {
+            strategy = getStrategy(table, tmd);
+            StorageService.calculatePendingRanges(strategy, table);
+            int replicationFactor = strategy.getReplicationFactor();
+            if (replicationFactor < 2)
+                continue;
+
+            for (ConsistencyLevel c : ConsistencyLevel.values())
+            {
+
+                if (c == ConsistencyLevel.EACH_QUORUM || c == ConsistencyLevel.LOCAL_QUORUM)
+                    continue;
+
+                for (int i = 0; i < replicationFactor; i++)
+                {
+                    hintedNodes.clear();
+
+                    for (int j = 0; j < i; j++)
+                    {
+                        hintedNodes.put(hosts.get(j), hosts.get(j));
+                    }
+
+                    IWriteResponseHandler writeHandler = strategy.getWriteResponseHandler(hosts, hintedNodes, c);
+
+                    IReadCommand command = new IReadCommand()
+                    {
+                        public String getKeyspace()
+                        {
+                            return table;
+                        }
+                    };
+                    RowRepairResolver resolver = new RowRepairResolver(table, ByteBufferUtil.bytes("foo"));
+                    ReadCallback<Row> readHandler = StorageProxy.getReadCallback(resolver, command, c, new ArrayList<InetAddress>(hintedNodes.keySet()));
+
+                    boolean isWriteUnavailable = false;
+                    boolean isReadUnavailable = false;
+                    try
+                    {
+                        writeHandler.assureSufficientLiveNodes();
+                    }
+                    catch (UnavailableException e)
+                    {
+                        isWriteUnavailable = true;
+                    }
+
+                    try
+                    {
+                        readHandler.assureSufficientLiveNodes();
+                    }
+                    catch (UnavailableException e)
+                    {
+                        isReadUnavailable = true;
+                    }
+
+                    //these should always match (in this kind of test)
+                    assertTrue(isWriteUnavailable == isReadUnavailable);
+
+                    switch (c)
+                    {
+                        case ALL:
+                            if (isWriteUnavailable)
+                                assertTrue(hintedNodes.size() < replicationFactor);
+                            else
+                                assertTrue(hintedNodes.size() >= replicationFactor);
+
+                            break;
+                        case ONE:
+                        case ANY:
+                            if (isWriteUnavailable)
+                                assertTrue(hintedNodes.size() == 0);
+                            else
+                                assertTrue(hintedNodes.size() > 0);
+                            break;
+                        case TWO:
+                            if (isWriteUnavailable)
+                                assertTrue(hintedNodes.size() < 2);
+                            else
+                                assertTrue(hintedNodes.size() >= 2);
+                            break;
+                        case THREE:
+                            if (isWriteUnavailable)
+                                assertTrue(hintedNodes.size() < 3);
+                            else
+                                assertTrue(hintedNodes.size() >= 3);
+                            break;
+                        case QUORUM:
+                            if (isWriteUnavailable)
+                                assertTrue(hintedNodes.size() < (replicationFactor / 2 + 1));
+                            else
+                                assertTrue(hintedNodes.size() >= (replicationFactor / 2 + 1));
+                            break;
+                        default:
+                            fail("Unhandled CL: " + c);
+
+                    }
+                }
+            }
+            return;
+        }
+
+        fail("Test requires at least one table with RF > 1");
+    }
+
+    private AbstractReplicationStrategy getStrategy(String table, TokenMetadata tmd) throws ConfigurationException
+    {
+        return AbstractReplicationStrategy.createReplicationStrategy(table,
+                                                                     "org.apache.cassandra.locator.SimpleStrategy",
+                                                                     tmd,
+                                                                     new SimpleSnitch(),
+                                                                     null);
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/EmbeddedCassandraServiceTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/EmbeddedCassandraServiceTest.java
index e69de29b..143efa70 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/EmbeddedCassandraServiceTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/EmbeddedCassandraServiceTest.java
@@ -0,0 +1,112 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.charset.CharacterCodingException;
+
+import com.google.common.base.Charsets;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.config.KSMetaData;
+import org.apache.cassandra.thrift.*;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.thrift.TException;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.protocol.TProtocol;
+import org.apache.thrift.transport.TFramedTransport;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+
+/**
+ * Example how to use an embedded cassandra service.
+ *
+ * Tests connect to localhost:9160 when the embedded server is running.
+ *
+ */
+public class EmbeddedCassandraServiceTest extends CleanupHelper
+{
+
+    private static EmbeddedCassandraService cassandra;
+
+    /**
+     * Set embedded cassandra up and spawn it in a new thread.
+     *
+     * @throws TTransportException
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    @BeforeClass
+    public static void setup() throws TTransportException, IOException, InterruptedException, ConfigurationException
+    {
+        cassandra = new EmbeddedCassandraService();
+        cassandra.start();
+    }
+
+    @Test
+    public void testEmbeddedCassandraService()
+    throws AuthenticationException, AuthorizationException, InvalidRequestException, UnavailableException, TimedOutException, TException, NotFoundException, CharacterCodingException
+    {
+        Cassandra.Client client = getClient();
+        client.set_keyspace("Keyspace1");
+
+        ByteBuffer key_user_id = ByteBufferUtil.bytes("1");
+        
+        long timestamp = System.currentTimeMillis();
+        ColumnPath cp = new ColumnPath("Standard1");
+        ColumnParent par = new ColumnParent("Standard1");
+        cp.column = ByteBufferUtil.bytes("name");
+
+        // insert
+        client.insert(key_user_id, par, new Column(ByteBufferUtil.bytes("name"),
+                      ByteBufferUtil.bytes("Ran"), timestamp), ConsistencyLevel.ONE);
+
+        // read
+        ColumnOrSuperColumn got = client.get(key_user_id, cp, ConsistencyLevel.ONE);
+
+        // assert
+        assertNotNull("Got a null ColumnOrSuperColumn", got);
+        assertEquals("Ran", ByteBufferUtil.string(got.getColumn().value));
+    }
+
+    /**
+     * Gets a connection to the localhost client
+     *
+     * @return
+     * @throws TTransportException
+     */
+    private Cassandra.Client getClient() throws TTransportException
+    {
+        TTransport tr = new TFramedTransport(new TSocket("localhost", DatabaseDescriptor.getRpcPort()));
+        TProtocol proto = new TBinaryProtocol(tr);
+        Cassandra.Client client = new Cassandra.Client(proto);
+        tr.open();
+        return client;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/InitClientTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/InitClientTest.java
index e69de29b..f5fcbf31 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/InitClientTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/InitClientTest.java
@@ -0,0 +1,35 @@
+package org.apache.cassandra.service;
+
+import org.junit.Test;
+
+import java.io.IOException;
+
+import org.apache.cassandra.config.ConfigurationException;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+public class InitClientTest // extends CleanupHelper
+{
+    @Test
+    public void testInitClientStartup() throws IOException, ConfigurationException
+    {
+        StorageService.instance.initClient();
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/MoveTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/MoveTest.java
index 3f24d583..ea87565f 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/MoveTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/MoveTest.java
@@ -1 +1,625 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.service;
+
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.util.*;
+
+import org.apache.cassandra.config.ConfigurationException;
+import org.junit.Test;
+
+import static org.junit.Assert.*;
+import com.google.common.collect.HashMultimap;
+import com.google.common.collect.Multimap;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.*;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.VersionedValue;
+import org.apache.cassandra.locator.AbstractReplicationStrategy;
+import org.apache.cassandra.locator.SimpleSnitch;
+import org.apache.cassandra.locator.TokenMetadata;
+
+public class MoveTest extends CleanupHelper
+{
+    /**
+     * Test whether write endpoints is correct when the node is leaving. Uses
+     * StorageService.onChange and does not manipulate token metadata directly.
+     */
+    @Test
+    public void newTestWriteEndpointsDuringLeave() throws Exception
+    {
+        StorageService ss = StorageService.instance;
+        final int RING_SIZE = 6;
+        final int LEAVING_NODE = 3;
+
+        TokenMetadata tmd = ss.getTokenMetadata();
+        tmd.clearUnsafe();
+        IPartitioner partitioner = new RandomPartitioner();
+        VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(partitioner);
+
+        IPartitioner oldPartitioner = ss.setPartitionerUnsafe(partitioner);
+
+        ArrayList<Token> endpointTokens = new ArrayList<Token>();
+        ArrayList<Token> keyTokens = new ArrayList<Token>();
+        List<InetAddress> hosts = new ArrayList<InetAddress>();
+
+        Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, RING_SIZE);
+
+        Map<Token, List<InetAddress>> expectedEndpoints = new HashMap<Token, List<InetAddress>>();
+        for (String table : DatabaseDescriptor.getNonSystemTables())
+        {
+            for (Token token : keyTokens)
+            {
+                List<InetAddress> endpoints = new ArrayList<InetAddress>();
+                Iterator<Token> tokenIter = TokenMetadata.ringIterator(tmd.sortedTokens(), token, false);
+                while (tokenIter.hasNext())
+                {
+                    endpoints.add(tmd.getEndpoint(tokenIter.next()));
+                }
+                expectedEndpoints.put(token, endpoints);
+            }
+        }
+
+        // Third node leaves
+        ss.onChange(hosts.get(LEAVING_NODE),
+                ApplicationState.STATUS,
+                valueFactory.leaving(endpointTokens.get(LEAVING_NODE)));
+        assertTrue(tmd.isLeaving(hosts.get(LEAVING_NODE)));
+
+        AbstractReplicationStrategy strategy;
+        for (String table : DatabaseDescriptor.getNonSystemTables())
+        {
+            strategy = getStrategy(table, tmd);
+            for (Token token : keyTokens)
+            {
+                int replicationFactor = strategy.getReplicationFactor();
+
+                HashSet<InetAddress> actual = new HashSet<InetAddress>(tmd.getWriteEndpoints(token, table, strategy.calculateNaturalEndpoints(token, tmd)));
+                HashSet<InetAddress> expected = new HashSet<InetAddress>();
+
+                for (int i = 0; i < replicationFactor; i++)
+                {
+                    expected.add(expectedEndpoints.get(token).get(i));
+                }
+
+                // if the leaving node is in the endpoint list,
+                // then we should expect it plus one extra for when it's gone
+                if (expected.contains(hosts.get(LEAVING_NODE)))
+                    expected.add(expectedEndpoints.get(token).get(replicationFactor));
+
+                assertEquals("mismatched endpoint sets", expected, actual);
+            }
+        }
+
+        ss.setPartitionerUnsafe(oldPartitioner);
+    }
+
+    /**
+     * Test pending ranges and write endpoints when multiple nodes are on the move
+     * simultaneously
+     */
+    @Test
+    public void testSimultaneousMove() throws UnknownHostException, ConfigurationException
+    {
+        StorageService ss = StorageService.instance;
+        final int RING_SIZE = 10;
+        TokenMetadata tmd = ss.getTokenMetadata();
+        tmd.clearUnsafe();
+        IPartitioner partitioner = new RandomPartitioner();
+        VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(partitioner);
+
+        IPartitioner oldPartitioner = ss.setPartitionerUnsafe(partitioner);
+
+        ArrayList<Token> endpointTokens = new ArrayList<Token>();
+        ArrayList<Token> keyTokens = new ArrayList<Token>();
+        List<InetAddress> hosts = new ArrayList<InetAddress>();
+
+        // create a ring or 10 nodes
+        Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, RING_SIZE);
+
+        // nodes 6, 8 and 9 leave
+        final int[] LEAVING = new int[] {6, 8, 9};
+        for (int leaving : LEAVING)
+            ss.onChange(hosts.get(leaving), ApplicationState.STATUS, valueFactory.leaving(endpointTokens.get(leaving)));
+
+        // boot two new nodes with keyTokens.get(5) and keyTokens.get(7)
+        InetAddress boot1 = InetAddress.getByName("127.0.1.1");
+        ss.onChange(boot1, ApplicationState.STATUS, valueFactory.bootstrapping(keyTokens.get(5)));
+        InetAddress boot2 = InetAddress.getByName("127.0.1.2");
+        ss.onChange(boot2, ApplicationState.STATUS, valueFactory.bootstrapping(keyTokens.get(7)));
+
+        Collection<InetAddress> endpoints = null;
+
+        /* don't require test update every time a new keyspace is added to test/conf/cassandra.yaml */
+        Map<String, AbstractReplicationStrategy> tableStrategyMap = new HashMap<String, AbstractReplicationStrategy>();
+        for (int i=1; i<=4; i++)
+        {
+            tableStrategyMap.put("Keyspace" + i, getStrategy("Keyspace" + i, tmd));
+        }
+
+        // pre-calculate the results.
+        Map<String, Multimap<Token, InetAddress>> expectedEndpoints = new HashMap<String, Multimap<Token, InetAddress>>();
+        expectedEndpoints.put("Keyspace1", HashMultimap.<Token, InetAddress>create());
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("5"), makeAddrs("127.0.0.2"));
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("15"), makeAddrs("127.0.0.3"));
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("25"), makeAddrs("127.0.0.4"));
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("35"), makeAddrs("127.0.0.5"));
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("45"), makeAddrs("127.0.0.6"));
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("55"), makeAddrs("127.0.0.7", "127.0.0.8", "127.0.1.1"));
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("65"), makeAddrs("127.0.0.8"));
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("75"), makeAddrs("127.0.0.9", "127.0.1.2", "127.0.0.1"));
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("85"), makeAddrs("127.0.0.10", "127.0.0.1"));
+        expectedEndpoints.get("Keyspace1").putAll(new BigIntegerToken("95"), makeAddrs("127.0.0.1"));
+        expectedEndpoints.put("Keyspace2", HashMultimap.<Token, InetAddress>create());
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("5"), makeAddrs("127.0.0.2"));
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("15"), makeAddrs("127.0.0.3"));
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("25"), makeAddrs("127.0.0.4"));
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("35"), makeAddrs("127.0.0.5"));
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("45"), makeAddrs("127.0.0.6"));
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("55"), makeAddrs("127.0.0.7", "127.0.0.8", "127.0.1.1"));
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("65"), makeAddrs("127.0.0.8"));
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("75"), makeAddrs("127.0.0.9", "127.0.1.2", "127.0.0.1"));
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("85"), makeAddrs("127.0.0.10", "127.0.0.1"));
+        expectedEndpoints.get("Keyspace2").putAll(new BigIntegerToken("95"), makeAddrs("127.0.0.1"));
+        expectedEndpoints.put("Keyspace3", HashMultimap.<Token, InetAddress>create());
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("5"), makeAddrs("127.0.0.2", "127.0.0.3", "127.0.0.4", "127.0.0.5", "127.0.0.6"));
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("15"), makeAddrs("127.0.0.3", "127.0.0.4", "127.0.0.5", "127.0.0.6", "127.0.0.7", "127.0.1.1", "127.0.0.8"));
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("25"), makeAddrs("127.0.0.4", "127.0.0.5", "127.0.0.6", "127.0.0.7", "127.0.0.8", "127.0.1.2", "127.0.0.1", "127.0.1.1"));
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("35"), makeAddrs("127.0.0.5", "127.0.0.6", "127.0.0.7", "127.0.0.8", "127.0.0.9", "127.0.1.2", "127.0.0.1", "127.0.0.2", "127.0.1.1"));
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("45"), makeAddrs("127.0.0.6", "127.0.0.7", "127.0.0.8", "127.0.0.9", "127.0.0.10", "127.0.1.2", "127.0.0.1", "127.0.0.2", "127.0.1.1", "127.0.0.3"));
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("55"), makeAddrs("127.0.0.7", "127.0.0.8", "127.0.0.9", "127.0.0.10", "127.0.0.1", "127.0.0.2", "127.0.0.3", "127.0.0.4", "127.0.1.1", "127.0.1.2"));
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("65"), makeAddrs("127.0.0.8", "127.0.0.9", "127.0.0.10", "127.0.0.1", "127.0.0.2", "127.0.1.2", "127.0.0.3", "127.0.0.4"));
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("75"), makeAddrs("127.0.0.9", "127.0.0.10", "127.0.0.1", "127.0.0.2", "127.0.0.3", "127.0.1.2", "127.0.0.4", "127.0.0.5"));
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("85"), makeAddrs("127.0.0.10", "127.0.0.1", "127.0.0.2", "127.0.0.3", "127.0.0.4", "127.0.0.5"));
+        expectedEndpoints.get("Keyspace3").putAll(new BigIntegerToken("95"), makeAddrs("127.0.0.1", "127.0.0.2", "127.0.0.3", "127.0.0.4", "127.0.0.5"));
+        expectedEndpoints.put("Keyspace4", HashMultimap.<Token, InetAddress>create());
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("5"), makeAddrs("127.0.0.2", "127.0.0.3", "127.0.0.4"));
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("15"), makeAddrs("127.0.0.3", "127.0.0.4", "127.0.0.5"));
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("25"), makeAddrs("127.0.0.4", "127.0.0.5", "127.0.0.6"));
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("35"), makeAddrs("127.0.0.5", "127.0.0.6", "127.0.0.7", "127.0.1.1", "127.0.0.8"));
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("45"), makeAddrs("127.0.0.6", "127.0.0.7", "127.0.0.8", "127.0.1.2", "127.0.0.1", "127.0.1.1"));
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("55"), makeAddrs("127.0.0.7", "127.0.0.8", "127.0.0.9", "127.0.0.1", "127.0.0.2", "127.0.1.1", "127.0.1.2"));
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("65"), makeAddrs("127.0.0.8", "127.0.0.9", "127.0.0.10", "127.0.1.2", "127.0.0.1", "127.0.0.2"));
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("75"), makeAddrs("127.0.0.9", "127.0.0.10", "127.0.0.1", "127.0.1.2", "127.0.0.2", "127.0.0.3"));
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("85"), makeAddrs("127.0.0.10", "127.0.0.1", "127.0.0.2", "127.0.0.3"));
+        expectedEndpoints.get("Keyspace4").putAll(new BigIntegerToken("95"), makeAddrs("127.0.0.1", "127.0.0.2", "127.0.0.3"));
+
+        for (Map.Entry<String, AbstractReplicationStrategy> tableStrategy : tableStrategyMap.entrySet())
+        {
+            String table = tableStrategy.getKey();
+            AbstractReplicationStrategy strategy = tableStrategy.getValue();
+
+            for (int i = 0; i < keyTokens.size(); i++)
+            {
+                endpoints = tmd.getWriteEndpoints(keyTokens.get(i), table, strategy.getNaturalEndpoints(keyTokens.get(i)));
+                assertTrue(expectedEndpoints.get(table).get(keyTokens.get(i)).size() == endpoints.size());
+                assertTrue(expectedEndpoints.get(table).get(keyTokens.get(i)).containsAll(endpoints));
+            }
+
+            // just to be sure that things still work according to the old tests, run them:
+            if (strategy.getReplicationFactor() != 3)
+                continue;
+            // tokens 5, 15 and 25 should go three nodes
+            for (int i=0; i<3; ++i)
+            {
+                endpoints = tmd.getWriteEndpoints(keyTokens.get(i), table, strategy.getNaturalEndpoints(keyTokens.get(i)));
+                assertTrue(endpoints.size() == 3);
+                assertTrue(endpoints.contains(hosts.get(i+1)));
+                assertTrue(endpoints.contains(hosts.get(i+2)));
+                assertTrue(endpoints.contains(hosts.get(i+3)));
+            }
+
+            // token 35 should go to nodes 4, 5, 6, 7 and boot1
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(3), table, strategy.getNaturalEndpoints(keyTokens.get(3)));
+            assertTrue(endpoints.size() == 5);
+            assertTrue(endpoints.contains(hosts.get(4)));
+            assertTrue(endpoints.contains(hosts.get(5)));
+            assertTrue(endpoints.contains(hosts.get(6)));
+            assertTrue(endpoints.contains(hosts.get(7)));
+            assertTrue(endpoints.contains(boot1));
+
+            // token 45 should go to nodes 5, 6, 7, 0, boot1 and boot2
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(4), table, strategy.getNaturalEndpoints(keyTokens.get(4)));
+            assertTrue(endpoints.size() == 6);
+            assertTrue(endpoints.contains(hosts.get(5)));
+            assertTrue(endpoints.contains(hosts.get(6)));
+            assertTrue(endpoints.contains(hosts.get(7)));
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(boot1));
+            assertTrue(endpoints.contains(boot2));
+
+            // token 55 should go to nodes 6, 7, 8, 0, 1, boot1 and boot2
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(5), table, strategy.getNaturalEndpoints(keyTokens.get(5)));
+            assertTrue(endpoints.size() == 7);
+            assertTrue(endpoints.contains(hosts.get(6)));
+            assertTrue(endpoints.contains(hosts.get(7)));
+            assertTrue(endpoints.contains(hosts.get(8)));
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(hosts.get(1)));
+            assertTrue(endpoints.contains(boot1));
+            assertTrue(endpoints.contains(boot2));
+
+            // token 65 should go to nodes 7, 8, 9, 0, 1 and boot2
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(6), table, strategy.getNaturalEndpoints(keyTokens.get(6)));
+            assertTrue(endpoints.size() == 6);
+            assertTrue(endpoints.contains(hosts.get(7)));
+            assertTrue(endpoints.contains(hosts.get(8)));
+            assertTrue(endpoints.contains(hosts.get(9)));
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(hosts.get(1)));
+            assertTrue(endpoints.contains(boot2));
+
+            // token 75 should to go nodes 8, 9, 0, 1, 2 and boot2
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(7), table, strategy.getNaturalEndpoints(keyTokens.get(7)));
+            assertTrue(endpoints.size() == 6);
+            assertTrue(endpoints.contains(hosts.get(8)));
+            assertTrue(endpoints.contains(hosts.get(9)));
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(hosts.get(1)));
+            assertTrue(endpoints.contains(hosts.get(2)));
+            assertTrue(endpoints.contains(boot2));
+
+            // token 85 should go to nodes 9, 0, 1 and 2
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(8), table, strategy.getNaturalEndpoints(keyTokens.get(8)));
+            assertTrue(endpoints.size() == 4);
+            assertTrue(endpoints.contains(hosts.get(9)));
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(hosts.get(1)));
+            assertTrue(endpoints.contains(hosts.get(2)));
+
+            // token 95 should go to nodes 0, 1 and 2
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(9), table, strategy.getNaturalEndpoints(keyTokens.get(9)));
+            assertTrue(endpoints.size() == 3);
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(hosts.get(1)));
+            assertTrue(endpoints.contains(hosts.get(2)));
+
+        }
+
+        // Now finish node 6 and node 9 leaving, as well as boot1 (after this node 8 is still
+        // leaving and boot2 in progress
+        ss.onChange(hosts.get(LEAVING[0]), ApplicationState.STATUS, valueFactory.left(endpointTokens.get(LEAVING[0])));
+        ss.onChange(hosts.get(LEAVING[2]), ApplicationState.STATUS, valueFactory.left(endpointTokens.get(LEAVING[2])));
+        ss.onChange(boot1, ApplicationState.STATUS, valueFactory.normal(keyTokens.get(5)));
+
+        // adjust precalcuated results.  this changes what the epected endpoints are.
+        expectedEndpoints.get("Keyspace1").get(new BigIntegerToken("55")).removeAll(makeAddrs("127.0.0.7", "127.0.0.8"));
+        expectedEndpoints.get("Keyspace1").get(new BigIntegerToken("85")).removeAll(makeAddrs("127.0.0.10"));
+        expectedEndpoints.get("Keyspace2").get(new BigIntegerToken("55")).removeAll(makeAddrs("127.0.0.7", "127.0.0.8"));
+        expectedEndpoints.get("Keyspace2").get(new BigIntegerToken("85")).removeAll(makeAddrs("127.0.0.10"));
+        expectedEndpoints.get("Keyspace3").get(new BigIntegerToken("15")).removeAll(makeAddrs("127.0.0.7", "127.0.0.8"));
+        expectedEndpoints.get("Keyspace3").get(new BigIntegerToken("25")).removeAll(makeAddrs("127.0.0.7", "127.0.1.2", "127.0.0.1"));
+        expectedEndpoints.get("Keyspace3").get(new BigIntegerToken("35")).removeAll(makeAddrs("127.0.0.7", "127.0.0.2"));
+        expectedEndpoints.get("Keyspace3").get(new BigIntegerToken("45")).removeAll(makeAddrs("127.0.0.7", "127.0.0.10", "127.0.0.3"));
+        expectedEndpoints.get("Keyspace3").get(new BigIntegerToken("55")).removeAll(makeAddrs("127.0.0.7", "127.0.0.10", "127.0.0.4"));
+        expectedEndpoints.get("Keyspace3").get(new BigIntegerToken("65")).removeAll(makeAddrs("127.0.0.10"));
+        expectedEndpoints.get("Keyspace3").get(new BigIntegerToken("75")).removeAll(makeAddrs("127.0.0.10"));
+        expectedEndpoints.get("Keyspace3").get(new BigIntegerToken("85")).removeAll(makeAddrs("127.0.0.10"));
+        expectedEndpoints.get("Keyspace4").get(new BigIntegerToken("35")).removeAll(makeAddrs("127.0.0.7", "127.0.0.8"));
+        expectedEndpoints.get("Keyspace4").get(new BigIntegerToken("45")).removeAll(makeAddrs("127.0.0.7", "127.0.1.2", "127.0.0.1"));
+        expectedEndpoints.get("Keyspace4").get(new BigIntegerToken("55")).removeAll(makeAddrs("127.0.0.2", "127.0.0.7"));
+        expectedEndpoints.get("Keyspace4").get(new BigIntegerToken("65")).removeAll(makeAddrs("127.0.0.10"));
+        expectedEndpoints.get("Keyspace4").get(new BigIntegerToken("75")).removeAll(makeAddrs("127.0.0.10"));
+        expectedEndpoints.get("Keyspace4").get(new BigIntegerToken("85")).removeAll(makeAddrs("127.0.0.10"));
+
+        for (Map.Entry<String, AbstractReplicationStrategy> tableStrategy : tableStrategyMap.entrySet())
+        {
+            String table = tableStrategy.getKey();
+            AbstractReplicationStrategy strategy = tableStrategy.getValue();
+
+            for (int i = 0; i < keyTokens.size(); i++)
+            {
+                endpoints = tmd.getWriteEndpoints(keyTokens.get(i), table, strategy.getNaturalEndpoints(keyTokens.get(i)));
+                assertTrue(expectedEndpoints.get(table).get(keyTokens.get(i)).size() == endpoints.size());
+                assertTrue(expectedEndpoints.get(table).get(keyTokens.get(i)).containsAll(endpoints));
+            }
+
+            if (strategy.getReplicationFactor() != 3)
+                continue;
+            // leave this stuff in to guarantee the old tests work the way they were supposed to.
+            // tokens 5, 15 and 25 should go three nodes
+            for (int i=0; i<3; ++i)
+            {
+                endpoints = tmd.getWriteEndpoints(keyTokens.get(i), table, strategy.getNaturalEndpoints(keyTokens.get(i)));
+                assertTrue(endpoints.size() == 3);
+                assertTrue(endpoints.contains(hosts.get(i+1)));
+                assertTrue(endpoints.contains(hosts.get(i+2)));
+                assertTrue(endpoints.contains(hosts.get(i+3)));
+            }
+
+            // token 35 goes to nodes 4, 5 and boot1
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(3), table, strategy.getNaturalEndpoints(keyTokens.get(3)));
+            assertTrue(endpoints.size() == 3);
+            assertTrue(endpoints.contains(hosts.get(4)));
+            assertTrue(endpoints.contains(hosts.get(5)));
+            assertTrue(endpoints.contains(boot1));
+
+            // token 45 goes to nodes 5, boot1 and node7
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(4), table, strategy.getNaturalEndpoints(keyTokens.get(4)));
+            assertTrue(endpoints.size() == 3);
+            assertTrue(endpoints.contains(hosts.get(5)));
+            assertTrue(endpoints.contains(boot1));
+            assertTrue(endpoints.contains(hosts.get(7)));
+
+            // token 55 goes to boot1, 7, boot2, 8 and 0
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(5), table, strategy.getNaturalEndpoints(keyTokens.get(5)));
+            assertTrue(endpoints.size() == 5);
+            assertTrue(endpoints.contains(boot1));
+            assertTrue(endpoints.contains(hosts.get(7)));
+            assertTrue(endpoints.contains(boot2));
+            assertTrue(endpoints.contains(hosts.get(8)));
+            assertTrue(endpoints.contains(hosts.get(0)));
+
+            // token 65 goes to nodes 7, boot2, 8, 0 and 1
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(6), table, strategy.getNaturalEndpoints(keyTokens.get(6)));
+            assertTrue(endpoints.size() == 5);
+            assertTrue(endpoints.contains(hosts.get(7)));
+            assertTrue(endpoints.contains(boot2));
+            assertTrue(endpoints.contains(hosts.get(8)));
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(hosts.get(1)));
+
+            // token 75 goes to nodes boot2, 8, 0, 1 and 2
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(7), table, strategy.getNaturalEndpoints(keyTokens.get(7)));
+            assertTrue(endpoints.size() == 5);
+            assertTrue(endpoints.contains(boot2));
+            assertTrue(endpoints.contains(hosts.get(8)));
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(hosts.get(1)));
+            assertTrue(endpoints.contains(hosts.get(2)));
+
+            // token 85 goes to nodes 0, 1 and 2
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(8), table, strategy.getNaturalEndpoints(keyTokens.get(8)));
+            assertTrue(endpoints.size() == 3);
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(hosts.get(1)));
+            assertTrue(endpoints.contains(hosts.get(2)));
+
+            // token 95 goes to nodes 0, 1 and 2
+            endpoints = tmd.getWriteEndpoints(keyTokens.get(9), table, strategy.getNaturalEndpoints(keyTokens.get(9)));
+            assertTrue(endpoints.size() == 3);
+            assertTrue(endpoints.contains(hosts.get(0)));
+            assertTrue(endpoints.contains(hosts.get(1)));
+            assertTrue(endpoints.contains(hosts.get(2)));
+        }
+
+        ss.setPartitionerUnsafe(oldPartitioner);
+    }
+
+    @Test
+    public void testStateJumpToBootstrap() throws UnknownHostException
+    {
+        StorageService ss = StorageService.instance;
+        TokenMetadata tmd = ss.getTokenMetadata();
+        tmd.clearUnsafe();
+        IPartitioner partitioner = new RandomPartitioner();
+        VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(partitioner);
+
+        IPartitioner oldPartitioner = ss.setPartitionerUnsafe(partitioner);
+
+        ArrayList<Token> endpointTokens = new ArrayList<Token>();
+        ArrayList<Token> keyTokens = new ArrayList<Token>();
+        List<InetAddress> hosts = new ArrayList<InetAddress>();
+
+        // create a ring or 5 nodes
+        Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, 7);
+
+        // node 2 leaves
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.leaving(endpointTokens.get(2)));
+
+        // don't bother to test pending ranges here, that is extensively tested by other
+        // tests. Just check that the node is in appropriate lists.
+        assertTrue(tmd.isMember(hosts.get(2)));
+        assertTrue(tmd.isLeaving(hosts.get(2)));
+        assertTrue(tmd.getBootstrapTokens().isEmpty());
+
+        // Bootstrap the node immedidiately to keyTokens.get(4) without going through STATE_LEFT
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.bootstrapping(keyTokens.get(4)));
+
+        assertFalse(tmd.isMember(hosts.get(2)));
+        assertFalse(tmd.isLeaving(hosts.get(2)));
+        assertTrue(tmd.getBootstrapTokens().get(keyTokens.get(4)).equals(hosts.get(2)));
+
+        // Bootstrap node hosts.get(3) to keyTokens.get(1)
+        ss.onChange(hosts.get(3), ApplicationState.STATUS, valueFactory.bootstrapping(keyTokens.get(1)));
+
+        assertFalse(tmd.isMember(hosts.get(3)));
+        assertFalse(tmd.isLeaving(hosts.get(3)));
+        assertTrue(tmd.getBootstrapTokens().get(keyTokens.get(4)).equals(hosts.get(2)));
+        assertTrue(tmd.getBootstrapTokens().get(keyTokens.get(1)).equals(hosts.get(3)));
+
+        // Bootstrap node hosts.get(2) further to keyTokens.get(3)
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.bootstrapping(keyTokens.get(3)));
+
+        assertFalse(tmd.isMember(hosts.get(2)));
+        assertFalse(tmd.isLeaving(hosts.get(2)));
+        assertTrue(tmd.getBootstrapTokens().get(keyTokens.get(3)).equals(hosts.get(2)));
+        assertTrue(tmd.getBootstrapTokens().get(keyTokens.get(4)) == null);
+        assertTrue(tmd.getBootstrapTokens().get(keyTokens.get(1)).equals(hosts.get(3)));
+
+        // Go to normal again for both nodes
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.normal(keyTokens.get(3)));
+        ss.onChange(hosts.get(3), ApplicationState.STATUS, valueFactory.normal(keyTokens.get(2)));
+
+        assertTrue(tmd.isMember(hosts.get(2)));
+        assertFalse(tmd.isLeaving(hosts.get(2)));
+        assertTrue(tmd.getToken(hosts.get(2)).equals(keyTokens.get(3)));
+        assertTrue(tmd.isMember(hosts.get(3)));
+        assertFalse(tmd.isLeaving(hosts.get(3)));
+        assertTrue(tmd.getToken(hosts.get(3)).equals(keyTokens.get(2)));
+
+        assertTrue(tmd.getBootstrapTokens().isEmpty());
+
+        ss.setPartitionerUnsafe(oldPartitioner);
+    }
+
+    @Test
+    public void testStateJumpToNormal() throws UnknownHostException
+    {
+        StorageService ss = StorageService.instance;
+        TokenMetadata tmd = ss.getTokenMetadata();
+        tmd.clearUnsafe();
+        IPartitioner partitioner = new RandomPartitioner();
+        VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(partitioner);
+
+        IPartitioner oldPartitioner = ss.setPartitionerUnsafe(partitioner);
+
+        ArrayList<Token> endpointTokens = new ArrayList<Token>();
+        ArrayList<Token> keyTokens = new ArrayList<Token>();
+        List<InetAddress> hosts = new ArrayList<InetAddress>();
+
+        // create a ring or 5 nodes
+        Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, 6);
+
+        // node 2 leaves
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.leaving(endpointTokens.get(2)));
+
+        assertTrue(tmd.isLeaving(hosts.get(2)));
+        assertTrue(tmd.getToken(hosts.get(2)).equals(endpointTokens.get(2)));
+
+        // back to normal
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.normal(keyTokens.get(2)));
+
+        assertTrue(tmd.getLeavingEndpoints().isEmpty());
+        assertTrue(tmd.getToken(hosts.get(2)).equals(keyTokens.get(2)));
+
+        // node 3 goes through leave and left and then jumps to normal at its new token
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.leaving(keyTokens.get(2)));
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.left(keyTokens.get(2)));
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.normal(keyTokens.get(4)));
+
+        assertTrue(tmd.getBootstrapTokens().isEmpty());
+        assertTrue(tmd.getLeavingEndpoints().isEmpty());
+        assertTrue(tmd.getToken(hosts.get(2)).equals(keyTokens.get(4)));
+
+        ss.setPartitionerUnsafe(oldPartitioner);
+    }
+
+    @Test
+    public void testStateJumpToLeaving() throws UnknownHostException
+    {
+        StorageService ss = StorageService.instance;
+        TokenMetadata tmd = ss.getTokenMetadata();
+        tmd.clearUnsafe();
+        IPartitioner partitioner = new RandomPartitioner();
+        VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(partitioner);
+
+        IPartitioner oldPartitioner = ss.setPartitionerUnsafe(partitioner);
+
+        ArrayList<Token> endpointTokens = new ArrayList<Token>();
+        ArrayList<Token> keyTokens = new ArrayList<Token>();
+        List<InetAddress> hosts = new ArrayList<InetAddress>();
+
+        // create a ring or 5 nodes
+        Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, 6);
+
+        // node 2 leaves with _different_ token
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.leaving(keyTokens.get(0)));
+
+        assertTrue(tmd.getToken(hosts.get(2)).equals(keyTokens.get(0)));
+        assertTrue(tmd.isLeaving(hosts.get(2)));
+        assertTrue(tmd.getEndpoint(endpointTokens.get(2)) == null);
+
+        // go to boostrap
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.bootstrapping(keyTokens.get(1)));
+
+        assertFalse(tmd.isLeaving(hosts.get(2)));
+        assertTrue(tmd.getBootstrapTokens().size() == 1);
+        assertTrue(tmd.getBootstrapTokens().get(keyTokens.get(1)).equals(hosts.get(2)));
+
+        // jump to leaving again
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.leaving(keyTokens.get(1)));
+
+        assertTrue(tmd.getEndpoint(keyTokens.get(1)).equals(hosts.get(2)));
+        assertTrue(tmd.isLeaving(hosts.get(2)));
+        assertTrue(tmd.getBootstrapTokens().isEmpty());
+
+        // go to state left
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.left(keyTokens.get(1)));
+
+        assertFalse(tmd.isMember(hosts.get(2)));
+        assertFalse(tmd.isLeaving(hosts.get(2)));
+
+        ss.setPartitionerUnsafe(oldPartitioner);
+    }
+
+    @Test
+    public void testStateJumpToLeft() throws UnknownHostException
+    {
+        StorageService ss = StorageService.instance;
+        TokenMetadata tmd = ss.getTokenMetadata();
+        tmd.clearUnsafe();
+        IPartitioner partitioner = new RandomPartitioner();
+        VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(partitioner);
+
+        IPartitioner oldPartitioner = ss.setPartitionerUnsafe(partitioner);
+
+        ArrayList<Token> endpointTokens = new ArrayList<Token>();
+        ArrayList<Token> keyTokens = new ArrayList<Token>();
+        List<InetAddress> hosts = new ArrayList<InetAddress>();
+
+        // create a ring of 6 nodes
+        Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, 7);
+
+        // node hosts.get(2) goes jumps to left
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.left(endpointTokens.get(2)));
+
+        assertFalse(tmd.isMember(hosts.get(2)));
+
+        // node hosts.get(4) goes to bootstrap
+        ss.onChange(hosts.get(3), ApplicationState.STATUS, valueFactory.bootstrapping(keyTokens.get(1)));
+
+        assertFalse(tmd.isMember(hosts.get(3)));
+        assertTrue(tmd.getBootstrapTokens().size() == 1);
+        assertTrue(tmd.getBootstrapTokens().get(keyTokens.get(1)).equals(hosts.get(3)));
+
+        // and then directly to 'left'
+        ss.onChange(hosts.get(2), ApplicationState.STATUS, valueFactory.left(keyTokens.get(1)));
+
+        assertTrue(tmd.getBootstrapTokens().size() == 0);
+        assertFalse(tmd.isMember(hosts.get(2)));
+        assertFalse(tmd.isLeaving(hosts.get(2)));
+
+        ss.setPartitionerUnsafe(oldPartitioner);
+    }
+
+    private static Collection<InetAddress> makeAddrs(String... hosts) throws UnknownHostException
+    {
+        ArrayList<InetAddress> addrs = new ArrayList<InetAddress>(hosts.length);
+        for (String host : hosts)
+            addrs.add(InetAddress.getByName(host));
+        return addrs;
+    }
+
+    private AbstractReplicationStrategy getStrategy(String table, TokenMetadata tmd) throws ConfigurationException
+    {
+        return AbstractReplicationStrategy.createReplicationStrategy(
+                table,
+                "org.apache.cassandra.locator.SimpleStrategy",
+                tmd,
+                new SimpleSnitch(),
+                null);
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/RemoveTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/RemoveTest.java
index e69de29b..93f73807 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/RemoveTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/RemoveTest.java
@@ -0,0 +1,229 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.concurrent.Stage;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.RandomPartitioner;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.gms.VersionedValue;
+import org.apache.cassandra.locator.TokenMetadata;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.sink.IMessageSink;
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.streaming.StreamUtil;
+import org.apache.cassandra.utils.FBUtilities;
+
+import static org.junit.Assert.*;
+
+public class RemoveTest extends CleanupHelper
+{
+    StorageService ss = StorageService.instance;
+    TokenMetadata tmd = ss.getTokenMetadata();
+    IPartitioner oldPartitioner;
+    ArrayList<Token> endpointTokens = new ArrayList<Token>();
+    ArrayList<Token> keyTokens = new ArrayList<Token>();
+    List<InetAddress> hosts = new ArrayList<InetAddress>();
+    InetAddress removalhost;
+    Token removaltoken;
+
+    @Before
+    public void setup() throws IOException, ConfigurationException
+    {
+        tmd.clearUnsafe();
+        IPartitioner partitioner = new RandomPartitioner();
+
+        oldPartitioner = ss.setPartitionerUnsafe(partitioner);
+
+        // create a ring of 5 nodes
+        Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, 6);
+
+        MessagingService.instance().listen(FBUtilities.getLocalAddress());
+        Gossiper.instance.start(1);
+        for (int i = 0; i < 6; i++)
+        {
+            Gossiper.instance.initializeNodeUnsafe(hosts.get(i), 1);
+        }
+        removalhost = hosts.get(5);
+        hosts.remove(removalhost);
+        removaltoken = endpointTokens.get(5);
+        endpointTokens.remove(removaltoken);
+    }
+
+    @After
+    public void tearDown()
+    {
+        SinkManager.clear();
+        MessagingService.instance().shutdown();
+        ss.setPartitionerUnsafe(oldPartitioner);
+    }
+
+    @Test(expected = UnsupportedOperationException.class)
+    public void testBadToken()
+    {
+        final String token = StorageService.getPartitioner().getTokenFactory().toString(keyTokens.get(2));
+        ss.removeToken(token);
+
+    }
+
+    @Test(expected = UnsupportedOperationException.class)
+    public void testLocalToken()
+    {
+        //first token should be localhost
+        final String token = StorageService.getPartitioner().getTokenFactory().toString(endpointTokens.get(0));
+        ss.removeToken(token);
+    }
+
+    @Test
+    public void testRemoveToken() throws InterruptedException
+    {
+        IPartitioner partitioner = StorageService.getPartitioner();
+
+        final String token = partitioner.getTokenFactory().toString(removaltoken);
+        ReplicationSink rSink = new ReplicationSink();
+        SinkManager.add(rSink);
+
+        // start removal in background and send replication confirmations
+        final AtomicBoolean success = new AtomicBoolean(false);
+        Thread remover = new Thread()
+        {
+            public void run()
+            {
+                try
+                {
+                    ss.removeToken(token);
+                }
+                catch (Exception e)
+                {
+                    System.err.println(e);
+                    e.printStackTrace();
+                    return;
+                }
+                success.set(true);
+            }
+        };
+        remover.start();
+
+        Thread.sleep(1000); // make sure removal is waiting for confirmation
+
+        assertTrue(tmd.isLeaving(removalhost));
+        assertEquals(1, tmd.getLeavingEndpoints().size());
+
+        for (InetAddress host : hosts)
+        {
+            Message msg = new Message(host, StorageService.Verb.REPLICATION_FINISHED, new byte[0]);
+            MessagingService.instance().sendRR(msg, FBUtilities.getLocalAddress());
+        }
+
+        remover.join();
+
+        assertTrue(success.get());
+        assertTrue(tmd.getLeavingEndpoints().isEmpty());
+    }
+
+    @Test
+    public void testStartRemoving()
+    {
+        IPartitioner partitioner = StorageService.getPartitioner();
+        VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(partitioner);
+
+        NotificationSink nSink = new NotificationSink();
+        ReplicationSink rSink = new ReplicationSink();
+        SinkManager.add(nSink);
+        SinkManager.add(rSink);
+
+        assertEquals(0, tmd.getLeavingEndpoints().size());
+
+        ss.onChange(hosts.get(1),
+                    ApplicationState.STATUS,
+                    valueFactory.removingNonlocal(endpointTokens.get(1), removaltoken));
+
+        assertEquals(1, nSink.callCount);
+        assertTrue(tmd.isLeaving(removalhost));
+        assertEquals(1, tmd.getLeavingEndpoints().size());
+    }
+
+    @Test
+    public void testFinishRemoving()
+    {
+        IPartitioner partitioner = StorageService.getPartitioner();
+        VersionedValue.VersionedValueFactory valueFactory = new VersionedValue.VersionedValueFactory(partitioner);
+
+        assertEquals(0, tmd.getLeavingEndpoints().size());
+
+        ss.onChange(hosts.get(1),
+                    ApplicationState.STATUS,
+                    valueFactory.removedNonlocal(endpointTokens.get(1), removaltoken));
+
+        assertFalse(Gossiper.instance.getLiveMembers().contains(removalhost));
+        assertFalse(tmd.isMember(removalhost));
+    }
+
+    class ReplicationSink implements IMessageSink
+    {
+        public Message handleMessage(Message msg, String id, InetAddress to)
+        {
+            if (!msg.getVerb().equals(StorageService.Verb.STREAM_REQUEST))
+                return msg;
+
+            StreamUtil.finishStreamRequest(msg, to);
+
+            return null;
+        }
+    }
+
+    class NotificationSink implements IMessageSink
+    {
+        public int callCount = 0;
+
+        public Message handleMessage(Message msg, String id, InetAddress to)
+        {
+            if (msg.getVerb().equals(StorageService.Verb.REPLICATION_FINISHED))
+            {
+                callCount++;
+                assertEquals(Stage.MISC, msg.getMessageType());
+                // simulate a response from remote server
+                Message response = msg.getReply(FBUtilities.getLocalAddress(), new byte[]{ });
+                MessagingService.instance().sendReply(response, id, FBUtilities.getLocalAddress());
+                return null;
+            }
+            else
+            {
+                return msg;
+            }
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/RowResolverTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/RowResolverTest.java
index e69de29b..4c847cc4 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/RowResolverTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/RowResolverTest.java
@@ -0,0 +1,96 @@
+package org.apache.cassandra.service;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import java.util.Arrays;
+
+import org.apache.cassandra.SchemaLoader;
+import org.junit.Test;
+
+import org.apache.cassandra.db.ColumnFamily;
+
+import static org.apache.cassandra.db.TableTest.assertColumns;
+import static org.apache.cassandra.Util.column;
+import static junit.framework.Assert.assertNull;
+
+public class RowResolverTest extends SchemaLoader
+{
+    @Test
+    public void testResolveSupersetNewer()
+    {
+        ColumnFamily cf1 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf1.addColumn(column("c1", "v1", 0));
+
+        ColumnFamily cf2 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf2.addColumn(column("c1", "v2", 1));
+
+        ColumnFamily resolved = RowRepairResolver.resolveSuperset(Arrays.asList(cf1, cf2));
+        assertColumns(resolved, "c1");
+        assertColumns(ColumnFamily.diff(cf1, resolved), "c1");
+        assertNull(ColumnFamily.diff(cf2, resolved));
+    }
+
+    @Test
+    public void testResolveSupersetDisjoint()
+    {
+        ColumnFamily cf1 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf1.addColumn(column("c1", "v1", 0));
+
+        ColumnFamily cf2 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf2.addColumn(column("c2", "v2", 1));
+
+        ColumnFamily resolved = RowRepairResolver.resolveSuperset(Arrays.asList(cf1, cf2));
+        assertColumns(resolved, "c1", "c2");
+        assertColumns(ColumnFamily.diff(cf1, resolved), "c2");
+        assertColumns(ColumnFamily.diff(cf2, resolved), "c1");
+    }
+
+    @Test
+    public void testResolveSupersetNullOne()
+    {
+        ColumnFamily cf2 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf2.addColumn(column("c2", "v2", 1));
+
+        ColumnFamily resolved = RowRepairResolver.resolveSuperset(Arrays.asList(null, cf2));
+        assertColumns(resolved, "c2");
+        assertColumns(ColumnFamily.diff(null, resolved), "c2");
+        assertNull(ColumnFamily.diff(cf2, resolved));
+    }
+
+    @Test
+    public void testResolveSupersetNullTwo()
+    {
+        ColumnFamily cf1 = ColumnFamily.create("Keyspace1", "Standard1");
+        cf1.addColumn(column("c1", "v1", 0));
+
+        ColumnFamily resolved = RowRepairResolver.resolveSuperset(Arrays.asList(cf1, null));
+        assertColumns(resolved, "c1");
+        assertNull(ColumnFamily.diff(cf1, resolved));
+        assertColumns(ColumnFamily.diff(null, resolved), "c1");
+    }
+
+    @Test
+    public void testResolveSupersetNullBoth()
+    {
+        assertNull(RowRepairResolver.resolveSuperset(Arrays.<ColumnFamily>asList(null, null)));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/SerializationsTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/SerializationsTest.java
index e69de29b..10fa525f 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/SerializationsTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/SerializationsTest.java
@@ -0,0 +1,101 @@
+package org.apache.cassandra.service;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import org.apache.cassandra.AbstractSerializationsTester;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.RandomPartitioner;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+import org.junit.Test;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+public class SerializationsTest extends AbstractSerializationsTester
+{
+    private void testTreeRequestWrite() throws IOException
+    {
+        DataOutputStream out = getOutput("service.TreeRequest.bin");
+        AntiEntropyService.TreeRequestVerbHandler.SERIALIZER.serialize(Statics.req, out);
+        Message.serializer().serialize(AntiEntropyService.TreeRequestVerbHandler.makeVerb(Statics.req), out);
+        out.close();
+    }
+    
+    @Test
+    public void testTreeRequestRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testTreeRequestWrite();
+        
+        DataInputStream in = getInput("service.TreeRequest.bin");
+        assert AntiEntropyService.TreeRequestVerbHandler.SERIALIZER.deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private void testTreeResponseWrite() throws IOException
+    {
+        AntiEntropyService.Validator v0 = new AntiEntropyService.Validator(Statics.req);
+        IPartitioner part = new RandomPartitioner();
+        MerkleTree mt = new MerkleTree(part, MerkleTree.RECOMMENDED_DEPTH, Integer.MAX_VALUE);
+        List<Token> tokens = new ArrayList<Token>();
+        for (int i = 0; i < 10; i++)
+        {
+            Token t = part.getRandomToken();
+            tokens.add(t);
+            mt.split(t);
+        }
+        AntiEntropyService.Validator v1 = new AntiEntropyService.Validator(Statics.req, mt);
+        DataOutputStream out = getOutput("service.TreeResponse.bin");
+        AntiEntropyService.TreeResponseVerbHandler.SERIALIZER.serialize(v0, out);
+        AntiEntropyService.TreeResponseVerbHandler.SERIALIZER.serialize(v1, out);
+        Message.serializer().serialize(AntiEntropyService.TreeResponseVerbHandler.makeVerb(FBUtilities.getLocalAddress(), v0), out);
+        Message.serializer().serialize(AntiEntropyService.TreeResponseVerbHandler.makeVerb(FBUtilities.getLocalAddress(), v1), out);
+        out.close();
+    }
+    
+    @Test
+    public void testTreeResponseRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testTreeResponseWrite();
+        
+        DataInputStream in = getInput("service.TreeResponse.bin");
+        assert AntiEntropyService.TreeResponseVerbHandler.SERIALIZER.deserialize(in) != null;
+        assert AntiEntropyService.TreeResponseVerbHandler.SERIALIZER.deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private static class Statics
+    {
+        private static final AntiEntropyService.CFPair pair = new AntiEntropyService.CFPair("Keyspace1", "Standard1");
+        private static final AntiEntropyService.TreeRequest req = new AntiEntropyService.TreeRequest("sessionId", FBUtilities.getLocalAddress(), pair);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageProxyTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageProxyTest.java
index 3f24d583..9396d16f 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageProxyTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageProxyTest.java
@@ -1 +1,115 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.service;
+
+import java.net.InetAddress;
+import java.util.List;
+
+import org.junit.BeforeClass;
+import org.junit.Test;
+import static org.junit.Assert.assertEquals;
+
+import org.apache.cassandra.CleanupHelper;
+import static org.apache.cassandra.Util.range;
+import static org.apache.cassandra.Util.bounds;
+import static org.apache.cassandra.Util.token;
+
+import org.apache.cassandra.dht.AbstractBounds;
+import org.apache.cassandra.locator.TokenMetadata;
+
+public class StorageProxyTest extends CleanupHelper
+{
+    @BeforeClass
+    public static void beforeClass() throws Throwable
+    {
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+        tmd.updateNormalToken(token("1"), InetAddress.getByName("127.0.0.1"));
+        tmd.updateNormalToken(token("6"), InetAddress.getByName("127.0.0.6"));
+    }
+
+    private void testGRR(AbstractBounds queryRange, AbstractBounds... expected)
+    {
+        List<AbstractBounds> restricted = StorageProxy.getRestrictedRanges(queryRange);
+        assertEquals(restricted.toString(), expected.length, restricted.size());
+        for (int i = 0; i < expected.length; i++)
+            assertEquals("Mismatch for index " + i + ": " + restricted, expected[i], restricted.get(i));
+    }
+
+    @Test
+    public void testGRR() throws Throwable
+    {
+        // no splits
+        testGRR(range("2", "5"), range("2", "5"));
+        testGRR(bounds("2", "5"), bounds("2", "5"));
+        // single split
+        testGRR(range("2", "7"), range("2", "6"), range("6", "7"));
+        testGRR(bounds("2", "7"), bounds("2", "6"), range("6", "7"));
+        // single split starting from min
+        testGRR(range("", "2"), range("", "1"), range("1", "2"));
+        testGRR(bounds("", "2"), bounds("", "1"), range("1", "2"));
+        // single split ending with max
+        testGRR(range("5", ""), range("5", "6"), range("6", ""));
+        testGRR(bounds("5", ""), bounds("5", "6"), range("6", ""));
+        // two splits
+        testGRR(range("0", "7"), range("0", "1"), range("1", "6"), range("6", "7"));
+        testGRR(bounds("0", "7"), bounds("0", "1"), range("1", "6"), range("6", "7"));
+    }
+
+    @Test
+    public void testGRRExact() throws Throwable
+    {
+        // min
+        testGRR(range("1", "5"), range("1", "5"));
+        testGRR(bounds("1", "5"), bounds("1", "1"), range("1", "5"));
+        // max
+        testGRR(range("2", "6"), range("2", "6"));
+        testGRR(bounds("2", "6"), bounds("2", "6"));
+        // both
+        testGRR(range("1", "6"), range("1", "6"));
+        testGRR(bounds("1", "6"), bounds("1", "1"), range("1", "6"));
+    }
+
+    @Test
+    public void testGRRWrapped() throws Throwable
+    {
+        // one token in wrapped range
+        testGRR(range("7", "0"), range("7", ""), range("", "0"));
+        // two tokens in wrapped range
+        testGRR(range("5", "0"), range("5", "6"), range("6", ""), range("", "0"));
+        testGRR(range("7", "2"), range("7", ""), range("", "1"), range("1", "2"));
+        // full wraps
+        testGRR(range("0", "0"), range("0", "1"), range("1", "6"), range("6", ""), range("", "0"));
+        testGRR(range("", ""), range("", "1"), range("1", "6"), range("6", ""));
+        // wrap on member tokens
+        testGRR(range("6", "6"), range("6", ""), range("", "1"), range("1", "6"));
+        testGRR(range("6", "1"), range("6", ""), range("", "1"));
+        // end wrapped
+        testGRR(range("5", ""), range("5", "6"), range("6", ""));
+    }
+
+    @Test
+    public void testGRRExactBounds() throws Throwable
+    {
+        // equal tokens are special cased as non-wrapping for bounds
+        testGRR(bounds("0", "0"), bounds("0", "0"));
+        // completely empty bounds match everything
+        testGRR(bounds("", ""), bounds("", "1"), range("1", "6"), range("6", ""));
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceAccessor.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceAccessor.java
index 3f24d583..1b0ee860 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceAccessor.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceAccessor.java
@@ -1 +1,30 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import org.apache.cassandra.locator.TokenMetadata;
+
+public class StorageServiceAccessor
+{
+    public static TokenMetadata setTokenMetadata(TokenMetadata tmd)
+    {
+        return StorageService.instance.setTokenMetadataUnsafe(tmd);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceClientTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceClientTest.java
index e69de29b..6deecbe0 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceClientTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceClientTest.java
@@ -0,0 +1,48 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.service;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.junit.Test;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.assertFalse;
+
+import java.io.File;
+import java.io.IOException;
+
+public class StorageServiceClientTest
+{
+    @Test
+    public void testClientOnlyMode() throws IOException, ConfigurationException
+    {
+        CleanupHelper.mkdirs();
+        CleanupHelper.cleanup();
+        StorageService.instance.initClient();
+
+        // verify that no storage directories were created.
+        for (String path : DatabaseDescriptor.getAllDataFileLocations())
+        {
+            assertFalse(new File(path).exists());
+        }
+        StorageService.instance.stopClient();
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceServerTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceServerTest.java
index e69de29b..d9c98628 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceServerTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/service/StorageServiceServerTest.java
@@ -0,0 +1,71 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.service;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.ConfigurationException;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+public class StorageServiceServerTest
+{
+    @Test
+    public void testRegularMode() throws IOException, InterruptedException, ConfigurationException
+    {
+        CleanupHelper.mkdirs();
+        CleanupHelper.cleanup();
+        StorageService.instance.initServer();
+        for (String path : DatabaseDescriptor.getAllDataFileLocations())
+        {
+            // verify that storage directories are there.
+            assertTrue(new File(path).exists());
+        }
+        // a proper test would be to call decommission here, but decommission() mixes both shutdown and datatransfer
+        // calls.  This test is only interested in the shutdown-related items which a properly handled by just
+        // stopping the client.
+        //StorageService.instance.decommission();
+        StorageService.instance.stopClient();
+    }
+
+    @Test
+    public void testGetAllRangesEmpty()
+    {
+        List<Token> toks = Collections.emptyList();
+        assertEquals(Collections.emptyList(), StorageService.instance.getAllRanges(toks));
+    }
+
+    @Test
+    public void testSnapshot() throws IOException
+    {
+        // no need to insert extra data, even an "empty" database will have a little information in the system keyspace
+        StorageService.instance.takeAllSnapshot(null);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/BootstrapTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/BootstrapTest.java
index e69de29b..7e2d8698 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/BootstrapTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/BootstrapTest.java
@@ -0,0 +1,53 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.streaming;
+
+import static junit.framework.Assert.assertEquals;
+
+import java.io.File;
+import java.io.IOException;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.io.sstable.Descriptor;
+import org.apache.cassandra.utils.Pair;
+
+import java.util.Arrays;
+
+import org.junit.Test;
+
+public class BootstrapTest extends SchemaLoader
+{
+    @Test
+    public void testGetNewNames() throws IOException
+    {
+        Descriptor desc = Descriptor.fromFilename(new File("Keyspace1", "Standard1-500-Data.db").toString());
+        assert !desc.isLatestVersion; // deliberately test old version; see CASSANDRA-2283
+        PendingFile inContext = new PendingFile(null, desc, "Data.db", Arrays.asList(new Pair<Long,Long>(0L, 1L)));
+
+        PendingFile outContext = StreamIn.getContextMapping(inContext);
+        // filename and generation are expected to have changed
+        assert !inContext.getFilename().equals(outContext.getFilename());
+
+        // nothing else should
+        assertEquals(inContext.component, outContext.component);
+        assertEquals(desc.ksname, outContext.desc.ksname);
+        assertEquals(desc.cfname, outContext.desc.cfname);
+        assertEquals(desc.version, outContext.desc.version);
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/SerializationsTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/SerializationsTest.java
index e69de29b..2cd913c4 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/SerializationsTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/SerializationsTest.java
@@ -0,0 +1,205 @@
+package org.apache.cassandra.streaming;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import org.apache.cassandra.AbstractSerializationsTester;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.BigIntegerToken;
+import org.apache.cassandra.dht.BytesToken;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.io.sstable.Descriptor;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.Pair;
+import org.junit.Test;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+public class SerializationsTest extends AbstractSerializationsTester
+{
+    private void testPendingFileWrite() throws IOException
+    {
+        // make sure to test serializing null and a pf with no sstable.
+        PendingFile normal = makePendingFile(true, 100);
+        PendingFile noSections = makePendingFile(true, 0);
+        PendingFile noSST = makePendingFile(false, 100);
+        
+        DataOutputStream out = getOutput("streaming.PendingFile.bin");
+        PendingFile.serializer().serialize(normal, out);
+        PendingFile.serializer().serialize(noSections, out);
+        PendingFile.serializer().serialize(noSST, out);
+        PendingFile.serializer().serialize(null, out);
+        out.close();
+    }
+    
+    @Test
+    public void testPendingFileRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testPendingFileWrite();
+        
+        DataInputStream in = getInput("streaming.PendingFile.bin");
+        assert PendingFile.serializer().deserialize(in) != null;
+        assert PendingFile.serializer().deserialize(in) != null;
+        assert PendingFile.serializer().deserialize(in) != null;
+        assert PendingFile.serializer().deserialize(in) == null;
+        in.close();
+    }
+    
+    private void testStreamHeaderWrite() throws IOException
+    {
+        StreamHeader sh0 = new StreamHeader("Keyspace1", 123L, makePendingFile(true, 100));
+        StreamHeader sh1 = new StreamHeader("Keyspace1", 124L, makePendingFile(false, 100));
+        Collection<PendingFile> files = new ArrayList<PendingFile>();
+        for (int i = 0; i < 50; i++)
+            files.add(makePendingFile(i % 2 == 0, 100));
+        StreamHeader sh2 = new StreamHeader("Keyspace1", 125L, makePendingFile(true, 100), files);
+        StreamHeader sh3 = new StreamHeader("Keyspace1", 125L, null, files);
+        StreamHeader sh4 = new StreamHeader("Keyspace1", 125L, makePendingFile(true, 100), new ArrayList<PendingFile>());
+        
+        DataOutputStream out = getOutput("streaming.StreamHeader.bin");
+        StreamHeader.serializer().serialize(sh0, out);
+        StreamHeader.serializer().serialize(sh1, out);
+        StreamHeader.serializer().serialize(sh2, out);
+        StreamHeader.serializer().serialize(sh3, out);
+        StreamHeader.serializer().serialize(sh4, out);
+        out.close();
+    }
+    
+    @Test
+    public void testStreamHeaderRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testStreamHeaderWrite();
+        
+        DataInputStream in = getInput("streaming.StreamHeader.bin");
+        assert StreamHeader.serializer().deserialize(in) != null;
+        assert StreamHeader.serializer().deserialize(in) != null;
+        assert StreamHeader.serializer().deserialize(in) != null;
+        assert StreamHeader.serializer().deserialize(in) != null;
+        assert StreamHeader.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private void testStreamReplyWrite() throws IOException
+    {
+        StreamReply rep = new StreamReply("this is a file", 123L, StreamReply.Status.FILE_FINISHED);
+        DataOutputStream out = getOutput("streaming.StreamReply.bin");
+        StreamReply.serializer.serialize(rep, out);
+        Message.serializer().serialize(rep.createMessage(), out);
+        out.close();
+    }
+    
+    @Test
+    public void testStreamReplyRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testStreamReplyWrite();
+        
+        DataInputStream in = getInput("streaming.StreamReply.bin");
+        assert StreamReply.serializer.deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private static PendingFile makePendingFile(boolean sst, int numSecs)
+    {
+        Descriptor desc = new Descriptor("z", new File("path/doesn't/matter"), "Keyspace1", "Standard1", 23, false);
+        List<Pair<Long, Long>> sections = new ArrayList<Pair<Long, Long>>();
+        for (int i = 0; i < numSecs; i++)
+            sections.add(new Pair<Long, Long>(new Long(i), new Long(i * i)));
+        return new PendingFile(sst ? makeSSTable() : null, desc, SSTable.COMPONENT_DATA, sections);
+    }
+    
+    private void testStreamRequestMessageWrite() throws IOException
+    {
+        Collection<Range> ranges = new ArrayList<Range>();
+        for (int i = 0; i < 5; i++)
+            ranges.add(new Range(new BytesToken(ByteBufferUtil.bytes(Integer.toString(10*i))), new BytesToken(ByteBufferUtil.bytes(Integer.toString(10*i+5)))));
+        StreamRequestMessage msg0 = new StreamRequestMessage(FBUtilities.getLocalAddress(), ranges, "Keyspace1", 123L);
+        StreamRequestMessage msg1 = new StreamRequestMessage(FBUtilities.getLocalAddress(), makePendingFile(true, 100), 124L);
+        StreamRequestMessage msg2 = new StreamRequestMessage(FBUtilities.getLocalAddress(), makePendingFile(false, 100), 124L);
+        
+        DataOutputStream out = getOutput("streaming.StreamRequestMessage.bin");
+        StreamRequestMessage.serializer().serialize(msg0, out);
+        StreamRequestMessage.serializer().serialize(msg1, out);
+        StreamRequestMessage.serializer().serialize(msg2, out);
+        Message.serializer().serialize(msg0.makeMessage(), out);
+        Message.serializer().serialize(msg1.makeMessage(), out);
+        Message.serializer().serialize(msg2.makeMessage(), out);
+        out.close();
+    }
+    
+    @Test
+    public void testStreamRequestMessageRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testStreamRequestMessageWrite();
+        
+        DataInputStream in = getInput("streaming.StreamRequestMessage.bin");
+        assert StreamRequestMessage.serializer().deserialize(in) != null;
+        assert StreamRequestMessage.serializer().deserialize(in) != null;
+        assert StreamRequestMessage.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        assert Message.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private static SSTable makeSSTable()
+    {
+        Table t = Table.open("Keyspace1");
+        for (int i = 0; i < 100; i++)
+        {
+            RowMutation rm = new RowMutation(t.name, ByteBufferUtil.bytes(Long.toString(System.nanoTime())));
+            rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes("cola")), ByteBufferUtil.bytes("value"), 0);
+            try
+            {
+                rm.apply();
+            }
+            catch (IOException ex) 
+            {
+                throw new RuntimeException(ex);
+            }
+        }
+        try
+        {
+            t.getColumnFamilyStore("Standard1").forceBlockingFlush();
+            return t.getColumnFamilyStore("Standard1").getSSTables().iterator().next();
+        }
+        catch (Exception any)
+        {
+            throw new RuntimeException(any);
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/StreamUtil.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/StreamUtil.java
index 3f24d583..4890f2df 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/StreamUtil.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/StreamUtil.java
@@ -1 +1,55 @@
   + native
+/**
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.streaming;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.net.InetAddress;
+
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+
+public class StreamUtil
+{
+
+    /**
+     * Takes an stream request message and creates an empty status response. Exists here because StreamRequestMessage
+     * is package protected.
+     */
+    static public void finishStreamRequest(Message msg, InetAddress to) 
+    {
+        byte[] body = msg.getMessageBody();
+        ByteArrayInputStream bufIn = new ByteArrayInputStream(body);
+
+        try
+        {
+            StreamRequestMessage srm = StreamRequestMessage.serializer().deserialize(new DataInputStream(bufIn));
+            StreamInSession session = StreamInSession.get(to, srm.sessionId);
+            session.closeIfFinished();
+        }
+        catch (Exception e)
+        {
+            System.err.println(e); 
+            e.printStackTrace();
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
index 3f24d583..7766133a 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
@@ -1 +1,205 @@
   + native
+package org.apache.cassandra.streaming;
+
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+import static junit.framework.Assert.assertEquals;
+import static org.apache.cassandra.Util.column;
+
+import java.net.InetAddress;
+import java.nio.ByteBuffer;
+import java.util.*;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.columniterator.IdentityQueryFilter;
+import org.apache.cassandra.db.filter.IFilter;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.io.sstable.SSTableUtils;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.thrift.IndexClause;
+import org.apache.cassandra.thrift.IndexExpression;
+import org.apache.cassandra.thrift.IndexOperator;
+import org.apache.cassandra.utils.FBUtilities;
+
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+public class StreamingTransferTest extends CleanupHelper
+{
+    public static final InetAddress LOCAL = FBUtilities.getLocalAddress();
+
+    @BeforeClass
+    public static void setup() throws Exception
+    {
+        StorageService.instance.initServer();
+    }
+
+    @Test
+    public void testTransferTable() throws Exception
+    {
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfs = table.getColumnFamilyStore("Indexed1");
+        
+        // write a temporary SSTable, and unregister it
+        for (int i = 1; i <= 3; i++)
+        {
+            String key = "key" + i;
+            RowMutation rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes(key));
+            ColumnFamily cf = ColumnFamily.create(table.name, cfs.columnFamily);
+            cf.addColumn(column(key, "v", 0));
+            cf.addColumn(new Column(ByteBufferUtil.bytes("birthdate"), ByteBufferUtil.bytes((long) i), 0));
+            rm.add(cf);
+            rm.apply();
+        }
+        cfs.forceBlockingFlush();
+        assert cfs.getSSTables().size() == 1;
+        SSTableReader sstable = cfs.getSSTables().iterator().next();
+        cfs.removeAllSSTables();
+
+        // transfer the first and last key
+        IPartitioner p = StorageService.getPartitioner();
+        List<Range> ranges = new ArrayList<Range>();
+        ranges.add(new Range(p.getMinimumToken(), p.getToken(ByteBufferUtil.bytes("key1"))));
+        ranges.add(new Range(p.getToken(ByteBufferUtil.bytes("key2")), p.getMinimumToken()));
+        StreamOutSession session = StreamOutSession.create(table.name, LOCAL, null);
+        StreamOut.transferSSTables(session, Arrays.asList(sstable), ranges);
+        session.await();
+
+        // confirm that the SSTable was transferred and registered
+        List<Row> rows = Util.getRangeSlice(cfs);
+        assertEquals(2, rows.size());
+        assert rows.get(0).key.key.equals( ByteBufferUtil.bytes("key1"));
+        assert rows.get(1).key.key.equals( ByteBufferUtil.bytes("key3"));
+        assertEquals(2, rows.get(0).cf.getColumnsMap().size());
+        assertEquals(2, rows.get(1).cf.getColumnsMap().size());
+        assert rows.get(1).cf.getColumn(ByteBufferUtil.bytes("key3")) != null;
+
+        // and that the index and filter were properly recovered
+        assert null != cfs.getColumnFamily(QueryFilter.getIdentityFilter(Util.dk("key1"), new QueryPath(cfs.columnFamily)));
+        assert null != cfs.getColumnFamily(QueryFilter.getIdentityFilter(Util.dk("key3"), new QueryPath(cfs.columnFamily)));
+
+        // and that the secondary index works
+        IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(3L));
+        IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
+        IFilter filter = new IdentityQueryFilter();
+        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+        rows = cfs.scan(clause, range, filter);
+        assertEquals(1, rows.size());
+        assert rows.get(0).key.key.equals( ByteBufferUtil.bytes("key3")) ;
+    }
+
+    @Test
+    public void testTransferTableMultiple() throws Exception
+    {
+        // write a temporary SSTable, but don't register it
+        Set<String> content = new HashSet<String>();
+        content.add("transfer1");
+        content.add("transfer2");
+        content.add("transfer3");
+        SSTableReader sstable = SSTableUtils.prepare().write(content);
+        String tablename = sstable.getTableName();
+        String cfname = sstable.getColumnFamilyName();
+
+        Set<String> content2 = new HashSet<String>();
+        content2.add("test");
+        content2.add("test2");
+        content2.add("test3");
+        SSTableReader sstable2 = SSTableUtils.prepare().write(content2);
+
+        // transfer the first and last key
+        IPartitioner p = StorageService.getPartitioner();
+        List<Range> ranges = new ArrayList<Range>();
+        ranges.add(new Range(p.getMinimumToken(), p.getToken(ByteBufferUtil.bytes("transfer1"))));
+        ranges.add(new Range(p.getToken(ByteBufferUtil.bytes("test2")), p.getMinimumToken()));
+        StreamOutSession session = StreamOutSession.create(tablename, LOCAL, null);
+        StreamOut.transferSSTables(session, Arrays.asList(sstable, sstable2), ranges);
+        session.await();
+
+        // confirm that the SSTable was transferred and registered
+        ColumnFamilyStore cfstore = Table.open(tablename).getColumnFamilyStore(cfname);
+        List<Row> rows = Util.getRangeSlice(cfstore);
+        assertEquals(6, rows.size());
+        assert rows.get(0).key.key.equals(ByteBufferUtil.bytes("test"));
+        assert rows.get(3).key.key.equals(ByteBufferUtil.bytes("transfer1"));
+        assert rows.get(0).cf.getColumnsMap().size() == 1;
+        assert rows.get(3).cf.getColumnsMap().size() == 1;
+
+        // these keys fall outside of the ranges and should not be transferred.
+        assert null != cfstore.getColumnFamily(QueryFilter.getIdentityFilter(Util.dk("transfer2"), new QueryPath("Standard1")));
+        assert null != cfstore.getColumnFamily(QueryFilter.getIdentityFilter(Util.dk("transfer3"), new QueryPath("Standard1")));
+        
+        // and that the index and filter were properly recovered
+        assert null != cfstore.getColumnFamily(QueryFilter.getIdentityFilter(Util.dk("test"), new QueryPath("Standard1")));
+        assert null != cfstore.getColumnFamily(QueryFilter.getIdentityFilter(Util.dk("transfer1"), new QueryPath("Standard1")));
+    }
+
+    @Test
+    public void testTransferOfMultipleColumnFamilies() throws Exception
+    {
+        String keyspace = "Keyspace1";
+        IPartitioner p = StorageService.getPartitioner();
+        String[] columnFamilies = new String[] { "Standard1", "Standard2", "Standard3" };
+        List<SSTableReader> ssTableReaders = new ArrayList<SSTableReader>();
+
+        // ranges to transfer
+        List<Range> ranges = new ArrayList<Range>();
+
+        for (String cf : columnFamilies)
+        {
+            Set<String> content = new HashSet<String>();
+
+            content.add("data-" + cf + "-1");
+            content.add("data-" + cf + "-2");
+            content.add("data-" + cf + "-3");
+
+            SSTableUtils.Context context = SSTableUtils.prepare().ks(keyspace).cf(cf);
+
+            ssTableReaders.add(context.write(content));
+            ranges.add(new Range(p.getMinimumToken(), p.getToken(ByteBufferUtil.bytes("data-" + cf + "-3"))));
+        }
+
+        StreamOutSession session = StreamOutSession.create(keyspace, LOCAL, null);
+        StreamOut.transferSSTables(session, ssTableReaders, ranges);
+
+        session.await();
+
+        for (String cf : columnFamilies)
+        {
+            ColumnFamilyStore store = Table.open(keyspace).getColumnFamilyStore(cf);
+            List<Row> rows = Util.getRangeSlice(store);
+
+            assert rows.size() >= 3;
+
+            for (int i = 0; i < 3; i++)
+            {
+                String expectedKey = "data-" + cf + "-" + (i + 1);
+                assertEquals(p.decorateKey(ByteBufferUtil.bytes(expectedKey)), rows.get(i).key);
+            }
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/tools/SSTableExportTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
index 3f24d583..b9ce64bf 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
@@ -1 +1,219 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.tools;
+
+import java.io.File;
+import java.io.FileReader;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.ExpiringColumn;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.sstable.Descriptor;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableWriter;
+import org.apache.cassandra.utils.ByteBufferUtil;
+
+import static org.apache.cassandra.io.sstable.SSTableUtils.tempSSTableFile;
+import static org.apache.cassandra.utils.ByteBufferUtil.bytesToHex;
+import static org.apache.cassandra.utils.ByteBufferUtil.hexToBytes;
+import static org.junit.Assert.assertTrue;
+
+import org.apache.cassandra.Util;
+
+import org.json.simple.JSONArray;
+import org.json.simple.JSONObject;
+import org.json.simple.JSONValue;
+import org.json.simple.parser.ParseException;
+import org.junit.Test;
+
+public class SSTableExportTest extends SchemaLoader
+{
+    public String asHex(String str)
+    {
+        return bytesToHex(ByteBufferUtil.bytes(str));
+    }
+
+    @Test
+    public void testEnumeratekeys() throws IOException
+    {
+        File tempSS = tempSSTableFile("Keyspace1", "Standard1");
+        ColumnFamily cfamily = ColumnFamily.create("Keyspace1", "Standard1");
+        SSTableWriter writer = new SSTableWriter(tempSS.getPath(), 2);
+        
+        // Add rowA
+        cfamily.addColumn(new QueryPath("Standard1", null, ByteBufferUtil.bytes("colA")), ByteBufferUtil.bytes("valA"), System.currentTimeMillis());
+        writer.append(Util.dk("rowA"), cfamily);
+        cfamily.clear();
+        
+        // Add rowB
+        cfamily.addColumn(new QueryPath("Standard1", null, ByteBufferUtil.bytes("colB")), ByteBufferUtil.bytes("valB"), System.currentTimeMillis());
+        writer.append(Util.dk("rowB"), cfamily);
+        cfamily.clear();
+     
+        writer.closeAndOpenReader();
+        
+        // Enumerate and verify
+        File temp = File.createTempFile("Standard1", ".txt");
+        SSTableExport.enumeratekeys(writer.getFilename(), new PrintStream(temp.getPath()));
+
+        
+        FileReader file = new FileReader(temp);
+        char[] buf = new char[(int) temp.length()];
+        file.read(buf);
+        String output = new String(buf);
+
+        String sep = System.getProperty("line.separator");
+        assert output.equals(asHex("rowA") + sep + asHex("rowB") + sep) : output;
+    }
+
+    @Test
+    public void testExportSimpleCf() throws IOException
+    {
+        File tempSS = tempSSTableFile("Keyspace1", "Standard1");
+        ColumnFamily cfamily = ColumnFamily.create("Keyspace1", "Standard1");
+        SSTableWriter writer = new SSTableWriter(tempSS.getPath(), 2);
+        
+        int nowInSec = (int)(System.currentTimeMillis() / 1000) + 42; //live for 42 seconds
+        // Add rowA
+        cfamily.addColumn(new QueryPath("Standard1", null, ByteBufferUtil.bytes("colA")), ByteBufferUtil.bytes("valA"), System.currentTimeMillis());
+        cfamily.addColumn(null, new ExpiringColumn(ByteBufferUtil.bytes("colExp"), ByteBufferUtil.bytes("valExp"), System.currentTimeMillis(), 42, nowInSec));
+        writer.append(Util.dk("rowA"), cfamily);
+        cfamily.clear();
+        
+        // Add rowB
+        cfamily.addColumn(new QueryPath("Standard1", null, ByteBufferUtil.bytes("colB")), ByteBufferUtil.bytes("valB"), System.currentTimeMillis());
+        writer.append(Util.dk("rowB"), cfamily);
+        cfamily.clear();
+
+        // Add rowExclude
+        cfamily.addColumn(new QueryPath("Standard1", null, ByteBufferUtil.bytes("colX")), ByteBufferUtil.bytes("valX"), System.currentTimeMillis());
+        writer.append(Util.dk("rowExclude"), cfamily);
+        cfamily.clear();
+
+        SSTableReader reader = writer.closeAndOpenReader();
+        
+        // Export to JSON and verify
+        File tempJson = File.createTempFile("Standard1", ".json");
+        SSTableExport.export(reader, new PrintStream(tempJson.getPath()), new String[]{asHex("rowExclude")});
+        
+        JSONObject json = (JSONObject)JSONValue.parse(new FileReader(tempJson));
+        
+        JSONArray rowA = (JSONArray)json.get(asHex("rowA"));
+        JSONArray colA = (JSONArray)rowA.get(0);
+        assert hexToBytes((String)colA.get(1)).equals(ByteBufferUtil.bytes("valA"));
+
+        JSONArray colExp = (JSONArray)rowA.get(1);
+        assert ((Long)colExp.get(4)) == 42;
+        assert ((Long)colExp.get(5)) == nowInSec;
+        
+        JSONArray rowB = (JSONArray)json.get(asHex("rowB"));
+        JSONArray colB = (JSONArray)rowB.get(0);
+        assert !(Boolean)colB.get(3);
+
+        JSONArray rowExclude = (JSONArray)json.get(asHex("rowExclude"));
+        assert rowExclude == null;
+    }
+
+    @Test
+    public void testExportSuperCf() throws IOException
+    {
+        File tempSS = tempSSTableFile("Keyspace1", "Super4");
+        ColumnFamily cfamily = ColumnFamily.create("Keyspace1", "Super4");
+        SSTableWriter writer = new SSTableWriter(tempSS.getPath(), 2);
+        
+        // Add rowA
+        cfamily.addColumn(new QueryPath("Super4", ByteBufferUtil.bytes("superA"), ByteBufferUtil.bytes("colA")), ByteBufferUtil.bytes("valA"), System.currentTimeMillis());
+        writer.append(Util.dk("rowA"), cfamily);
+        cfamily.clear();
+        
+        // Add rowB
+        cfamily.addColumn(new QueryPath("Super4", ByteBufferUtil.bytes("superB"), ByteBufferUtil.bytes("colB")), ByteBufferUtil.bytes("valB"), System.currentTimeMillis());
+        writer.append(Util.dk("rowB"), cfamily);
+        cfamily.clear();
+
+        // Add rowExclude
+        cfamily.addColumn(new QueryPath("Super4", ByteBufferUtil.bytes("superX"), ByteBufferUtil.bytes("colX")), ByteBufferUtil.bytes("valX"), System.currentTimeMillis());
+        writer.append(Util.dk("rowExclude"), cfamily);
+        cfamily.clear();
+
+        SSTableReader reader = writer.closeAndOpenReader();
+        
+        // Export to JSON and verify
+        File tempJson = File.createTempFile("Super4", ".json");
+        SSTableExport.export(reader, new PrintStream(tempJson.getPath()), new String[]{asHex("rowExclude")});
+        
+        JSONObject json = (JSONObject)JSONValue.parse(new FileReader(tempJson));
+        
+        JSONObject rowA = (JSONObject)json.get(asHex("rowA"));
+        JSONObject superA = (JSONObject)rowA.get(cfamily.getComparator().getString(ByteBufferUtil.bytes("superA")));
+        JSONArray subColumns = (JSONArray)superA.get("subColumns");
+        JSONArray colA = (JSONArray)subColumns.get(0);
+        JSONObject rowExclude = (JSONObject)json.get(asHex("rowExclude"));
+        assert hexToBytes((String)colA.get(1)).equals(ByteBufferUtil.bytes("valA"));
+        assert !(Boolean)colA.get(3);
+        assert rowExclude == null;
+    }
+    
+    @Test
+    public void testRoundTripStandardCf() throws IOException, ParseException
+    {
+        File tempSS = tempSSTableFile("Keyspace1", "Standard1");
+        ColumnFamily cfamily = ColumnFamily.create("Keyspace1", "Standard1");
+        SSTableWriter writer = new SSTableWriter(tempSS.getPath(), 2);
+        
+        // Add rowA
+        cfamily.addColumn(new QueryPath("Standard1", null, ByteBufferUtil.bytes("name")), ByteBufferUtil.bytes("val"), System.currentTimeMillis());
+        writer.append(Util.dk("rowA"), cfamily);
+        cfamily.clear();
+
+        // Add rowExclude
+        cfamily.addColumn(new QueryPath("Standard1", null, ByteBufferUtil.bytes("name")), ByteBufferUtil.bytes("val"), System.currentTimeMillis());
+        writer.append(Util.dk("rowExclude"), cfamily);
+        cfamily.clear();
+
+        SSTableReader reader = writer.closeAndOpenReader();
+        
+        // Export to JSON and verify
+        File tempJson = File.createTempFile("Standard1", ".json");
+        SSTableExport.export(reader, new PrintStream(tempJson.getPath()), new String[]{asHex("rowExclude")});
+        
+        // Import JSON to another SSTable file
+        File tempSS2 = tempSSTableFile("Keyspace1", "Standard1");
+        SSTableImport.importJson(tempJson.getPath(), "Keyspace1", "Standard1", tempSS2.getPath());
+
+        reader = SSTableReader.open(Descriptor.fromFilename(tempSS2.getPath()));
+        QueryFilter qf = QueryFilter.getNamesFilter(Util.dk("rowA"), new QueryPath("Standard1", null, null), ByteBufferUtil.bytes("name"));
+        ColumnFamily cf = qf.getSSTableColumnIterator(reader).getColumnFamily();
+        assertTrue(cf != null);
+        assertTrue(cf.getColumn(ByteBufferUtil.bytes("name")).value().equals(hexToBytes("76616c")));
+
+        qf = QueryFilter.getNamesFilter(Util.dk("rowExclude"), new QueryPath("Standard1", null, null), ByteBufferUtil.bytes("name"));
+        cf = qf.getSSTableColumnIterator(reader).getColumnFamily();
+        assert cf == null;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/tools/SSTableImportTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
index e69de29b..bd267dfb 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
@@ -0,0 +1,102 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.tools;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.DeletedColumn;
+import org.apache.cassandra.db.ExpiringColumn;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.filter.QueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.columniterator.IColumnIterator;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.sstable.Descriptor;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import static org.apache.cassandra.utils.ByteBufferUtil.hexToBytes;
+import static org.apache.cassandra.io.sstable.SSTableUtils.tempSSTableFile;
+import static org.junit.Assert.assertEquals;
+
+import org.apache.cassandra.Util;
+
+import org.json.simple.parser.ParseException;
+import org.junit.Test;
+
+public class SSTableImportTest extends SchemaLoader
+{   
+    @Test
+    public void testImportSimpleCf() throws IOException
+    {
+        // Import JSON to temp SSTable file
+        String jsonUrl = getClass().getClassLoader().getResource("SimpleCF.json").getPath();
+        File tempSS = tempSSTableFile("Keyspace1", "Standard1");
+        SSTableImport.importJson(jsonUrl, "Keyspace1", "Standard1", tempSS.getPath());
+
+        // Verify results
+        SSTableReader reader = SSTableReader.open(Descriptor.fromFilename(tempSS.getPath()));
+        QueryFilter qf = QueryFilter.getIdentityFilter(Util.dk("rowA"), new QueryPath("Standard1"));
+        IColumnIterator iter = qf.getSSTableColumnIterator(reader);
+        ColumnFamily cf = iter.getColumnFamily();
+        while (iter.hasNext()) cf.addColumn(iter.next());
+        assert cf.getColumn(ByteBufferUtil.bytes("colAA")).value().equals(hexToBytes("76616c4141"));
+        assert !(cf.getColumn(ByteBufferUtil.bytes("colAA")) instanceof DeletedColumn);
+        IColumn expCol = cf.getColumn(ByteBufferUtil.bytes("colAC"));
+        assert expCol.value().equals(hexToBytes("76616c4143"));
+        assert expCol instanceof ExpiringColumn;
+        assert ((ExpiringColumn)expCol).getTimeToLive() == 42 && expCol.getLocalDeletionTime() == 2000000000;
+    }
+
+    @Test
+    public void testImportSuperCf() throws IOException, ParseException
+    {
+        String jsonUrl = getClass().getClassLoader().getResource("SuperCF.json").getPath();
+        File tempSS = tempSSTableFile("Keyspace1", "Super4");
+        SSTableImport.importJson(jsonUrl, "Keyspace1", "Super4", tempSS.getPath());
+        
+        // Verify results
+        SSTableReader reader = SSTableReader.open(Descriptor.fromFilename(tempSS.getPath()));
+        QueryFilter qf = QueryFilter.getNamesFilter(Util.dk("rowA"), new QueryPath("Super4", null, null), ByteBufferUtil.bytes("superA"));
+        ColumnFamily cf = qf.getSSTableColumnIterator(reader).getColumnFamily();
+        IColumn superCol = cf.getColumn(ByteBufferUtil.bytes("superA"));
+        assert superCol != null;
+        assert superCol.getSubColumns().size() > 0;
+        IColumn subColumn = superCol.getSubColumn(ByteBufferUtil.bytes("colAA"));
+        assert subColumn.value().equals(hexToBytes("76616c75654141"));
+    }
+
+    @Test
+    public void testImportUnsortedMode() throws IOException
+    {
+        String jsonUrl = getClass().getClassLoader().getResource("UnsortedSuperCF.json").getPath();
+        File tempSS = tempSSTableFile("Keyspace1", "Super4");
+
+        ColumnFamily columnFamily = ColumnFamily.create("Keyspace1", "Super4");
+        IPartitioner<?> partitioner = DatabaseDescriptor.getPartitioner();
+
+        SSTableImport.setKeyCountToImport(3);
+        int result = SSTableImport.importSorted(jsonUrl, columnFamily, tempSS.getPath(), partitioner);
+        assert result == -1;
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/BloomFilterTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/BloomFilterTest.java
index e69de29b..31a3369f 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/BloomFilterTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/BloomFilterTest.java
@@ -0,0 +1,140 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.utils;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Set;
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+
+import org.apache.cassandra.io.util.DataOutputBuffer;
+
+import org.junit.Before;
+import org.junit.Test;
+
+public class BloomFilterTest
+{
+    public BloomFilter bf;
+
+    public BloomFilterTest()
+    {
+        bf = BloomFilter.getFilter(10000L, FilterTestHelper.MAX_FAILURE_RATE);
+    }
+
+    public static BloomFilter testSerialize(BloomFilter f) throws IOException
+    {
+        f.add(ByteBufferUtil.bytes("a"));
+        DataOutputBuffer out = new DataOutputBuffer();
+        f.serializer().serialize(f, out);
+
+        ByteArrayInputStream in = new ByteArrayInputStream(out.getData(), 0, out.getLength());
+        BloomFilter f2 = f.serializer().deserialize(new DataInputStream(in));
+
+        assert f2.isPresent(ByteBufferUtil.bytes("a"));
+        assert !f2.isPresent(ByteBufferUtil.bytes("b"));
+        return f2;
+    }
+
+
+    @Before
+    public void clear()
+    {
+        bf.clear();
+    }
+
+    @Test(expected = UnsupportedOperationException.class)
+    public void testBloomLimits1()
+    {
+        int maxBuckets = BloomCalculations.probs.length - 1;
+        int maxK = BloomCalculations.probs[maxBuckets].length - 1;
+
+        // possible
+        BloomCalculations.computeBloomSpec(maxBuckets, BloomCalculations.probs[maxBuckets][maxK]);
+
+        // impossible, throws
+        BloomCalculations.computeBloomSpec(maxBuckets, BloomCalculations.probs[maxBuckets][maxK] / 2);
+    }
+
+    @Test
+    public void testOne()
+    {
+        bf.add(ByteBufferUtil.bytes("a"));
+        assert bf.isPresent(ByteBufferUtil.bytes("a"));
+        assert !bf.isPresent(ByteBufferUtil.bytes("b"));
+    }
+
+    @Test
+    public void testFalsePositivesInt()
+    {
+        FilterTestHelper.testFalsePositives(bf, FilterTestHelper.intKeys(), FilterTestHelper.randomKeys2());
+    }
+
+    @Test
+    public void testFalsePositivesRandom()
+    {
+        FilterTestHelper.testFalsePositives(bf, FilterTestHelper.randomKeys(), FilterTestHelper.randomKeys2());
+    }
+
+    @Test
+    public void testWords()
+    {
+        if (KeyGenerator.WordGenerator.WORDS == 0)
+        {
+            return;
+        }
+        BloomFilter bf2 = BloomFilter.getFilter(KeyGenerator.WordGenerator.WORDS / 2, FilterTestHelper.MAX_FAILURE_RATE);
+        int skipEven = KeyGenerator.WordGenerator.WORDS % 2 == 0 ? 0 : 2;
+        FilterTestHelper.testFalsePositives(bf2,
+                                            new KeyGenerator.WordGenerator(skipEven, 2),
+                                            new KeyGenerator.WordGenerator(1, 2));
+    }
+
+    @Test
+    public void testSerialize() throws IOException
+    {
+        BloomFilterTest.testSerialize(bf);
+    }
+
+    public void testManyHashes(Iterator<ByteBuffer> keys)
+    {
+        int MAX_HASH_COUNT = 128;
+        Set<Long> hashes = new HashSet<Long>();
+        long collisions = 0;
+        while (keys.hasNext())
+        {
+            hashes.clear();
+            ByteBuffer buf = keys.next();
+            for (long hashIndex : BloomFilter.getHashBuckets(buf, MAX_HASH_COUNT, 1024 * 1024))
+            {
+                hashes.add(hashIndex);
+            }
+            collisions += (MAX_HASH_COUNT - hashes.size());
+        }
+        assert collisions <= 100;
+    }
+
+    @Test
+    public void testManyRandom()
+    {
+        testManyHashes(FilterTestHelper.randomKeys());
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/BoundedStatsDequeTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/BoundedStatsDequeTest.java
index e69de29b..9ead13ce 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/BoundedStatsDequeTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/BoundedStatsDequeTest.java
@@ -0,0 +1,78 @@
+package org.apache.cassandra.utils;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import static org.junit.Assert.*;
+
+import java.util.Iterator;
+
+import org.junit.Test;
+
+public class BoundedStatsDequeTest
+{
+
+    @Test
+    public void test()
+    {
+        int size = 4;
+        
+        BoundedStatsDeque bsd = new BoundedStatsDeque(size);
+        //check the values for an empty result
+        assertEquals(0, bsd.size());
+        assertEquals(0, bsd.sum(), 0.001d);
+        assertEquals(Double.NaN, bsd.mean(), 0.001d);
+        assertEquals(Double.NaN, bsd.variance(), 0.001d);
+        assertEquals(Double.NaN, bsd.stdev(), 0.001d);
+        assertEquals(0, bsd.sumOfDeviations(), 0.001d);
+        
+        bsd.add(1d); //this one falls out, over limit
+        bsd.add(2d);
+        bsd.add(3d);
+        bsd.add(4d);
+        bsd.add(5d);
+        
+        //verify that everything is in there
+        Iterator<Double> iter = bsd.iterator();
+        assertTrue(iter.hasNext());
+        assertEquals(2d, iter.next(), 0);
+        assertTrue(iter.hasNext());
+        assertEquals(3d, iter.next(), 0);
+        assertTrue(iter.hasNext());
+        assertEquals(4d, iter.next(), 0);
+        assertTrue(iter.hasNext());
+        assertEquals(5d, iter.next(), 0);
+        assertFalse(iter.hasNext());
+        
+        //check results
+        assertEquals(size, bsd.size());
+        assertEquals(14, bsd.sum(), 0.001d);
+        assertEquals(3.5, bsd.mean(), 0.001d);
+        assertEquals(1.25, bsd.variance(), 0.001d);
+        assertEquals(1.1180d, bsd.stdev(), 0.001d);
+        assertEquals(5, bsd.sumOfDeviations(), 0.001d);
+        
+        //check that it clears properly
+        bsd.clear();
+        assertFalse(bsd.iterator().hasNext());
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/ByteBufferUtilTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/ByteBufferUtilTest.java
index e69de29b..1c384abe 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/ByteBufferUtilTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/ByteBufferUtilTest.java
@@ -0,0 +1,237 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import static org.junit.Assert.assertArrayEquals;
+import static org.junit.Assert.assertEquals;
+
+import java.io.IOException;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.nio.ByteBuffer;
+import java.nio.charset.CharacterCodingException;
+import java.util.Arrays;
+
+import com.google.common.base.Charsets;
+import org.junit.Test;
+
+public class ByteBufferUtilTest
+{
+    private static final String s = "cassandra";
+
+    private ByteBuffer fromStringWithPosition(String s, int pos, boolean direct)
+    {
+        int l = s.length();
+        ByteBuffer bb;
+        if (direct)
+        {
+            bb = ByteBuffer.allocateDirect(l + pos);
+        }
+        else
+        {
+            ByteBuffer tmp = ByteBuffer.allocate(l + pos + 3);
+            tmp.position(3);
+            bb = tmp.slice(); // make bb have a non null arrayOffset
+        }
+        bb.position(pos);
+        bb.mark();
+        bb.put(s.getBytes());
+        bb.reset();
+        assert bb.position() == pos;
+        return bb;
+    }
+
+    @Test
+    public void testString() throws Exception
+    {
+        assert s.equals(ByteBufferUtil.string(ByteBufferUtil.bytes(s)));
+
+        int pos = 10;
+        ByteBuffer bb = fromStringWithPosition(s, 10, false);
+        assert s.equals(ByteBufferUtil.string(bb, 10, s.length()));
+
+        bb = fromStringWithPosition(s, 10, true);
+        assert s.equals(ByteBufferUtil.string(bb, 10, s.length()));
+    }
+
+    @Test
+    public void testGetArray()
+    {
+        byte[] t = s.getBytes();
+
+        ByteBuffer bb = ByteBufferUtil.bytes(s);
+        assertArrayEquals(t, ByteBufferUtil.getArray(bb));
+
+        bb = fromStringWithPosition(s, 10, false);
+        assertArrayEquals(t, ByteBufferUtil.getArray(bb));
+
+        bb = fromStringWithPosition(s, 10, true);
+        assertArrayEquals(t, ByteBufferUtil.getArray(bb));
+    }
+
+    @Test
+    public void testLastIndexOf()
+    {
+        ByteBuffer bb = ByteBufferUtil.bytes(s);
+        checkLastIndexOf(bb);
+
+        bb = fromStringWithPosition(s, 10, false);
+        checkLastIndexOf(bb);
+
+        bb = fromStringWithPosition(s, 10, true);
+        checkLastIndexOf(bb);
+    }
+
+    private void checkLastIndexOf(ByteBuffer bb)
+    {
+        assert bb.position() + 8 == ByteBufferUtil.lastIndexOf(bb, (byte)'a', bb.position() + 8);
+        assert bb.position() + 4 == ByteBufferUtil.lastIndexOf(bb, (byte)'a', bb.position() + 7);
+        assert bb.position() + 3 == ByteBufferUtil.lastIndexOf(bb, (byte)'s', bb.position() + 8);
+        assert -1 == ByteBufferUtil.lastIndexOf(bb, (byte)'o', bb.position() + 8);
+        assert -1 == ByteBufferUtil.lastIndexOf(bb, (byte)'d', bb.position() + 5);
+    }
+
+    @Test
+    public void testClone()
+    {
+        ByteBuffer bb = ByteBufferUtil.bytes(s);
+        ByteBuffer clone1 = ByteBufferUtil.clone(bb);
+        assert bb != clone1;
+        assert bb.equals(clone1);
+        assert bb.array() != clone1.array();
+
+        bb = fromStringWithPosition(s, 10, false);
+        ByteBuffer clone2 = ByteBufferUtil.clone(bb);
+        assert bb != clone2;
+        assert bb.equals(clone2);
+        assert clone1.equals(clone2);
+        assert bb.array() != clone2.array();
+
+        bb = fromStringWithPosition(s, 10, true);
+        ByteBuffer clone3 = ByteBufferUtil.clone(bb);
+        assert bb != clone3;
+        assert bb.equals(clone3);
+        assert clone1.equals(clone3);
+    }
+
+    @Test
+    public void testArrayCopy()
+    {
+        ByteBuffer bb = ByteBufferUtil.bytes(s);
+        checkArrayCopy(bb);
+
+        bb = fromStringWithPosition(s, 10, false);
+        checkArrayCopy(bb);
+
+        bb = fromStringWithPosition(s, 10, true);
+        checkArrayCopy(bb);
+    }
+
+    private void checkArrayCopy(ByteBuffer bb)
+    {
+
+        byte[] bytes = new byte[s.length()];
+        ByteBufferUtil.arrayCopy(bb, bb.position(), bytes, 0, s.length());
+        assertArrayEquals(s.getBytes(), bytes);
+
+        bytes = new byte[5];
+        ByteBufferUtil.arrayCopy(bb, bb.position() + 3, bytes, 1, 4);
+        assertArrayEquals(Arrays.copyOfRange(s.getBytes(), 3, 7), Arrays.copyOfRange(bytes, 1, 5));
+    }
+
+    @Test
+    public void testReadWrite() throws IOException
+    {
+        ByteBuffer bb = ByteBufferUtil.bytes(s);
+        checkReadWrite(bb);
+
+        bb = fromStringWithPosition(s, 10, false);
+        checkReadWrite(bb);
+
+        bb = fromStringWithPosition(s, 10, true);
+        checkReadWrite(bb);
+    }
+
+    private void checkReadWrite(ByteBuffer bb) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream out = new DataOutputStream(bos);
+        ByteBufferUtil.writeWithLength(bb, out);
+        ByteBufferUtil.writeWithShortLength(bb, out);
+
+        DataInputStream in = new DataInputStream(new ByteArrayInputStream(bos.toByteArray()));
+        assert bb.equals(ByteBufferUtil.readWithLength(in));
+        assert bb.equals(ByteBufferUtil.readWithShortLength(in));
+    }
+
+    @Test
+    public void testInputStream() throws IOException
+    {
+        ByteBuffer bb = ByteBuffer.allocate(13);
+        bb.putInt(255);
+        bb.put((byte) -3);
+        bb.putLong(42L);
+        bb.clear();
+
+        DataInputStream in = new DataInputStream(ByteBufferUtil.inputStream(bb));
+        assert in.readInt() == 255;
+        assert in.readByte() == (byte)-3;
+        assert in.readLong() == 42L;
+    }
+
+    @Test
+    public void testIntBytesConversions()
+    {
+        // positive, negative, 1 and 2 byte cases, including a few edges that would foul things up unless you're careful
+        // about masking away sign extension.
+        int[] ints = new int[]
+        {
+            -20, -127, -128, 0, 1, 127, 128, 65534, 65535, -65534, -65535
+        };
+
+        for (int i : ints) {
+            ByteBuffer ba = ByteBufferUtil.bytes(i);
+            int actual = ByteBufferUtil.toInt(ba);
+            assertEquals(i, actual);
+        }
+    }
+
+    @Test(expected=CharacterCodingException.class)
+    public void testDecode() throws IOException
+    {
+        ByteBuffer bytes = ByteBuffer.wrap(new byte[]{(byte)0xff, (byte)0xfe});
+        ByteBufferUtil.string(bytes);
+    }
+
+    @Test
+    public void testHexBytesConversion()
+    {
+        for (int i = Byte.MIN_VALUE; i <= Byte.MAX_VALUE; i++)
+        {
+            ByteBuffer bb = ByteBuffer.allocate(1);
+            bb.put((byte)i);
+            bb.clear();
+            String s = ByteBufferUtil.bytesToHex(bb);
+            ByteBuffer bb2 = ByteBufferUtil.hexToBytes(s);
+            assert bb.equals(bb2);
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/EstimatedHistogramTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/EstimatedHistogramTest.java
index e69de29b..c3826d11 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/EstimatedHistogramTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/EstimatedHistogramTest.java
@@ -0,0 +1,74 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.utils;
+
+import org.junit.Test;
+
+import static org.junit.Assert.*;
+
+
+public class EstimatedHistogramTest
+{
+    @Test
+    public void testSimple()
+    {
+        // 0 and 1 map to the same, first bucket
+        EstimatedHistogram histogram = new EstimatedHistogram();
+        histogram.add(0);
+        assertEquals(1, histogram.get(0));
+        histogram.add(1);
+        assertEquals(2, histogram.get(0));
+    }
+
+    @Test
+    public void testOverflow()
+    {
+        EstimatedHistogram histogram = new EstimatedHistogram(1);
+        histogram.add(100);
+        assert histogram.isOverflowed();
+        assertEquals(Long.MAX_VALUE, histogram.max());
+    }
+
+    @Test
+    public void testMinMax()
+    {
+        EstimatedHistogram histogram = new EstimatedHistogram();
+        histogram.add(16);
+        assertEquals(15, histogram.min());
+        assertEquals(17, histogram.max());
+    }
+
+    @Test
+    public void testFindingCorrectBuckets()
+    {
+        EstimatedHistogram histogram = new EstimatedHistogram();
+        histogram.add(23282687);
+        assert !histogram.isOverflowed();
+        assertEquals(1, histogram.getBuckets(false)[histogram.buckets.length() - 2]);
+
+        histogram.add(9);
+        assertEquals(1, histogram.getBuckets(false)[8]);
+
+        histogram.add(20);
+        histogram.add(21);
+        histogram.add(22);
+        assertEquals(2, histogram.getBuckets(false)[13]);
+        assertEquals(5021848, histogram.mean());
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/FBUtilitiesTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/FBUtilitiesTest.java
index 3f24d583..fd28a897 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/FBUtilitiesTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/FBUtilitiesTest.java
@@ -1 +1,73 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import static org.junit.Assert.assertArrayEquals;
+import static org.junit.Assert.assertEquals;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.charset.CharacterCodingException;
+import java.util.Arrays;
+
+import com.google.common.base.Charsets;
+import org.junit.Test;
+
+public class FBUtilitiesTest 
+{
+	@Test
+    public void testHexBytesConversion()
+    {
+        for (int i = Byte.MIN_VALUE; i <= Byte.MAX_VALUE; i++)
+        {
+            byte[] b = new byte[]{ (byte)i };
+            String s = FBUtilities.bytesToHex(b);
+            byte[] c = FBUtilities.hexToBytes(s);
+            assertArrayEquals(b, c);
+        }
+    }
+    
+    @Test
+    public void testHexToBytesStringConversion()
+    {
+        String[] values = new String[]
+        {
+            "0",
+            "10",
+            "100",
+            "101",
+            "f",
+            "ff"
+        };
+        byte[][] expected = new byte[][]
+        {
+            new byte[] { 0x00 },
+            new byte[] { 0x10 },
+            new byte[] { 0x01, 0x00 },
+            new byte[] { 0x01, 0x01 },
+            new byte[] { 0x0f },
+            new byte[] { (byte)0x000000ff }
+        };
+        
+        for (int i = 0; i < values.length; i++)
+            assert Arrays.equals(FBUtilities.hexToBytes(values[i]), expected[i]);
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/FilterTestHelper.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/FilterTestHelper.java
index e69de29b..34a7ad87 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/FilterTestHelper.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/FilterTestHelper.java
@@ -0,0 +1,82 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.utils;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Set;
+
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.junit.Test;
+
+public class FilterTestHelper
+{
+    // used by filter subclass tests
+
+    static final double MAX_FAILURE_RATE = 0.1;
+    public static final BloomCalculations.BloomSpecification spec = BloomCalculations.computeBloomSpec(15, MAX_FAILURE_RATE);
+    static final int ELEMENTS = 10000;
+
+    static final ResetableIterator<ByteBuffer> intKeys()
+    {
+        return new KeyGenerator.IntGenerator(ELEMENTS);
+    }
+
+    static final ResetableIterator<ByteBuffer> randomKeys()
+    {
+        return new KeyGenerator.RandomStringGenerator(314159, ELEMENTS);
+    }
+
+    static final ResetableIterator<ByteBuffer> randomKeys2()
+    {
+        return new KeyGenerator.RandomStringGenerator(271828, ELEMENTS);
+    }
+
+    public static void testFalsePositives(Filter f, ResetableIterator<ByteBuffer> keys, ResetableIterator<ByteBuffer> otherkeys)
+    {
+        assert keys.size() == otherkeys.size();
+
+        while (keys.hasNext())
+        {
+            f.add(keys.next());
+        }
+
+        int fp = 0;
+        while (otherkeys.hasNext())
+        {
+            if (f.isPresent(otherkeys.next()))
+            {
+                fp++;
+            }
+        }
+
+        double fp_ratio = fp / (keys.size() * BloomCalculations.probs[spec.bucketsPerElement][spec.K]);
+        assert fp_ratio < 1.03 : fp_ratio;
+    }
+
+    public void testTrue()
+    {
+      assert true;
+    }
+
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/KeyGenerator.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/KeyGenerator.java
index 3f24d583..df2504ba 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/KeyGenerator.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/KeyGenerator.java
@@ -1 +1,171 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.utils;
+
+import java.io.BufferedReader;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.nio.ByteBuffer;
+import java.util.Random;
+
+public class KeyGenerator {
+    private static ByteBuffer randomKey(Random r) {
+        byte[] bytes = new byte[48];
+        r.nextBytes(bytes);
+        return ByteBuffer.wrap(bytes);
+    }
+
+    static class RandomStringGenerator implements ResetableIterator<ByteBuffer> {
+        int i, n, seed;
+        Random random;
+
+        RandomStringGenerator(int seed, int n) {
+            i = 0;
+            this.seed = seed;
+            this.n = n;
+            reset();
+        }
+
+        public int size() {
+            return n;
+        }
+
+        public void reset() {
+            random = new Random(seed);
+        }
+
+        public boolean hasNext() {
+            return i < n;
+        }
+
+        public ByteBuffer next() {
+            i++;
+            return randomKey(random);
+        }
+
+        public void remove() {
+            throw new UnsupportedOperationException();
+        }
+    }
+
+    static class IntGenerator implements ResetableIterator<ByteBuffer> {
+        private int i, start, n;
+
+        IntGenerator(int n) {
+            this(0, n);
+        }
+
+        IntGenerator(int start, int n) {
+            this.start = start;
+            this.n = n;
+            reset();
+        }
+
+        public int size() {
+            return n - start;
+        }
+
+        public void reset() {
+            i = start;
+        }
+
+        public boolean hasNext() {
+            return i < n;
+        }
+
+        public ByteBuffer next() {
+            return ByteBufferUtil.bytes(Integer.toString(i++));
+        }
+
+        public void remove() {
+            throw new UnsupportedOperationException();
+        }
+    }
+
+    static class WordGenerator implements ResetableIterator<ByteBuffer> {
+        static int WORDS;
+
+        static {
+            try {
+                BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream("/usr/share/dict/words")));
+                while (br.ready()) {
+                    br.readLine();
+                    WORDS++;
+                }
+            } catch (IOException e) {
+                WORDS = 0;
+            }
+        }
+
+        BufferedReader reader;
+        private int modulo;
+        private int skip;
+        byte[] next;
+
+        WordGenerator(int skip, int modulo) {
+            this.skip = skip;
+            this.modulo = modulo;
+            reset();
+        }
+
+        public int size() {
+            return (1 + WORDS - skip) / modulo;
+        }
+
+        public void reset() {
+            try {
+                reader = new BufferedReader(new InputStreamReader(new FileInputStream("/usr/share/dict/words")));
+            } catch (FileNotFoundException e) {
+                throw new RuntimeException(e);
+            }
+            for (int i = 0; i < skip; i++) {
+                try {
+                    reader.readLine();
+                } catch (IOException e) {
+                    throw new RuntimeException(e);
+                }
+            }
+            next();
+        }
+
+        public boolean hasNext() {
+            return next != null;
+        }
+
+        public ByteBuffer next() {
+            try {
+                byte[] s = next;
+                for (int i = 0; i < modulo; i++) {
+                    String line = reader.readLine();
+                    next = line == null ? null : line.getBytes();
+                }
+                return s == null ? null : ByteBuffer.wrap(s);
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        }
+
+        public void remove() {
+            throw new UnsupportedOperationException();
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/LegacyBloomFilterTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/LegacyBloomFilterTest.java
index 3f24d583..07a5529f 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/LegacyBloomFilterTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/LegacyBloomFilterTest.java
@@ -1 +1,139 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.utils;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Set;
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+
+import org.junit.Before;
+import org.junit.Test;
+
+public class LegacyBloomFilterTest
+{
+    public LegacyBloomFilter bf;
+
+    public LegacyBloomFilterTest()
+    {
+        bf = LegacyBloomFilter.getFilter(FilterTestHelper.ELEMENTS, FilterTestHelper.MAX_FAILURE_RATE);
+    }
+
+    public static Filter testSerialize(LegacyBloomFilter f) throws IOException
+    {
+        f.add(ByteBufferUtil.bytes("a"));
+        DataOutputBuffer out = new DataOutputBuffer();
+        f.serializer().serialize(f, out);
+
+        ByteArrayInputStream in = new ByteArrayInputStream(out.getData(), 0, out.getLength());
+        LegacyBloomFilter f2 = f.serializer().deserialize(new DataInputStream(in));
+
+        assert f2.isPresent(ByteBufferUtil.bytes("a"));
+        assert !f2.isPresent(ByteBufferUtil.bytes("b"));
+        return f2;
+    }
+
+
+    @Before
+    public void clear()
+    {
+        bf.clear();
+    }
+
+    @Test(expected=UnsupportedOperationException.class)
+    public void testBloomLimits1()
+    {
+        int maxBuckets = BloomCalculations.probs.length - 1;
+        int maxK = BloomCalculations.probs[maxBuckets].length - 1;
+
+        // possible
+        BloomCalculations.computeBloomSpec(maxBuckets, BloomCalculations.probs[maxBuckets][maxK]);
+
+        // impossible, throws
+        BloomCalculations.computeBloomSpec(maxBuckets, BloomCalculations.probs[maxBuckets][maxK] / 2);
+    }
+
+    @Test
+    public void testOne()
+    {
+        bf.add(ByteBufferUtil.bytes("a"));
+        assert bf.isPresent(ByteBufferUtil.bytes("a"));
+        assert !bf.isPresent(ByteBufferUtil.bytes("b"));
+    }
+
+    @Test
+    public void testFalsePositivesInt()
+    {
+        FilterTestHelper.testFalsePositives(bf, FilterTestHelper.intKeys(), FilterTestHelper.randomKeys2());
+    }
+
+    @Test
+    public void testFalsePositivesRandom()
+    {
+        FilterTestHelper.testFalsePositives(bf, FilterTestHelper.randomKeys(), FilterTestHelper.randomKeys2());
+    }
+
+    @Test
+    public void testWords()
+    {
+        if (KeyGenerator.WordGenerator.WORDS == 0)
+        {
+            return;
+        }
+        LegacyBloomFilter bf2 = LegacyBloomFilter.getFilter(KeyGenerator.WordGenerator.WORDS / 2, FilterTestHelper.MAX_FAILURE_RATE);
+        int skipEven = KeyGenerator.WordGenerator.WORDS % 2 == 0 ? 0 : 2;
+        FilterTestHelper.testFalsePositives(bf2,
+                                      new KeyGenerator.WordGenerator(skipEven, 2),
+                                      new KeyGenerator.WordGenerator(1, 2));
+    }
+
+    @Test
+    public void testSerialize() throws IOException
+    {
+        LegacyBloomFilterTest.testSerialize(bf);
+    }
+
+    public void testManyHashes(Iterator<ByteBuffer> keys)
+    {
+        int MAX_HASH_COUNT = 128;
+        Set<Integer> hashes = new HashSet<Integer>();
+        int collisions = 0;
+        while (keys.hasNext())
+        {
+            hashes.clear();
+            for (int hashIndex : LegacyBloomFilter.getHashBuckets(keys.next(), MAX_HASH_COUNT, 1024 * 1024))
+            {
+                hashes.add(hashIndex);
+            }
+            collisions += (MAX_HASH_COUNT - hashes.size());
+        }
+        assert collisions <= 100;
+    }
+
+    @Test
+    public void testManyRandom()
+    {
+        testManyHashes(FilterTestHelper.randomKeys());
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/MerkleTreeTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/MerkleTreeTest.java
index 3f24d583..39ec1229 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/MerkleTreeTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/MerkleTreeTest.java
@@ -1 +1,615 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyten ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.utils;
+
+import static org.apache.cassandra.utils.MerkleTree.RECOMMENDED_DEPTH;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.math.BigInteger;
+import java.nio.ByteBuffer;
+import java.util.ArrayDeque;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Random;
+
+import org.apache.cassandra.dht.BigIntegerToken;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.RandomPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.utils.MerkleTree.Hashable;
+import org.apache.cassandra.utils.MerkleTree.RowHash;
+import org.apache.cassandra.utils.MerkleTree.TreeRange;
+import org.apache.cassandra.utils.MerkleTree.TreeRangeIterator;
+import org.junit.Before;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.AbstractIterator;
+
+public class MerkleTreeTest
+{
+    private static final Logger logger = LoggerFactory.getLogger(MerkleTreeTest.class);
+
+    public static byte[] DUMMY = "blah".getBytes();
+
+    /**
+     * If a test assumes that the tree is 8 units wide, then it should set this value
+     * to 8.
+     */
+    public static BigInteger TOKEN_SCALE = new BigInteger("8");
+
+    protected IPartitioner partitioner;
+    protected MerkleTree mt;
+
+    @Before
+    public void clear()
+    {
+        TOKEN_SCALE = new BigInteger("8");
+        partitioner = new RandomPartitioner();
+        mt = new MerkleTree(partitioner, RECOMMENDED_DEPTH, Integer.MAX_VALUE);
+    }
+
+    public static void assertHashEquals(final byte[] left, final byte[] right)
+    {
+        assertHashEquals("", left, right);
+    }
+
+    public static void assertHashEquals(String message, final byte[] left, final byte[] right)
+    {
+        String lstring = left == null ? "null" : FBUtilities.bytesToHex(left);
+        String rstring = right == null ? "null" : FBUtilities.bytesToHex(right);
+        assertEquals(message, lstring, rstring);
+    }
+
+    /**
+     * The value returned by this method is affected by TOKEN_SCALE: setting TOKEN_SCALE
+     * to 8 means that passing -1 through 8 for this method will return values mapped
+     * between -1 and Token.MAX_VALUE.
+     */
+    public static BigIntegerToken tok(int i)
+    {
+        if (i == -1)
+            return new BigIntegerToken(new BigInteger("-1"));
+        BigInteger md5_max = new BigInteger("2").pow(127);
+        BigInteger bint = md5_max.divide(TOKEN_SCALE).multiply(new BigInteger(""+i));
+        return new BigIntegerToken(bint);
+    }
+
+    @Test
+    public void testSplit()
+    {
+        // split the range  (zero, zero] into:
+        //  (zero,four], (four,six], (six,seven] and (seven, zero]
+        mt.split(tok(4));
+        mt.split(tok(6));
+        mt.split(tok(7));
+
+        assertEquals(4, mt.size());
+        assertEquals(new Range(tok(7), tok(-1)), mt.get(tok(-1)));
+        assertEquals(new Range(tok(-1), tok(4)), mt.get(tok(3)));
+        assertEquals(new Range(tok(-1), tok(4)), mt.get(tok(4)));
+        assertEquals(new Range(tok(4), tok(6)), mt.get(tok(6)));
+        assertEquals(new Range(tok(6), tok(7)), mt.get(tok(7)));
+
+        // check depths
+        assertEquals((byte)1, mt.get(tok(4)).depth);
+        assertEquals((byte)2, mt.get(tok(6)).depth);
+        assertEquals((byte)3, mt.get(tok(7)).depth);
+        assertEquals((byte)3, mt.get(tok(-1)).depth);
+
+        try
+        {
+            mt.split(tok(-1));
+            fail("Shouldn't be able to split outside the initial range.");
+        }
+        catch (AssertionError e)
+        {
+            // pass
+        }
+    }
+
+    @Test
+    public void testSplitLimitDepth()
+    {
+        mt = new MerkleTree(partitioner, (byte)2, Integer.MAX_VALUE);
+
+        assertTrue(mt.split(tok(4)));
+        assertTrue(mt.split(tok(2)));
+        assertEquals(3, mt.size());
+        
+        // should fail to split below hashdepth
+        assertFalse(mt.split(tok(1)));
+        assertEquals(3, mt.size());
+        assertEquals(new Range(tok(4), tok(-1)), mt.get(tok(-1)));
+        assertEquals(new Range(tok(-1), tok(2)), mt.get(tok(2)));
+        assertEquals(new Range(tok(2), tok(4)), mt.get(tok(4)));
+    }
+
+    @Test
+    public void testSplitLimitSize()
+    {
+        mt = new MerkleTree(partitioner, RECOMMENDED_DEPTH, 2);
+
+        assertTrue(mt.split(tok(4)));
+        assertEquals(2, mt.size());
+        
+        // should fail to split above maxsize
+        assertFalse(mt.split(tok(2)));
+        assertEquals(2, mt.size());
+        assertEquals(new Range(tok(4), tok(-1)), mt.get(tok(-1)));
+        assertEquals(new Range(tok(-1), tok(4)), mt.get(tok(4)));
+    }
+
+    @Test
+    public void testCompact()
+    {
+        // (zero, one], (one,two], ... (seven, zero]
+        mt.split(tok(4));
+        mt.split(tok(2)); 
+        mt.split(tok(6));
+        mt.split(tok(1));
+        mt.split(tok(3));
+        mt.split(tok(5));
+        mt.split(tok(7));
+
+        // compact (zero,two] and then (four,six]
+        mt.compact(tok(1));
+        mt.compact(tok(5));
+        assertEquals(6, mt.size());
+        assertEquals(new Range(tok(-1), tok(2)), mt.get(tok(2)));
+        assertEquals(new Range(tok(2), tok(3)), mt.get(tok(3)));
+        assertEquals(new Range(tok(3), tok(4)), mt.get(tok(4)));
+        assertEquals(new Range(tok(4), tok(6)), mt.get(tok(5)));
+        assertEquals(new Range(tok(6), tok(7)), mt.get(tok(7)));
+        assertEquals(new Range(tok(7), tok(-1)), mt.get(tok(-1)));
+        // compacted ranges should be at depth 2, and the rest at 3
+        for (int i : new int[]{2,6}){ assertEquals((byte)2, mt.get(tok(i)).depth); }
+        for (int i : new int[]{3,4,7,-1}){ assertEquals((byte)3, mt.get(tok(i)).depth); }
+
+        // compact (two,four] and then (six,zero]
+        mt.compact(tok(3));
+        mt.compact(tok(7));
+        assertEquals(4, mt.size());
+        assertEquals(new Range(tok(-1), tok(2)), mt.get(tok(2)));
+        assertEquals(new Range(tok(2), tok(4)), mt.get(tok(4)));
+        assertEquals(new Range(tok(4), tok(6)), mt.get(tok(5)));
+        assertEquals(new Range(tok(6), tok(-1)), mt.get(tok(-1)));
+        for (int i : new int[]{2,4,5,-1}){ assertEquals((byte)2, mt.get(tok(i)).depth); }
+
+        // compact (zero,four]
+        mt.compact(tok(2));
+        assertEquals(3, mt.size());
+        assertEquals(new Range(tok(-1), tok(4)), mt.get(tok(2)));
+        assertEquals(new Range(tok(4), tok(6)), mt.get(tok(6)));
+        assertEquals(new Range(tok(6), tok(-1)), mt.get(tok(-1)));
+
+        // compact (four, zero]
+        mt.compact(tok(6));
+        assertEquals(2, mt.size());
+        assertEquals(new Range(tok(-1), tok(4)), mt.get(tok(2)));
+        assertEquals(new Range(tok(4), tok(-1)), mt.get(tok(6)));
+        assertEquals((byte)1, mt.get(tok(2)).depth);
+        assertEquals((byte)1, mt.get(tok(6)).depth);
+
+        // compact (zero, zero] (the root)
+        mt.compact(tok(4));
+        assertEquals(1, mt.size());
+        assertEquals(new Range(tok(-1), tok(-1)), mt.get(tok(-1)));
+        assertEquals((byte)0, mt.get(tok(-1)).depth);
+    }
+
+    @Test
+    public void testCompactHash()
+    {
+        byte[] val = DUMMY;
+        byte[] valXval = hashed(val, 1, 1);
+
+        // (zero, four], (four,zero]
+        mt.split(tok(4));
+
+        // validate both ranges
+        mt.get(tok(4)).hash(val);
+        mt.get(tok(-1)).hash(val);
+
+        // compact (zero, eight]
+        mt.compact(tok(4));
+        assertHashEquals(valXval, mt.get(tok(-1)).hash());
+    }
+
+    @Test
+    public void testInvalids()
+    {
+        Iterator<TreeRange> ranges;
+        
+        // (zero, zero]
+        ranges = mt.invalids(new Range(tok(-1), tok(-1)));
+        assertEquals(new Range(tok(-1), tok(-1)), ranges.next());
+        assertFalse(ranges.hasNext());
+
+        // all invalid
+        mt.split(tok(4));
+        mt.split(tok(2));
+        mt.split(tok(6));
+        mt.split(tok(3));
+        mt.split(tok(5));
+        ranges = mt.invalids(new Range(tok(-1), tok(-1)));
+        assertEquals(new Range(tok(-1), tok(2)), ranges.next());
+        assertEquals(new Range(tok(2), tok(3)), ranges.next());
+        assertEquals(new Range(tok(3), tok(4)), ranges.next());
+        assertEquals(new Range(tok(4), tok(5)), ranges.next());
+        assertEquals(new Range(tok(5), tok(6)), ranges.next());
+        assertEquals(new Range(tok(6), tok(-1)), ranges.next());
+        assertFalse(ranges.hasNext());
+        
+        // some invalid
+        mt.get(tok(2)).hash("non-null!".getBytes());
+        mt.get(tok(4)).hash("non-null!".getBytes());
+        mt.get(tok(5)).hash("non-null!".getBytes());
+        mt.get(tok(-1)).hash("non-null!".getBytes());
+        ranges = mt.invalids(new Range(tok(-1), tok(-1)));
+        assertEquals(new Range(tok(2), tok(3)), ranges.next());
+        assertEquals(new Range(tok(5), tok(6)), ranges.next());
+        assertFalse(ranges.hasNext());
+        
+        // some invalid in left subrange
+        ranges = mt.invalids(new Range(tok(-1), tok(6)));
+        assertEquals(new Range(tok(2), tok(3)), ranges.next());
+        assertEquals(new Range(tok(5), tok(6)), ranges.next());
+        assertFalse(ranges.hasNext());
+
+        // some invalid in right subrange
+        ranges = mt.invalids(new Range(tok(2), tok(-1)));
+        assertEquals(new Range(tok(2), tok(3)), ranges.next());
+        assertEquals(new Range(tok(5), tok(6)), ranges.next());
+        assertFalse(ranges.hasNext());
+    }
+
+    @Test
+    public void testHashFull()
+    {
+        byte[] val = DUMMY;
+        Range range = new Range(tok(-1), tok(-1));
+
+        // (zero, zero]
+        assertNull(mt.hash(range));
+        
+        // validate the range
+        mt.get(tok(-1)).hash(val);
+        
+        assertHashEquals(val, mt.hash(range));
+    }
+
+    @Test
+    public void testHashPartial()
+    {
+        byte[] val = DUMMY;
+        byte[] leftval = hashed(val, 1, 1);
+        byte[] partialval = hashed(val, 1);
+        Range left = new Range(tok(-1), tok(4));
+        Range partial = new Range(tok(2), tok(4));
+        Range right = new Range(tok(4), tok(-1));
+        Range linvalid = new Range(tok(1), tok(4));
+        Range rinvalid = new Range(tok(4), tok(6));
+
+        // (zero,two] (two,four] (four, zero]
+        mt.split(tok(4));
+        mt.split(tok(2));
+        assertNull(mt.hash(left));
+        assertNull(mt.hash(partial));
+        assertNull(mt.hash(right));
+        assertNull(mt.hash(linvalid));
+        assertNull(mt.hash(rinvalid));
+        
+        // validate the range
+        mt.get(tok(2)).hash(val);
+        mt.get(tok(4)).hash(val);
+        mt.get(tok(-1)).hash(val);
+        
+        assertHashEquals(leftval, mt.hash(left));
+        assertHashEquals(partialval, mt.hash(partial));
+        assertHashEquals(val, mt.hash(right));
+        assertNull(mt.hash(linvalid));
+        assertNull(mt.hash(rinvalid));
+    }
+
+    @Test
+    public void testHashInner()
+    {
+        byte[] val = DUMMY;
+        byte[] lchildval = hashed(val, 3, 3, 2);
+        byte[] rchildval = hashed(val, 2, 2);
+        byte[] fullval = hashed(val, 3, 3, 2, 2, 2);
+        Range full = new Range(tok(-1), tok(-1));
+        Range lchild = new Range(tok(-1), tok(4));
+        Range rchild = new Range(tok(4), tok(-1));
+        Range invalid = new Range(tok(1), tok(-1));
+
+        // (zero,one] (one, two] (two,four] (four, six] (six, zero]
+        mt.split(tok(4));
+        mt.split(tok(2));
+        mt.split(tok(6));
+        mt.split(tok(1));
+        assertNull(mt.hash(full));
+        assertNull(mt.hash(lchild));
+        assertNull(mt.hash(rchild));
+        assertNull(mt.hash(invalid));
+        
+        // validate the range
+        mt.get(tok(1)).hash(val);
+        mt.get(tok(2)).hash(val);
+        mt.get(tok(4)).hash(val);
+        mt.get(tok(6)).hash(val);
+        mt.get(tok(-1)).hash(val);
+        
+        assertHashEquals(fullval, mt.hash(full));
+        assertHashEquals(lchildval, mt.hash(lchild));
+        assertHashEquals(rchildval, mt.hash(rchild));
+        assertNull(mt.hash(invalid));
+    }
+
+    @Test
+    public void testHashDegenerate()
+    {
+        TOKEN_SCALE = new BigInteger("32");
+
+        byte[] val = DUMMY;
+        byte[] childfullval = hashed(val, 5, 5, 4);
+        byte[] fullval = hashed(val, 5, 5, 4, 3, 2, 1);
+        Range childfull = new Range(tok(-1), tok(4));
+        Range full = new Range(tok(-1), tok(-1));
+        Range invalid = new Range(tok(4), tok(-1));
+
+        mt = new MerkleTree(partitioner, RECOMMENDED_DEPTH, Integer.MAX_VALUE);
+        mt.split(tok(16));
+        mt.split(tok(8));
+        mt.split(tok(4));
+        mt.split(tok(2));
+        mt.split(tok(1));
+        assertNull(mt.hash(full));
+        assertNull(mt.hash(childfull));
+        assertNull(mt.hash(invalid));
+        
+        // validate the range
+        mt.get(tok(1)).hash(val);
+        mt.get(tok(2)).hash(val);
+        mt.get(tok(4)).hash(val);
+        mt.get(tok(8)).hash(val);
+        mt.get(tok(16)).hash(val);
+        mt.get(tok(-1)).hash(val);
+
+        assertHashEquals(fullval, mt.hash(full));
+        assertHashEquals(childfullval, mt.hash(childfull));
+        assertNull(mt.hash(invalid));
+    }
+
+    @Test
+    public void testHashRandom()
+    {
+        int max = 1000000;
+        TOKEN_SCALE = new BigInteger("" + max);
+
+        mt = new MerkleTree(partitioner, RECOMMENDED_DEPTH, 32);
+        Random random = new Random();
+        while (true)
+        {
+            if (!mt.split(tok(random.nextInt(max))))
+                break;
+        }
+
+        // validate the tree
+        TreeRangeIterator ranges = mt.invalids(new Range(tok(-1), tok(-1)));
+        for (TreeRange range : ranges)
+            range.addHash(new RowHash(range.right, new byte[0]));
+
+        assert null != mt.hash(new Range(tok(-1), tok(-1))) :
+            "Could not hash tree " + mt;
+    }
+
+    /**
+     * Generate two trees with different splits, but containing the same keys, and
+     * check that they compare equally.
+     *
+     * The set of keys used in this test is: #{2,4,6,8,12,14,0}
+     */
+    @Test
+    public void testValidateTree()
+    {
+        TOKEN_SCALE = new BigInteger("16"); // this test needs slightly more resolution
+
+        Range full = new Range(tok(-1), tok(-1));
+        Iterator<TreeRange> ranges;
+        MerkleTree mt2 = new MerkleTree(partitioner, RECOMMENDED_DEPTH, Integer.MAX_VALUE);
+
+        mt.split(tok(8));
+        mt.split(tok(4));
+        mt.split(tok(12));
+        mt.split(tok(6));
+        mt.split(tok(10));
+        
+        ranges = mt.invalids(full);
+        ranges.next().addAll(new HIterator(2, 4)); // (-1,4]: depth 2
+        ranges.next().addAll(new HIterator(6)); // (4,6]
+        ranges.next().addAll(new HIterator(8)); // (6,8]
+        ranges.next().addAll(new HIterator(/*empty*/ new int[0])); // (8,10]
+        ranges.next().addAll(new HIterator(12)); // (10,12]
+        ranges.next().addAll(new HIterator(14, -1)); // (12,-1]: depth 2
+
+
+        mt2.split(tok(8));
+        mt2.split(tok(4));
+        mt2.split(tok(12));
+        mt2.split(tok(2));
+        mt2.split(tok(10));
+        mt2.split(tok(9));
+        mt2.split(tok(11));
+
+        ranges = mt2.invalids(full);
+        ranges.next().addAll(new HIterator(2)); // (-1,2]
+        ranges.next().addAll(new HIterator(4)); // (2,4]
+        ranges.next().addAll(new HIterator(6, 8)); // (4,8]: depth 2
+        ranges.next().addAll(new HIterator(/*empty*/ new int[0])); // (8,9]
+        ranges.next().addAll(new HIterator(/*empty*/ new int[0])); // (9,10]
+        ranges.next().addAll(new HIterator(/*empty*/ new int[0])); // (10,11]: depth 4
+        ranges.next().addAll(new HIterator(12)); // (11,12]: depth 4
+        ranges.next().addAll(new HIterator(14, -1)); // (12,-1]: depth 2
+
+        byte[] mthash = mt.hash(full);
+        byte[] mt2hash = mt2.hash(full);
+        assertHashEquals("Tree hashes did not match: " + mt + " && " + mt2, mthash, mt2hash);
+    }
+
+    @Test
+    public void testSerialization() throws Exception
+    {
+        Range full = new Range(tok(-1), tok(-1));
+        ByteArrayOutputStream bout = new ByteArrayOutputStream();
+        ObjectOutputStream oout = new ObjectOutputStream(bout);
+    
+        // populate and validate the tree
+        mt.maxsize(256);
+        mt.init();
+        for (TreeRange range : mt.invalids(full))
+            range.addAll(new HIterator(range.right));
+
+        byte[] initialhash = mt.hash(full);
+        oout.writeObject(mt);
+        oout.close();
+
+        ByteArrayInputStream bin = new ByteArrayInputStream(bout.toByteArray());
+        ObjectInputStream oin = new ObjectInputStream(bin);
+        MerkleTree restored = (MerkleTree)oin.readObject();
+    
+        // restore partitioner after serialization
+        restored.partitioner(partitioner);
+
+        assertHashEquals(initialhash, restored.hash(full));
+    }
+
+    @Test
+    public void testDifference()
+    {
+        Range full = new Range(tok(-1), tok(-1));
+        int maxsize = 16;
+        mt.maxsize(maxsize);
+        MerkleTree mt2 = new MerkleTree(partitioner, RECOMMENDED_DEPTH, maxsize);
+        mt.init();
+        mt2.init();
+
+        TreeRange leftmost = null;
+        TreeRange middle = null;
+        TreeRange rightmost = null;
+
+        // compact the leftmost, and split the rightmost
+        Iterator<TreeRange> ranges = mt.invalids(full);
+        leftmost = ranges.next();
+        rightmost = null;
+        while (ranges.hasNext())
+            rightmost = ranges.next();
+        mt.compact(leftmost.right);
+        leftmost = mt.get(leftmost.right); // leftmost is now a larger range
+        mt.split(rightmost.right);
+        
+        // set the hash for the left neighbor of rightmost
+        middle = mt.get(rightmost.left);
+        middle.hash("arbitrary!".getBytes());
+        byte depth = middle.depth;
+
+        // add dummy hashes to the rest of both trees
+        for (TreeRange range : mt.invalids(full))
+            range.addAll(new HIterator(range.right));
+        for (TreeRange range : mt2.invalids(full))
+            range.addAll(new HIterator(range.right));
+        
+        // trees should disagree for leftmost, (middle.left, rightmost.right]
+        List<TreeRange> diffs = MerkleTree.difference(mt, mt2);
+        assertEquals(diffs + " contains wrong number of differences:", 2, diffs.size());
+        assertTrue(diffs.contains(leftmost));
+        assertTrue(diffs.contains(new Range(middle.left, rightmost.right)));
+    }
+
+    /**
+     * Return the root hash of a binary tree with leaves at the given depths
+     * and with the given hash val in each leaf.
+     */
+    byte[] hashed(byte[] val, Integer... depths)
+    {
+        ArrayDeque<Integer> dstack = new ArrayDeque<Integer>();
+        ArrayDeque<byte[]> hstack = new ArrayDeque<byte[]>();
+        Iterator<Integer> depthiter = Arrays.asList(depths).iterator();
+        if (depthiter.hasNext())
+        {
+            dstack.push(depthiter.next());
+            hstack.push(val);
+        }
+        while (depthiter.hasNext())
+        {
+            Integer depth = depthiter.next();
+            byte[] hash = val;
+            while (dstack.peek() == depth)
+            {
+                // consume the stack
+                hash = Hashable.binaryHash(hstack.pop(), hash);
+                depth = dstack.pop()-1;
+            }
+            dstack.push(depth);
+            hstack.push(hash);
+        }
+        assert hstack.size() == 1;
+        return hstack.pop();
+    }
+
+    static class HIterator extends AbstractIterator<RowHash>
+    {
+        private Iterator<Token> tokens;
+
+        public HIterator(int... tokens)
+        {
+            List<Token> tlist = new LinkedList<Token>();
+            for (int token : tokens)
+                tlist.add(tok(token));
+            this.tokens = tlist.iterator();
+        }
+    
+        public HIterator(Token... tokens)
+        {
+            this.tokens = Arrays.asList(tokens).iterator();
+        }
+    
+        @Override
+        public RowHash computeNext()
+        {
+            if (tokens.hasNext())
+                return new RowHash(tokens.next(), DUMMY);
+            return endOfData();
+        }
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/ResetableIterator.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/ResetableIterator.java
index e69de29b..8d7c9d97 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/ResetableIterator.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/ResetableIterator.java
@@ -0,0 +1,27 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.utils;
+
+import java.util.Iterator;
+
+public interface ResetableIterator<T> extends Iterator<T> {
+    public void reset();
+
+    int size();
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/SerializationsTest.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/SerializationsTest.java
index 3f24d583..a0c6d9cf 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/SerializationsTest.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/SerializationsTest.java
@@ -1 +1,118 @@
   + native
+package org.apache.cassandra.utils;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import org.apache.cassandra.AbstractSerializationsTester;
+import org.apache.cassandra.service.StorageService;
+import org.junit.Test;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+public class SerializationsTest extends AbstractSerializationsTester
+{
+    
+    private void testBloomFilterWrite() throws IOException
+    {
+        BloomFilter bf = BloomFilter.getFilter(1000000, 0.0001);
+        for (int i = 0; i < 100; i++)
+            bf.add(StorageService.getPartitioner().getTokenFactory().toByteArray(StorageService.getPartitioner().getRandomToken()));
+        DataOutputStream out = getOutput("utils.BloomFilter.bin");
+        BloomFilter.serializer().serialize(bf, out);
+        out.close();
+    }
+    
+    @Test
+    public void testBloomFilterRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testBloomFilterWrite();
+        
+        DataInputStream in = getInput("utils.BloomFilter.bin");
+        assert BloomFilter.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private void testLegacyBloomFilterWrite() throws IOException
+    {
+        LegacyBloomFilter a = LegacyBloomFilter.getFilter(1000000, 1000);
+        LegacyBloomFilter b = LegacyBloomFilter.getFilter(1000000, 0.0001);
+        for (int i = 0; i < 100; i++)
+        {
+            ByteBuffer key = StorageService.getPartitioner().getTokenFactory().toByteArray(StorageService.getPartitioner().getRandomToken()); 
+            a.add(key);
+            b.add(key);
+        }
+        DataOutputStream out = getOutput("utils.LegacyBloomFilter.bin");
+        LegacyBloomFilter.serializer().serialize(a, out);
+        LegacyBloomFilter.serializer().serialize(b, out);
+        out.close();
+    }
+    
+    @Test
+    public void testLegacyBloomFilterRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testLegacyBloomFilterWrite();
+        
+        DataInputStream in = getInput("utils.LegacyBloomFilter.bin");
+        assert LegacyBloomFilter.serializer().deserialize(in) != null;
+        in.close();
+    }
+    
+    private void testEstimatedHistogramWrite() throws IOException
+    {
+        EstimatedHistogram hist0 = new EstimatedHistogram();
+        EstimatedHistogram hist1 = new EstimatedHistogram(5000);
+        long[] offsets = new long[1000];
+        long[] data = new long[offsets.length + 1];
+        for (int i = 0; i < offsets.length; i++)
+        {
+            offsets[i] = i;
+            data[i] = 10 * i;
+        }
+        data[offsets.length] = 100000;
+        EstimatedHistogram hist2 = new EstimatedHistogram(offsets, data);
+        
+        DataOutputStream out = getOutput("utils.EstimatedHistogram.bin");
+        EstimatedHistogram.serializer.serialize(hist0, out);
+        EstimatedHistogram.serializer.serialize(hist1, out);
+        EstimatedHistogram.serializer.serialize(hist2, out);
+        out.close();
+    }
+    
+    @Test
+    public void testEstimatedHistogramRead() throws IOException
+    {
+        if (EXECUTE_WRITES)
+            testEstimatedHistogramWrite();
+        
+        DataInputStream in = getInput("utils.EstimatedHistogram.bin");
+        assert EstimatedHistogram.serializer.deserialize(in) != null;
+        assert EstimatedHistogram.serializer.deserialize(in) != null;
+        assert EstimatedHistogram.serializer.deserialize(in) != null;
+        in.close();
+    }
+}
diff --git a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/UUIDTests.java b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/UUIDTests.java
index 3f24d583..34bc58f1 100644
--- a/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/UUIDTests.java
+++ b/cassandra/branches/cassandra-0.7.6-2/test/unit/org/apache/cassandra/utils/UUIDTests.java
@@ -1 +1,79 @@
   + native
+package org.apache.cassandra.utils;
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+
+
+import org.apache.cassandra.db.marshal.TimeUUIDType;
+import org.junit.Test;
+
+import java.math.BigInteger;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.nio.ByteBuffer;
+import java.util.UUID;
+
+
+public class UUIDTests
+{
+    @Test
+    public void verifyType1() throws UnknownHostException
+    {
+        
+        UUID uuid = UUIDGen.makeType1UUIDFromHost(InetAddress.getByName("127.0.0.1"));
+        assert uuid.version() == 1;
+    }
+
+    @Test
+    public void verifyOrdering1() throws UnknownHostException
+    {
+        UUID one = UUIDGen.makeType1UUIDFromHost(InetAddress.getByName("127.0.0.1"));
+        UUID two = UUIDGen.makeType1UUIDFromHost(InetAddress.getByName("127.0.0.2"));
+        assert one.timestamp() < two.timestamp();
+    }
+
+
+    @Test
+    public void testDecomposeAndRaw() throws UnknownHostException
+    {
+        UUID a = UUIDGen.makeType1UUIDFromHost(InetAddress.getByName("127.0.0.1"));
+        byte[] decomposed = UUIDGen.decompose(a);
+        UUID b = UUIDGen.getUUID(ByteBuffer.wrap(decomposed));
+        assert a.equals(b);
+    }
+
+    @Test
+    public void testTimeUUIDType() throws UnknownHostException
+    {
+        TimeUUIDType comp = TimeUUIDType.instance;
+        ByteBuffer first = ByteBuffer.wrap(UUIDGen.decompose(UUIDGen.makeType1UUIDFromHost(InetAddress.getByName("127.0.0.1"))));
+        ByteBuffer second = ByteBuffer.wrap(UUIDGen.decompose(UUIDGen.makeType1UUIDFromHost(InetAddress.getByName("127.0.0.1"))));
+        assert comp.compare(first, second) < 0;
+        assert comp.compare(second, first) > 0;
+        ByteBuffer sameAsFirst = ByteBuffer.wrap(UUIDGen.decompose(UUIDGen.getUUID(first)));
+        assert comp.compare(first, sameAsFirst) == 0;
+    }
+
+    private void assertNonZero(BigInteger i)
+    {
+        assert i.toString(2).indexOf("1") > -1;
+    }
+}
