diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityNode.java b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityNode.java
index 617f2b58..c1cb866b 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityNode.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityNode.java
@@ -36,6 +36,8 @@
   private int column;
   private double sensitivity;
   
+  public EigencutsSensitivityNode() {}
+
   public EigencutsSensitivityNode(int i, int j, double s) {
     row = i;
     column = j;
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java
index 5f30ff99..5218099b 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java
@@ -21,6 +21,7 @@
 import java.util.List;
 import java.util.Set;
 
+import com.google.common.collect.Iterables;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -258,17 +259,16 @@ public void testCanopyReducerManhattan() throws Exception {
 
     List<VectorWritable> points = getPointsWritable();
     reducer.reduce(new Text("centroid"), points, context);
-    Set<Text> keys = writer.getKeys();
-    assertEquals("Number of centroids", 3, keys.size());
+    Iterable<Text> keys = writer.getKeysInInsertionOrder();
+    assertEquals("Number of centroids", 3, Iterables.size(keys));
     int i = 0;
     for (Text key : keys) {
       List<ClusterWritable> data = writer.getValue(key);
       ClusterWritable clusterWritable = data.get(0);
-	  Canopy canopy = (Canopy)clusterWritable.getValue();
-	  assertEquals(manhattanCentroids.get(i).asFormatString()
-          + " is not equal to "
-          + canopy.computeCentroid().asFormatString(), manhattanCentroids
-          .get(i), canopy.computeCentroid());
+      Canopy canopy = (Canopy) clusterWritable.getValue();
+      assertEquals(manhattanCentroids.get(i).asFormatString() + " is not equal to "
+          + canopy.computeCentroid().asFormatString(),
+          manhattanCentroids.get(i), canopy.computeCentroid());
       i++;
     }
   }
@@ -282,29 +282,27 @@ public void testCanopyReducerManhattan() throws Exception {
   public void testCanopyReducerEuclidean() throws Exception {
     CanopyReducer reducer = new CanopyReducer();
     Configuration conf = getConfiguration();
-    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY,
-        "org.apache.mahout.common.distance.EuclideanDistanceMeasure");
+    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, "org.apache.mahout.common.distance.EuclideanDistanceMeasure");
     conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));
     conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(2.1));
     conf.set(CanopyConfigKeys.CF_KEY, "0");
     DummyRecordWriter<Text, ClusterWritable> writer = new DummyRecordWriter<Text, ClusterWritable>();
-    Reducer<Text, VectorWritable, Text, ClusterWritable>.Context context = DummyRecordWriter
-        .build(reducer, conf, writer, Text.class, VectorWritable.class);
+    Reducer<Text, VectorWritable, Text, ClusterWritable>.Context context =
+        DummyRecordWriter.build(reducer, conf, writer, Text.class, VectorWritable.class);
     reducer.setup(context);
 
     List<VectorWritable> points = getPointsWritable();
     reducer.reduce(new Text("centroid"), points, context);
-    Set<Text> keys = writer.getKeys();
-    assertEquals("Number of centroids", 3, keys.size());
+    Iterable<Text> keys = writer.getKeysInInsertionOrder();
+    assertEquals("Number of centroids", 3, Iterables.size(keys));
     int i = 0;
     for (Text key : keys) {
       List<ClusterWritable> data = writer.getValue(key);
       ClusterWritable clusterWritable = data.get(0);
-      Canopy canopy = (Canopy)clusterWritable.getValue();
-      assertEquals(euclideanCentroids.get(i).asFormatString()
-          + " is not equal to "
-          + canopy.computeCentroid().asFormatString(), euclideanCentroids
-          .get(i), canopy.computeCentroid());
+      Canopy canopy = (Canopy) clusterWritable.getValue();
+      assertEquals(euclideanCentroids.get(i).asFormatString() + " is not equal to "
+          + canopy.computeCentroid().asFormatString(),
+          euclideanCentroids.get(i), canopy.computeCentroid());
       i++;
     }
   }
@@ -346,16 +344,14 @@ public void testCanopyGenManhattanMR() throws Exception {
       assertEquals("2nd key", "C-1", key.toString());
       c = new Pair<Double,Double>(clusterWritable.getValue().getCenter().get(0),
     		  clusterWritable.getValue().getCenter().get(1));
-      assertTrue("center "+c+" not found", findAndRemove(c, refCenters, EPSILON));
+      assertTrue("center " + c + " not found", findAndRemove(c, refCenters, EPSILON));
       assertFalse("more to come", reader.next(key, clusterWritable));
     } finally {
       Closeables.close(reader, true);
     }
   }
 
-  static boolean findAndRemove(Pair<Double, Double> target,
-                               Collection<Pair<Double, Double>> list,
-                               double epsilon) {
+  static boolean findAndRemove(Pair<Double, Double> target, Collection<Pair<Double, Double>> list, double epsilon) {
     for (Pair<Double,Double> curr : list) {
       if ( (Math.abs(target.getFirst() - curr.getFirst()) < epsilon) 
            && (Math.abs(target.getSecond() - curr.getSecond()) < epsilon) ) {
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java b/mahout/trunk/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java
index 324dbc27..e203eeb4 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java
@@ -29,8 +29,6 @@
 import org.apache.hadoop.mapreduce.Reducer;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
@@ -45,47 +43,47 @@
 
 public final class DummyRecordWriter<K extends Writable, V extends Writable> extends RecordWriter<K, V> {
 
-  private static final Logger log = LoggerFactory.getLogger(DummyRecordWriter.class);
-
+  private final List<K> keysInInsertionOrder = Lists.newArrayList();
   private final Map<K, List<V>> data = Maps.newHashMap();
 
   @Override
   public void write(K key, V value) {
+
     // if the user reuses the same writable class, we need to create a new one
     // otherwise the Map content will be modified after the insert
     try {
-      if (!(key instanceof NullWritable)) {
-        K newKey = (K) key.getClass().newInstance();
-        cloneWritable(key, newKey);
-        key = newKey;
-      }
-      V newValue = (V) value.getClass().newInstance();
-      cloneWritable(value, newValue);
-      value = newValue;
-    } catch (InstantiationException e) {
-      log.error(e.getMessage());
-    } catch (IllegalAccessException e) {
-      log.error(e.getMessage());
-    } catch (IOException e) {
-      log.error(e.getMessage());
-    }
+
+      K keyToUse = key instanceof NullWritable ? key : (K) cloneWritable(key);
+      V valueToUse = (V) cloneWritable(value);
+
+      keysInInsertionOrder.add(keyToUse);
 
     List<V> points = data.get(key);
     if (points == null) {
       points = Lists.newArrayList();
-      data.put(key, points);
+        data.put(keyToUse, points);
+      }
+      points.add(valueToUse);
+
+    } catch (IOException e) {
+      throw new RuntimeException(e.getMessage(), e);
     }
-    points.add(value);
   }
 
-  private void cloneWritable(Writable from, Writable to) throws IOException {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    DataOutputStream dos = new DataOutputStream(baos);
-    from.write(dos);
-    dos.close();
-    ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());
-    DataInputStream dis = new DataInputStream(bais);
-    to.readFields(dis);
+  private Writable cloneWritable(Writable original) throws IOException {
+
+    Writable clone;
+    try {
+      clone = original.getClass().asSubclass(Writable.class).newInstance();
+    } catch (Exception e) {
+      throw new RuntimeException("Unable to instantiate writable!", e);
+    }
+    ByteArrayOutputStream bytes = new ByteArrayOutputStream();
+
+    original.write(new DataOutputStream(bytes));
+    clone.readFields(new DataInputStream(new ByteArrayInputStream(bytes.toByteArray())));
+
+    return clone;
   }
 
   @Override
@@ -104,6 +102,10 @@ public void close(TaskAttemptContext context) {
     return data.keySet();
   }
 
+  public Iterable<K> getKeysInInsertionOrder() {
+    return keysInInsertionOrder;
+  }
+
   public static <K1, V1, K2, V2> Mapper<K1, V1, K2, V2>.Context build(Mapper<K1, V1, K2, V2> mapper,
                                                                       Configuration configuration,
                                                                       RecordWriter<K2, V2> output) {
