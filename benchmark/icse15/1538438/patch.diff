diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJobTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJobTest.java
index b9082089..9d37da2d 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJobTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJobTest.java
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils;
 import org.apache.mahout.cf.taste.impl.TasteTestCase;
 import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
@@ -339,7 +340,10 @@ public void recommenderJobWithIDMapping() throws Exception {
     int numIterations = 5;
     double lambda = 0.065;
 
-    int success = alsFactorization.run(new String[] {
+    Configuration conf = getConfiguration();
+
+    int success = ToolRunner.run(alsFactorization, new String[] {
+        "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"),
         "--input", inputFile.getAbsolutePath(),
         "--output", intermediateDir.getAbsolutePath(),
         "--tempDir", tmpDir.getAbsolutePath(),
@@ -356,7 +360,8 @@ public void recommenderJobWithIDMapping() throws Exception {
 
     RecommenderJob recommender = new RecommenderJob();
 
-    success = recommender.run(new String[] {
+    success = ToolRunner.run(recommender, new String[] {
+        "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"),
         "--input", intermediateDir.getAbsolutePath() + "/userRatings/",
         "--userFeatures", intermediateDir.getAbsolutePath() + "/U/",
         "--itemFeatures", intermediateDir.getAbsolutePath() + "/M/",
diff --git a/mahout/trunk/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromMailArchivesTest.java b/mahout/trunk/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromMailArchivesTest.java
index b281d35e..46bbcf9e 100644
--- a/mahout/trunk/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromMailArchivesTest.java
+++ b/mahout/trunk/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromMailArchivesTest.java
@@ -129,7 +129,7 @@ public void testSequential() throws Exception {
   @Test
   public void testMapReduce() throws Exception {
 
-    Path tmpDir = this.getTestTempDirPath();
+    Path tmpDir = getTestTempDirPath();
     Path mrOutputDir = new Path(tmpDir, "mail-archives-out-mr");
     Configuration configuration = getConfiguration();
     FileSystem fs = FileSystem.get(configuration);
@@ -137,6 +137,7 @@ public void testMapReduce() throws Exception {
     File expectedInputFile = new File(inputDir.toString());
 
     String[] args = {
+      "-Dhadoop.tmp.dir=" + configuration.get("hadoop.tmp.dir"),
       "--input", expectedInputFile.getAbsolutePath(),
       "--output", mrOutputDir.toString(),
       "--charset", "UTF-8",
diff --git a/mahout/trunk/integration/src/test/java/org/apache/mahout/text/TestSequenceFilesFromDirectory.java b/mahout/trunk/integration/src/test/java/org/apache/mahout/text/TestSequenceFilesFromDirectory.java
index 148ac129..f2cc40ba 100644
--- a/mahout/trunk/integration/src/test/java/org/apache/mahout/text/TestSequenceFilesFromDirectory.java
+++ b/mahout/trunk/integration/src/test/java/org/apache/mahout/text/TestSequenceFilesFromDirectory.java
@@ -126,6 +126,7 @@ public void testSequenceFileFromDirectoryMapReduce() throws Exception {
     createFilesFromArrays(conf, inputDir, DATA1);
 
     SequenceFilesFromDirectory.main(new String[]{
+      "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"),
       "--input", inputDir.toString(),
       "--output", mrOutputDir.toString(),
       "--chunkSize", "64",
@@ -143,6 +144,7 @@ public void testSequenceFileFromDirectoryMapReduce() throws Exception {
     logger.info("\n\n ---- recursive dirs: {}", dirs);
 
     SequenceFilesFromDirectory.main(new String[]{
+      "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"),
       "--input", inputDirRecur.toString(),
       "--output", mrOutputDirRecur.toString(),
       "--chunkSize", "64",
