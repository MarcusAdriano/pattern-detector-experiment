diff --git a/incubator/cassandra/branches/cassandra-0.6/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java b/incubator/cassandra/branches/cassandra-0.6/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
index edb05d43..a360f481 100644
--- a/incubator/cassandra/branches/cassandra-0.6/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
+++ b/incubator/cassandra/branches/cassandra-0.6/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
@@ -37,6 +37,24 @@
 import org.apache.thrift.transport.TSocket;
 import org.apache.thrift.transport.TTransportException;
 
+/**
+ * Hadoop InputFormat allowing map/reduce against Cassandra rows within one ColumnFamily.
+ *
+ * At minimum, you need to set the CF and predicate (description of columns to extract from each row)
+ * in your Hadoop job Configuration.  The ConfigHelper class is provided to make this
+ * simple:
+ *   ConfigHelper.setColumnFamily
+ *   ConfigHelper.setSlicePredicate
+ *
+ * You can also configure the number of rows per InputSplit with
+ *   ConfigHelper.setInputSplitSize
+ * This should be "as big as possible, but no bigger."  Each InputSplit is read from Cassandra
+ * with a single get_slice_range query, and the per-call overhead of get_slice_range is high,
+ * so larger split sizes are better -- but if it is too large, you will run out of memory,
+ * since no paging is done (yet).
+ *
+ * The default split size is 4096 rows.
+ */
 public class ColumnFamilyInputFormat extends InputFormat<String, SortedMap<byte[], IColumn>>
 {
 
diff --git a/incubator/cassandra/branches/cassandra-0.6/src/java/org/apache/cassandra/hadoop/ConfigHelper.java b/incubator/cassandra/branches/cassandra-0.6/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
index b64212bc..2afe8aeb 100644
--- a/incubator/cassandra/branches/cassandra-0.6/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
+++ b/incubator/cassandra/branches/cassandra-0.6/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
@@ -16,7 +16,15 @@
     private static final String COLUMNFAMILY_CONFIG = "cassandra.input.columnfamily";
     private static final String PREDICATE_CONFIG = "cassandra.input.predicate";
     private static final String INPUT_SPLIT_SIZE_CONFIG = "cassandra.input.split.size";
+    private static final int DEFAULT_SPLIT_SIZE = 4096;
 
+    /**
+     * Set the keyspace and column family for this job.
+     *
+     * @param conf Job configuration you are about to run
+     * @param keyspace
+     * @param columnFamily
+     */
     public static void setColumnFamily(Configuration conf, String keyspace, String columnFamily)
     {
         if (keyspace == null)
@@ -44,7 +52,7 @@ public static void setColumnFamily(Configuration conf, String keyspace, String c
      * This affects the number of maps created, if the number is too small
      * the overhead of each map will take up the bulk of the job time.
      *
-     * @param conf Job configuration you are about to run.
+     * @param conf Job configuration you are about to run
      * @param splitsize Size of the input split
      */
     public static void setInputSplitSize(Configuration conf, int splitsize)
@@ -54,9 +62,15 @@ public static void setInputSplitSize(Configuration conf, int splitsize)
 
     public static int getInputSplitSize(Configuration conf)
     {
-        return conf.getInt(INPUT_SPLIT_SIZE_CONFIG, 4096);
+        return conf.getInt(INPUT_SPLIT_SIZE_CONFIG, DEFAULT_SPLIT_SIZE);
     }
 
+    /**
+     * Set the predicate that determines what columns will be selected from each row.
+     *
+     * @param conf Job configuration you are about to run
+     * @param predicate
+     */
     public static void setSlicePredicate(Configuration conf, SlicePredicate predicate)
     {
         conf.set(PREDICATE_CONFIG, predicateToString(predicate));
