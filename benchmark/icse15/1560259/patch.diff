diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorMapper.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorMapper.java
index e69de29b..ce58df31 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorMapper.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorMapper.java
@@ -0,0 +1,50 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.List;
+
+import com.google.common.collect.Lists;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+
+/**
+ * 
+ *  outputs the pattern for each item in the pattern, so that reducer can group them
+ * and select the top K frequent patterns
+ * 
+ */
+public class AggregatorMapper extends Mapper<Text,TopKStringPatterns,Text,TopKStringPatterns> {
+  
+  @Override
+  protected void map(Text key, TopKStringPatterns values, Context context) throws IOException,
+                                                                          InterruptedException {
+    for (Pair<List<String>,Long> pattern : values.getPatterns()) {
+      for (String item : pattern.getFirst()) {
+        List<Pair<List<String>,Long>> patternSingularList = Lists.newArrayList();
+        patternSingularList.add(pattern);
+        context.setStatus("Aggregator Mapper:Grouping Patterns for " + item);
+        context.write(new Text(item), new TopKStringPatterns(patternSingularList));
+      }
+    }
+    
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorReducer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorReducer.java
index e69de29b..55c021de 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorReducer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorReducer.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+
+/**
+ * 
+ *  groups all Frequent Patterns containing an item and outputs the top K patterns
+ * containing that particular item
+ * 
+ */
+public class AggregatorReducer extends Reducer<Text,TopKStringPatterns,Text,TopKStringPatterns> {
+  
+  private int maxHeapSize = 50;
+  
+  @Override
+  protected void reduce(Text key, Iterable<TopKStringPatterns> values, Context context) throws IOException,
+                                                                                       InterruptedException {
+    TopKStringPatterns patterns = new TopKStringPatterns();
+    for (TopKStringPatterns value : values) {
+      context.setStatus("Aggregator Reducer: Selecting TopK patterns for: " + key);
+      patterns = patterns.merge(value, maxHeapSize);
+    }
+    context.write(key, patterns);
+    
+  }
+  
+  @Override
+  protected void setup(Context context) throws IOException, InterruptedException {
+    super.setup(context);
+    Parameters params = new Parameters(context.getConfiguration().get("pfp.parameters", ""));
+    maxHeapSize = Integer.valueOf(params.get("maxHeapSize", "50"));
+    
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/CountDescendingPairComparator.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/CountDescendingPairComparator.java
index e69de29b..ee093ce7 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/CountDescendingPairComparator.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/CountDescendingPairComparator.java
@@ -0,0 +1,41 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.Serializable;
+import java.util.Comparator;
+
+import org.apache.mahout.common.Pair;
+
+/**
+ * Defines an ordering on {@link Pair}s whose second element is a count. The ordering places those with
+ * high count first (that is, descending), and for those of equal count, orders by the first element in the
+ * pair, ascending. It is used in several places in the FPM code.
+ */
+public final class CountDescendingPairComparator<A extends Comparable<? super A>,B extends Comparable<? super B>>
+  implements Comparator<Pair<A,B>>, Serializable {
+
+  @Override
+  public int compare(Pair<A,B> a, Pair<A,B> b) {
+    int ret = b.getSecond().compareTo(a.getSecond());
+    if (ret != 0) {
+      return ret;
+    }
+    return a.getFirst().compareTo(b.getFirst());
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java
index e69de29b..c774524f 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java
@@ -0,0 +1,215 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.nio.charset.Charset;
+import java.util.Collection;
+import java.util.List;
+
+import com.google.common.collect.Sets;
+import com.google.common.io.Closeables;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.mahout.common.AbstractJob;
+import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.ContextStatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.convertors.SequenceFileOutputCollector;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.StringOutputConverter;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public final class FPGrowthDriver extends AbstractJob {
+
+  private static final Logger log = LoggerFactory.getLogger(FPGrowthDriver.class);
+
+  private FPGrowthDriver() {
+  }
+
+  public static void main(String[] args) throws Exception {
+    ToolRunner.run(new Configuration(), new FPGrowthDriver(), args);
+  }
+
+  /**
+   * Run TopK FPGrowth given the input file,
+   */
+  @Override
+  public int run(String[] args) throws Exception {
+    addInputOption();
+    addOutputOption();
+
+    addOption("minSupport", "s", "(Optional) The minimum number of times a co-occurrence must be present."
+              + " Default Value: 3", "3");
+    addOption("maxHeapSize", "k", "(Optional) Maximum Heap Size k, to denote the requirement to mine top K items."
+              + " Default value: 50", "50");
+    addOption("numGroups", "g", "(Optional) Number of groups the features should be divided in the map-reduce version."
+              + " Doesn't work in sequential version Default Value:" + PFPGrowth.NUM_GROUPS_DEFAULT,
+              Integer.toString(PFPGrowth.NUM_GROUPS_DEFAULT));
+    addOption("splitterPattern", "regex", "Regular Expression pattern used to split given string transaction into"
+            + " itemsets. Default value splits comma separated itemsets.  Default Value:"
+            + " \"[ ,\\t]*[,|\\t][ ,\\t]*\" ", "[ ,\t]*[,|\t][ ,\t]*");
+    addOption("numTreeCacheEntries", "tc", "(Optional) Number of entries in the tree cache to prevent duplicate"
+            + " tree building. (Warning) a first level conditional FP-Tree might consume a lot of memory, "
+            + "so keep this value small, but big enough to prevent duplicate tree building. "
+            + "Default Value:5 Recommended Values: [5-10]", "5");
+    addOption("method", "method", "Method of processing: sequential|mapreduce", "sequential");
+    addOption("encoding", "e", "(Optional) The file encoding.  Default value: UTF-8", "UTF-8");
+    addFlag("useFPG2", "2", "Use an alternate FPG implementation");
+
+    if (parseArguments(args) == null) {
+      return -1;
+    }
+
+    Parameters params = new Parameters();
+
+    if (hasOption("minSupport")) {
+      String minSupportString = getOption("minSupport");
+      params.set("minSupport", minSupportString);
+    }
+    if (hasOption("maxHeapSize")) {
+      String maxHeapSizeString = getOption("maxHeapSize");
+      params.set("maxHeapSize", maxHeapSizeString);
+    }
+    if (hasOption("numGroups")) {
+      String numGroupsString = getOption("numGroups");
+      params.set("numGroups", numGroupsString);
+    }
+
+    if (hasOption("numTreeCacheEntries")) {
+      String numTreeCacheString = getOption("numTreeCacheEntries");
+      params.set("treeCacheSize", numTreeCacheString);
+    }
+
+    if (hasOption("splitterPattern")) {
+      String patternString = getOption("splitterPattern");
+      params.set("splitPattern", patternString);
+    }
+
+    String encoding = "UTF-8";
+    if (hasOption("encoding")) {
+      encoding = getOption("encoding");
+    }
+    params.set("encoding", encoding);
+
+    if (hasOption("useFPG2")) {
+      params.set(PFPGrowth.USE_FPG2, "true");
+    }
+
+    Path inputDir = getInputPath();
+    Path outputDir = getOutputPath();
+
+    params.set("input", inputDir.toString());
+    params.set("output", outputDir.toString());
+
+    String classificationMethod = getOption("method");
+    if ("sequential".equalsIgnoreCase(classificationMethod)) {
+      runFPGrowth(params);
+    } else if ("mapreduce".equalsIgnoreCase(classificationMethod)) {
+      Configuration conf = new Configuration();
+      HadoopUtil.delete(conf, outputDir);
+      PFPGrowth.runPFPGrowth(params);
+    }
+
+    return 0;
+  }
+
+  private static void runFPGrowth(Parameters params) throws IOException {
+    log.info("Starting Sequential FPGrowth");
+    int maxHeapSize = Integer.valueOf(params.get("maxHeapSize", "50"));
+    int minSupport = Integer.valueOf(params.get("minSupport", "3"));
+
+    Path output = new Path(params.get("output", "output.txt"));
+    Path input = new Path(params.get("input"));
+
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(output.toUri(), conf);
+
+    Charset encoding = Charset.forName(params.get("encoding"));
+
+    String pattern = params.get("splitPattern", PFPGrowth.SPLITTER.toString());
+
+    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output, Text.class, TopKStringPatterns.class);
+
+    FSDataInputStream inputStream = null;
+    FSDataInputStream inputStreamAgain = null;
+
+    Collection<String> features = Sets.newHashSet();
+
+    if ("true".equals(params.get(PFPGrowth.USE_FPG2))) {
+      org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj<String> fp 
+        = new org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj<String>();
+
+      try {
+        inputStream = fs.open(input);
+        inputStreamAgain = fs.open(input);
+        fp.generateTopKFrequentPatterns(
+                new StringRecordIterator(new FileLineIterable(inputStream, encoding, false), pattern),
+                fp.generateFList(
+                        new StringRecordIterator(new FileLineIterable(inputStreamAgain, encoding, false), pattern),
+                        minSupport),
+                minSupport,
+                maxHeapSize,
+                features,
+                new StringOutputConverter(new SequenceFileOutputCollector<Text, TopKStringPatterns>(writer))
+        );
+      } finally {
+        Closeables.close(writer, false);
+        Closeables.close(inputStream, true);
+        Closeables.close(inputStreamAgain, true);
+      }
+    } else {
+      FPGrowth<String> fp = new FPGrowth<String>();
+
+
+      inputStream = fs.open(input);
+      inputStreamAgain = fs.open(input);
+      try {
+        fp.generateTopKFrequentPatterns(
+                new StringRecordIterator(new FileLineIterable(inputStream, encoding, false), pattern),
+                fp.generateFList(
+                        new StringRecordIterator(new FileLineIterable(inputStreamAgain, encoding, false), pattern),
+                        minSupport),
+                minSupport,
+                maxHeapSize,
+                features,
+                new StringOutputConverter(new SequenceFileOutputCollector<Text, TopKStringPatterns>(writer)),
+                new ContextStatusUpdater(null));
+      } finally {
+        Closeables.close(writer, false);
+        Closeables.close(inputStream, true);
+        Closeables.close(inputStreamAgain, true);
+      }
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowth.readFrequentPattern(conf, output);
+    for (Pair<String, TopKStringPatterns> entry : frequentPatterns) {
+      log.info("Dumping Patterns for Feature: {} \n{}", entry.getFirst(), entry.getSecond());
+    }
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java
index e69de29b..834a8756 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.util.Iterator;
+
+import com.google.common.collect.AbstractIterator;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.math.list.IntArrayList;
+
+/**
+ * Iterates over multiple transaction trees to produce a single iterator of transactions
+ */
+public final class MultiTransactionTreeIterator extends AbstractIterator<IntArrayList> {
+  
+  private final Iterator<Pair<IntArrayList,Long>> pIterator;
+  private IntArrayList current;
+  private long currentMaxCount;
+  private long currentCount;
+  
+  public MultiTransactionTreeIterator(Iterator<Pair<IntArrayList,Long>> iterator) {
+    this.pIterator = iterator;
+  }
+
+  @Override
+  protected IntArrayList computeNext() {
+    if (currentCount >= currentMaxCount) {
+      if (pIterator.hasNext()) {
+        Pair<IntArrayList,Long> nextValue = pIterator.next();
+        current = nextValue.getFirst();
+        currentMaxCount = nextValue.getSecond();
+        currentCount = 0;
+      } else {
+        return endOfData();
+      }
+    }
+    currentCount++;
+    return current;
+  }
+  
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
index e69de29b..fe4bde00 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
@@ -0,0 +1,346 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.List;
+import java.util.PriorityQueue;
+import java.util.regex.Pattern;
+
+import com.google.common.collect.Lists;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
+import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.iterator.sequencefile.PathType;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
+import org.apache.mahout.math.list.IntArrayList;
+
+/**
+ * 
+ * Parallel FP Growth Driver Class. Runs each stage of PFPGrowth as described in the paper
+ * http://infolab.stanford.edu/~echang/recsys08-69.pdf
+ * 
+ */
+public final class PFPGrowth {
+  
+  public static final String ENCODING = "encoding";
+  public static final String F_LIST = "fList";
+  public static final String NUM_GROUPS = "numGroups";
+  public static final int NUM_GROUPS_DEFAULT = 1000;
+  public static final String MAX_PER_GROUP = "maxPerGroup";
+  public static final String OUTPUT = "output";
+  public static final String MIN_SUPPORT = "minSupport";
+  public static final String MAX_HEAP_SIZE = "maxHeapSize";
+  public static final String INPUT = "input";
+  public static final String PFP_PARAMETERS = "pfp.parameters";
+  public static final String FILE_PATTERN = "part-*";
+  public static final String FP_GROWTH = "fpgrowth";
+  public static final String FREQUENT_PATTERNS = "frequentpatterns";
+  public static final String PARALLEL_COUNTING = "parallelcounting";
+  public static final String SPLIT_PATTERN = "splitPattern";
+  public static final String USE_FPG2 = "use_fpg2";
+
+  public static final Pattern SPLITTER = Pattern.compile("[ ,\t]*[,|\t][ ,\t]*");
+  
+  private PFPGrowth() { }
+  
+  /**
+   * Generates the fList from the serialized string representation
+   * 
+   * @return Deserialized Feature Frequency List
+   */
+  public static List<Pair<String,Long>> readFList(Configuration conf) throws IOException {
+    List<Pair<String,Long>> list = Lists.newArrayList();
+
+    Path[] files = HadoopUtil.getCachedFiles(conf);
+    if (files.length != 1) {
+      throw new IOException("Cannot read Frequency list from Distributed Cache (" + files.length + ')');
+    }
+
+    for (Pair<Text,LongWritable> record
+         : new SequenceFileIterable<Text,LongWritable>(files[0], true, conf)) {
+      list.add(new Pair<String,Long>(record.getFirst().toString(), record.getSecond().get()));
+    }
+    return list;
+  }
+  
+  /**
+   * Serializes the fList and returns the string representation of the List
+   */
+  public static void saveFList(Iterable<Pair<String,Long>> flist, Parameters params, Configuration conf)
+    throws IOException {
+    Path flistPath = new Path(params.get(OUTPUT), F_LIST);
+    FileSystem fs = FileSystem.get(flistPath.toUri(), conf);
+    flistPath = fs.makeQualified(flistPath);
+    HadoopUtil.delete(conf, flistPath);
+    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, flistPath, Text.class, LongWritable.class);
+    try {
+      for (Pair<String,Long> pair : flist) {
+        writer.append(new Text(pair.getFirst()), new LongWritable(pair.getSecond()));
+      }
+    } finally {
+      writer.close();
+    }
+    DistributedCache.addCacheFile(flistPath.toUri(), conf);
+  }
+  
+  /**
+   * read the feature frequency List which is built at the end of the Parallel counting job
+   * 
+   * @return Feature Frequency List
+   */
+  public static List<Pair<String,Long>> readFList(Parameters params) {
+    int minSupport = Integer.valueOf(params.get(MIN_SUPPORT, "3"));
+    Configuration conf = new Configuration();
+    
+    Path parallelCountingPath = new Path(params.get(OUTPUT), PARALLEL_COUNTING);
+
+    PriorityQueue<Pair<String,Long>> queue = new PriorityQueue<Pair<String,Long>>(11,
+        new Comparator<Pair<String,Long>>() {
+          @Override
+          public int compare(Pair<String,Long> o1, Pair<String,Long> o2) {
+            int ret = o2.getSecond().compareTo(o1.getSecond());
+            if (ret != 0) {
+              return ret;
+            }
+            return o1.getFirst().compareTo(o2.getFirst());
+          }
+        });
+
+    for (Pair<Text,LongWritable> record
+         : new SequenceFileDirIterable<Text,LongWritable>(new Path(parallelCountingPath, FILE_PATTERN),
+                                                        PathType.GLOB, null, null, true, conf)) {
+      long value = record.getSecond().get();
+      if (value >= minSupport) {
+        queue.add(new Pair<String,Long>(record.getFirst().toString(), value));
+      }
+    }
+    List<Pair<String,Long>> fList = Lists.newArrayList();
+    while (!queue.isEmpty()) {
+      fList.add(queue.poll());
+    }
+    return fList;
+  }
+
+  public static int getGroup(int itemId, int maxPerGroup) {
+    return itemId / maxPerGroup;
+  }
+
+  public static IntArrayList getGroupMembers(int groupId, int maxPerGroup, int numFeatures) {
+    int start = groupId * maxPerGroup;
+    int end = start + maxPerGroup;
+    if (end > numFeatures) {
+      end = numFeatures;
+    }
+    IntArrayList ret = new IntArrayList();
+    for (int i = start; i < end; i++) {
+      ret.add(i);
+    }
+    return ret;
+  }
+  
+  /**
+   * Read the Frequent Patterns generated from Text
+   * 
+   * @return List of TopK patterns for each string frequent feature
+   */
+  public static List<Pair<String,TopKStringPatterns>> readFrequentPattern(Parameters params) throws IOException {
+    
+    Configuration conf = new Configuration();
+    
+    Path frequentPatternsPath = new Path(params.get(OUTPUT), FREQUENT_PATTERNS);
+    FileSystem fs = FileSystem.get(frequentPatternsPath.toUri(), conf);
+    FileStatus[] outputFiles = fs.globStatus(new Path(frequentPatternsPath, FILE_PATTERN));
+    
+    List<Pair<String,TopKStringPatterns>> ret = Lists.newArrayList();
+    for (FileStatus fileStatus : outputFiles) {
+      ret.addAll(FPGrowth.readFrequentPattern(conf, fileStatus.getPath()));
+    }
+    return ret;
+  }
+  
+  /**
+   * @param params params
+   * @param conf Configuration
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   * @throws IOException
+   *
+   * */
+  public static void runPFPGrowth(Parameters params, Configuration conf) throws IOException,
+                                                                        InterruptedException,
+                                                                        ClassNotFoundException {
+    conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,"
+                + "org.apache.hadoop.io.serializer.WritableSerialization");
+    startParallelCounting(params, conf);
+
+    // save feature list to dcache
+    List<Pair<String,Long>> fList = readFList(params);
+    saveFList(fList, params, conf);
+
+    // set param to control group size in MR jobs
+    int numGroups = params.getInt(NUM_GROUPS, NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) {
+      maxPerGroup++;
+    }
+    params.set(MAX_PER_GROUP, Integer.toString(maxPerGroup));
+
+    startParallelFPGrowth(params, conf);
+    startAggregating(params, conf);
+  }
+  
+  /**
+   * 
+   * @param params
+   *          params should contain input and output locations as a string value, the additional parameters
+   *          include minSupport(3), maxHeapSize(50), numGroups(1000)
+   */
+  public static void runPFPGrowth(Parameters params) throws IOException,
+                                                    InterruptedException,
+                                                    ClassNotFoundException {
+    Configuration conf = new Configuration();
+    runPFPGrowth(params, conf);
+  }
+  
+  /**
+   * Run the aggregation Job to aggregate the different TopK patterns and group each Pattern by the features
+   * present in it and thus calculate the final Top K frequent Patterns for each feature
+   */
+  public static void startAggregating(Parameters params, Configuration conf)
+    throws IOException, InterruptedException, ClassNotFoundException {
+    
+    conf.set(PFP_PARAMETERS, params.toString());
+    conf.set("mapred.compress.map.output", "true");
+    conf.set("mapred.output.compression.type", "BLOCK");
+    
+    Path input = new Path(params.get(OUTPUT), FP_GROWTH);
+    Job job = new Job(conf, "PFP Aggregator Driver running over input: " + input);
+    job.setJarByClass(PFPGrowth.class);
+    
+    job.setOutputKeyClass(Text.class);
+    job.setOutputValueClass(TopKStringPatterns.class);
+    
+    FileInputFormat.addInputPath(job, input);
+    Path outPath = new Path(params.get(OUTPUT), FREQUENT_PATTERNS);
+    FileOutputFormat.setOutputPath(job, outPath);
+    
+    job.setInputFormatClass(SequenceFileInputFormat.class);
+    job.setMapperClass(AggregatorMapper.class);
+    job.setCombinerClass(AggregatorReducer.class);
+    job.setReducerClass(AggregatorReducer.class);
+    job.setOutputFormatClass(SequenceFileOutputFormat.class);
+    
+    HadoopUtil.delete(conf, outPath);
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
+  }
+  
+  /**
+   * Count the frequencies of various features in parallel using Map/Reduce
+   */
+  public static void startParallelCounting(Parameters params, Configuration conf)
+    throws IOException, InterruptedException, ClassNotFoundException {
+    conf.set(PFP_PARAMETERS, params.toString());
+    
+    conf.set("mapred.compress.map.output", "true");
+    conf.set("mapred.output.compression.type", "BLOCK");
+    
+    String input = params.get(INPUT);
+    Job job = new Job(conf, "Parallel Counting Driver running over input: " + input);
+    job.setJarByClass(PFPGrowth.class);
+    
+    job.setOutputKeyClass(Text.class);
+    job.setOutputValueClass(LongWritable.class);
+    
+    FileInputFormat.addInputPath(job, new Path(input));
+    Path outPath = new Path(params.get(OUTPUT), PARALLEL_COUNTING);
+    FileOutputFormat.setOutputPath(job, outPath);
+    
+    HadoopUtil.delete(conf, outPath);
+    
+    job.setInputFormatClass(TextInputFormat.class);
+    job.setMapperClass(ParallelCountingMapper.class);
+    job.setCombinerClass(ParallelCountingReducer.class);
+    job.setReducerClass(ParallelCountingReducer.class);
+    job.setOutputFormatClass(SequenceFileOutputFormat.class);
+    
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
+    
+  }
+  
+  /**
+   * Run the Parallel FPGrowth Map/Reduce Job to calculate the Top K features of group dependent shards
+   */
+  public static void startParallelFPGrowth(Parameters params, Configuration conf)
+    throws IOException, InterruptedException, ClassNotFoundException {
+    conf.set(PFP_PARAMETERS, params.toString());
+    conf.set("mapred.compress.map.output", "true");
+    conf.set("mapred.output.compression.type", "BLOCK");
+    Path input = new Path(params.get(INPUT));
+    Job job = new Job(conf, "PFP Growth Driver running over input" + input);
+    job.setJarByClass(PFPGrowth.class);
+    
+    job.setMapOutputKeyClass(IntWritable.class);
+    job.setMapOutputValueClass(TransactionTree.class);
+    
+    job.setOutputKeyClass(Text.class);
+    job.setOutputValueClass(TopKStringPatterns.class);
+    
+    FileInputFormat.addInputPath(job, input);
+    Path outPath = new Path(params.get(OUTPUT), FP_GROWTH);
+    FileOutputFormat.setOutputPath(job, outPath);
+    
+    HadoopUtil.delete(conf, outPath);
+    
+    job.setInputFormatClass(TextInputFormat.class);
+    job.setMapperClass(ParallelFPGrowthMapper.class);
+    job.setCombinerClass(ParallelFPGrowthCombiner.class);
+    job.setReducerClass(ParallelFPGrowthReducer.class);
+    job.setOutputFormatClass(SequenceFileOutputFormat.class);
+    
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingMapper.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingMapper.java
index e69de29b..626c9af0 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingMapper.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingMapper.java
@@ -0,0 +1,64 @@
+/**
+w * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Set;
+import java.util.regex.Pattern;
+
+import com.google.common.collect.Sets;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.mahout.common.Parameters;
+
+/**
+ * 
+ *  maps all items in a particular transaction like the way it is done in Hadoop
+ * WordCount example
+ * 
+ */
+public class ParallelCountingMapper extends Mapper<LongWritable,Text,Text,LongWritable> {
+  
+  private static final LongWritable ONE = new LongWritable(1);
+  
+  private Pattern splitter;
+  
+  @Override
+  protected void map(LongWritable offset, Text input, Context context) throws IOException,
+                                                                      InterruptedException {
+    
+    String[] items = splitter.split(input.toString());
+    Set<String> uniqueItems = Sets.newHashSet(Arrays.asList(items));
+    for (String item : uniqueItems) {
+      if (item.trim().isEmpty()) {
+        continue;
+      }
+      context.setStatus("Parallel Counting Mapper: " + item);
+      context.write(new Text(item), ONE);
+    }
+  }
+  
+  @Override
+  protected void setup(Context context) throws IOException, InterruptedException {
+    super.setup(context);
+    Parameters params = new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
+    splitter = Pattern.compile(params.get(PFPGrowth.SPLIT_PATTERN, PFPGrowth.SPLITTER.toString()));
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingReducer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingReducer.java
index e69de29b..242e8b42 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingReducer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingReducer.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Reducer;
+
+/**
+ *  sums up the item count and output the item and the count This can also be
+ * used as a local Combiner. A simple summing reducer
+ */
+public class ParallelCountingReducer extends Reducer<Text,LongWritable,Text,LongWritable> {
+  
+  @Override
+  protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException,
+                                                                                 InterruptedException {
+    long sum = 0;
+    for (LongWritable value : values) {
+      context.setStatus("Parallel Counting Reducer :" + key);
+      sum += value.get();
+    }
+    context.setStatus("Parallel Counting Reducer: " + key + " => " + sum);
+    context.write(key, new LongWritable(sum));
+    
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java
index e69de29b..76eaf514 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java
@@ -0,0 +1,46 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+
+import org.apache.mahout.math.list.IntArrayList;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.mahout.common.Pair;
+
+/**
+ *  takes each group of dependent transactions and\ compacts it in a
+ * TransactionTree structure
+ */
+public class ParallelFPGrowthCombiner extends Reducer<IntWritable,TransactionTree,IntWritable,TransactionTree> {
+  
+  @Override
+  protected void reduce(IntWritable key, Iterable<TransactionTree> values, Context context)
+    throws IOException, InterruptedException {
+    TransactionTree cTree = new TransactionTree();
+    for (TransactionTree tr : values) {
+      for (Pair<IntArrayList,Long> p : tr) {
+        cTree.addPattern(p.getFirst(), p.getSecond());
+      }
+    }
+    context.write(key, cTree.getCompressedTree());
+  }
+  
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java
index e69de29b..b1027896 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java
@@ -0,0 +1,99 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.regex.Pattern;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.mahout.common.Pair;
+
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.math.list.IntArrayList;
+import org.apache.mahout.math.map.OpenObjectIntHashMap;
+import org.apache.mahout.math.set.OpenIntHashSet;
+
+/**
+ *  maps each transaction to all unique items groups in the transaction. mapper
+ * outputs the group id as key and the transaction as value
+ * 
+ */
+public class ParallelFPGrowthMapper extends Mapper<LongWritable,Text,IntWritable,TransactionTree> {
+
+  private final OpenObjectIntHashMap<String> fMap = new OpenObjectIntHashMap<String>();
+  private Pattern splitter;
+  private int maxPerGroup;
+  private final IntWritable wGroupID = new IntWritable();
+
+  @Override
+  protected void map(LongWritable offset, Text input, Context context)
+    throws IOException, InterruptedException {
+
+    String[] items = splitter.split(input.toString());
+
+    OpenIntHashSet itemSet = new OpenIntHashSet();
+
+    for (String item : items) {
+      if (fMap.containsKey(item) && !item.trim().isEmpty()) {
+        itemSet.add(fMap.get(item));
+      }
+    }
+
+    IntArrayList itemArr = new IntArrayList(itemSet.size());
+    itemSet.keys(itemArr);
+    itemArr.sort();
+
+    OpenIntHashSet groups = new OpenIntHashSet();
+    for (int j = itemArr.size() - 1; j >= 0; j--) {
+      // generate group dependent shards
+      int item = itemArr.get(j);
+      int groupID = PFPGrowth.getGroup(item, maxPerGroup);
+        
+      if (!groups.contains(groupID)) {
+        IntArrayList tempItems = new IntArrayList(j + 1);
+        tempItems.addAllOfFromTo(itemArr, 0, j);
+        context.setStatus("Parallel FPGrowth: Generating Group Dependent transactions for: " + item);
+        wGroupID.set(groupID);
+        context.write(wGroupID, new TransactionTree(tempItems, 1L));
+      }
+      groups.add(groupID);
+    }
+    
+  }
+  
+  @Override
+  protected void setup(Context context) throws IOException, InterruptedException {
+    super.setup(context);
+
+    int i = 0;
+    for (Pair<String,Long> e : PFPGrowth.readFList(context.getConfiguration())) {
+      fMap.put(e.getFirst(), i++);
+    }
+    
+    Parameters params = 
+      new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
+
+    splitter = Pattern.compile(params.get(PFPGrowth.SPLIT_PATTERN,
+                                          PFPGrowth.SPLITTER.toString()));
+    
+    maxPerGroup = params.getInt(PFPGrowth.MAX_PER_GROUP, 0);
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java
index e69de29b..c510de61 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java
@@ -0,0 +1,144 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map.Entry;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
+import org.apache.commons.lang3.mutable.MutableLong;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.fpm.pfpgrowth.convertors.ContextStatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.convertors.ContextWriteOutputCollector;
+import org.apache.mahout.fpm.pfpgrowth.convertors.integer.IntegerStringOutputConverter;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthIds;
+import org.apache.mahout.math.list.IntArrayList;
+import org.apache.mahout.math.list.LongArrayList;
+
+/**
+ *  takes each group of transactions and runs Vanilla FPGrowth on it and
+ * outputs the the Top K frequent Patterns for each group.
+ * 
+ */
+public final class ParallelFPGrowthReducer extends Reducer<IntWritable,TransactionTree,Text,TopKStringPatterns> {
+
+  private final List<String> featureReverseMap = Lists.newArrayList();
+  private final LongArrayList freqList = new LongArrayList();
+  private int maxHeapSize = 50;
+  private int minSupport = 3;
+  private int numFeatures;
+  private int maxPerGroup;
+  private boolean useFP2;
+
+  private static final class IteratorAdapter implements Iterator<Pair<List<Integer>,Long>> {
+    private final Iterator<Pair<IntArrayList,Long>> innerIter;
+
+    private IteratorAdapter(Iterator<Pair<IntArrayList,Long>> transactionIter) {
+      innerIter = transactionIter;
+    }
+
+    @Override
+    public boolean hasNext() {
+      return innerIter.hasNext();
+    }
+
+    @Override
+    public Pair<List<Integer>,Long> next() {
+      Pair<IntArrayList,Long> innerNext = innerIter.next();
+      return new Pair<List<Integer>,Long>(innerNext.getFirst().toList(), innerNext.getSecond());
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  @Override
+  protected void reduce(IntWritable key, Iterable<TransactionTree> values, Context context) throws IOException {
+    TransactionTree cTree = new TransactionTree();
+    for (TransactionTree tr : values) {
+      for (Pair<IntArrayList,Long> p : tr) {
+        cTree.addPattern(p.getFirst(), p.getSecond());
+      }
+    }
+    
+    List<Pair<Integer,Long>> localFList = Lists.newArrayList();
+    for (Entry<Integer,MutableLong> fItem : cTree.generateFList().entrySet()) {
+      localFList.add(new Pair<Integer,Long>(fItem.getKey(), fItem.getValue().toLong()));
+    }
+    
+    Collections.sort(localFList, new CountDescendingPairComparator<Integer,Long>());
+    
+    if (useFP2) {
+      FPGrowthIds.generateTopKFrequentPatterns(
+          cTree.iterator(),
+          freqList,
+          minSupport,
+          maxHeapSize,
+          PFPGrowth.getGroupMembers(key.get(), maxPerGroup, numFeatures),
+          new IntegerStringOutputConverter(
+              new ContextWriteOutputCollector<IntWritable, TransactionTree, Text, TopKStringPatterns>(context),
+              featureReverseMap)
+      );
+    } else {
+      FPGrowth<Integer> fpGrowth = new FPGrowth<Integer>();
+      fpGrowth.generateTopKFrequentPatterns(
+          new IteratorAdapter(cTree.iterator()),
+          localFList,
+          minSupport,
+          maxHeapSize,
+          Sets.newHashSet(PFPGrowth.getGroupMembers(key.get(),
+                                                         maxPerGroup, 
+                                                         numFeatures).toList()),
+          new IntegerStringOutputConverter(
+              new ContextWriteOutputCollector<IntWritable,TransactionTree,Text,TopKStringPatterns>(context),
+              featureReverseMap),
+          new ContextStatusUpdater<IntWritable,TransactionTree,Text,TopKStringPatterns>(context));
+    }
+  }
+  
+  @Override
+  protected void setup(Context context) throws IOException, InterruptedException {
+    
+    super.setup(context);
+    Parameters params = new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
+    
+    for (Pair<String,Long> e : PFPGrowth.readFList(context.getConfiguration())) {
+      featureReverseMap.add(e.getFirst());
+      freqList.add(e.getSecond());
+    }
+    
+    maxHeapSize = Integer.valueOf(params.get(PFPGrowth.MAX_HEAP_SIZE, "50"));
+    minSupport = Integer.valueOf(params.get(PFPGrowth.MIN_SUPPORT, "3"));
+
+    maxPerGroup = params.getInt(PFPGrowth.MAX_PER_GROUP, 0);
+    numFeatures = featureReverseMap.size();
+    useFP2 = "true".equals(params.get(PFPGrowth.USE_FPG2));
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java
index e69de29b..12f79079 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java
@@ -0,0 +1,375 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.mahout.math.list.IntArrayList;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import org.apache.commons.lang3.mutable.MutableLong;
+import org.apache.hadoop.io.VIntWritable;
+import org.apache.hadoop.io.VLongWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.mahout.common.Pair;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * A compact representation of transactions modeled on the lines to
+ * {@link org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPTree} This reduces plenty of space and speeds up
+ * Map/Reduce of {@link PFPGrowth} algorithm by reducing data size passed from the Mapper to the reducer where
+ * {@link org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth} mining is done
+ */
+public final class TransactionTree implements Writable, Iterable<Pair<IntArrayList,Long>> {
+
+  private static final Logger log = LoggerFactory.getLogger(TransactionTree.class);
+
+  private static final int DEFAULT_CHILDREN_INITIAL_SIZE = 2;
+  private static final int DEFAULT_INITIAL_SIZE = 8;
+  private static final float GROWTH_RATE = 1.5f;
+  private static final int ROOTNODEID = 0;
+  
+  private int[] attribute;
+  private int[] childCount;
+  private int[][] nodeChildren;
+  private long[] nodeCount;
+  private int nodes;
+  private boolean representedAsList;
+  private List<Pair<IntArrayList,Long>> transactionSet;
+  
+  public TransactionTree() {
+    this(DEFAULT_INITIAL_SIZE);
+  }
+  
+  public TransactionTree(int size) {
+    if (size < DEFAULT_INITIAL_SIZE) {
+      size = DEFAULT_INITIAL_SIZE;
+    }
+    childCount = new int[size];
+    attribute = new int[size];
+    nodeCount = new long[size];
+    nodeChildren = new int[size][];
+    createRootNode();
+    representedAsList = false;
+  }
+
+  public TransactionTree(IntArrayList items, Long support) {
+    representedAsList = true;
+    transactionSet = Lists.newArrayList();
+    transactionSet.add(new Pair<IntArrayList,Long>(items, support));
+  }
+  
+  public TransactionTree(List<Pair<IntArrayList,Long>> transactionSet) {
+    representedAsList = true;
+    this.transactionSet = transactionSet;
+  }
+  
+  public void addChild(int parentNodeId, int childnodeId) {
+    int length = childCount[parentNodeId];
+    if (length >= nodeChildren[parentNodeId].length) {
+      resizeChildren(parentNodeId);
+    }
+    nodeChildren[parentNodeId][length++] = childnodeId;
+    childCount[parentNodeId] = length;
+    
+  }
+  
+  public void addCount(int nodeId, long nextNodeCount) {
+    if (nodeId < nodes) {
+      this.nodeCount[nodeId] += nextNodeCount;
+    }
+  }
+
+  public int addPattern(IntArrayList myList, long addCount) {
+    int temp = ROOTNODEID;
+    int ret = 0;
+    boolean addCountMode = true;
+    for (int idx = 0; idx < myList.size(); idx++) {
+      int attributeValue = myList.get(idx);
+      int child;
+      if (addCountMode) {
+        child = childWithAttribute(temp, attributeValue);
+        if (child == -1) {
+          addCountMode = false;
+        } else {
+          addCount(child, addCount);
+          temp = child;
+        }
+      }
+      if (!addCountMode) {
+        child = createNode(temp, attributeValue, addCount);
+        temp = child;
+        ret++;
+      }
+    }
+    return ret;
+  }
+  
+  public int attribute(int nodeId) {
+    return this.attribute[nodeId];
+  }
+  
+  public int childAtIndex(int nodeId, int index) {
+    if (childCount[nodeId] < index) {
+      return -1;
+    }
+    return nodeChildren[nodeId][index];
+  }
+  
+  public int childCount() {
+    int sum = 0;
+    for (int i = 0; i < nodes; i++) {
+      sum += childCount[i];
+    }
+    return sum;
+  }
+  
+  public int childCount(int nodeId) {
+    return childCount[nodeId];
+  }
+  
+  public int childWithAttribute(int nodeId, int childAttribute) {
+    int length = childCount[nodeId];
+    for (int i = 0; i < length; i++) {
+      if (attribute[nodeChildren[nodeId][i]] == childAttribute) {
+        return nodeChildren[nodeId][i];
+      }
+    }
+    return -1;
+  }
+  
+  public long count(int nodeId) {
+    return nodeCount[nodeId];
+  }
+  
+  public Map<Integer,MutableLong> generateFList() {
+    Map<Integer,MutableLong> frequencyList = Maps.newHashMap();
+    for (Pair<IntArrayList, Long> p : this) {
+      IntArrayList items = p.getFirst();
+      for (int idx = 0; idx < items.size(); idx++) {
+        if (!frequencyList.containsKey(items.get(idx))) {
+          frequencyList.put(items.get(idx), new MutableLong(0));
+        }
+        frequencyList.get(items.get(idx)).add(p.getSecond());
+      }
+    }
+    return frequencyList;
+  }
+  
+  public TransactionTree getCompressedTree() {
+    TransactionTree ctree = new TransactionTree();
+    Iterator<Pair<IntArrayList,Long>> it = iterator();
+    int node = 0;
+    int size = 0;
+    List<Pair<IntArrayList,Long>> compressedTransactionSet = Lists.newArrayList();
+    while (it.hasNext()) {
+      Pair<IntArrayList,Long> p = it.next();
+      p.getFirst().sort();
+      compressedTransactionSet.add(p);
+      node += ctree.addPattern(p.getFirst(), p.getSecond());
+      size += p.getFirst().size() + 2;
+    }
+
+    if (log.isDebugEnabled()) {
+      log.debug("Nodes in UnCompressed Tree: {} ", nodes);
+      log.debug("UnCompressed Tree Size: {}", (this.nodes * 4 * 4 + this.childCount() * 4) / 1000000.0);
+      log.debug("Nodes in Compressed Tree: {} ", node);
+      log.debug("Compressed Tree Size: {}", (node * 4 * 4 + ctree.childCount() * 4) / 1000000.0);
+      log.debug("TransactionSet Size: {}", size * 4 / 1000000.0);
+    }
+    if (node * 4 * 4 + ctree.childCount() * 4 <= size * 4) {
+      return ctree;
+    } else {
+      return new TransactionTree(compressedTransactionSet);
+    }
+  }
+  
+  @Override
+  public Iterator<Pair<IntArrayList,Long>> iterator() {
+    if (this.isTreeEmpty() && !representedAsList) {
+      throw new IllegalStateException("This is a bug. Please report this to mahout-user list");
+    } else if (representedAsList) {
+      return transactionSet.iterator();
+    } else {
+      return new TransactionTreeIterator(this);
+    }
+  }
+  
+  public boolean isTreeEmpty() {
+    return nodes <= 1;
+  }
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    representedAsList = in.readBoolean();
+    
+    VIntWritable vInt = new VIntWritable();
+    VLongWritable vLong = new VLongWritable();
+    
+    if (representedAsList) {
+      transactionSet = Lists.newArrayList();
+      vInt.readFields(in);
+      int numTransactions = vInt.get();
+      for (int i = 0; i < numTransactions; i++) {
+        vLong.readFields(in);
+        Long support = vLong.get();
+        
+        vInt.readFields(in);
+        int length = vInt.get();
+        
+        int[] items = new int[length];
+        for (int j = 0; j < length; j++) {
+          vInt.readFields(in);
+          items[j] = vInt.get();
+        }
+        Pair<IntArrayList,Long> transaction = new Pair<IntArrayList,Long>(new IntArrayList(items), support);
+        transactionSet.add(transaction);
+      }
+    } else {
+      vInt.readFields(in);
+      nodes = vInt.get();
+      attribute = new int[nodes];
+      nodeCount = new long[nodes];
+      childCount = new int[nodes];
+      nodeChildren = new int[nodes][];
+      for (int i = 0; i < nodes; i++) {
+        vInt.readFields(in);
+        attribute[i] = vInt.get();
+        vLong.readFields(in);
+        nodeCount[i] = vLong.get();
+        vInt.readFields(in);
+        int childCountI = vInt.get();
+        childCount[i] = childCountI;
+        nodeChildren[i] = new int[childCountI];
+        for (int j = 0; j < childCountI; j++) {
+          vInt.readFields(in);
+          nodeChildren[i][j] = vInt.get();
+        }
+      }
+    }
+  }
+  
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeBoolean(representedAsList);
+    VIntWritable vInt = new VIntWritable();
+    VLongWritable vLong = new VLongWritable();
+    if (representedAsList) {
+      int transactionSetSize = transactionSet.size();
+      vInt.set(transactionSetSize);
+      vInt.write(out);
+      for (Pair<IntArrayList, Long> transaction : transactionSet) {
+        vLong.set(transaction.getSecond());
+        vLong.write(out);
+
+        vInt.set(transaction.getFirst().size());
+        vInt.write(out);
+
+        IntArrayList items = transaction.getFirst();
+        for (int idx = 0; idx < items.size(); idx++) {
+          int item = items.get(idx);
+          vInt.set(item);
+          vInt.write(out);
+        }
+      }
+    } else {
+      vInt.set(nodes);
+      vInt.write(out);
+      for (int i = 0; i < nodes; i++) {
+        vInt.set(attribute[i]);
+        vInt.write(out);
+        vLong.set(nodeCount[i]);
+        vLong.write(out);
+        vInt.set(childCount[i]);
+        vInt.write(out);
+        int max = childCount[i];
+        for (int j = 0; j < max; j++) {
+          vInt.set(nodeChildren[i][j]);
+          vInt.write(out);
+        }
+      }
+    }
+  }
+  
+  private int createNode(int parentNodeId, int attributeValue, long count) {
+    if (nodes >= this.attribute.length) {
+      resize();
+    }
+    
+    childCount[nodes] = 0;
+    this.attribute[nodes] = attributeValue;
+    nodeCount[nodes] = count;
+    if (nodeChildren[nodes] == null) {
+      nodeChildren[nodes] = new int[DEFAULT_CHILDREN_INITIAL_SIZE];
+    }
+    
+    int childNodeId = nodes++;
+    addChild(parentNodeId, childNodeId);
+    return childNodeId;
+  }
+  
+  private void createRootNode() {
+    childCount[nodes] = 0;
+    attribute[nodes] = -1;
+    nodeCount[nodes] = 0;
+    if (nodeChildren[nodes] == null) {
+      nodeChildren[nodes] = new int[DEFAULT_CHILDREN_INITIAL_SIZE];
+    }
+    nodes++;
+  }
+  
+  private void resize() {
+    int size = (int) (GROWTH_RATE * nodes);
+    if (size < DEFAULT_INITIAL_SIZE) {
+      size = DEFAULT_INITIAL_SIZE;
+    }
+    
+    int[] oldChildCount = childCount;
+    int[] oldAttribute = attribute;
+    long[] oldnodeCount = nodeCount;
+    int[][] oldNodeChildren = nodeChildren;
+    
+    childCount = new int[size];
+    attribute = new int[size];
+    nodeCount = new long[size];
+    nodeChildren = new int[size][];
+    
+    System.arraycopy(oldChildCount, 0, this.childCount, 0, nodes);
+    System.arraycopy(oldAttribute, 0, this.attribute, 0, nodes);
+    System.arraycopy(oldnodeCount, 0, this.nodeCount, 0, nodes);
+    System.arraycopy(oldNodeChildren, 0, this.nodeChildren, 0, nodes);
+  }
+  
+  private void resizeChildren(int nodeId) {
+    int length = childCount[nodeId];
+    int size = (int) (GROWTH_RATE * length);
+    if (size < DEFAULT_CHILDREN_INITIAL_SIZE) {
+      size = DEFAULT_CHILDREN_INITIAL_SIZE;
+    }
+    int[] oldNodeChildren = nodeChildren[nodeId];
+    nodeChildren[nodeId] = new int[size];
+    System.arraycopy(oldNodeChildren, 0, this.nodeChildren[nodeId], 0, length);
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java
index e69de29b..a243c055 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java
@@ -0,0 +1,91 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.util.Iterator;
+import java.util.Stack;
+
+import org.apache.mahout.math.list.IntArrayList;
+
+import com.google.common.collect.AbstractIterator;
+import org.apache.mahout.common.Pair;
+
+/**
+ * Generates a List of transactions view of Transaction Tree by doing Depth First Traversal on the tree
+ * structure
+ */
+final class TransactionTreeIterator extends AbstractIterator<Pair<IntArrayList,Long>> {
+
+  private final Stack<int[]> depth = new Stack<int[]>();
+  private final TransactionTree transactionTree;
+
+  TransactionTreeIterator(TransactionTree transactionTree) {
+    this.transactionTree = transactionTree;
+    depth.push(new int[] {0, -1});
+  }
+
+  @Override
+  protected Pair<IntArrayList, Long> computeNext() {
+
+    if (depth.isEmpty()) {
+      return endOfData();
+    }
+    
+    long sum;
+    int childId;
+    do {
+      int[] top = depth.peek();
+      while (top[1] + 1 == transactionTree.childCount(top[0])) {
+        depth.pop();
+        top = depth.peek();
+      }
+      if (depth.isEmpty()) {
+        return endOfData();
+      }
+      top[1]++;
+      childId = transactionTree.childAtIndex(top[0], top[1]);
+      depth.push(new int[] {childId, -1});
+      
+      sum = 0;
+      for (int i = transactionTree.childCount(childId) - 1; i >= 0; i--) {
+        sum += transactionTree.count(transactionTree.childAtIndex(childId, i));
+      }
+    } while (sum == transactionTree.count(childId));
+
+    Iterator<int[]> it = depth.iterator();
+    it.next();
+    IntArrayList data = new IntArrayList();
+    while (it.hasNext()) {
+      data.add(transactionTree.attribute(it.next()[0]));
+    }
+
+    Pair<IntArrayList,Long> returnable = new Pair<IntArrayList,Long>(data, transactionTree.count(childId) - sum);
+
+    int[] top = depth.peek();
+    while (top[1] + 1 == transactionTree.childCount(top[0])) {
+      depth.pop();
+      if (depth.isEmpty()) {
+        break;
+      }
+      top = depth.peek();
+    }
+    return returnable;
+  }
+
+
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextStatusUpdater.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextStatusUpdater.java
index e69de29b..8fbb4c6a 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextStatusUpdater.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextStatusUpdater.java
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.convertors;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapreduce.Reducer;
+
+/**
+ * Updates the Context object of a {@link Reducer} class
+ * 
+ * @param <IK>
+ * @param <IV>
+ * @param <K>
+ * @param <V>
+ */
+public class ContextStatusUpdater<IK extends Writable,IV extends Writable,K extends Writable,V extends Writable>
+    implements StatusUpdater {
+  
+  private static final long PERIOD = 10000; // Update every 10 seconds
+  
+  private final Reducer<IK,IV,K,V>.Context context;
+  
+  private long time = System.currentTimeMillis();
+  
+  public ContextStatusUpdater(Reducer<IK,IV,K,V>.Context context) {
+    this.context = context;
+  }
+  
+  @Override
+  public void update(String status) {
+    long curTime = System.currentTimeMillis();
+    if (curTime - time > PERIOD && context != null) {
+      time = curTime;
+      context.setStatus("Processing FPTree: " + status);
+    }
+    
+  }
+  
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextWriteOutputCollector.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextWriteOutputCollector.java
index e69de29b..02cbf6e3 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextWriteOutputCollector.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextWriteOutputCollector.java
@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.convertors;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * An output collector for {@link Reducer} for PFPGrowth which updates the status as well as writes the
+ * patterns generated by the algorithm
+ * 
+ * @param <IK>
+ * @param <IV>
+ * @param <K>
+ * @param <V>
+ */
+public class ContextWriteOutputCollector<IK extends Writable,IV extends Writable,K extends Writable,V extends Writable>
+    implements OutputCollector<K,V> {
+  
+  private static final Logger log = LoggerFactory.getLogger(ContextWriteOutputCollector.class);
+  
+  private final Reducer<IK,IV,K,V>.Context context;
+  
+  public ContextWriteOutputCollector(Reducer<IK,IV,K,V>.Context context) {
+    this.context = context;
+  }
+  
+  @Override
+  public final void collect(K key, V value) throws IOException {
+    try {
+      context.setStatus("Writing Top K patterns for: " + key);
+      context.write(key, value);
+    } catch (InterruptedException e) {
+      log.error("{}", e.toString());
+    }
+  }
+  
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/SequenceFileOutputCollector.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/SequenceFileOutputCollector.java
index e69de29b..cd8df3af 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/SequenceFileOutputCollector.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/SequenceFileOutputCollector.java
@@ -0,0 +1,45 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.convertors;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.OutputCollector;
+
+/**
+ * Collects the {@link Writable} key and {@link Writable} value, and writes them into a {@link SequenceFile}
+ * 
+ * @param <K>
+ * @param <V>
+ */
+public class SequenceFileOutputCollector<K extends Writable,V extends Writable> implements
+    OutputCollector<K,V> {
+  private final SequenceFile.Writer writer;
+  
+  public SequenceFileOutputCollector(SequenceFile.Writer writer) {
+    this.writer = writer;
+  }
+  
+  @Override
+  public final void collect(K key, V value) throws IOException {
+    writer.append(key, value);
+  }
+  
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/StatusUpdater.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/StatusUpdater.java
index e69de29b..ff01cefc 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/StatusUpdater.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/StatusUpdater.java
@@ -0,0 +1,27 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.convertors;
+
+/**
+ * An interface of a Status updater
+ * 
+ */
+public interface StatusUpdater {
+  
+  void update(String status);
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TopKPatternsOutputConverter.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TopKPatternsOutputConverter.java
index e69de29b..a29575fa 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TopKPatternsOutputConverter.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TopKPatternsOutputConverter.java
@@ -0,0 +1,70 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.convertors;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+
+import com.google.common.collect.Lists;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FrequentPatternMaxHeap;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.Pattern;
+
+/**
+ * An output converter which converts the output patterns and collects them in a
+ * {@link FrequentPatternMaxHeap}
+ * 
+ * @param <A>
+ */
+public final class TopKPatternsOutputConverter<A extends Comparable<? super A>> implements
+    OutputCollector<Integer,FrequentPatternMaxHeap> {
+  
+  private final OutputCollector<A,List<Pair<List<A>,Long>>> collector;
+  
+  private final Map<Integer,A> reverseMapping;
+  
+  public TopKPatternsOutputConverter(OutputCollector<A,List<Pair<List<A>,Long>>> collector,
+                                     Map<Integer,A> reverseMapping) {
+    this.collector = collector;
+    this.reverseMapping = reverseMapping;
+  }
+  
+  @Override
+  public void collect(Integer key, FrequentPatternMaxHeap value) throws IOException {
+    List<Pair<List<A>,Long>> perAttributePatterns = Lists.newArrayList();
+    PriorityQueue<Pattern> t = value.getHeap();
+    while (!t.isEmpty()) {
+      Pattern itemSet = t.poll();
+      List<A> frequentPattern = Lists.newArrayList();
+      for (int j = 0; j < itemSet.length(); j++) {
+        frequentPattern.add(reverseMapping.get(itemSet.getPattern()[j]));
+      }
+      Collections.sort(frequentPattern);
+      
+      Pair<List<A>,Long> returnItemSet = new Pair<List<A>,Long>(frequentPattern, itemSet.support());
+      perAttributePatterns.add(returnItemSet);
+    }
+    Collections.reverse(perAttributePatterns);
+    
+    collector.collect(reverseMapping.get(key), perAttributePatterns);
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TransactionIterator.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TransactionIterator.java
index e69de29b..94d15199 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TransactionIterator.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TransactionIterator.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.convertors;
+
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import com.google.common.base.Function;
+import com.google.common.collect.ForwardingIterator;
+import com.google.common.collect.Iterators;
+import org.apache.mahout.common.Pair;
+
+/**
+ * Iterates over a Transaction and outputs the transaction integer id mapping and the support of the
+ * transaction
+ */
+public class TransactionIterator<T> extends ForwardingIterator<Pair<int[],Long>> {
+
+  private final int[] transactionBuffer;
+  private final Iterator<Pair<int[],Long>> delegate;
+
+  public TransactionIterator(Iterator<Pair<List<T>,Long>> transactions, final Map<T,Integer> attributeIdMapping) {
+    transactionBuffer = new int[attributeIdMapping.size()];
+    delegate = Iterators.transform(
+        transactions,
+        new Function<Pair<List<T>,Long>, Pair<int[],Long>>() {
+          @Override
+          public Pair<int[],Long> apply(Pair<List<T>,Long> from) {
+            if (from == null) {
+              return null;
+            }
+            int index = 0;
+            for (T attribute : from.getFirst()) {
+              if (attributeIdMapping.containsKey(attribute)) {
+                transactionBuffer[index++] = attributeIdMapping.get(attribute);
+              }
+            }
+            int[] transactionList = new int[index];
+            System.arraycopy(transactionBuffer, 0, transactionList, 0, index);
+            return new Pair<int[],Long>(transactionList, from.getSecond());
+          }
+        });
+  }
+
+  @Override
+  protected Iterator<Pair<int[],Long>> delegate() {
+    return delegate;
+  }
+
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/integer/IntegerStringOutputConverter.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/integer/IntegerStringOutputConverter.java
index e69de29b..df736ea1 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/integer/IntegerStringOutputConverter.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/integer/IntegerStringOutputConverter.java
@@ -0,0 +1,61 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.convertors.integer;
+
+import java.io.IOException;
+import java.util.List;
+
+import com.google.common.collect.Lists;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+
+/**
+ * Collects the Patterns with Integer id and Long support and converts them to Pattern of Strings based on a
+ * reverse feature lookup map.
+ */
+public final class IntegerStringOutputConverter implements
+    OutputCollector<Integer,List<Pair<List<Integer>,Long>>> {
+  
+  private final OutputCollector<Text,TopKStringPatterns> collector;
+  
+  private final List<String> featureReverseMap;
+  
+  public IntegerStringOutputConverter(OutputCollector<Text,TopKStringPatterns> collector,
+                                      List<String> featureReverseMap) {
+    this.collector = collector;
+    this.featureReverseMap = featureReverseMap;
+  }
+  
+  @Override
+  public void collect(Integer key, List<Pair<List<Integer>,Long>> value) throws IOException {
+    String stringKey = featureReverseMap.get(key);
+    List<Pair<List<String>,Long>> stringValues = Lists.newArrayList();
+    for (Pair<List<Integer>,Long> e : value) {
+      List<String> pattern = Lists.newArrayList();
+      for (Integer i : e.getFirst()) {
+        pattern.add(featureReverseMap.get(i));
+      }
+      stringValues.add(new Pair<List<String>,Long>(pattern, e.getSecond()));
+    }
+    
+    collector.collect(new Text(stringKey), new TopKStringPatterns(stringValues));
+  }
+  
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/StringOutputConverter.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/StringOutputConverter.java
index e69de29b..65cf3724 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/StringOutputConverter.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/StringOutputConverter.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.convertors.string;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.Pair;
+
+/**
+ * Collects a string pattern in a MaxHeap and outputs the top K patterns
+ * 
+ */
+public final class StringOutputConverter implements OutputCollector<String,List<Pair<List<String>,Long>>> {
+  
+  private final OutputCollector<Text,TopKStringPatterns> collector;
+  
+  public StringOutputConverter(OutputCollector<Text,TopKStringPatterns> collector) {
+    this.collector = collector;
+  }
+  
+  @Override
+  public void collect(String key,
+                      List<Pair<List<String>,Long>> value) throws IOException {
+    collector.collect(new Text(key), new TopKStringPatterns(value));
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/TopKStringPatterns.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/TopKStringPatterns.java
index e69de29b..85481af0 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/TopKStringPatterns.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/TopKStringPatterns.java
@@ -0,0 +1,145 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.convertors.string;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+
+import com.google.common.collect.Lists;
+import org.apache.hadoop.io.Writable;
+import org.apache.mahout.common.Pair;
+
+/**
+ * A class which collects Top K string patterns
+ *
+ */
+public final class TopKStringPatterns implements Writable {
+  private final List<Pair<List<String>,Long>> frequentPatterns;
+  
+  public TopKStringPatterns() {
+    frequentPatterns = Lists.newArrayList();
+  }
+  
+  public TopKStringPatterns(Collection<Pair<List<String>, Long>> patterns) {
+    frequentPatterns = Lists.newArrayList();
+    frequentPatterns.addAll(patterns);
+  }
+  
+  public Iterator<Pair<List<String>,Long>> iterator() {
+    return frequentPatterns.iterator();
+  }
+  
+  public List<Pair<List<String>,Long>> getPatterns() {
+    return frequentPatterns;
+  }
+  
+  public TopKStringPatterns merge(TopKStringPatterns pattern, int heapSize) {
+    List<Pair<List<String>,Long>> patterns = Lists.newArrayList();
+    Iterator<Pair<List<String>,Long>> myIterator = frequentPatterns.iterator();
+    Iterator<Pair<List<String>,Long>> otherIterator = pattern.iterator();
+    Pair<List<String>,Long> myItem = null;
+    Pair<List<String>,Long> otherItem = null;
+    for (int i = 0; i < heapSize; i++) {
+      if (myItem == null && myIterator.hasNext()) {
+        myItem = myIterator.next();
+      }
+      if (otherItem == null && otherIterator.hasNext()) {
+        otherItem = otherIterator.next();
+      }
+      if (myItem != null && otherItem != null) {
+        int cmp = myItem.getSecond().compareTo(otherItem.getSecond());
+        if (cmp == 0) {
+          cmp = myItem.getFirst().size() - otherItem.getFirst().size();
+          if (cmp == 0) {
+            for (int j = 0; j < myItem.getFirst().size(); j++) {
+              cmp = myItem.getFirst().get(j).compareTo(
+                otherItem.getFirst().get(j));
+              if (cmp != 0) {
+                break;
+              }
+            }
+          }
+        }
+        if (cmp <= 0) {
+          patterns.add(otherItem);
+          if (cmp == 0) {
+            myItem = null;
+          }
+          otherItem = null;
+        } else if (cmp > 0) {
+          patterns.add(myItem);
+          myItem = null;
+        }
+      } else if (myItem != null) {
+        patterns.add(myItem);
+        myItem = null;
+      } else if (otherItem != null) {
+        patterns.add(otherItem);
+        otherItem = null;
+      } else {
+        break;
+      }
+    }
+    return new TopKStringPatterns(patterns);
+  }
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    frequentPatterns.clear();
+    int length = in.readInt();
+    for (int i = 0; i < length; i++) {
+      List<String> items = Lists.newArrayList();
+      int itemsetLength = in.readInt();
+      long support = in.readLong();
+      for (int j = 0; j < itemsetLength; j++) {
+        items.add(in.readUTF());
+      }
+      frequentPatterns.add(new Pair<List<String>,Long>(items, support));
+    }
+  }
+  
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeInt(frequentPatterns.size());
+    for (Pair<List<String>,Long> pattern : frequentPatterns) {
+      out.writeInt(pattern.getFirst().size());
+      out.writeLong(pattern.getSecond());
+      for (String item : pattern.getFirst()) {
+        out.writeUTF(item);
+      }
+    }
+  }
+  
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    String sep = "";
+    for (Pair<List<String>,Long> pattern : frequentPatterns) {
+      sb.append(sep);
+      sb.append(pattern.toString());
+      sep = ", ";
+      
+    }
+    return sb.toString();
+    
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPGrowth.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPGrowth.java
index e69de29b..8dc52011 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPGrowth.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPGrowth.java
@@ -0,0 +1,707 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import org.apache.commons.lang3.mutable.MutableLong;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
+import org.apache.mahout.fpm.pfpgrowth.CountDescendingPairComparator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.convertors.TopKPatternsOutputConverter;
+import org.apache.mahout.fpm.pfpgrowth.convertors.TransactionIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.math.map.OpenIntIntHashMap;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Implementation of PFGrowth Algorithm with FP-Bonsai pruning
+ *
+ * @param <A> object type used as the cell items in a transaction list
+ */
+public class FPGrowth<A extends Comparable<? super A>> {
+
+  private static final Logger log = LoggerFactory.getLogger(FPGrowth.class);
+
+  public static List<Pair<String,TopKStringPatterns>> readFrequentPattern(Configuration conf, Path path) {
+    List<Pair<String,TopKStringPatterns>> ret = Lists.newArrayList();
+    // key is feature value is count
+    for (Pair<Writable,TopKStringPatterns> record
+         : new SequenceFileIterable<Writable,TopKStringPatterns>(path, true, conf)) {
+      ret.add(new Pair<String,TopKStringPatterns>(record.getFirst().toString(),
+                                                  new TopKStringPatterns(record.getSecond().getPatterns())));
+    }
+    return ret;
+  }
+
+  /**
+   * Generate the Feature Frequency list from the given transaction whose
+   * frequency > minSupport
+   *
+   * @param transactions
+   *          Iterator over the transaction database
+   * @param minSupport
+   *          minSupport of the feature to be included
+   * @return the List of features and their associated frequency as a Pair
+   */
+  public final List<Pair<A,Long>> generateFList(Iterator<Pair<List<A>,Long>> transactions, int minSupport) {
+
+    Map<A,MutableLong> attributeSupport = Maps.newHashMap();
+    while (transactions.hasNext()) {
+      Pair<List<A>,Long> transaction = transactions.next();
+      for (A attribute : transaction.getFirst()) {
+        if (attributeSupport.containsKey(attribute)) {
+          attributeSupport.get(attribute).add(transaction.getSecond().longValue());
+        } else {
+          attributeSupport.put(attribute, new MutableLong(transaction.getSecond()));
+        }
+      }
+    }
+    List<Pair<A,Long>> fList = Lists.newArrayList();
+    for (Entry<A,MutableLong> e : attributeSupport.entrySet()) {
+      long value = e.getValue().longValue();
+      if (value >= minSupport) {
+        fList.add(new Pair<A,Long>(e.getKey(), value));
+      }
+    }
+
+    Collections.sort(fList, new CountDescendingPairComparator<A,Long>());
+
+    return fList;
+  }
+
+  /**
+   * Generate Top K Frequent Patterns for every feature in returnableFeatures
+   * given a stream of transactions and the minimum support
+   *
+   * @param transactionStream
+   *          Iterator of transaction
+   * @param frequencyList
+   *          list of frequent features and their support value
+   * @param minSupport
+   *          minimum support of the transactions
+   * @param k
+   *          Number of top frequent patterns to keep
+   * @param returnableFeatures
+   *          set of features for which the frequent patterns are mined. If the
+   *          set is empty or null, then top K patterns for every frequent item (an item
+   *          whose support> minSupport) is generated
+   * @param output
+   *          The output collector to which the the generated patterns are
+   *          written
+   * @throws IOException
+   */
+  public final void generateTopKFrequentPatterns(Iterator<Pair<List<A>,Long>> transactionStream,
+                                                 Collection<Pair<A, Long>> frequencyList,
+                                                 long minSupport,
+                                                 int k,
+                                                 Collection<A> returnableFeatures,
+                                                 OutputCollector<A,List<Pair<List<A>,Long>>> output,
+                                                 StatusUpdater updater) throws IOException {
+
+    Map<Integer,A> reverseMapping = Maps.newHashMap();
+    Map<A,Integer> attributeIdMapping = Maps.newHashMap();
+
+    int id = 0;
+    for (Pair<A,Long> feature : frequencyList) {
+      A attrib = feature.getFirst();
+      Long frequency = feature.getSecond();
+      if (frequency >= minSupport) {
+        attributeIdMapping.put(attrib, id);
+        reverseMapping.put(id++, attrib);
+      }
+    }
+
+    long[] attributeFrequency = new long[attributeIdMapping.size()];
+    for (Pair<A,Long> feature : frequencyList) {
+      A attrib = feature.getFirst();
+      Long frequency = feature.getSecond();
+      if (frequency < minSupport) {
+        break;
+      }
+      attributeFrequency[attributeIdMapping.get(attrib)] = frequency;
+    }
+
+    log.info("Number of unique items {}", frequencyList.size());
+
+    Collection<Integer> returnFeatures = Sets.newHashSet();
+    if (returnableFeatures != null && !returnableFeatures.isEmpty()) {
+      for (A attrib : returnableFeatures) {
+        if (attributeIdMapping.containsKey(attrib)) {
+          returnFeatures.add(attributeIdMapping.get(attrib));
+          log.info("Adding Pattern {}=>{}", attrib, attributeIdMapping
+            .get(attrib));
+        }
+      }
+    } else {
+      for (int j = 0; j < attributeIdMapping.size(); j++) {
+        returnFeatures.add(j);
+      }
+    }
+
+    log.info("Number of unique pruned items {}", attributeIdMapping.size());
+    generateTopKFrequentPatterns(new TransactionIterator<A>(transactionStream,
+        attributeIdMapping), attributeFrequency, minSupport, k, reverseMapping
+        .size(), returnFeatures, new TopKPatternsOutputConverter<A>(output,
+            reverseMapping), updater);
+
+  }
+
+  /**
+   * Top K FpGrowth Algorithm
+   *
+   * @param tree
+   *          to be mined
+   * @param minSupportValue
+   *          minimum support of the pattern to keep
+   * @param k
+   *          Number of top frequent patterns to keep
+   * @param requiredFeatures
+   *          Set of integer id's of features to mine
+   * @param outputCollector
+   *          the Collector class which converts the given frequent pattern in
+   *          integer to A
+   * @return Top K Frequent Patterns for each feature and their support
+   */
+  private Map<Integer,FrequentPatternMaxHeap> fpGrowth(FPTree tree,
+                                                       long minSupportValue,
+                                                       int k,
+                                                       Collection<Integer> requiredFeatures,
+                                                       TopKPatternsOutputConverter<A> outputCollector,
+                                                       StatusUpdater updater) throws IOException {
+
+    Map<Integer,FrequentPatternMaxHeap> patterns = Maps.newHashMap();
+    FPTreeDepthCache treeCache = new FPTreeDepthCache();
+    for (int i = tree.getHeaderTableCount() - 1; i >= 0; i--) {
+      int attribute = tree.getAttributeAtIndex(i);
+      if (requiredFeatures.contains(attribute)) {
+        log.info("Mining FTree Tree for all patterns with {}", attribute);
+        MutableLong minSupport = new MutableLong(minSupportValue);
+        FrequentPatternMaxHeap frequentPatterns = growth(tree, minSupport, k,
+                                                         treeCache, 0, attribute, updater);
+        patterns.put(attribute, frequentPatterns);
+        outputCollector.collect(attribute, frequentPatterns);
+
+        minSupportValue = Math.max(minSupportValue, minSupport.longValue() / 2);
+        log.info("Found {} Patterns with Least Support {}", patterns.get(
+            attribute).count(), patterns.get(attribute).leastSupport());
+      }
+    }
+    log.info("Tree Cache: First Level: Cache hits={} Cache Misses={}",
+      treeCache.getHits(), treeCache.getMisses());
+    return patterns;
+  }
+
+  private static FrequentPatternMaxHeap generateSinglePathPatterns(FPTree tree,
+                                                                   int k,
+                                                                   long minSupport) {
+    FrequentPatternMaxHeap frequentPatterns = new FrequentPatternMaxHeap(k, false);
+
+    int tempNode = FPTree.ROOTNODEID;
+    Pattern frequentItem = new Pattern();
+    while (tree.childCount(tempNode) != 0) {
+      if (tree.childCount(tempNode) > 1) {
+        log.info("This should not happen {} {}", tree.childCount(tempNode),
+          tempNode);
+      }
+      tempNode = tree.childAtIndex(tempNode, 0);
+      if (tree.count(tempNode) >= minSupport) {
+        frequentItem.add(tree.attribute(tempNode), tree.count(tempNode));
+      }
+    }
+    if (frequentItem.length() > 0) {
+      frequentPatterns.insert(frequentItem);
+    }
+
+    return frequentPatterns;
+  }
+
+  /**
+   * Internal TopKFrequentPattern Generation algorithm, which represents the A's
+   * as integers and transforms features to use only integers
+   *
+   * @param transactions
+   *          Transaction database Iterator
+   * @param attributeFrequency
+   *          array representing the Frequency of the corresponding attribute id
+   * @param minSupport
+   *          minimum support of the pattern to be mined
+   * @param k
+   *          Max value of the Size of the Max-Heap in which Patterns are held
+   * @param featureSetSize
+   *          number of features
+   * @param returnFeatures
+   *          the id's of the features for which Top K patterns have to be mined
+   * @param topKPatternsOutputCollector
+   *          the outputCollector which transforms the given Pattern in integer
+   *          format to the corresponding A Format
+   */
+  private void generateTopKFrequentPatterns(
+    Iterator<Pair<int[],Long>> transactions,
+    long[] attributeFrequency,
+    long minSupport,
+    int k,
+    int featureSetSize,
+    Collection<Integer> returnFeatures, TopKPatternsOutputConverter<A> topKPatternsOutputCollector,
+    StatusUpdater updater) throws IOException {
+
+    FPTree tree = new FPTree(featureSetSize);
+    for (int i = 0; i < featureSetSize; i++) {
+      tree.addHeaderCount(i, attributeFrequency[i]);
+    }
+
+    // Constructing initial FPTree from the list of transactions
+    int nodecount = 0;
+    // int attribcount = 0;
+    int i = 0;
+    while (transactions.hasNext()) {
+      Pair<int[],Long> transaction = transactions.next();
+      Arrays.sort(transaction.getFirst());
+      // attribcount += transaction.length;
+      nodecount += treeAddCount(tree, transaction.getFirst(), transaction.getSecond(), minSupport, attributeFrequency);
+      i++;
+      if (i % 10000 == 0) {
+        log.info("FPTree Building: Read {} Transactions", i);
+      }
+    }
+
+    log.info("Number of Nodes in the FP Tree: {}", nodecount);
+
+    fpGrowth(tree, minSupport, k, returnFeatures, topKPatternsOutputCollector, updater);
+  }
+
+  private static FrequentPatternMaxHeap growth(FPTree tree,
+                                               MutableLong minSupportMutable,
+                                               int k,
+                                               FPTreeDepthCache treeCache,
+                                               int level,
+                                               int currentAttribute,
+                                               StatusUpdater updater) {
+
+    FrequentPatternMaxHeap frequentPatterns = new FrequentPatternMaxHeap(k,
+      true);
+
+    int i = Arrays.binarySearch(tree.getHeaderTableAttributes(),
+      currentAttribute);
+    if (i < 0) {
+      return frequentPatterns;
+    }
+
+    int headerTableCount = tree.getHeaderTableCount();
+
+    while (i < headerTableCount) {
+      int attribute = tree.getAttributeAtIndex(i);
+      long count = tree.getHeaderSupportCount(attribute);
+      if (count < minSupportMutable.longValue()) {
+        i++;
+        continue;
+      }
+      updater.update("FPGrowth Algorithm for a given feature: " + attribute);
+      FPTree conditionalTree = treeCache.getFirstLevelTree(attribute);
+      if (conditionalTree.isEmpty()) {
+        traverseAndBuildConditionalFPTreeData(tree.getHeaderNext(attribute),
+          minSupportMutable.longValue(), conditionalTree, tree);
+        // printTree(conditionalTree);
+
+      }
+
+      FrequentPatternMaxHeap returnedPatterns;
+      if (attribute == currentAttribute) {
+
+        returnedPatterns = growthTopDown(conditionalTree, minSupportMutable, k,
+          treeCache, level + 1, true, currentAttribute, updater);
+
+        frequentPatterns = mergeHeap(frequentPatterns, returnedPatterns,
+          attribute, count, true);
+      } else {
+        returnedPatterns = growthTopDown(conditionalTree, minSupportMutable, k,
+          treeCache, level + 1, false, currentAttribute, updater);
+        frequentPatterns = mergeHeap(frequentPatterns, returnedPatterns,
+          attribute, count, false);
+      }
+      if (frequentPatterns.isFull() && minSupportMutable.longValue() < frequentPatterns.leastSupport()) {
+        minSupportMutable.setValue(frequentPatterns.leastSupport());
+      }
+      i++;
+    }
+
+    return frequentPatterns;
+  }
+
+  private static FrequentPatternMaxHeap growthBottomUp(FPTree tree,
+                                                       MutableLong minSupportMutable,
+                                                       int k,
+                                                       FPTreeDepthCache treeCache,
+                                                       int level,
+                                                       boolean conditionalOfCurrentAttribute,
+                                                       int currentAttribute,
+                                                       StatusUpdater updater) {
+
+    FrequentPatternMaxHeap frequentPatterns = new FrequentPatternMaxHeap(k,
+      false);
+
+    if (!conditionalOfCurrentAttribute) {
+      int index = Arrays.binarySearch(tree.getHeaderTableAttributes(),
+        currentAttribute);
+      if (index < 0) {
+        return frequentPatterns;
+      } else {
+        int attribute = tree.getAttributeAtIndex(index);
+        long count = tree.getHeaderSupportCount(attribute);
+        if (count < minSupportMutable.longValue()) {
+          return frequentPatterns;
+        }
+      }
+    }
+
+    if (tree.singlePath()) {
+      return generateSinglePathPatterns(tree, k, minSupportMutable.longValue());
+    }
+
+    updater.update("Bottom Up FP Growth");
+    for (int i = tree.getHeaderTableCount() - 1; i >= 0; i--) {
+      int attribute = tree.getAttributeAtIndex(i);
+      long count = tree.getHeaderSupportCount(attribute);
+      if (count < minSupportMutable.longValue()) {
+        continue;
+      }
+      FPTree conditionalTree = treeCache.getTree(level);
+
+      FrequentPatternMaxHeap returnedPatterns;
+      if (conditionalOfCurrentAttribute) {
+        traverseAndBuildConditionalFPTreeData(tree.getHeaderNext(attribute),
+          minSupportMutable.longValue(), conditionalTree, tree);
+        returnedPatterns = growthBottomUp(conditionalTree, minSupportMutable,
+          k, treeCache, level + 1, true, currentAttribute, updater);
+
+        frequentPatterns = mergeHeap(frequentPatterns, returnedPatterns,
+          attribute, count, true);
+      } else {
+        if (attribute == currentAttribute) {
+          traverseAndBuildConditionalFPTreeData(tree.getHeaderNext(attribute),
+            minSupportMutable.longValue(), conditionalTree, tree);
+          returnedPatterns = growthBottomUp(conditionalTree, minSupportMutable,
+            k, treeCache, level + 1, true, currentAttribute, updater);
+
+          frequentPatterns = mergeHeap(frequentPatterns, returnedPatterns,
+            attribute, count, true);
+        } else if (attribute > currentAttribute) {
+          traverseAndBuildConditionalFPTreeData(tree.getHeaderNext(attribute),
+            minSupportMutable.longValue(), conditionalTree, tree);
+          returnedPatterns = growthBottomUp(conditionalTree, minSupportMutable,
+            k, treeCache, level + 1, false, currentAttribute, updater);
+          frequentPatterns = mergeHeap(frequentPatterns, returnedPatterns,
+            attribute, count, false);
+        }
+      }
+
+      if (frequentPatterns.isFull() && minSupportMutable.longValue() < frequentPatterns.leastSupport()) {
+        minSupportMutable.setValue(frequentPatterns.leastSupport());
+      }
+    }
+
+    return frequentPatterns;
+  }
+
+  private static FrequentPatternMaxHeap growthTopDown(FPTree tree,
+                                                      MutableLong minSupportMutable,
+                                                      int k,
+                                                      FPTreeDepthCache treeCache,
+                                                      int level,
+                                                      boolean conditionalOfCurrentAttribute,
+                                                      int currentAttribute,
+                                                      StatusUpdater updater) {
+
+    FrequentPatternMaxHeap frequentPatterns = new FrequentPatternMaxHeap(k,
+      true);
+
+    if (!conditionalOfCurrentAttribute) {
+      int index = Arrays.binarySearch(tree.getHeaderTableAttributes(),
+        currentAttribute);
+      if (index < 0) {
+        return frequentPatterns;
+      } else {
+        int attribute = tree.getAttributeAtIndex(index);
+        long count = tree.getHeaderSupportCount(attribute);
+        if (count < minSupportMutable.longValue()) {
+          return frequentPatterns;
+        }
+      }
+    }
+
+    if (tree.singlePath()) {
+      return generateSinglePathPatterns(tree, k, minSupportMutable.longValue());
+    }
+
+    updater.update("Top Down Growth:");
+
+    for (int i = 0; i < tree.getHeaderTableCount(); i++) {
+      int attribute = tree.getAttributeAtIndex(i);
+      long count = tree.getHeaderSupportCount(attribute);
+      if (count < minSupportMutable.longValue()) {
+        continue;
+      }
+
+      FPTree conditionalTree = treeCache.getTree(level);
+
+      FrequentPatternMaxHeap returnedPatterns;
+      if (conditionalOfCurrentAttribute) {
+        traverseAndBuildConditionalFPTreeData(tree.getHeaderNext(attribute),
+          minSupportMutable.longValue(), conditionalTree, tree);
+
+        returnedPatterns = growthBottomUp(conditionalTree, minSupportMutable,
+          k, treeCache, level + 1, true, currentAttribute, updater);
+        frequentPatterns = mergeHeap(frequentPatterns, returnedPatterns,
+          attribute, count, true);
+
+      } else {
+        if (attribute == currentAttribute) {
+          traverseAndBuildConditionalFPTreeData(tree.getHeaderNext(attribute),
+            minSupportMutable.longValue(), conditionalTree, tree);
+          returnedPatterns = growthBottomUp(conditionalTree, minSupportMutable,
+            k, treeCache, level + 1, true, currentAttribute, updater);
+          frequentPatterns = mergeHeap(frequentPatterns, returnedPatterns,
+            attribute, count, true);
+
+        } else if (attribute > currentAttribute) {
+          traverseAndBuildConditionalFPTreeData(tree.getHeaderNext(attribute),
+            minSupportMutable.longValue(), conditionalTree, tree);
+          returnedPatterns = growthBottomUp(conditionalTree, minSupportMutable,
+            k, treeCache, level + 1, false, currentAttribute, updater);
+          frequentPatterns = mergeHeap(frequentPatterns, returnedPatterns,
+            attribute, count, false);
+
+        }
+      }
+      if (frequentPatterns.isFull() && minSupportMutable.longValue() < frequentPatterns.leastSupport()) {
+        minSupportMutable.setValue(frequentPatterns.leastSupport());
+      }
+    }
+
+    return frequentPatterns;
+  }
+
+  private static FrequentPatternMaxHeap mergeHeap(FrequentPatternMaxHeap frequentPatterns,
+                                                  FrequentPatternMaxHeap returnedPatterns,
+                                                  int attribute,
+                                                  long count,
+                                                  boolean addAttribute) {
+    frequentPatterns.addAll(returnedPatterns, attribute, count);
+    if (frequentPatterns.addable(count) && addAttribute) {
+      Pattern p = new Pattern();
+      p.add(attribute, count);
+      frequentPatterns.insert(p);
+    }
+
+    return frequentPatterns;
+  }
+
+  private static void traverseAndBuildConditionalFPTreeData(int firstConditionalNode,
+                                                            long minSupport,
+                                                            FPTree conditionalTree,
+                                                            FPTree tree) {
+
+    // Build Subtable
+    int conditionalNode = firstConditionalNode;
+
+    while (conditionalNode != -1) {
+      long nextNodeCount = tree.count(conditionalNode);
+      int pathNode = tree.parent(conditionalNode);
+      int prevConditional = -1;
+
+      while (pathNode != 0) { // dummy root node
+        int attribute = tree.attribute(pathNode);
+        if (tree.getHeaderSupportCount(attribute) < minSupport) {
+          pathNode = tree.parent(pathNode);
+          continue;
+        }
+        // update and increment the headerTable Counts
+        conditionalTree.addHeaderCount(attribute, nextNodeCount);
+
+        int conditional = tree.conditional(pathNode);
+        // if its a new conditional tree node
+
+        if (conditional == 0) {
+          tree.setConditional(pathNode, conditionalTree.createConditionalNode(
+            attribute, 0));
+          conditional = tree.conditional(pathNode);
+          conditionalTree.addHeaderNext(attribute, conditional);
+        } else {
+          conditionalTree.setSinglePath(false);
+        }
+
+        if (prevConditional != -1) { // if there is a child element
+          int prevParent = conditionalTree.parent(prevConditional);
+          if (prevParent == -1) {
+            conditionalTree.setParent(prevConditional, conditional);
+          } else if (prevParent != conditional) {
+            throw new IllegalStateException();
+          }
+        }
+
+        conditionalTree.addCount(conditional, nextNodeCount);
+        prevConditional = conditional;
+
+        pathNode = tree.parent(pathNode);
+
+      }
+
+      if (prevConditional != -1) {
+        int prevParent = conditionalTree.parent(prevConditional);
+        if (prevParent == -1) {
+          conditionalTree.setParent(prevConditional, FPTree.ROOTNODEID);
+        } else if (prevParent != FPTree.ROOTNODEID) {
+          throw new IllegalStateException();
+        }
+        if (conditionalTree.childCount(FPTree.ROOTNODEID) > 1 && conditionalTree.singlePath()) {
+          conditionalTree.setSinglePath(false);
+        }
+      }
+      conditionalNode = tree.next(conditionalNode);
+    }
+
+    tree.clearConditional();
+    conditionalTree.reorderHeaderTable();
+    pruneFPTree(minSupport, conditionalTree);
+    // prune Conditional Tree
+
+  }
+
+  private static void pruneFPTree(long minSupport, FPTree tree) {
+    for (int i = 0; i < tree.getHeaderTableCount(); i++) {
+      int currentAttribute = tree.getAttributeAtIndex(i);
+      if (tree.getHeaderSupportCount(currentAttribute) < minSupport) {
+        int nextNode = tree.getHeaderNext(currentAttribute);
+        tree.removeHeaderNext(currentAttribute);
+        while (nextNode != -1) {
+
+          int mychildCount = tree.childCount(nextNode);
+
+          int parentNode = tree.parent(nextNode);
+
+          for (int j = 0; j < mychildCount; j++) {
+            Integer myChildId = tree.childAtIndex(nextNode, j);
+            tree.replaceChild(parentNode, nextNode, myChildId);
+          }
+          nextNode = tree.next(nextNode);
+        }
+
+      }
+    }
+
+    for (int i = 0; i < tree.getHeaderTableCount(); i++) {
+      int currentAttribute = tree.getAttributeAtIndex(i);
+      int nextNode = tree.getHeaderNext(currentAttribute);
+
+      OpenIntIntHashMap prevNode = new OpenIntIntHashMap();
+      int justPrevNode = -1;
+      while (nextNode != -1) {
+
+        int parent = tree.parent(nextNode);
+
+        if (prevNode.containsKey(parent)) {
+          int prevNodeId = prevNode.get(parent);
+          if (tree.childCount(prevNodeId) <= 1 && tree.childCount(nextNode) <= 1) {
+            tree.addCount(prevNodeId, tree.count(nextNode));
+            tree.addCount(nextNode, -1 * tree.count(nextNode));
+            if (tree.childCount(nextNode) == 1) {
+              tree.addChild(prevNodeId, tree.childAtIndex(nextNode, 0));
+              tree.setParent(tree.childAtIndex(nextNode, 0), prevNodeId);
+            }
+            tree.setNext(justPrevNode, tree.next(nextNode));
+          }
+        } else {
+          prevNode.put(parent, nextNode);
+        }
+        justPrevNode = nextNode;
+        nextNode = tree.next(nextNode);
+      }
+    }
+
+    // prune Conditional Tree
+
+  }
+
+  /**
+   * Create FPTree with node counts incremented by addCount variable given the
+   * root node and the List of Attributes in transaction sorted by support
+   *
+   * @param tree
+   *          object to which the transaction has to be added to
+   * @param myList
+   *          List of transactions sorted by support
+   * @param addCount
+   *          amount by which the Node count has to be incremented
+   * @param minSupport
+   *          the MutableLong value which contains the current value(dynamic) of
+   *          support
+   * @param attributeFrequency
+   *          the list of attributes and their frequency
+   * @return the number of new nodes added
+   */
+  private static int treeAddCount(FPTree tree,
+                                  int[] myList,
+                                  long addCount,
+                                  long minSupport,
+                                  long[] attributeFrequency) {
+
+    int temp = FPTree.ROOTNODEID;
+    int ret = 0;
+    boolean addCountMode = true;
+
+    for (int attribute : myList) {
+      if (attributeFrequency[attribute] < minSupport) {
+        return ret;
+      }
+      int child;
+      if (addCountMode) {
+        child = tree.childWithAttribute(temp, attribute);
+        if (child == -1) {
+          addCountMode = false;
+        } else {
+          tree.addCount(child, addCount);
+          temp = child;
+        }
+      }
+      if (!addCountMode) {
+        child = tree.createNode(temp, attribute, addCount);
+        temp = child;
+        ret++;
+      }
+    }
+
+    return ret;
+
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTree.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTree.java
index e69de29b..3664c782 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTree.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTree.java
@@ -0,0 +1,433 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth;
+
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.TreeSet;
+
+/**
+ * The Frequent Pattern Tree datastructure used for mining patterns using
+ * {@link FPGrowth} algorithm
+ * 
+ */
+public class FPTree {
+  
+  public static final int ROOTNODEID = 0;
+  private static final int DEFAULT_CHILDREN_INITIAL_SIZE = 2;
+  private static final int DEFAULT_HEADER_TABLE_INITIAL_SIZE = 4;
+  private static final int DEFAULT_INITIAL_SIZE = 8;
+  private static final float GROWTH_RATE = 1.5f;
+  private static final int HEADERTABLEBLOCKSIZE = 2;
+  private static final int HT_LAST = 1;
+  private static final int HT_NEXT = 0;
+  
+  private int[] attribute;
+  private int[] childCount;
+  private int[] conditional;
+  private long[] headerTableAttributeCount;
+  private int[] headerTableAttributes;
+  private int headerTableCount;
+  private int[] headerTableLookup;
+  private int[][] headerTableProperties;
+  private int[] next;
+  private int[][] nodeChildren;
+  private long[] nodeCount;
+  private int nodes;
+  private int[] parent;
+  private boolean singlePath;
+  private final Collection<Integer> sortedSet = new TreeSet<Integer>();
+  
+  public FPTree() {
+    this(DEFAULT_INITIAL_SIZE);
+  }
+  
+  public FPTree(int size) {
+    if (size < DEFAULT_INITIAL_SIZE) {
+      size = DEFAULT_INITIAL_SIZE;
+    }
+    
+    parent = new int[size];
+    next = new int[size];
+    childCount = new int[size];
+    attribute = new int[size];
+    nodeCount = new long[size];
+    
+    nodeChildren = new int[size][];
+    conditional = new int[size];
+    
+    headerTableAttributes = new int[DEFAULT_HEADER_TABLE_INITIAL_SIZE];
+    headerTableAttributeCount = new long[DEFAULT_HEADER_TABLE_INITIAL_SIZE];
+    headerTableLookup = new int[DEFAULT_HEADER_TABLE_INITIAL_SIZE];
+    Arrays.fill(headerTableLookup, -1);
+    headerTableProperties = new int[DEFAULT_HEADER_TABLE_INITIAL_SIZE][];
+    
+    singlePath = true;
+    createRootNode();
+  }
+  
+  public final void addChild(int parentNodeId, int childnodeId) {
+    int length = childCount[parentNodeId];
+    if (length >= nodeChildren[parentNodeId].length) {
+      resizeChildren(parentNodeId);
+    }
+    nodeChildren[parentNodeId][length++] = childnodeId;
+    childCount[parentNodeId] = length;
+    
+    if (length > 1 && singlePath) {
+      singlePath = false;
+    }
+  }
+  
+  public final void addCount(int nodeId, long count) {
+    if (nodeId < nodes) {
+      this.nodeCount[nodeId] += count;
+    }
+  }
+  
+  public final void addHeaderCount(int attributeValue, long count) {
+    int index = getHeaderIndex(attributeValue);
+    headerTableAttributeCount[index] += count;
+  }
+  
+  public final void addHeaderNext(int attributeValue, int nodeId) {
+    int index = getHeaderIndex(attributeValue);
+    if (headerTableProperties[index][HT_NEXT] == -1) {
+      headerTableProperties[index][HT_NEXT] = nodeId;
+      headerTableProperties[index][HT_LAST] = nodeId;
+    } else {
+      setNext(headerTableProperties[index][HT_LAST], nodeId);
+      headerTableProperties[index][HT_LAST] = nodeId;
+    }
+  }
+  
+  public final int attribute(int nodeId) {
+    return this.attribute[nodeId];
+  }
+  
+  public final int childAtIndex(int nodeId, int index) {
+    if (childCount[nodeId] < index) {
+      return -1;
+    }
+    return nodeChildren[nodeId][index];
+  }
+  
+  public final int childCount(int nodeId) {
+    return childCount[nodeId];
+  }
+  
+  public final int childWithAttribute(int nodeId, int childAttribute) {
+    int length = childCount[nodeId];
+    for (int i = 0; i < length; i++) {
+      if (attribute[nodeChildren[nodeId][i]] == childAttribute) {
+        return nodeChildren[nodeId][i];
+      }
+    }
+    return -1;
+  }
+  
+  public final void clear() {
+    nodes = 0;
+    headerTableCount = 0;
+    singlePath = true;
+    Arrays.fill(headerTableLookup, -1);
+    sortedSet.clear();
+    createRootNode();
+  }
+  
+  public final void clearConditional() {
+    for (int i = nodes - 1; i >= 0; i--) {
+      conditional[i] = 0;
+    }
+  }
+  
+  public final int conditional(int nodeId) {
+    return this.conditional[nodeId];
+  }
+  
+  public final long count(int nodeId) {
+    return nodeCount[nodeId];
+  }
+  
+  public final int createConditionalNode(int attributeValue, long count) {
+    if (nodes >= this.attribute.length) {
+      resize();
+    }
+    childCount[nodes] = 0;
+    next[nodes] = -1;
+    parent[nodes] = -1;
+    conditional[nodes] = 0;
+    this.attribute[nodes] = attributeValue;
+    nodeCount[nodes] = count;
+    
+    if (nodeChildren[nodes] == null) {
+      nodeChildren[nodes] = new int[DEFAULT_CHILDREN_INITIAL_SIZE];
+    }
+    
+    return nodes++;
+  }
+  
+  public final int createNode(int parentNodeId, int attributeValue, long count) {
+    if (nodes >= this.attribute.length) {
+      resize();
+    }
+    
+    childCount[nodes] = 0;
+    next[nodes] = -1;
+    parent[nodes] = parentNodeId;
+    this.attribute[nodes] = attributeValue;
+    nodeCount[nodes] = count;
+    
+    conditional[nodes] = 0;
+    if (nodeChildren[nodes] == null) {
+      nodeChildren[nodes] = new int[DEFAULT_CHILDREN_INITIAL_SIZE];
+    }
+    
+    int childNodeId = nodes++;
+    addChild(parentNodeId, childNodeId);
+    addHeaderNext(attributeValue, childNodeId);
+    return childNodeId;
+  }
+  
+  public final void createRootNode() {
+    childCount[nodes] = 0;
+    next[nodes] = -1;
+    parent[nodes] = 0;
+    attribute[nodes] = -1;
+    nodeCount[nodes] = 0;
+    if (nodeChildren[nodes] == null) {
+      nodeChildren[nodes] = new int[DEFAULT_CHILDREN_INITIAL_SIZE];
+    }
+    nodes++;
+  }
+  
+  public final int getAttributeAtIndex(int index) {
+    return headerTableAttributes[index];
+  }
+  
+  public final int getHeaderNext(int attributeValue) {
+    int index = getHeaderIndex(attributeValue);
+    return headerTableProperties[index][HT_NEXT];
+  }
+  
+  public final long getHeaderSupportCount(int attributeValue) {
+    int index = getHeaderIndex(attributeValue);
+    return headerTableAttributeCount[index];
+  }
+  
+  public final int[] getHeaderTableAttributes() {
+    int[] attributes = new int[headerTableCount];
+    System.arraycopy(headerTableAttributes, 0, attributes, 0, headerTableCount);
+    return attributes;
+  }
+  
+  public final int getHeaderTableCount() {
+    return headerTableCount;
+  }
+  
+  public final boolean isEmpty() {
+    return nodes <= 1;
+  }
+  
+  public final int next(int nodeId) {
+    return next[nodeId];
+  }
+  
+  public final int parent(int nodeId) {
+    return parent[nodeId];
+  }
+  
+  public final void removeHeaderNext(int attributeValue) {
+    int index = getHeaderIndex(attributeValue);
+    headerTableProperties[index][HT_NEXT] = -1;
+  }
+  
+  public final void reorderHeaderTable() {
+    // Arrays.sort(headerTableAttributes, 0, headerTableCount);
+    int i = 0;
+    for (int attr : sortedSet) {
+      headerTableAttributes[i++] = attr;
+    }
+  }
+  
+  public void replaceChild(int parentNodeId, int replacableNode, int childnodeId) {
+    int max = childCount[parentNodeId];
+    for (int i = 0; i < max; i++) {
+      if (nodeChildren[parentNodeId][i] == replacableNode) {
+        nodeChildren[parentNodeId][i] = childnodeId;
+        parent[childnodeId] = parentNodeId;
+      }
+    }
+  }
+  
+  public final void setConditional(int nodeId, int conditionalNode) {
+    if (nodeId < nodes) {
+      this.conditional[nodeId] = conditionalNode;
+    }
+  }
+  
+  public final void setNext(int nodeId, int nextNode) {
+    if (nodeId < nodes) {
+      this.next[nodeId] = nextNode;
+    }
+  }
+  
+  public final void setParent(int nodeId, int parentNode) {
+    if (nodeId < nodes) {
+      this.parent[nodeId] = parentNode;
+      
+      int length = childCount[parentNode];
+      if (length >= nodeChildren[parentNode].length) {
+        resizeChildren(parentNode);
+      }
+      nodeChildren[parentNode][length++] = nodeId;
+      childCount[parentNode] = length;
+    }
+  }
+  
+  public final void setSinglePath(boolean bit) {
+    singlePath = bit;
+  }
+  
+  public final boolean singlePath() {
+    return singlePath;
+  }
+  
+  private int getHeaderIndex(int attributeValue) {
+    if (attributeValue >= headerTableLookup.length) {
+      resizeHeaderLookup(attributeValue);
+    }
+    int index = headerTableLookup[attributeValue];
+    if (index == -1) { // if attribute didnt exist;
+      if (headerTableCount >= headerTableAttributes.length) {
+        resizeHeaderTable();
+      }
+      headerTableAttributes[headerTableCount] = attributeValue;
+      if (headerTableProperties[headerTableCount] == null) {
+        headerTableProperties[headerTableCount] = new int[HEADERTABLEBLOCKSIZE];
+      }
+      headerTableAttributeCount[headerTableCount] = 0;
+      headerTableProperties[headerTableCount][HT_NEXT] = -1;
+      headerTableProperties[headerTableCount][HT_LAST] = -1;
+      index = headerTableCount++;
+      headerTableLookup[attributeValue] = index;
+      sortedSet.add(attributeValue);
+    }
+    return index;
+  }
+  
+  private void resize() {
+    int size = (int) (GROWTH_RATE * nodes);
+    if (size < DEFAULT_INITIAL_SIZE) {
+      size = DEFAULT_INITIAL_SIZE;
+    }
+    
+    int[] oldChildCount = childCount;
+    int[] oldAttribute = attribute;
+    long[] oldnodeCount = nodeCount;
+    int[] oldParent = parent;
+    int[] oldNext = next;
+    int[][] oldNodeChildren = nodeChildren;
+    int[] oldConditional = conditional;
+    
+    childCount = new int[size];
+    attribute = new int[size];
+    nodeCount = new long[size];
+    parent = new int[size];
+    next = new int[size];
+    
+    nodeChildren = new int[size][];
+    conditional = new int[size];
+    
+    System.arraycopy(oldChildCount, 0, this.childCount, 0, nodes);
+    System.arraycopy(oldAttribute, 0, this.attribute, 0, nodes);
+    System.arraycopy(oldnodeCount, 0, this.nodeCount, 0, nodes);
+    System.arraycopy(oldParent, 0, this.parent, 0, nodes);
+    System.arraycopy(oldNext, 0, this.next, 0, nodes);
+    System.arraycopy(oldNodeChildren, 0, this.nodeChildren, 0, nodes);
+    System.arraycopy(oldConditional, 0, this.conditional, 0, nodes);
+  }
+  
+  private void resizeChildren(int nodeId) {
+    int length = childCount[nodeId];
+    int size = (int) (GROWTH_RATE * length);
+    if (size < DEFAULT_CHILDREN_INITIAL_SIZE) {
+      size = DEFAULT_CHILDREN_INITIAL_SIZE;
+    }
+    int[] oldNodeChildren = nodeChildren[nodeId];
+    nodeChildren[nodeId] = new int[size];
+    System.arraycopy(oldNodeChildren, 0, this.nodeChildren[nodeId], 0, length);
+  }
+  
+  private void resizeHeaderLookup(int attributeValue) {
+    int size = (int) (attributeValue * GROWTH_RATE);
+    int[] oldLookup = headerTableLookup;
+    headerTableLookup = new int[size];
+    Arrays.fill(headerTableLookup, oldLookup.length, size, -1);
+    System.arraycopy(oldLookup, 0, this.headerTableLookup, 0, oldLookup.length);
+  }
+  
+  private void resizeHeaderTable() {
+    int size = (int) (GROWTH_RATE * headerTableCount);
+    if (size < DEFAULT_HEADER_TABLE_INITIAL_SIZE) {
+      size = DEFAULT_HEADER_TABLE_INITIAL_SIZE;
+    }
+    
+    int[] oldAttributes = headerTableAttributes;
+    long[] oldAttributeCount = headerTableAttributeCount;
+    int[][] oldProperties = headerTableProperties;
+    headerTableAttributes = new int[size];
+    headerTableAttributeCount = new long[size];
+    headerTableProperties = new int[size][];
+    System.arraycopy(oldAttributes, 0, this.headerTableAttributes, 0,
+      headerTableCount);
+    System.arraycopy(oldAttributeCount, 0, this.headerTableAttributeCount, 0,
+      headerTableCount);
+    System.arraycopy(oldProperties, 0, this.headerTableProperties, 0,
+      headerTableCount);
+  }
+
+  private void toStringHelper(StringBuilder sb, int currNode, String prefix) {
+    if (childCount[currNode] == 0) {
+      sb.append(prefix).append("-{attr:").append(attribute[currNode])
+        .append(", id: ").append(currNode)
+        .append(", cnt:").append(nodeCount[currNode]).append("}\n");
+    } else {
+      StringBuilder newPre = new StringBuilder(prefix);
+      newPre.append("-{attr:").append(attribute[currNode])
+        .append(", id: ").append(currNode)
+        .append(", cnt:").append(nodeCount[currNode]).append('}');
+      StringBuilder fakePre = new StringBuilder();
+      while (fakePre.length() < newPre.length()) {
+        fakePre.append(' ');
+      }
+      for (int i = 0; i < childCount[currNode]; i++) {
+        toStringHelper(sb, nodeChildren[currNode][i], (i == 0 ? newPre : fakePre).toString() + '-' + i + "->");
+      }
+    }
+  }
+  
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder("[FPTree\n");
+    toStringHelper(sb, 0, "  ");
+    sb.append("\n]\n");
+    return sb.toString();
+  }
+
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTreeDepthCache.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTreeDepthCache.java
index e69de29b..d13138ed 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTreeDepthCache.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTreeDepthCache.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth;
+
+import com.google.common.collect.Lists;
+
+import java.util.List;
+
+/**
+ * Caches large FPTree {@link Object} for each level of the recursive
+ * {@link FPGrowth} algorithm to reduce allocation overhead.
+ */
+public class FPTreeDepthCache {
+
+  private final LeastKCache<Integer,FPTree> firstLevelCache = new LeastKCache<Integer,FPTree>(5);
+  private int hits;
+  private int misses;
+  private final List<FPTree> treeCache = Lists.newArrayList();
+  
+  public final FPTree getFirstLevelTree(Integer attr) {
+    FPTree tree = firstLevelCache.get(attr);
+    if (tree != null) {
+      hits++;
+      return tree;
+    } else {
+      misses++;
+      FPTree conditionalTree = new FPTree();
+      firstLevelCache.set(attr, conditionalTree);
+      return conditionalTree;
+    }
+  }
+  
+  public final int getHits() {
+    return hits;
+  }
+  
+  public final int getMisses() {
+    return misses;
+  }
+  
+  public final FPTree getTree(int level) {
+    while (treeCache.size() < level + 1) {
+      FPTree cTree = new FPTree();
+      treeCache.add(cTree);
+    }
+    FPTree conditionalTree = treeCache.get(level);
+    conditionalTree.clear();
+    return conditionalTree;
+  }
+  
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java
index e69de29b..9b8a494d 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java
@@ -0,0 +1,176 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth;
+
+import java.util.PriorityQueue;
+import java.util.Set;
+
+import com.google.common.collect.Sets;
+import org.apache.mahout.math.map.OpenLongObjectHashMap;
+
+/**  keeps top K Attributes in a TreeSet */
+public final class FrequentPatternMaxHeap {
+  
+  private int count;
+  private Pattern least;
+  private final int maxSize;
+  private final boolean subPatternCheck;
+  private final OpenLongObjectHashMap<Set<Pattern>> patternIndex;
+  private final PriorityQueue<Pattern> queue;
+  
+  public FrequentPatternMaxHeap(int numResults, boolean subPatternCheck) {
+    maxSize = numResults;
+    queue = new PriorityQueue<Pattern>(maxSize);
+    this.subPatternCheck = subPatternCheck;
+    patternIndex = new OpenLongObjectHashMap<Set<Pattern>>();
+    for (Pattern p : queue) {
+      Long index = p.support();
+      Set<Pattern> patternList;
+      if (!patternIndex.containsKey(index)) {
+        patternList = Sets.newHashSet();
+        patternIndex.put(index, patternList);
+      }
+      patternList = patternIndex.get(index);
+      patternList.add(p);
+      
+    }
+  }
+  
+  public boolean addable(long support) {
+    return count < maxSize || least.support() <= support;
+  }
+  
+  public PriorityQueue<Pattern> getHeap() {
+    if (subPatternCheck) {
+      PriorityQueue<Pattern> ret = new PriorityQueue<Pattern>(maxSize);
+      for (Pattern p : queue) {
+        if (patternIndex.get(p.support()).contains(p)) {
+          ret.add(p);
+        }
+      }
+      return ret;
+    }
+    return queue;
+  }
+  
+  public void addAll(FrequentPatternMaxHeap patterns,
+                     int attribute,
+                     long attributeSupport) {
+    for (Pattern pattern : patterns.getHeap()) {
+      long support = Math.min(attributeSupport, pattern.support());
+      if (this.addable(support)) {
+        pattern.add(attribute, support);
+        this.insert(pattern);
+      }
+    }
+  }
+  
+  public void insert(Pattern frequentPattern) {
+    if (frequentPattern.length() == 0) {
+      return;
+    }
+    
+    if (count == maxSize) {
+      if (frequentPattern.compareTo(least) > 0 && addPattern(frequentPattern)) {
+        Pattern evictedItem = queue.poll();
+        least = queue.peek();
+        if (subPatternCheck) {
+          patternIndex.get(evictedItem.support()).remove(evictedItem);
+        }
+      }
+    } else {
+      if (addPattern(frequentPattern)) {
+        count++;
+        if (least == null) {
+          least = frequentPattern;
+        } else {
+          if (least.compareTo(frequentPattern) < 0) {
+            least = frequentPattern;
+          }
+        }
+      }
+    }
+  }
+  
+  public int count() {
+    return count;
+  }
+  
+  public boolean isFull() {
+    return count == maxSize;
+  }
+  
+  public long leastSupport() {
+    if (least == null) {
+      return 0;
+    }
+    return least.support();
+  }
+  
+  private boolean addPattern(Pattern frequentPattern) {
+    if (subPatternCheck) {
+      Long index = frequentPattern.support();
+      if (patternIndex.containsKey(index)) {
+        Set<Pattern> indexSet = patternIndex.get(index);
+        boolean replace = false;
+        Pattern replacablePattern = null;
+        for (Pattern p : indexSet) {
+          if (frequentPattern.isSubPatternOf(p)) {
+            return false;
+          } else if (p.isSubPatternOf(frequentPattern)) {
+            replace = true;
+            replacablePattern = p;
+            break;
+          }
+        }
+        if (replace) {
+          indexSet.remove(replacablePattern);
+          if (!indexSet.contains(frequentPattern) && queue.add(frequentPattern)) {
+            indexSet.add(frequentPattern);
+          }
+          return false;
+        }
+        queue.add(frequentPattern);
+        indexSet.add(frequentPattern);
+      } else {
+        queue.add(frequentPattern);
+        Set<Pattern> patternList;
+        if (!patternIndex.containsKey(index)) {
+          patternList = Sets.newHashSet();
+          patternIndex.put(index, patternList);
+        }
+        patternList = patternIndex.get(index);
+        patternList.add(frequentPattern);
+      }
+    } else {
+      queue.add(frequentPattern);
+    }
+    return true;
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder("FreqPatHeap{");
+    String sep = "";
+    for (Pattern p : getHeap()) {
+      sb.append(sep).append(p);
+      sep = ", ";
+    }
+    return sb.toString();
+  }
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/LeastKCache.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/LeastKCache.java
index e69de29b..c792393a 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/LeastKCache.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/LeastKCache.java
@@ -0,0 +1,57 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth;
+
+import com.google.common.collect.Maps;
+
+import java.util.Collections;
+import java.util.Map;
+import java.util.PriorityQueue;
+
+public class LeastKCache<K extends Comparable<? super K>,V> {
+  
+  private final int capacity;
+  private final Map<K,V> cache;
+  private final PriorityQueue<K> queue;
+  
+  public LeastKCache(int capacity) {
+    this.capacity = capacity;
+    cache = Maps.newHashMapWithExpectedSize(capacity);
+    queue = new PriorityQueue<K>(capacity + 1, Collections.reverseOrder());
+  }
+
+  public final V get(K key) {
+    return cache.get(key);
+  }
+  
+  public final void set(K key, V value) {
+    if (!contains(key)) {
+      queue.add(key);
+    }
+    cache.put(key, value);
+    while (queue.size() > capacity) {
+      K k = queue.poll();
+      cache.remove(k);
+    }
+  }
+  
+  public final boolean contains(K key) {
+    return cache.containsKey(key);
+  }
+  
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java
index e69de29b..64938ad7 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java
@@ -0,0 +1,162 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth;
+
+import com.google.common.primitives.Longs;
+
+import java.util.Arrays;
+
+/**
+ * A  in FPGrowth is a list of items (here int) and the
+ * support(the number of times the pattern is seen in the dataset)
+ * 
+ */
+public class Pattern implements Comparable<Pattern> {
+  
+  private static final int DEFAULT_INITIAL_SIZE = 2;
+  
+  private static final float GROWTH_RATE = 1.5f;
+  
+  private boolean dirty = true;
+  
+  private int hashCode;
+  
+  private int length;
+  
+  private int[] pattern;
+  
+  private long support = Long.MAX_VALUE;
+  
+  public Pattern() {
+    this(DEFAULT_INITIAL_SIZE);
+  }
+  
+  private Pattern(int size) {
+    if (size < DEFAULT_INITIAL_SIZE) {
+      size = DEFAULT_INITIAL_SIZE;
+    }
+    this.pattern = new int[size];
+    dirty = true;
+  }
+  
+  public final void add(int id, long supportCount) {
+    dirty = true;
+    if (length >= pattern.length) {
+      resize();
+    }
+    this.pattern[length++] = id;
+    Arrays.sort(this.pattern, 0, length);
+    this.support = supportCount > this.support ? this.support : supportCount;
+  }
+
+  public final int[] getPattern() {
+    return this.pattern;
+  }
+
+  public final boolean isSubPatternOf(Pattern frequentPattern) {
+    int[] otherPattern = frequentPattern.getPattern();
+    int otherLength = frequentPattern.length();
+    if (this.length() > frequentPattern.length()) {
+      return false;
+    }
+    int i = 0;
+    int otherI = 0;
+    while (i < length && otherI < otherLength) {
+      if (otherPattern[otherI] == pattern[i]) {
+        otherI++;
+        i++;
+      } else if (otherPattern[otherI] < pattern[i]) {
+        otherI++;
+      } else {
+        return false;
+      }
+    }
+    return otherI != otherLength || i == length;
+  }
+
+  public final int length() {
+    return this.length;
+  }
+
+  public final long support() {
+    return this.support;
+  }
+
+  private void resize() {
+    int size = (int) (GROWTH_RATE * length);
+    if (size < DEFAULT_INITIAL_SIZE) {
+      size = DEFAULT_INITIAL_SIZE;
+    }
+    int[] oldpattern = pattern;
+    this.pattern = new int[size];
+    System.arraycopy(oldpattern, 0, this.pattern, 0, length);
+  }
+  
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) {
+      return true;
+    }
+    if (obj == null) {
+      return false;
+    }
+    if (getClass() != obj.getClass()) {
+      return false;
+    }
+    Pattern other = (Pattern) obj;
+    // expensive check done only if length and support matches    
+    return length == other.length && support == other.support && Arrays.equals(pattern, other.pattern);
+  }
+  
+  @Override
+  public int hashCode() {
+    if (!dirty) {
+      return hashCode;
+    }
+    int result = Arrays.hashCode(pattern);
+    result = 31 * result + Longs.hashCode(support);
+    result = 31 * result + length;
+    hashCode = result;
+    return result;
+  }
+  
+  @Override
+  public final String toString() {
+    int[] arr = new int[length];
+    System.arraycopy(pattern, 0, arr, 0, length);
+    return Arrays.toString(arr) + '-' + support;
+  }
+  
+  @Override
+  public int compareTo(Pattern cr2) {
+    long support2 = cr2.support();
+    int length2 = cr2.length();
+    if (support == support2) {
+      if (length < length2) {
+        return -1;
+      } else if (length > length2) {
+        return 1;
+      } else {
+        return 0;
+      }
+    } else {
+      return support > support2 ? 1 : -1;
+    }
+  }
+  
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java
index e69de29b..171baf7a 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java
@@ -0,0 +1,319 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth2;
+
+import java.io.IOException;
+import java.util.AbstractMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+
+import org.apache.commons.lang3.mutable.MutableLong;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.fpm.pfpgrowth.convertors.TopKPatternsOutputConverter;
+import org.apache.mahout.math.list.LongArrayList;
+import org.apache.mahout.math.list.IntArrayList;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import  org.apache.mahout.fpm.pfpgrowth.fpgrowth.Pattern;
+import  org.apache.mahout.fpm.pfpgrowth.fpgrowth.FrequentPatternMaxHeap;
+/**
+ * Implementation of PFGrowth Algorithm
+ */
+public final class FPGrowthIds {
+
+  private static final Logger log = LoggerFactory.getLogger(FPGrowthIds.class);
+
+  private FPGrowthIds() {
+  }
+
+ /**
+   * Generate Top K Frequent Patterns for every feature in returnableFeatures
+   * given a stream of transactions and the minimum support
+   *
+   *
+  * @param transactionStream
+  *          Iterator of transaction
+  * @param attributeFrequency
+  *          list of frequent features and their support value
+  * @param minSupport
+  *          minimum support of the transactions
+  * @param k
+  *          Number of top frequent patterns to keep
+  * @param returnableFeatures
+  *          set of features for which the frequent patterns are mined. If the
+  *          set is empty or null, then top K patterns for every frequent item (an item
+  *          whose support> minSupport) is generated
+  * @param output
+  *          The output collector to which the the generated patterns are
+  *          written
+  * @throws IOException
+   */
+  public static void generateTopKFrequentPatterns(Iterator<Pair<IntArrayList, Long>> transactionStream,
+                                                  LongArrayList attributeFrequency,
+                                                  long minSupport,
+                                                  int k,
+                                                  IntArrayList returnableFeatures,
+                                                  OutputCollector<Integer, List<Pair<List<Integer>, Long>>> output) throws IOException {
+
+    for (int i = 0; i < attributeFrequency.size(); i++) {
+      if (attributeFrequency.get(i) < minSupport) {
+        attributeFrequency.setSize(i);
+        attributeFrequency.trimToSize();
+        break;
+      }
+    }
+
+    log.info("Number of unique items {}", attributeFrequency.size());
+
+    if (returnableFeatures == null || returnableFeatures.isEmpty()) {
+      returnableFeatures = new IntArrayList();
+      for (int j = 0; j < attributeFrequency.size(); j++) {
+        returnableFeatures.add(j);
+      }
+    }
+
+    log.info("Number of unique pruned items {}", attributeFrequency.size());
+    generateTopKFrequentPatterns(transactionStream, attributeFrequency,
+        minSupport, k, returnableFeatures,
+        new TopKPatternsOutputConverter<Integer>(output, new IdentityMapping()));
+  }
+
+  private static class IdentityMapping extends AbstractMap<Integer, Integer> {
+
+    @Override
+    public Set<Map.Entry<Integer,Integer>> entrySet() {
+      throw new IllegalStateException();
+    }
+
+    @Override
+    public Integer get(Object key) {
+      return (Integer) key;
+    }
+
+  }
+
+  /**
+   * Top K FpGrowth Algorithm
+   *
+   *
+   * @param tree
+   *          to be mined
+   * @param minSupportValue
+   *          minimum support of the pattern to keep
+   * @param k
+   *          Number of top frequent patterns to keep
+   * @param requiredFeatures
+   *          Set of integer id's of features to mine
+   * @param outputCollector
+   *          the Collector class which converts the given frequent pattern in
+   *          integer to A
+   * @return Top K Frequent Patterns for each feature and their support
+   */
+  private static Map<Integer,FrequentPatternMaxHeap> fpGrowth(FPTree tree,
+                                                              long minSupportValue,
+                                                              int k,
+                                                              IntArrayList requiredFeatures,
+                                                              TopKPatternsOutputConverter<Integer> outputCollector) throws IOException {
+
+    Map<Integer,FrequentPatternMaxHeap> patterns = Maps.newHashMap();
+    requiredFeatures.sort();
+    for (int attribute : tree.attrIterableRev()) {
+      if (requiredFeatures.binarySearch(attribute) >= 0) {
+        log.info("Mining FTree Tree for all patterns with {}", attribute);
+        MutableLong minSupport = new MutableLong(minSupportValue);
+        FrequentPatternMaxHeap frequentPatterns = growth(tree, minSupport, k,
+                                                         attribute);
+        patterns.put(attribute, frequentPatterns);
+        outputCollector.collect(attribute, frequentPatterns);
+
+        minSupportValue = Math.max(minSupportValue, minSupport.longValue() / 2);
+        log.info("Found {} Patterns with Least Support {}", patterns.get(
+            attribute).count(), patterns.get(attribute).leastSupport());
+      }
+    }
+    return patterns;
+  }
+
+      
+
+  /**
+   * Internal TopKFrequentPattern Generation algorithm, which represents the A's
+   * as integers and transforms features to use only integers
+   *
+   * @param transactions
+   *          Transaction database Iterator
+   * @param attributeFrequency
+   *          array representing the Frequency of the corresponding attribute id
+   * @param minSupport
+ *          minimum support of the pattern to be mined
+   * @param k
+*          Max value of the Size of the Max-Heap in which Patterns are held
+   * @param returnFeatures
+*          the id's of the features for which Top K patterns have to be mined
+   * @param topKPatternsOutputCollector
+*          the outputCollector which transforms the given Pattern in integer
+   */
+  private static void generateTopKFrequentPatterns(
+      Iterator<Pair<IntArrayList, Long>> transactions,
+      LongArrayList attributeFrequency,
+      long minSupport,
+      int k,
+      IntArrayList returnFeatures,
+      TopKPatternsOutputConverter<Integer> topKPatternsOutputCollector) throws IOException {
+
+    FPTree tree = new FPTree(attributeFrequency, minSupport);
+
+    // Constructing initial FPTree from the list of transactions
+    int i = 0;
+    while (transactions.hasNext()) {
+      Pair<IntArrayList,Long> transaction = transactions.next();
+      IntArrayList iArr = transaction.getFirst();
+      tree.accumulate(iArr, transaction.getSecond());
+      i++;
+      if (i % 10000 == 0) {
+        log.info("FPTree Building: Read {} Transactions", i);
+      }
+    }
+
+    fpGrowth(tree, minSupport, k, returnFeatures, topKPatternsOutputCollector);
+  }
+
+  /** 
+   * Run FP Growth recursively on tree, for the given target attribute
+   */
+  private static FrequentPatternMaxHeap growth(FPTree tree,
+                                               MutableLong minSupportMutable,
+                                               int k,
+                                               int currentAttribute) {
+
+    long currentAttributeCount = tree.headerCount(currentAttribute);
+
+    if (currentAttributeCount < minSupportMutable.longValue()) {
+      return new FrequentPatternMaxHeap(k, true);
+    }
+ 
+    FPTree condTree = tree.createMoreFreqConditionalTree(currentAttribute);
+
+    Pair<FPTree, FPTree> pAndQ = condTree.splitSinglePrefix();
+    FPTree p = pAndQ.getFirst();
+    FPTree q = pAndQ.getSecond();
+
+    FrequentPatternMaxHeap prefixPats = null;
+    if (p != null) {
+      prefixPats = mineSinglePrefix(p, k);
+    }
+
+    FrequentPatternMaxHeap suffixPats = new FrequentPatternMaxHeap(k, true);
+
+    Pattern thisPat = new Pattern();
+    thisPat.add(currentAttribute, currentAttributeCount);
+    suffixPats.insert(thisPat);
+
+    for (int attr : q.attrIterableRev())  {
+      mergeHeap(suffixPats,
+                growth(q, minSupportMutable, k, attr),
+                currentAttribute,
+                currentAttributeCount, true);
+    }
+
+    if (prefixPats != null) {
+      return cross(prefixPats, suffixPats, k);
+    }
+
+    return suffixPats;
+  }
+
+
+  /** 
+   * Return a set patterns which are the cross product of the patterns
+   * in pPats and qPats.  
+   */
+  private static FrequentPatternMaxHeap cross(FrequentPatternMaxHeap pPats, 
+                                              FrequentPatternMaxHeap qPats,
+                                              int k) {
+    FrequentPatternMaxHeap pats = new FrequentPatternMaxHeap(k, true);
+
+    for (Pattern p : pPats.getHeap()) {
+      int[] pints = p.getPattern();
+      for (Pattern q : qPats.getHeap()) {
+        int[] qints = q.getPattern();
+        
+        Pattern pq = new Pattern();
+        for (int pi = 0; pi < p.length(); pi++) {
+          pq.add(pints[pi], p.support());
+        }
+        for (int qi = 0; qi < q.length(); qi++) {
+          pq.add(qints[qi], q.support());
+        }
+        pats.insert(pq);
+      }
+    }
+
+    for (Pattern q : qPats.getHeap()) {
+      Pattern qq = new Pattern();
+      int[] qints = q.getPattern();
+      for (int qi = 0; qi < q.length(); qi++) {
+        qq.add(qints[qi], q.support());
+      }
+      pats.insert(qq);
+    }
+
+    return pats;
+  }
+
+  /**
+   * Mine all frequent patterns that can be created by following a prefix
+   * that is common to all sets in the given tree.
+   */
+  private static FrequentPatternMaxHeap mineSinglePrefix(FPTree tree, int k) {
+    FrequentPatternMaxHeap pats = new FrequentPatternMaxHeap(k, true);
+    FPTree.FPNode currNode = tree.root();
+    while (currNode.numChildren() == 1) {
+      currNode = currNode.children().iterator().next();
+      FrequentPatternMaxHeap singlePat = new FrequentPatternMaxHeap(k, true);
+      Pattern p = new Pattern();
+      p.add(currNode.attribute(), currNode.count());
+      singlePat.insert(p);
+      pats = cross(singlePat, pats, k);
+      pats.insert(p);
+    }
+
+    return pats;
+  }
+
+  private static void mergeHeap(FrequentPatternMaxHeap frequentPatterns,
+                                FrequentPatternMaxHeap returnedPatterns,
+                                int attribute,
+                                long count,
+                                boolean addAttribute) {
+    frequentPatterns.addAll(returnedPatterns, attribute, count);
+    if (frequentPatterns.addable(count) && addAttribute) {
+      Pattern p = new Pattern();
+      p.add(attribute, count);
+      frequentPatterns.insert(p);
+    }
+  }
+}
+
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java
index e69de29b..6dece3a6 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java
@@ -0,0 +1,380 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth2;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.commons.lang3.mutable.MutableLong;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
+import org.apache.mahout.fpm.pfpgrowth.CountDescendingPairComparator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.TopKPatternsOutputConverter;
+import org.apache.mahout.fpm.pfpgrowth.convertors.TransactionIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import  org.apache.mahout.fpm.pfpgrowth.fpgrowth.Pattern;
+import  org.apache.mahout.fpm.pfpgrowth.fpgrowth.FrequentPatternMaxHeap;
+/**
+ * Implementation of PFGrowth Algorithm
+ *
+ * @param <A> object type used as the cell items in a transaction list
+ */
+public class FPGrowthObj<A extends Comparable<? super A>> {
+
+  private static final Logger log = LoggerFactory.getLogger(FPGrowthObj.class);
+
+  public static List<Pair<String,TopKStringPatterns>> readFrequentPattern(Configuration conf, Path path) {
+    List<Pair<String,TopKStringPatterns>> ret = Lists.newArrayList();
+    // key is feature value is count
+    for (Pair<Writable,TopKStringPatterns> record
+         : new SequenceFileIterable<Writable,TopKStringPatterns>(path, true, conf)) {
+      ret.add(new Pair<String,TopKStringPatterns>(record.getFirst().toString(),
+                                                  new TopKStringPatterns(record.getSecond().getPatterns())));
+    }
+    return ret;
+  }
+
+  /**
+   * Generate the Feature Frequency list from the given transaction whose
+   * frequency > minSupport
+   *
+   * @param transactions
+   *          Iterator over the transaction database
+   * @param minSupport
+   *          minSupport of the feature to be included
+   * @return the List of features and their associated frequency as a Pair
+   */
+  public final List<Pair<A,Long>> generateFList(Iterator<Pair<List<A>,Long>> transactions, int minSupport) {
+
+    Map<A,MutableLong> attributeSupport = Maps.newHashMap();
+    while (transactions.hasNext()) {
+      Pair<List<A>,Long> transaction = transactions.next();
+      for (A attribute : transaction.getFirst()) {
+        if (attributeSupport.containsKey(attribute)) {
+          attributeSupport.get(attribute).add(transaction.getSecond().longValue());
+        } else {
+          attributeSupport.put(attribute, new MutableLong(transaction.getSecond()));
+        }
+      }
+    }
+    List<Pair<A,Long>> fList = Lists.newArrayList();
+    for (Entry<A,MutableLong> e : attributeSupport.entrySet()) {
+      long value = e.getValue().longValue();
+      if (value >= minSupport) {
+        fList.add(new Pair<A,Long>(e.getKey(), value));
+      }
+    }
+
+    Collections.sort(fList, new CountDescendingPairComparator<A,Long>());
+
+    return fList;
+  }
+
+ /**
+   * Generate Top K Frequent Patterns for every feature in returnableFeatures
+   * given a stream of transactions and the minimum support
+   *
+   *
+  * @param transactionStream
+  *          Iterator of transaction
+  * @param frequencyList
+  *          list of frequent features and their support value
+  * @param minSupport
+  *          minimum support of the transactions
+  * @param k
+  *          Number of top frequent patterns to keep
+  * @param returnableFeatures
+  *          set of features for which the frequent patterns are mined. If the
+  *          set is empty or null, then top K patterns for every frequent item (an item
+  *          whose support> minSupport) is generated
+  * @param output
+  *          The output collector to which the the generated patterns are
+  *          written
+  * @throws IOException
+   */
+  public final void generateTopKFrequentPatterns(Iterator<Pair<List<A>, Long>> transactionStream,
+                                                 Collection<Pair<A, Long>> frequencyList,
+                                                 long minSupport,
+                                                 int k,
+                                                 Collection<A> returnableFeatures,
+                                                 OutputCollector<A, List<Pair<List<A>, Long>>> output) throws IOException {
+
+    Map<Integer,A> reverseMapping = Maps.newHashMap();
+    Map<A,Integer> attributeIdMapping = Maps.newHashMap();
+
+    int id = 0;
+    for (Pair<A,Long> feature : frequencyList) {
+      A attrib = feature.getFirst();
+      Long frequency = feature.getSecond();
+      if (frequency >= minSupport) {
+        attributeIdMapping.put(attrib, id);
+        reverseMapping.put(id++, attrib);
+      }
+    }
+
+    long[] attributeFrequency = new long[attributeIdMapping.size()];
+    for (Pair<A,Long> feature : frequencyList) {
+      A attrib = feature.getFirst();
+      Long frequency = feature.getSecond();
+      if (frequency < minSupport) {
+        break;
+      }
+      attributeFrequency[attributeIdMapping.get(attrib)] = frequency;
+    }
+
+    log.info("Number of unique items {}", frequencyList.size());
+
+    Collection<Integer> returnFeatures = Sets.newHashSet();
+    if (returnableFeatures != null && !returnableFeatures.isEmpty()) {
+      for (A attrib : returnableFeatures) {
+        if (attributeIdMapping.containsKey(attrib)) {
+          returnFeatures.add(attributeIdMapping.get(attrib));
+          log.info("Adding Pattern {}=>{}", attrib, attributeIdMapping
+            .get(attrib));
+        }
+      }
+    } else {
+      for (int j = 0; j < attributeIdMapping.size(); j++) {
+        returnFeatures.add(j);
+      }
+    }
+
+    log.info("Number of unique pruned items {}", attributeIdMapping.size());
+    generateTopKFrequentPatterns(new TransactionIterator<A>(transactionStream,
+        attributeIdMapping), attributeFrequency, minSupport, k, 
+        returnFeatures, new TopKPatternsOutputConverter<A>(output,
+            reverseMapping));
+  }
+
+  /**
+   * Top K FpGrowth Algorithm
+   *
+   *
+   * @param tree
+   *          to be mined
+   * @param minSupportValue
+   *          minimum support of the pattern to keep
+   * @param k
+   *          Number of top frequent patterns to keep
+   * @param requiredFeatures
+   *          Set of integer id's of features to mine
+   * @param outputCollector
+   *          the Collector class which converts the given frequent pattern in
+   *          integer to A
+   * @return Top K Frequent Patterns for each feature and their support
+   */
+  private Map<Integer,FrequentPatternMaxHeap> fpGrowth(FPTree tree,
+                                                       long minSupportValue,
+                                                       int k,
+                                                       Collection<Integer> requiredFeatures,
+                                                       TopKPatternsOutputConverter<A> outputCollector) throws IOException {
+
+    Map<Integer,FrequentPatternMaxHeap> patterns = Maps.newHashMap();
+    for (int attribute : tree.attrIterableRev()) {
+      if (requiredFeatures.contains(attribute)) {
+        log.info("Mining FTree Tree for all patterns with {}", attribute);
+        MutableLong minSupport = new MutableLong(minSupportValue);
+        FrequentPatternMaxHeap frequentPatterns = growth(tree, minSupport, k,
+                                                         attribute);
+        patterns.put(attribute, frequentPatterns);
+        outputCollector.collect(attribute, frequentPatterns);
+
+        minSupportValue = Math.max(minSupportValue, minSupport.longValue() / 2);
+        log.info("Found {} Patterns with Least Support {}", patterns.get(
+            attribute).count(), patterns.get(attribute).leastSupport());
+      }
+    }
+    return patterns;
+  }
+
+  /**
+   * Internal TopKFrequentPattern Generation algorithm, which represents the A's
+   * as integers and transforms features to use only integers
+   *
+   * @param transactions
+   *          Transaction database Iterator
+   * @param attributeFrequency
+   *          array representing the Frequency of the corresponding attribute id
+   * @param minSupport
+ *          minimum support of the pattern to be mined
+   * @param k
+*          Max value of the Size of the Max-Heap in which Patterns are held
+   * @param returnFeatures
+*          the id's of the features for which Top K patterns have to be mined
+   * @param topKPatternsOutputCollector
+*          the outputCollector which transforms the given Pattern in integer
+   */
+  private void generateTopKFrequentPatterns(
+      Iterator<Pair<int[], Long>> transactions,
+      long[] attributeFrequency,
+      long minSupport,
+      int k,
+      Collection<Integer> returnFeatures, TopKPatternsOutputConverter<A> topKPatternsOutputCollector) throws IOException {
+
+    FPTree tree = new FPTree(attributeFrequency, minSupport);
+
+    // Constructing initial FPTree from the list of transactions
+    int i = 0;
+    while (transactions.hasNext()) {
+      Pair<int[],Long> transaction = transactions.next();
+      List<Integer> iLst = Lists.newArrayList();
+      int[] iArr = transaction.getFirst();
+      for (int anIArr : iArr) {
+        iLst.add(anIArr);
+      }
+      tree.accumulate(iLst, transaction.getSecond());
+      i++;
+      if (i % 10000 == 0) {
+        log.info("FPTree Building: Read {} Transactions", i);
+      }
+    }
+
+    fpGrowth(tree, minSupport, k, returnFeatures, topKPatternsOutputCollector);
+  }
+
+  /** 
+   * Run FP Growth recursively on tree, for the given target attribute
+   */
+  private static FrequentPatternMaxHeap growth(FPTree tree,
+                                               MutableLong minSupportMutable,
+                                               int k,
+                                               int currentAttribute) {
+
+    long currentAttributeCount = tree.headerCount(currentAttribute);
+
+    if (currentAttributeCount < minSupportMutable.longValue()) {
+      return new FrequentPatternMaxHeap(k, true);
+    }
+ 
+    FPTree condTree = tree.createMoreFreqConditionalTree(currentAttribute);
+
+    Pair<FPTree, FPTree> pAndQ = condTree.splitSinglePrefix();
+    FPTree p = pAndQ.getFirst();
+    FPTree q = pAndQ.getSecond();
+
+    FrequentPatternMaxHeap prefixPats = null;
+    if (p != null) {
+      prefixPats = mineSinglePrefix(p, k);
+    }
+
+    FrequentPatternMaxHeap suffixPats = new FrequentPatternMaxHeap(k, true);
+
+    Pattern thisPat = new Pattern();
+    thisPat.add(currentAttribute, currentAttributeCount);
+    suffixPats.insert(thisPat);
+
+    for (int attr : q.attrIterableRev())  {
+      mergeHeap(suffixPats,
+                growth(q, minSupportMutable, k, attr),
+                currentAttribute,
+                currentAttributeCount, true);
+    }
+
+    if (prefixPats != null) {
+      return cross(prefixPats, suffixPats, k);
+    }
+
+    return suffixPats;
+  }
+
+
+  /** 
+   * Return a set patterns which are the cross product of the patterns
+   * in pPats and qPats.  
+   */
+  private static FrequentPatternMaxHeap cross(FrequentPatternMaxHeap pPats, 
+                                              FrequentPatternMaxHeap qPats,
+                                              int k) {
+    FrequentPatternMaxHeap pats = new FrequentPatternMaxHeap(k, true);
+
+    for (Pattern p : pPats.getHeap()) {
+      int[] pints = p.getPattern();
+      for (Pattern q : qPats.getHeap()) {
+        int[] qints = q.getPattern();
+        
+        Pattern pq = new Pattern();
+        for (int pi = 0; pi < p.length(); pi++) {
+          pq.add(pints[pi], p.support());
+        }
+        for (int qi = 0; qi < q.length(); qi++) {
+          pq.add(qints[qi], q.support());
+        }
+        pats.insert(pq);
+      }
+    }
+
+    for (Pattern q : qPats.getHeap()) {
+      Pattern qq = new Pattern();
+      int[] qints = q.getPattern();
+      for (int qi = 0; qi < q.length(); qi++) {
+        qq.add(qints[qi], q.support());
+      }
+      pats.insert(qq);
+    }
+
+    return pats;
+  }
+
+  /**
+   * Mine all frequent patterns that can be created by following a prefix
+   * that is common to all sets in the given tree.
+   */
+  private static FrequentPatternMaxHeap mineSinglePrefix(FPTree tree, int k) {
+    FrequentPatternMaxHeap pats = new FrequentPatternMaxHeap(k, true);
+    FPTree.FPNode currNode = tree.root();
+    while (currNode.numChildren() == 1) {
+      currNode = currNode.children().iterator().next();
+      FrequentPatternMaxHeap singlePat = new FrequentPatternMaxHeap(k, true);
+      Pattern p = new Pattern();
+      p.add(currNode.attribute(), currNode.count());
+      singlePat.insert(p);
+      pats = cross(singlePat, pats, k);
+      pats.insert(p);
+    }
+
+    return pats;
+  }
+
+  private static void mergeHeap(FrequentPatternMaxHeap frequentPatterns, FrequentPatternMaxHeap returnedPatterns,
+      int attribute, long count, boolean addAttribute) {
+    frequentPatterns.addAll(returnedPatterns, attribute, count);
+    if (frequentPatterns.addable(count) && addAttribute) {
+      Pattern p = new Pattern();
+      p.add(attribute, count);
+      frequentPatterns.insert(p);
+    }
+  }
+}
+
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java
index e69de29b..d181093e 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java
@@ -0,0 +1,376 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth2;
+
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+import com.google.common.collect.Lists;
+
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.math.list.IntArrayList;
+import org.apache.mahout.math.list.LongArrayList;
+import org.apache.mahout.math.map.OpenIntObjectHashMap;
+
+/**
+ * A straightforward implementation of FPTrees as described in Han et. al.
+ */
+public final class FPTree {
+
+  private final AttrComparator attrComparator = new AttrComparator();
+  private final FPNode root;
+  private final long minSupport;
+  private final LongArrayList attrCountList;
+  private final OpenIntObjectHashMap<List<FPNode>> attrNodeLists;
+
+  public static final class FPNode {
+    private final FPNode parent;
+    private final OpenIntObjectHashMap<FPNode> childMap;
+    private final int attribute;
+    private long count;
+
+    private FPNode(FPNode parent, int attribute, long count) {
+      this.parent = parent;
+      this.attribute = attribute;
+      this.count = count;
+      this.childMap = new OpenIntObjectHashMap<FPNode>();
+    }
+
+    private void addChild(FPNode child) {
+      this.childMap.put(child.attribute(), child);
+    }
+
+    public Iterable<FPNode> children() {
+      return childMap.values();
+    }
+
+    public int numChildren() {
+      return childMap.size();
+    }
+
+    public FPNode parent() {
+      return parent;
+    }
+
+    public FPNode child(int attribute) {
+      return childMap.get(attribute);
+    }
+
+    public int attribute() {
+      return attribute;
+    }
+
+    public void accumulate(long incr) {
+      count += incr;
+    }
+
+    public long count() {
+      return count;
+    }
+
+  }
+
+  /**
+   * Creates an FPTree using the attribute counts in attrCountList.
+   *
+   * Note that the counts in attrCountList are assumed to be complete;
+   * they are not updated as the tree is modified.
+   */
+  public FPTree(LongArrayList attrCountList, long minSupport) {
+    this.root = new FPNode(null, -1, 0);
+    this.attrCountList = attrCountList;
+    this.attrNodeLists = new OpenIntObjectHashMap<List<FPNode>>();
+    this.minSupport = minSupport;
+  }
+
+  /**
+   * Creates an FPTree using the attribute counts in attrCounts.
+   *
+   * Note that the counts in attrCounts are assumed to be complete;
+   * they are not updated as the tree is modified.
+   */
+  public FPTree(long[] attrCounts, long minSupport) {
+    this.root = new FPNode(null, -1, 0);
+    this.attrCountList = new LongArrayList();
+    for (int i = 0; i < attrCounts.length; i++) {
+      if (attrCounts[i] > 0) {
+        if (attrCountList.size() < (i + 1)) {
+          attrCountList.setSize(i + 1);
+        }
+        attrCountList.set(i, attrCounts[i]);
+      }
+    }
+    this.attrNodeLists = new OpenIntObjectHashMap<List<FPNode>>();
+    this.minSupport = minSupport;
+  }
+
+
+  /**
+   * Returns the count of the given attribute, as supplied on construction.
+   */
+  public long headerCount(int attribute) {
+    return attrCountList.get(attribute);
+  }
+
+  /**
+   * Returns the root node of the tree.
+   */
+  public FPNode root() {
+    return root;
+  }
+
+  /**
+   * Adds an itemset with the given occurrance count.
+   */
+  public void accumulate(IntArrayList argItems, long count) {
+    // boxed primitive used so we can use custom comparitor in sort
+    List<Integer> items = Lists.newArrayList();
+    for (int i = 0; i < argItems.size(); i++) {
+      items.add(argItems.get(i));
+    }
+    Collections.sort(items, attrComparator);
+    
+    FPNode currNode = root;
+    for (Integer item : items) {
+      long attrCount = 0;
+      if (item < attrCountList.size()) {
+        attrCount = attrCountList.get(item);
+      }
+      if (attrCount < minSupport) {
+        continue;
+      }
+
+      FPNode next = currNode.child(item);
+      if (next == null) {
+        next = new FPNode(currNode, item, count);
+        currNode.addChild(next);
+        List<FPNode> nodeList = attrNodeLists.get(item);
+        if (nodeList == null) {
+          nodeList = Lists.newArrayList();
+          attrNodeLists.put(item, nodeList);
+        }
+        nodeList.add(next);
+      } else {
+        next.accumulate(count);
+      }
+      currNode = next;
+    }
+  } 
+
+  /**
+   * Adds an itemset with the given occurrance count.
+   */
+  public void accumulate(List<Integer> argItems, long count) {
+    List<Integer> items = Lists.newArrayList();
+    items.addAll(argItems);
+    Collections.sort(items, attrComparator);
+    
+    FPNode currNode = root;
+    for (Integer item : items) {
+      long attrCount = attrCountList.get(item);
+      if (attrCount < minSupport) {
+        continue;
+      }
+
+      FPNode next = currNode.child(item);
+      if (next == null) {
+        next = new FPNode(currNode, item, count);
+        currNode.addChild(next);
+        List<FPNode> nodeList = attrNodeLists.get(item);
+        if (nodeList == null) {
+          nodeList = Lists.newArrayList();
+          attrNodeLists.put(item, nodeList);
+        }
+        nodeList.add(next);
+      } else {
+        next.accumulate(count);
+      }
+      currNode = next;
+    }
+  }
+
+  /**
+   * Returns an Iterable over the attributes in the tree, sorted by
+   * frequency (high to low).
+   */
+  public Iterable<Integer> attrIterableRev() {
+    List<Integer> attrs = Lists.newArrayList();
+    for (int i = 0; i < attrCountList.size(); i++) {
+      if (attrCountList.get(i) > 0) {
+        attrs.add(i);
+      }
+    }
+    Collections.sort(attrs, Collections.reverseOrder(attrComparator));
+    return attrs;
+  }
+
+  /**
+   * Returns a conditional FP tree based on the targetAttr, containing
+   * only items that are more frequent.
+   */
+  public FPTree createMoreFreqConditionalTree(int targetAttr) {
+    LongArrayList counts = new LongArrayList();
+    List<FPNode> nodeList = attrNodeLists.get(targetAttr);
+
+    for (FPNode currNode : nodeList) {
+      long pathCount = currNode.count();
+      while (currNode != root) {
+        int currAttr = currNode.attribute();
+        if (counts.size() <= currAttr) {
+          counts.setSize(currAttr + 1);
+        }
+        long count = counts.get(currAttr);
+        counts.set(currNode.attribute(), count + pathCount);
+        currNode = currNode.parent();
+      }
+    }
+    if (counts.get(targetAttr) != attrCountList.get(targetAttr)) {
+      throw new IllegalStateException("mismatched counts for targetAttr="
+                                          + targetAttr + ", (" + counts.get(targetAttr)
+                                          + " != " + attrCountList.get(targetAttr) + "); "
+                                          + "thisTree=" + this + '\n');
+    }
+    counts.set(targetAttr, 0L);
+
+    FPTree toRet = new FPTree(counts, minSupport);
+    IntArrayList attrLst = new IntArrayList();
+    for (FPNode currNode : attrNodeLists.get(targetAttr)) {
+      long count = currNode.count();
+      attrLst.clear();
+      while (currNode != root) {
+        if (currNode.count() < count) {
+          throw new IllegalStateException();
+        }
+        attrLst.add(currNode.attribute());
+        currNode = currNode.parent();
+      }
+
+      toRet.accumulate(attrLst, count);      
+    }    
+    return toRet;
+  }
+
+  // biggest count or smallest attr number goes first
+  private class AttrComparator implements Comparator<Integer> {
+    @Override
+    public int compare(Integer a, Integer b) {
+
+      long aCnt = 0;
+      if (a < attrCountList.size()) {
+        aCnt = attrCountList.get(a);
+      }
+      long bCnt = 0;
+      if (b < attrCountList.size()) {
+        bCnt = attrCountList.get(b);
+      }
+      if (aCnt == bCnt) {
+        return a - b;
+      }
+      return (bCnt - aCnt) < 0 ? -1 : 1;
+    }
+  }
+
+  /**
+   *  Return a pair of trees that result from separating a common prefix
+   *  (if one exists) from the lower portion of this tree.
+   */
+  public Pair<FPTree, FPTree> splitSinglePrefix() {
+    if (root.numChildren() != 1) {
+      return new Pair<FPTree, FPTree>(null, this);
+    }
+    LongArrayList pAttrCountList = new LongArrayList();
+    LongArrayList qAttrCountList = attrCountList.copy();
+
+    FPNode currNode = root;
+    while (currNode.numChildren() == 1) {
+      currNode = currNode.children().iterator().next();
+      if (pAttrCountList.size() <= currNode.attribute()) {
+        pAttrCountList.setSize(currNode.attribute() + 1);
+      }
+      pAttrCountList.set(currNode.attribute(), currNode.count());
+      qAttrCountList.set(currNode.attribute(), 0);
+    }
+
+    FPTree pTree = new FPTree(pAttrCountList, minSupport);
+    FPTree qTree = new FPTree(qAttrCountList, minSupport);
+    recursivelyAddPrefixPats(pTree, qTree, root, null);
+
+    return new Pair<FPTree, FPTree>(pTree, qTree);
+  }
+
+  private long recursivelyAddPrefixPats(FPTree pTree, FPTree qTree, FPNode node,
+                                        IntArrayList items) {
+    long count = node.count();
+    int attribute = node.attribute();
+    if (items == null) {
+      // at root
+      if (node != root) {
+        throw new IllegalStateException();
+      }
+      items = new IntArrayList();
+    } else {
+      items.add(attribute);
+    }
+    long added = 0;
+    for (FPNode child : node.children()) {
+      added += recursivelyAddPrefixPats(pTree, qTree, child, items);
+    }
+    if (added < count) {
+      long toAdd = count - added;
+      pTree.accumulate(items, toAdd);
+      qTree.accumulate(items, toAdd);
+      added += toAdd;
+    }
+    if (node != root) {
+      int lastIdx = items.size() - 1;
+      if (items.get(lastIdx) != attribute) {
+        throw new IllegalStateException();
+      }
+      items.remove(lastIdx);
+    }
+    return added;
+  }
+
+  private static void toStringHelper(StringBuilder sb, FPNode currNode, String prefix) {
+    if (currNode.numChildren() == 0) {
+      sb.append(prefix).append("-{attr:").append(currNode.attribute())
+        .append(", cnt:").append(currNode.count()).append("}\n");
+    } else {
+      StringBuilder newPre = new StringBuilder(prefix);
+      newPre.append("-{attr:").append(currNode.attribute())
+        .append(", cnt:").append(currNode.count()).append('}');
+      StringBuilder fakePre = new StringBuilder();
+      while (fakePre.length() < newPre.length()) {
+        fakePre.append(' ');
+      }
+      int i = 0;
+      for (FPNode child : currNode.children()) {
+        toStringHelper(sb, child, (i++ == 0 ? newPre : fakePre).toString() + '-' + i + "->");
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder("[FPTree\n");
+    toStringHelper(sb, root, "  ");
+    sb.append(']');
+    return sb.toString();
+  }
+
+}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/package-info.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/package-info.java
index e69de29b..4c810e61 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/package-info.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/package-info.java
@@ -0,0 +1,49 @@
+/**
+ * <h2>MapReduce (parallel) implementation of FP Growth Algorithm for frequent Itemset Mining</h2>
+ *
+ * <p>We have a Top K Parallel FPGrowth Implementation. What it means is that given a huge transaction list,
+ * we find all unique features(field values) and eliminates those features whose frequency in the whole dataset
+ * is less that {@code minSupport}. Using these remaining features N, we find the top K closed patterns for
+ * each of them, generating NK patterns. FPGrowth Algorithm is a generic implementation, we can use any object
+ * type to denote a feature. Current implementation requires you to use a String as the object type. You may
+ * implement a version for any object by creating {@link java.util.Iterator}s, Convertors
+ * and TopKPatternWritable for that particular object. For more information please refer the package
+ * {@code org.apache.mahout.fpm.pfpgrowth.convertors.string}.</p>
+ *
+ * {@code
+ * FPGrowth<String> fp = new FPGrowth<String>();
+ * Set<String> features = new HashSet<String>();
+ * fp.generateTopKStringFrequentPatterns(
+ *   new StringRecordIterator(
+ *     new FileLineIterable(new File(input), encoding, false), pattern),
+ *     fp.generateFList(
+ *       new StringRecordIterator(new FileLineIterable(new File(input), encoding, false), pattern), minSupport),
+ *     minSupport,
+ *     maxHeapSize,
+ *     features,
+ *     new StringOutputConvertor(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)));}
+ *
+ * <ul>
+ * <li>The first argument is the iterator of transaction in this case its {@code Iterator<List<String>>}</li>
+ * <li>The second argument is the output of generateFList function, which returns the frequent items and
+ *  their frequencies from the given database transaction iterator</li>
+ * <li>The third argument is the minimum Support of the pattern to be generated</li>
+ * <li>The fourth argument is the maximum number of patterns to be mined for each feature</li>
+ * <li>The fifth argument is the set of features for which the frequent patterns has to be mined</li>
+ * <li>The last argument is an output collector which takes [key, value] of Feature and TopK Patterns of the format
+ *  {@code [String, List<Pair<List<String>,Long>>]} and writes them to the appropriate writer class
+ *  which takes care of storing the object, in this case in a
+ *  {@link org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat}</li>
+ * </ul>
+ *
+ * <p>The command line launcher for string transaction data {@code org.apache.mahout.fpm.pfpgrowth.FPGrowthJob}
+ * has other features including specifying the regex pattern for spitting a string line of a transaction into
+ * the constituent features.</p>
+ *
+ * <p>The {@code numGroups} parameter in FPGrowthJob specifies the number of groups into which transactions
+ * have to be decomposed. The {@code numTreeCacheEntries} parameter specifies the number of generated
+ * conditional FP-Trees to be kept in memory so as not to regenerate them. Increasing this number
+ * increases the memory consumption but might improve speed until a certain point. This depends entirely on
+ * the dataset in question. A value of 5-10 is recommended for mining up to top 100 patterns for each feature.</p>
+ */
+package org.apache.mahout.fpm.pfpgrowth;
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest.java
index e69de29b..f561fb5d 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest.java
@@ -0,0 +1,134 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
+import org.junit.Test;
+
+import com.google.common.io.Resources;
+
+public final class FPGrowthRetailDataTest extends MahoutTestCase {
+
+  @Test
+  public void testSpecificCaseFromRetailDataMinSup500() throws IOException {
+    FPGrowth<String> fp = new FPGrowth<String>();
+    
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail.dat").openStream()), "\\s+");
+    int pattern_41_36_39 = 0;
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = next.getFirst();
+      if (items.contains("41") && items.contains("36") && items.contains("39")) {
+        pattern_41_36_39++;
+      }
+    }
+    
+    final Map<Set<String>,Long> results = Maps.newHashMap();
+    
+    Set<String> returnableFeatures = Sets.newHashSet();
+    returnableFeatures.add("41");
+    returnableFeatures.add("36");
+    returnableFeatures.add("39");
+    
+    fp.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource("retail.dat").openStream()), "\\s+"),
+
+      fp.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource("retail.dat")
+          .openStream()), "\\s+"), 500), 500, 1000, returnableFeatures,
+      new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+        @Override
+        public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+          for (Pair<List<String>,Long> v : value) {
+            List<String> l = v.getFirst();
+            results.put(Sets.newHashSet(l), v.getSecond());
+          }
+        }
+        
+      }, new StatusUpdater() {
+        
+        @Override
+        public void update(String status) {}
+      });
+    
+    assertEquals(Long.valueOf(pattern_41_36_39), results.get(returnableFeatures));
+    
+  }
+  
+  /*
+  @Test
+  public void testRetailDataMinSup100() throws IOException {
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail_results_with_min_sup_100.dat").openStream()), "\\s+");
+    final Map<Set<String>,Long> expectedResults = Maps.newHashMap();
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = new ArrayList<String>(next.getFirst());
+      String supportString = items.remove(items.size() - 1);
+      Long support = Long.parseLong(supportString.substring(1, supportString.length() - 1));
+      expectedResults.put(new HashSet<String>(items), support);
+    }
+    
+    FPGrowth<String> fp = new FPGrowth<String>();
+    
+    final Map<Set<String>,Long> results = new HashMap<Set<String>,Long>();
+    
+    fp.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource("retail.dat").openStream()), "\\s+"),
+
+      fp.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource("retail.dat")
+          .openStream()), "\\s+"), 100), 100, 1000, null,
+      new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+        @Override
+        public void collect(String key, List<Pair<List<String>,Long>> value) throws IOException {
+          
+          for (Pair<List<String>,Long> v : value) {
+            List<String> l = v.getFirst();
+            results.put(new HashSet<String>(l), v.getSecond());
+          }
+        }
+      }, new StatusUpdater() {
+        
+        @Override
+        public void update(String status) {}
+      });
+    
+    assertEquals(expectedResults.size(), results.size());
+    
+    for (Entry<Set<String>,Long> entry : results.entrySet()) {
+      Set<String> key = entry.getKey();
+      assertEquals(expectedResults.get(key), results.get(entry.getKey()));
+    }
+  }*/
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java
index e69de29b..ac2d42eb 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java
@@ -0,0 +1,83 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+
+import com.google.common.io.Resources;
+
+public final class FPGrowthRetailDataTest2 extends MahoutTestCase {
+
+  @Test
+  public void testSpecificCaseFromRetailDataMinSup500() throws IOException {
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+    
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail.dat").openStream()), "\\s+");
+    int pattern_41_36_39 = 0;
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = next.getFirst();
+      if (items.contains("41") && items.contains("36") && items.contains("39")) {
+        pattern_41_36_39++;
+      }
+    }
+    
+    final Map<Set<String>,Long> results = Maps.newHashMap();
+    
+    Set<String> returnableFeatures = Sets.newHashSet();
+    returnableFeatures.add("41");
+    returnableFeatures.add("36");
+    returnableFeatures.add("39");
+    
+    fp.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource("retail.dat").openStream()), "\\s+"),
+
+      fp.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource("retail.dat")
+          .openStream()), "\\s+"), 500), 500, 1000, returnableFeatures,
+      new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+        @Override
+        public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+          for (Pair<List<String>,Long> v : value) {
+            List<String> l = v.getFirst();
+            results.put(Sets.newHashSet(l), v.getSecond());
+          }
+        }
+        
+      });
+    
+    assertEquals(Long.valueOf(pattern_41_36_39), results.get(returnableFeatures));
+    
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java
index e69de29b..bfcd709d 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java
@@ -0,0 +1,169 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+
+import com.google.common.io.Resources;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public final class FPGrowthRetailDataTestVs extends MahoutTestCase {
+
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthRetailDataTestVs.class);
+
+  private static long bestResults(Map<Set<String>, Long> res, Set<String> feats) {
+    Long best = res.get(feats);
+    if (best != null) {
+      return best;
+    }
+    best = -1L;
+    for (Map.Entry<Set<String>, Long> ent : res.entrySet()) { 
+      Set<String> r = ent.getKey();
+      Long supp = ent.getValue();
+      if (supp <= best) {
+        continue;
+      }
+      boolean hasAll = true;
+      for (String f : feats) {
+        if (!r.contains(f)) {
+          hasAll = false;
+          break;
+        }
+      }
+      if (hasAll) {
+        best = supp;
+      }
+    }
+    return best;
+  }
+
+  private static class MapCollector implements OutputCollector<String,List<Pair<List<String>,Long>>> {
+    private final Map<Set<String>,Long> results;
+
+    private MapCollector(Map<Set<String>, Long> results) {
+      this.results = results;
+    }
+
+    @Override
+    public void collect(String key, List<Pair<List<String>,Long>> value) {
+      for (Pair<List<String>,Long> v : value) {
+        List<String> l = v.getFirst();
+        results.put(Sets.newHashSet(l), v.getSecond());
+        log.info("found pat ["+v.getSecond()+"]: "+ v.getFirst());
+      }
+    }
+  }
+
+  private static class DummyUpdater implements StatusUpdater {
+    @Override
+    public void update(String status) { }
+  }
+
+  @Test
+  public void testVsWithRetailData() throws IOException {
+    String inputFilename = "retail.dat";
+    int minSupport = 500;
+    Collection<String> returnableFeatures = Sets.newHashSet();
+    
+    org.apache.mahout.fpm.pfpgrowth.fpgrowth.
+      FPGrowth<String> fp1 = new org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth<String>();
+
+    Map<Set<String>,Long> results1 = Maps.newHashMap();
+    
+    fp1.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+      fp1.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+           .openStream()), "\\s+"), minSupport), minSupport, 100000, 
+      returnableFeatures,
+      new MapCollector(results1), new DummyUpdater());
+
+    FPGrowthObj<String> fp2 = new FPGrowthObj<String>();
+    Map<Set<String>,Long> initialResults2 = Maps.newHashMap();
+    fp2.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+      fp2.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+           .openStream()), "\\s+"), minSupport), minSupport, 100000,
+        Sets.<String>newHashSet(),
+      new MapCollector(initialResults2));
+
+    Map<Set<String>, Long> results2;
+    if (returnableFeatures.isEmpty()) {
+      results2 = initialResults2;
+    } else {
+      Map<Set<String>, Long> tmpResult = new HashMap<Set<String>, Long>();
+      for (Map.Entry<Set<String>, Long> result2 : initialResults2.entrySet()) {
+        Set<String> r2feats = result2.getKey();
+        boolean hasSome = false;
+        for (String rf : returnableFeatures) {
+          if (r2feats.contains(rf)) {
+            hasSome = true;
+            break;
+          }
+        }
+        if (hasSome) {
+          tmpResult.put(result2.getKey(), result2.getValue());
+        }
+      }
+      results2 = tmpResult;
+    }
+
+    boolean allMatch = hasAll(results1, results2);
+    log.info("checked "+results1.size()+" itemsets iterating through #1");
+
+    allMatch &= hasAll(results2, results1);
+    log.info("checked "+results2.size()+" itemsets iterating through #2");
+
+    assertTrue("Had mismatches!", allMatch);
+  }
+
+  public static boolean hasAll(Map<Set<String>, Long> ref, Map<Set<String>, Long> other) {
+    boolean hasAll = true;
+    for (Map.Entry<Set<String>, Long> refEnt : ref.entrySet()) {
+      Set<String> feats = refEnt.getKey();
+      long supp1 = refEnt.getValue();
+      long supp2 = bestResults(other, feats);
+      if (supp1 != supp2) {
+        hasAll = false;
+        log.info("mismatch checking results ["+supp1+" vs "+supp2+"]: "+feats);
+      }
+    }
+
+    return hasAll;
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java
index e69de29b..64e2544e 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java
@@ -0,0 +1,243 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+
+import com.google.common.io.Resources;
+
+public final class FPGrowthSyntheticDataTest extends MahoutTestCase {
+
+  @Test
+  public void testSpecificCasesFromSynthData() throws IOException {
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+    
+    String inputFilename = "FPGsynth.dat";
+
+    StringRecordIterator it =
+        new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+");
+    int patternCnt_10_13_1669 = 0;
+    int patternCnt_10_13 = 0;
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = next.getFirst();
+      if (items.contains("10") && items.contains("13")) {
+        patternCnt_10_13++;
+        if (items.contains("1669")) {
+          patternCnt_10_13_1669++;
+        }
+      }
+    }
+
+    int minSupport = 50;
+    if (patternCnt_10_13_1669 < minSupport) {
+      throw new IllegalStateException("the test is broken or data is missing ("
+                                          + patternCnt_10_13_1669 + ", "
+                                          + patternCnt_10_13 + ')');
+    }
+
+    final Map<Set<String>,Long> results = Maps.newHashMap();
+    
+    Set<String> features_10_13 = Sets.newHashSet();
+    features_10_13.add("10");
+    features_10_13.add("13");
+
+    Set<String> returnableFeatures = Sets.newHashSet();
+    returnableFeatures.add("10");
+    returnableFeatures.add("13");
+    returnableFeatures.add("1669");
+    
+    fp.generateTopKFrequentPatterns(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+                                    fp.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+                                                                                                   .openStream()), "\\s+"), minSupport), minSupport, 100000, 
+                                    returnableFeatures,
+                                    new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+                                      @Override
+                                        public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+                                        for (Pair<List<String>,Long> v : value) {
+                                          List<String> l = v.getFirst();
+                                          results.put(Sets.newHashSet(l), v.getSecond());
+                                          System.out.println("found pat ["+v.getSecond()+"]: "+ v.getFirst());
+                                        }
+                                      }
+        
+                                    });
+
+    assertEquals(patternCnt_10_13, highestSupport(results, features_10_13));
+    assertEquals(patternCnt_10_13_1669, highestSupport(results, returnableFeatures));
+    
+  }
+
+  private static long highestSupport(Map<Set<String>, Long> res, Set<String> feats) {
+    Long best= res.get(feats);
+    if (best != null) {
+      return best;
+    }
+    best = -1L;
+    for (Map.Entry<Set<String>, Long> ent : res.entrySet()) {
+      Set<String> r= ent.getKey();
+      Long supp= ent.getValue();
+      if (supp <= best) {
+        continue;
+      }
+      boolean hasAll= true;
+      for (String f : feats) {
+        if (!r.contains(f)) {
+          hasAll= false;
+          break;
+        }
+      }
+      if (hasAll) {
+        best = supp;
+      }
+    }
+    return best;
+  }
+
+  @Test
+  public void testVsWithSynthData() throws IOException {
+    Collection<String> returnableFeatures = Sets.newHashSet();
+
+    // not limiting features (or including too many) can cause
+    // the test to run a very long time
+    returnableFeatures.add("10");
+    returnableFeatures.add("13");
+    //    returnableFeatures.add("1669");
+    
+    FPGrowth<String> fp1 = new FPGrowth<String>();
+
+    final Map<Set<String>,Long> results1 = Maps.newHashMap();
+
+    String inputFilename = "FPGsynth.dat";
+    int minSupport = 100;
+    fp1.generateTopKFrequentPatterns(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+                                     fp1.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+                                                                                                     .openStream()), "\\s+"), minSupport), minSupport, 1000000, 
+                                     returnableFeatures,
+                                     new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+                                       @Override
+                                         public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+                                         for (Pair<List<String>,Long> v : value) {
+                                           List<String> l = v.getFirst();
+                                           results1.put(Sets.newHashSet(l), v.getSecond());
+                                           System.out.println("found pat ["+v.getSecond()+"]: "+ v.getFirst());
+                                         }
+                                       }
+        
+                                     }, new StatusUpdater() {
+        
+                                         @Override
+                                           public void update(String status) {}
+                                       });
+
+    FPGrowthObj<String> fp2 = new FPGrowthObj<String>();
+    final Map<Set<String>,Long> initialResults2 = Maps.newHashMap();
+    fp2.generateTopKFrequentPatterns(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+                                     fp2.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+                                                                                                     .openStream()), "\\s+"), minSupport), minSupport, 1000000,
+                                     Sets.<String>newHashSet(),
+                                     new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+                                       @Override
+                                         public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+                                         for (Pair<List<String>,Long> v : value) {
+                                           List<String> l = v.getFirst();
+                                           initialResults2.put(Sets.newHashSet(l), v.getSecond());
+                                           System.out.println("found pat ["+v.getSecond()+"]: "+ v.getFirst());
+                                         }
+                                       }
+        
+                                     });
+
+    Map<Set<String>, Long> results2;
+    if (returnableFeatures.isEmpty()) {
+      results2 = initialResults2;
+    } else {
+      Map<Set<String>, Long> tmpResult = new HashMap<Set<String>, Long>();
+      for (Map.Entry<Set<String>, Long> result2 : initialResults2.entrySet()) {
+        Set<String> r2feats = result2.getKey();
+        boolean hasSome = false;
+        for (String rf : returnableFeatures) {
+          if (r2feats.contains(rf)) {
+            hasSome = true;
+            break;
+          }
+        }
+        if (hasSome) {
+          tmpResult.put(result2.getKey(), result2.getValue());
+        }
+      }
+      results2 = tmpResult;
+    }
+
+    boolean allMatch = true;
+    int itemsetsChecked= 0;
+    for (Map.Entry<Set<String>, Long> result1 : results1.entrySet()) {
+      itemsetsChecked++;
+      Set<String> feats= result1.getKey();
+      long supp1= result1.getValue();
+      long supp2= highestSupport(results2, feats);
+      if (supp1 != supp2) {
+        allMatch= false;
+        System.out.println("mismatch checking results1 [ "+supp1+" vs "+supp2+"]: "+feats);
+      }
+    }
+    System.out.println("checked "+itemsetsChecked+" itemsets iterating through #1");
+
+    itemsetsChecked= 0;
+    for (Map.Entry<Set<String>, Long> result2 : results2.entrySet()) { 
+      itemsetsChecked++;
+      Set<String> feats= result2.getKey();
+      long supp2= result2.getValue();
+      long supp1= highestSupport(results1, feats);
+      if (supp1 != supp2) {
+        allMatch= false;
+        System.out.println("mismatch checking results2 [ "+supp1+" vs "+supp2+"]: "+feats);
+      }
+    }
+    System.out.println("checked "+itemsetsChecked+" itemsets iterating through #2");
+
+    assertTrue("Had mismatches!", allMatch);
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java
index e69de29b..de0ec046 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java
@@ -0,0 +1,192 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
+import com.google.common.io.Closeables;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.fpm.pfpgrowth.convertors.ContextStatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.convertors.SequenceFileOutputCollector;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.StringOutputConverter;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
+import org.junit.Test;
+
+public final class FPGrowthTest extends MahoutTestCase {
+
+  @Test
+  public void testMaxHeapFPGrowth() throws Exception {
+
+    FPGrowth<String> fp = new FPGrowth<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("E", "A", "D", "B"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("D", "A", "C", "E", "B"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("C", "A", "B", "E"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("B", "A", "D"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("D"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("D", "B"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("A", "D", "E"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("B", "C"), 1L));
+
+    Path path = getTestTempFilePath("fpgrowthTest.dat");
+    Configuration conf = getConfiguration();
+    FileSystem fs = FileSystem.get(path.toUri(), conf);
+
+    SequenceFile.Writer writer =
+        new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
+    try {
+    fp.generateTopKFrequentPatterns(
+        transactions.iterator(),
+        fp.generateFList(transactions.iterator(), 3),
+        3,
+        100,
+        Sets.<String>newHashSet(),
+        new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)),
+        new ContextStatusUpdater(null));
+    } finally {
+      Closeables.close(writer, false);
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowth.readFrequentPattern(conf, path);
+    assertEquals(
+      "[(C,([B, C],3)), "
+          + "(E,([A, E],4), ([A, B, E],3), ([A, D, E],3)), "
+          + "(A,([A],5), ([A, D],4), ([A, E],4), ([A, B],4), ([A, B, E],3), ([A, D, E],3), ([A, B, D],3)), "
+          + "(D,([D],6), ([B, D],4), ([A, D],4), ([A, D, E],3), ([A, B, D],3)), "
+          + "(B,([B],6), ([A, B],4), ([B, D],4), ([A, B, D],3), ([A, B, E],3), ([B, C],3))]",
+      frequentPatterns.toString());
+
+  }
+  
+  /**
+   * Trivial test for MAHOUT-617
+   */
+  @Test
+  public void testMaxHeapFPGrowthData1() throws Exception {
+
+    FPGrowth<String> fp = new FPGrowth<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X"), 12L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("Y"), 4L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y"), 10L));
+
+    Path path = getTestTempFilePath("fpgrowthTestData1.dat");
+    Configuration conf = getConfiguration();
+    FileSystem fs = FileSystem.get(path.toUri(), conf);
+    System.out.println(fp.generateFList(transactions.iterator(), 2));
+    SequenceFile.Writer writer =
+        new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
+    try {
+      fp.generateTopKFrequentPatterns(
+          transactions.iterator(),
+          fp.generateFList(transactions.iterator(), 2),
+          2,
+          100,
+          Sets.<String>newHashSet(),
+          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)),
+          new ContextStatusUpdater(null));
+    } finally {
+      Closeables.close(writer, false);
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowth.readFrequentPattern(conf, path);
+    assertEquals(
+      "[(Y,([Y],14), ([X, Y],10)), (X,([X],22), ([X, Y],10))]", frequentPatterns.toString());
+  }
+  
+  /**
+   * Trivial test for MAHOUT-617
+   */
+  @Test
+  public void testMaxHeapFPGrowthData2() throws Exception {
+
+    FPGrowth<String> fp = new FPGrowth<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X"), 12L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("Y"), 4L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y"), 10L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y", "Z"), 11L));
+
+    Path path = getTestTempFilePath("fpgrowthTestData2.dat");
+    Configuration conf = getConfiguration();
+    FileSystem fs = FileSystem.get(path.toUri(), conf);
+    System.out.println(fp.generateFList(transactions.iterator(), 2));
+    SequenceFile.Writer writer =
+        new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
+    try {
+      fp.generateTopKFrequentPatterns(
+          transactions.iterator(),
+          fp.generateFList(transactions.iterator(), 2),
+          2,
+          100,
+          Sets.<String>newHashSet(),
+          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)),
+          new ContextStatusUpdater(null));
+    } finally {
+      Closeables.close(writer, false);
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowth.readFrequentPattern(conf, path);
+    assertEquals(
+      "[(Z,([X, Y, Z],11)), (Y,([Y],25), ([X, Y],21), ([X, Y, Z],11)), (X,([X],33), ([X, Y],21), ([X, Y, Z],11))]",
+      frequentPatterns.toString());
+  }
+
+  /**
+   * Trivial test for MAHOUT-355
+   */
+  @Test
+  public void testNoNullPointerExceptionWhenReturnableFeaturesIsNull() throws Exception {
+
+    FPGrowth<String> fp = new FPGrowth<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("E", "A", "D", "B"), 1L));
+
+    OutputCollector<String, List<Pair<List<String>, Long>>> noOutput =
+        new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+      @Override
+      public void collect(String arg0, List<Pair<List<String>, Long>> arg1) { 
+      }
+    };
+
+    fp.generateTopKFrequentPatterns(
+        transactions.iterator(),
+        fp.generateFList(transactions.iterator(), 3),
+        3,
+        100,
+        null,
+        noOutput,
+        new ContextStatusUpdater(null));
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java
index e69de29b..76c93fbe 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java
@@ -0,0 +1,191 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
+import com.google.common.io.Closeables;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.fpm.pfpgrowth.convertors.SequenceFileOutputCollector;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.StringOutputConverter;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+
+public final class FPGrowthTest2 extends MahoutTestCase {
+
+  @Test
+  public void testMaxHeapFPGrowth() throws Exception {
+
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("E", "A", "D", "B"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("D", "A", "C", "E", "B"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("C", "A", "B", "E"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("B", "A", "D"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("D"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("D", "B"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("A", "D", "E"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("B", "C"), 1L));
+
+    Path path = getTestTempFilePath("fpgrowthTest.dat");
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(path.toUri(), conf);
+
+    SequenceFile.Writer writer =
+        new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
+    try {
+    fp.generateTopKFrequentPatterns(
+        transactions.iterator(),
+        fp.generateFList(transactions.iterator(), 3),
+        3,
+        100,
+        Sets.<String>newHashSet(),
+        new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer))
+    );
+    } finally {
+      Closeables.close(writer, false);
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowthObj.readFrequentPattern(conf, path);
+    assertEquals(
+      "[(C,([B, C],3)), "
+          + "(E,([A, E],4), ([A, B, E],3), ([A, D, E],3)), "
+          + "(A,([A],5), ([A, D],4), ([A, B],4), ([A, B, D],3)), "
+          + "(D,([D],6), ([B, D],4)), "
+          + "(B,([B],6))]",
+      frequentPatterns.toString());
+
+  }
+  
+  /**
+   * Trivial test for MAHOUT-617
+   */
+  @Test
+  public void testMaxHeapFPGrowthData1() throws Exception {
+
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X"), 12L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("Y"), 4L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y"), 10L));
+
+    Path path = getTestTempFilePath("fpgrowthTestData1.dat");
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(path.toUri(), conf);
+    System.out.println(fp.generateFList(transactions.iterator(), 2));
+    SequenceFile.Writer writer =
+        new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
+    try {
+      fp.generateTopKFrequentPatterns(
+          transactions.iterator(),
+          fp.generateFList(transactions.iterator(), 2),
+          2,
+          100,
+          Sets.<String>newHashSet(),
+          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer))
+      );
+    } finally {
+      Closeables.close(writer, false);
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowthObj.readFrequentPattern(conf, path);
+    assertEquals(
+      "[(Y,([Y],14), ([X, Y],10)), (X,([X],22))]", frequentPatterns.toString());
+  }
+  
+  /**
+   * Trivial test for MAHOUT-617
+   */
+  @Test
+  public void testMaxHeapFPGrowthData2() throws Exception {
+
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X"), 12L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("Y"), 4L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y"), 10L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y", "Z"), 11L));
+
+    Path path = getTestTempFilePath("fpgrowthTestData2.dat");
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(path.toUri(), conf);
+    System.out.println(fp.generateFList(transactions.iterator(), 2));
+    SequenceFile.Writer writer =
+        new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
+    try {
+      fp.generateTopKFrequentPatterns(
+          transactions.iterator(),
+          fp.generateFList(transactions.iterator(), 2),
+          2,
+          100,
+          Sets.<String>newHashSet(),
+          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer))
+      );
+    } finally {
+      Closeables.close(writer, false);
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowthObj.readFrequentPattern(conf, path);
+    assertEquals(
+      "[(Z,([X, Y, Z],11)), (Y,([Y],25), ([X, Y],21)), (X,([X],33))]",
+      frequentPatterns.toString());
+  }
+
+  /**
+   * Trivial test for MAHOUT-355
+   */
+  @Test
+  public void testNoNullPointerExceptionWhenReturnableFeaturesIsNull() throws Exception {
+
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("E", "A", "D", "B"), 1L));
+
+    OutputCollector<String, List<Pair<List<String>, Long>>> noOutput =
+        new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+      @Override
+      public void collect(String arg0, List<Pair<List<String>, Long>> arg1) { 
+      }
+    };
+
+    fp.generateTopKFrequentPatterns(
+        transactions.iterator(),
+        fp.generateFList(transactions.iterator(), 3),
+        3,
+        100,
+        null,
+        noOutput
+    );
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
index e69de29b..ca98a7d4 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
@@ -0,0 +1,210 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.io.Resources;
+
+public class PFPGrowthRetailDataTest extends MahoutTestCase {
+  
+  private final Parameters params = new Parameters();
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthRetailDataTest.class);
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    params.set(PFPGrowth.MIN_SUPPORT, "100");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
+    params.set(PFPGrowth.NUM_GROUPS, "50");
+    params.set(PFPGrowth.ENCODING, "UTF-8");
+    File inputDir = getTestTempDir("transactions");
+    File outputDir = getTestTempDir("frequentpatterns");
+    File input = new File(inputDir, "test.txt");
+    params.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    params.set(PFPGrowth.OUTPUT, outputDir.getAbsolutePath());
+
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+        "retail.dat").openStream()), "\\s+");
+      Collection<List<String>> transactions = Lists.newArrayList();
+      
+      while (it.hasNext()) {
+        Pair<List<String>,Long> next = it.next();
+        transactions.add(next.getFirst());
+      }
+      
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = ",";
+        }
+        writer.write("\n");
+      }
+      
+    } finally {
+      Closeables.close(writer, false);
+    }
+  }
+  
+   
+  /**
+   * Test Parallel FPGrowth on retail data using top-level runPFPGrowth() method
+   */ 
+  @Test
+  public void testRetailDataMinSup100() throws Exception {
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail_results_with_min_sup_100.dat").openStream()), "\\s+");
+    Map<Set<String>,Long> expectedResults = Maps.newHashMap();
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = Lists.newArrayList(next.getFirst());
+      String supportString = items.remove(items.size() - 1);
+      Long support = Long.parseLong(supportString.substring(1, supportString.length() - 1));
+      expectedResults.put(Sets.newHashSet(items), support);
+    }
+
+    PFPGrowth.runPFPGrowth(params, getConfiguration());
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+  
+    Map<Set<String>,Long> results = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results.put(Sets.newHashSet(entry.getFirst()), entry.getSecond());
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : results.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (expectedResults.get(key) == null) {
+        System.out.println("spurious (1): " + key+ " with " +entry.getValue());
+      } else {
+        if (!expectedResults.get(key).equals(results.get(entry.getKey()))) {
+          System.out.println("invalid (1): " + key + ", expected: " + expectedResults.get(key) + ", got: "
+                             + results.get(entry.getKey()));
+        } else {
+          System.out.println("matched (1): " + key + ", with: " + expectedResults.get(key));
+        }
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : expectedResults.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results.get(key) == null) {
+        System.out.println("missing (1): " + key+ " with " +entry.getValue());
+      }
+    }
+    assertEquals(expectedResults.size(), results.size());
+  }
+  
+
+  /**
+   * Test Parallel FPG on retail data, running various stages individually
+   */ 
+  @Test
+  public void testRetailDataMinSup100InSteps() throws Exception {
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail_results_with_min_sup_100.dat").openStream()), "\\s+");   
+    Map<Set<String>,Long> expectedResults = Maps.newHashMap();
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = Lists.newArrayList(next.getFirst());
+      String supportString = items.remove(items.size() - 1);
+      Long support = Long.parseLong(supportString.substring(1, supportString.length() - 1));
+      expectedResults.put(Sets.newHashSet(items), support);
+    }
+    Configuration conf = getConfiguration();
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startParallelCounting(params, conf);
+
+    List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
+    PFPGrowth.saveFList(fList, params, conf);
+    int numGroups = params.getInt(PFPGrowth.NUM_GROUPS, 
+                                  PFPGrowth.NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) {
+      maxPerGroup++;
+    }
+    params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
+
+    PFPGrowth.startParallelFPGrowth(params, conf);
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startAggregating(params, conf);
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+    
+    Map<Set<String>,Long> results = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results.put(Sets.newHashSet(entry.getFirst()), entry.getSecond());
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : results.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (expectedResults.get(key) == null) {
+        System.out.println("spurious (2): " + key);
+      } else {
+        if (!expectedResults.get(key).equals(results.get(entry.getKey()))) {
+          System.out.println("invalid (2): " + key + ", expected: " + expectedResults.get(key) + ", got: "
+                             + results.get(entry.getKey()));
+        } else {
+          System.out.println("matched (2): " + key + ", with: " + expectedResults.get(key));
+        }
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : expectedResults.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results.get(key) == null) {
+        System.out.println("missing: " + key);
+      }
+    }
+    assertEquals(expectedResults.size(), results.size());
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java
index e69de29b..71a6c5e9 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java
@@ -0,0 +1,210 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.io.Resources;
+
+public class PFPGrowthRetailDataTest2 extends MahoutTestCase {
+  
+  private final Parameters params = new Parameters();
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthRetailDataTest2.class);
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    params.set(PFPGrowth.MIN_SUPPORT, "100");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
+    params.set(PFPGrowth.NUM_GROUPS, "50");
+    params.set(PFPGrowth.ENCODING, "UTF-8");
+    params.set(PFPGrowth.USE_FPG2, "true");
+    File inputDir = getTestTempDir("transactions");
+    File outputDir = getTestTempDir("frequentpatterns");
+    File input = new File(inputDir, "test.txt");
+    params.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    params.set(PFPGrowth.OUTPUT, outputDir.getAbsolutePath());
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+        "retail.dat").openStream()), "\\s+");
+      Collection<List<String>> transactions = Lists.newArrayList();
+      
+      while (it.hasNext()) {
+        Pair<List<String>,Long> next = it.next();
+        transactions.add(next.getFirst());
+      }
+      
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = ",";
+        }
+        writer.write("\n");
+      }
+      
+    } finally {
+      Closeables.close(writer, false);
+    }
+  }
+  
+  /**
+   * Test Parallel FPGrowth on retail data using top-level runPFPGrowth() method
+   */ 
+  @Test
+  public void testRetailDataMinSup100() throws Exception {
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail_results_with_min_sup_100.dat").openStream()), "\\s+");
+    Map<Set<String>,Long> expectedResults = Maps.newHashMap();
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = Lists.newArrayList(next.getFirst());
+      String supportString = items.remove(items.size() - 1);
+      Long support = Long.parseLong(supportString.substring(1, supportString.length() - 1));
+      expectedResults.put(Sets.newHashSet(items), support);
+    }
+
+    PFPGrowth.runPFPGrowth(params);
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+    
+    Map<Set<String>,Long> results = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results.put(Sets.newHashSet(entry.getFirst()), entry.getSecond());
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : results.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (expectedResults.get(key) == null) {
+        System.out.println("spurious (1): " + key+ " with " +entry.getValue());
+      } else {
+        if (!expectedResults.get(key).equals(results.get(entry.getKey()))) {
+          System.out.println("invalid (1): " + key + ", expected: " + expectedResults.get(key) + ", got: "
+                             + results.get(entry.getKey()));
+        } else {
+          System.out.println("matched (1): " + key + ", with: " + expectedResults.get(key));
+        }
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : expectedResults.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results.get(key) == null) {
+        System.out.println("missing (1): " + key+ " with " +entry.getValue());
+      }
+    }
+    assertEquals(expectedResults.size(), results.size());
+  }
+
+  /**
+   * Test Parallel FPG on retail data, running various stages individually
+   */ 
+  @Test
+  public void testRetailDataMinSup100InSteps() throws Exception {
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail_results_with_min_sup_100.dat").openStream()), "\\s+");    
+    Map<Set<String>,Long> expectedResults = Maps.newHashMap();
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = Lists.newArrayList(next.getFirst());
+      String supportString = items.remove(items.size() - 1);
+      Long support = Long.parseLong(supportString.substring(1, supportString.length() - 1));
+      expectedResults.put(Sets.newHashSet(items), support);
+    }
+    Configuration conf = new Configuration();
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startParallelCounting(params, conf);
+
+    List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
+    PFPGrowth.saveFList(fList, params, conf);
+
+    int numGroups = params.getInt(PFPGrowth.NUM_GROUPS, 
+                                  PFPGrowth.NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) {
+      maxPerGroup++;
+    }
+    params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
+
+    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startParallelFPGrowth(params, conf);
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startAggregating(params, conf);
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+    
+    Map<Set<String>,Long> results = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results.put(Sets.newHashSet(entry.getFirst()), entry.getSecond());
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : results.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (expectedResults.get(key) == null) {
+        System.out.println("spurious (2): " + key + ", " + entry.getValue());
+      } else {
+        if (!expectedResults.get(key).equals(results.get(entry.getKey()))) {
+          System.out.println("invalid (2): " + key + ", expected: " + expectedResults.get(key) + ", got: "
+                             + results.get(entry.getKey()));
+        } else {
+          System.out.println("matched (2): " + key + ", with: " + expectedResults.get(key));
+        }         
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : expectedResults.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results.get(key) == null) {
+        System.out.println("missing2: " + key + ", " + entry.getValue());
+      }
+    }
+    assertEquals(expectedResults.size(), results.size());
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java
index e69de29b..9225b2c3 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java
@@ -0,0 +1,155 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.junit.Test;
+
+import com.google.common.base.Charsets;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import com.google.common.io.Resources;
+
+public final class PFPGrowthRetailDataTestVs extends MahoutTestCase {
+
+  private final Parameters paramsImpl1 = new Parameters();
+  private final Parameters paramsImpl2 = new Parameters();
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+
+    File inputDir = getTestTempDir("transactions");
+    File input = new File(inputDir, "test.txt");
+
+    paramsImpl1.set(PFPGrowth.MIN_SUPPORT, "100");
+    paramsImpl1.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
+    paramsImpl1.set(PFPGrowth.NUM_GROUPS, "50");
+    paramsImpl1.set(PFPGrowth.ENCODING, "UTF-8");
+    paramsImpl1.set(PFPGrowth.INPUT, input.getAbsolutePath());
+
+    paramsImpl2.set(PFPGrowth.MIN_SUPPORT, "100");
+    paramsImpl2.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
+    paramsImpl2.set(PFPGrowth.NUM_GROUPS, "50");
+    paramsImpl2.set(PFPGrowth.ENCODING, "UTF-8");
+    paramsImpl2.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    paramsImpl2.set(PFPGrowth.USE_FPG2, "true");
+
+    File outputDir1 = getTestTempDir("frequentpatterns1");
+    paramsImpl1.set(PFPGrowth.OUTPUT, outputDir1.getAbsolutePath());
+
+    File outputDir2 = getTestTempDir("frequentpatterns2");
+    paramsImpl2.set(PFPGrowth.OUTPUT, outputDir2.getAbsolutePath());
+
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+        "retail.dat").openStream()), "\\s+");
+      Collection<List<String>> transactions = Lists.newArrayList();
+      
+      while (it.hasNext()) {
+        Pair<List<String>,Long> next = it.next();
+        transactions.add(next.getFirst());
+      }
+      
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = ",";
+        }
+        writer.write("\n");
+      }
+      
+    } finally {
+      Closeables.close(writer, false);
+    }
+  }
+  
+   
+  /**
+   * Test Parallel FPGrowth on retail data using top-level runPFPGrowth() method
+   */ 
+  @Test
+  public void testParallelRetailVs() throws Exception {
+
+    PFPGrowth.runPFPGrowth(paramsImpl1);
+    List<Pair<String,TopKStringPatterns>> frequentPatterns1 = PFPGrowth.readFrequentPattern(paramsImpl1);
+    
+    Map<Set<String>,Long> results1 = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns1) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results1.put(Sets.newHashSet(entry.getFirst()), entry.getSecond());
+      }
+    }
+  
+    PFPGrowth.runPFPGrowth(paramsImpl2);
+    List<Pair<String,TopKStringPatterns>> frequentPatterns2 = PFPGrowth.readFrequentPattern(paramsImpl2);
+  
+    Map<Set<String>,Long> results2 = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns2) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results2.put(Sets.newHashSet(entry.getFirst()), entry.getSecond());
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : results1.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results2.get(key) == null) {
+        System.out.println("spurious (1): " + key+ " with " +entry.getValue());
+      } else {
+        if (!results2.get(key).equals(results1.get(entry.getKey()))) {
+          System.out.println("invalid (1): " + key + ", expected: " + results2.get(key) + ", got: "
+                             + results1.get(entry.getKey()));
+        } else {
+          System.out.println("matched (1): " + key + ", with: " + results2.get(key));
+        }
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : results2.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results1.get(key) == null) {
+        System.out.println("missing (1): " + key+ " with " +entry.getValue());
+      }
+    }
+    assertEquals(results2.size(), results1.size());
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java
index e69de29b..2eb454a5 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java
@@ -0,0 +1,157 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.io.Resources;
+
+public class PFPGrowthSynthDataTest2 extends MahoutTestCase {
+  
+  private final Parameters params = new Parameters();
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthSynthDataTest2.class);
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    params.set(PFPGrowth.MIN_SUPPORT, "100");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
+    params.set(PFPGrowth.NUM_GROUPS, "50");
+    params.set(PFPGrowth.ENCODING, "UTF-8");
+    params.set(PFPGrowth.USE_FPG2, "true");
+    params.set(PFPGrowth.SPLIT_PATTERN, " ");
+    File inputDir = getTestTempDir("transactions");
+    File outputDir = getTestTempDir("frequentpatterns");
+    File input = new File(inputDir, "synth_test.txt");
+    params.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    params.set(PFPGrowth.OUTPUT, outputDir.getAbsolutePath());
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+        "FPGsynth.dat").openStream()), "\\s+");
+      Collection<List<String>> transactions = Lists.newArrayList();
+      
+      while (it.hasNext()) {
+        Pair<List<String>,Long> next = it.next();
+        transactions.add(next.getFirst());
+      }
+      
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = " ";
+        }
+        writer.write("\n");
+      }
+      
+    } finally {
+      Closeables.close(writer, false);
+    }
+  }
+
+  @Test
+  public void testVsSequential() throws Exception {
+
+    Map<Set<String>,Long> parallelResult = Maps.newHashMap();
+
+    PFPGrowth.runPFPGrowth(params);
+    List<Pair<String,TopKStringPatterns>> tmpParallel = PFPGrowth.readFrequentPattern(params);
+    
+    for (Pair<String,TopKStringPatterns> topK : tmpParallel) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        parallelResult.put(Sets.newHashSet(entry.getFirst()), entry.getSecond());
+      }
+    }
+
+    String inputFilename= "FPGsynth.dat";
+    int minSupport= 100;
+
+    final Map<Set<String>,Long> seqResult = Maps.newHashMap();
+    
+    FPGrowthObj<String> fpSeq = new FPGrowthObj<String>();
+    fpSeq.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+      fpSeq.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+           .openStream()), "\\s+"), minSupport), minSupport, 1000000, 
+      null,
+      new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+        @Override
+        public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+          for (Pair<List<String>,Long> v : value) {
+            List<String> l = v.getFirst();
+            seqResult.put(Sets.newHashSet(l), v.getSecond());
+          }
+        }
+        
+      });
+
+    for (Entry<Set<String>,Long> entry : parallelResult.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (seqResult.get(key) == null) {
+        log.info("spurious (1): " + key+ " with " +entry.getValue());
+      } else {
+        if (seqResult.get(key).equals(parallelResult.get(entry.getKey()))) {
+          log.info("matched (1): " + key + ", with: " + seqResult.get(key));
+        } else {
+          log.info("invalid (1): " + key + ", expected: " + seqResult.get(key) + ", got: "
+                       + parallelResult.get(entry.getKey()));
+        }
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : seqResult.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (parallelResult.get(key) == null) {
+        log.info("missing (1): " + key+ " with " +entry.getValue());
+      }
+    }
+    assertEquals(seqResult.size(), parallelResult.size());
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
index e69de29b..813a0260 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
@@ -0,0 +1,134 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public final class PFPGrowthTest extends MahoutTestCase {
+  
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthTest.class);
+  
+  private final Parameters params = new Parameters();
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    params.set(PFPGrowth.MIN_SUPPORT, "3");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "4");
+    params.set(PFPGrowth.NUM_GROUPS, "2");
+    params.set(PFPGrowth.ENCODING, "UTF-8");
+    File inputDir = getTestTempDir("transactions");
+    File outputDir = getTestTempDir("frequentpatterns");
+    File input = new File(inputDir, "test.txt");
+    params.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    params.set(PFPGrowth.OUTPUT, outputDir.getAbsolutePath());
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      Collection<List<String>> transactions = Lists.newArrayList();
+      transactions.add(Arrays.asList("E", "A", "D", "B"));
+      transactions.add(Arrays.asList("D", "A", "C", "E", "B"));
+      transactions.add(Arrays.asList("C", "A", "B", "E"));
+      transactions.add(Arrays.asList("B", "A", "D"));
+      transactions.add(Arrays.asList("D", "D", "", "D", "D"));
+      transactions.add(Arrays.asList("D", "B"));
+      transactions.add(Arrays.asList("A", "D", "E"));
+      transactions.add(Arrays.asList("B", "C"));
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = ",";
+        }
+        writer.write("\n");
+      }
+    } finally {
+      Closeables.close(writer, false);
+    }
+    
+  }
+
+  /**
+   * Test Parallel FPGrowth on small example data using top-level
+   * runPFPGrowth() method
+   */ 
+  @Test
+  public void testStartParallelFPGrowth() throws Exception {
+    PFPGrowth.runPFPGrowth(params, getConfiguration());
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+
+    assertEquals("[(A,([A],5), ([D, A],4), ([B, A],4), ([A, E],4)), "
+                 + "(B,([B],6), ([B, D],4), ([B, A],4), ([B, D, A],3)), " 
+                 + "(C,([B, C],3)), "
+                 + "(D,([D],6), ([D, A],4), ([B, D],4), ([D, A, E],3)), "
+                 + "(E,([A, E],4), ([D, A, E],3), ([B, A, E],3))]", frequentPatterns.toString());
+  }
+
+  /**
+   * Test Parallel FPGrowth on small example data using top-level
+   * runPFPGrowth() method
+   */ 
+  @Test
+  public void testStartParallelFPGrowthInSteps() throws Exception {
+    Configuration conf = getConfiguration();
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startParallelCounting(params, conf);
+    log.info("Reading fList Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
+    log.info("{}", fList);
+    assertEquals("[(B,6), (D,6), (A,5), (E,4), (C,3)]", fList.toString());
+ 
+    PFPGrowth.saveFList(fList, params, conf);
+    int numGroups = params.getInt(PFPGrowth.NUM_GROUPS, 
+                                  PFPGrowth.NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) {
+      maxPerGroup++;
+    }
+    params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
+
+    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startParallelFPGrowth(params, conf);
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startAggregating(params, conf);
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+    assertEquals("[(A,([A],5), ([D, A],4), ([B, A],4), ([A, E],4)), "
+                 + "(B,([B],6), ([B, D],4), ([B, A],4), ([B, D, A],3)), " 
+                 + "(C,([B, C],3)), "
+                 + "(D,([D],6), ([D, A],4), ([B, D],4), ([D, A, E],3)), "
+                 + "(E,([A, E],4), ([D, A, E],3), ([B, A, E],3))]", frequentPatterns.toString());
+    
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java
index e69de29b..12f55821 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java
@@ -0,0 +1,136 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public final class PFPGrowthTest2 extends MahoutTestCase {
+  
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthTest.class);
+  
+  private final Parameters params = new Parameters();
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    params.set(PFPGrowth.MIN_SUPPORT, "3");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "4");
+    params.set(PFPGrowth.NUM_GROUPS, "2");
+    params.set(PFPGrowth.ENCODING, "UTF-8");
+    params.set(PFPGrowth.USE_FPG2, "true");
+    File inputDir = getTestTempDir("transactions");
+    File outputDir = getTestTempDir("frequentpatterns");
+    File input = new File(inputDir, "test.txt");
+    params.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    params.set(PFPGrowth.OUTPUT, outputDir.getAbsolutePath());
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      Collection<List<String>> transactions = Lists.newArrayList();
+      transactions.add(Arrays.asList("E", "A", "D", "B"));
+      transactions.add(Arrays.asList("D", "A", "C", "E", "B"));
+      transactions.add(Arrays.asList("C", "A", "B", "E"));
+      transactions.add(Arrays.asList("B", "A", "D"));
+      transactions.add(Arrays.asList("D", "D", "", "D", "D"));
+      transactions.add(Arrays.asList("D", "B"));
+      transactions.add(Arrays.asList("A", "D", "E"));
+      transactions.add(Arrays.asList("B", "C"));
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = ",";
+        }
+        writer.write("\n");
+      }
+    } finally {
+      Closeables.close(writer, false);
+    }
+    
+  }
+
+  /**
+   * Test Parallel FPGrowth on small example data using top-level
+   * runPFPGrowth() method
+   */ 
+  @Test
+  public void testStartParallelFPGrowth() throws Exception {
+    PFPGrowth.runPFPGrowth(params);
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+
+    assertEquals("[(A,([A],5), ([D, A],4), ([B, A],4), ([A, E],4)), "
+                 + "(B,([B],6), ([B, D],4), ([B, A],4), ([B, D, A],3)), " 
+                 + "(C,([B, C],3)), "
+                 + "(D,([D],6), ([D, A],4), ([B, D],4), ([D, A, E],3)), "
+                 + "(E,([A, E],4), ([D, A, E],3), ([B, A, E],3))]", frequentPatterns.toString());                                                                 
+  }
+
+  /**
+   * Test Parallel FPG on small example data, running various stages
+   * individually
+   */ 
+  @Test
+  public void testStartParallelFPGrowthInSteps() throws Exception {
+    Configuration conf = new Configuration();
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startParallelCounting(params, conf);
+    log.info("Reading fList Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
+    log.info("{}", fList);
+    assertEquals("[(B,6), (D,6), (A,5), (E,4), (C,3)]", fList.toString());
+ 
+    PFPGrowth.saveFList(fList, params, conf);
+    int numGroups = params.getInt(PFPGrowth.NUM_GROUPS, 
+                                  PFPGrowth.NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) {
+      maxPerGroup++;
+    }
+    params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
+
+    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startParallelFPGrowth(params, conf);
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
+    PFPGrowth.startAggregating(params, conf);
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+
+    assertEquals("[(A,([A],5), ([D, A],4), ([B, A],4), ([A, E],4)), "
+                 + "(B,([B],6), ([B, D],4), ([B, A],4), ([B, D, A],3)), " 
+                 + "(C,([B, C],3)), "
+                 + "(D,([D],6), ([D, A],4), ([B, D],4), ([D, A, E],3)), "
+                 + "(E,([A, E],4), ([D, A, E],3), ([B, A, E],3))]", frequentPatterns.toString());                                                                 
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java
index e69de29b..9fdc685d 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java
@@ -0,0 +1,121 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.util.Iterator;
+import java.util.Random;
+
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.RandomUtils;
+import org.apache.mahout.math.list.IntArrayList;
+import org.junit.Before;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public final class TransactionTreeTest extends MahoutTestCase {
+
+  private static final Logger log = LoggerFactory.getLogger(TransactionTreeTest.class);
+
+  private static final int MAX_DUPLICATION = 50;
+  private static final int MAX_FEATURES = 30;
+  private static final int MAX_TRANSACTIONS = 500000;
+  private static final int MEGABYTE = 1000000;
+  private static final int NUM_OF_FPTREE_FIELDS = 4;
+  private static final int SIZE_INT = 4;
+  private static final int SIZE_LONG = 8;
+  private static final int SKIP_RATE = 10;
+
+  private Random gen;
+
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    gen = RandomUtils.getRandom();
+  }
+
+  private IntArrayList generateRandomArray() {
+    IntArrayList list = new IntArrayList();
+    for (int i = 0; i < MAX_FEATURES; i++) {
+      if (gen.nextInt() % SKIP_RATE == 0) {
+        list.add(i);
+      }
+    }
+    return list;
+  }
+
+  @Test
+  public void testTransactionTree() {
+    
+    TransactionTree tree = new TransactionTree();
+    int nodes = 0;
+    int total = 0;
+    for (int i = 0; i < MAX_TRANSACTIONS; i++) {
+      IntArrayList array = generateRandomArray();
+      total += array.size();
+      nodes += tree.addPattern(array, 1 + gen.nextInt(MAX_DUPLICATION));
+    }
+
+    log.info("Input integers: {}", total);
+    log.info("Input data Size: {}", total * SIZE_INT / (double) MEGABYTE);
+    log.info("Nodes in Tree: {}", nodes);
+    log.info("Size of Tree: {}", (nodes * SIZE_INT * NUM_OF_FPTREE_FIELDS + tree.childCount() * SIZE_INT)
+        / (double) MEGABYTE);
+
+    TransactionTree vtree = new TransactionTree();
+    StringBuilder sb = new StringBuilder();
+    int count = 0;
+    int items = 0;
+    Iterator<Pair<IntArrayList,Long>> it = tree.iterator();
+    while (it.hasNext()) {
+      Pair<IntArrayList,Long> p = it.next();
+      vtree.addPattern(p.getFirst(), p.getSecond());
+      items += p.getFirst().size();
+      count++;
+      sb.append(p);
+    }
+
+    log.info("Number of transaction integers: {}", items);
+    log.info("Size of Transactions: {}", (items * SIZE_INT + count * SIZE_LONG) / (double) MEGABYTE);
+    log.info("Number of Transactions: {}", count);
+
+    tree.getCompressedTree();
+    it = vtree.iterator();
+    StringBuilder sb1 = new StringBuilder();
+    while (it.hasNext()) {
+      sb1.append(it.next());
+    }
+    assertEquals(sb.toString(), sb1.toString());
+
+    TransactionTree mtree = new TransactionTree();
+    MultiTransactionTreeIterator mt = new MultiTransactionTreeIterator(vtree.iterator());
+    while (mt.hasNext()) {
+      mtree.addPattern(mt.next(), 1);
+    }
+
+    it = mtree.iterator();
+    StringBuilder sb2 = new StringBuilder();
+    while (it.hasNext()) {
+      sb2.append(it.next());
+    }
+    assertEquals(sb.toString(), sb2.toString());
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeapTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeapTest.java
index e69de29b..b4d98be0 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeapTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeapTest.java
@@ -0,0 +1,64 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth;
+
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Random;
+
+import com.google.common.collect.Sets;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.RandomUtils;
+import org.junit.Test;
+
+public final class FrequentPatternMaxHeapTest extends MahoutTestCase {
+
+  @Test
+  public void testMapHeap() {
+    Random gen = RandomUtils.getRandom();
+
+    FrequentPatternMaxHeap pq = new FrequentPatternMaxHeap(50, true);
+    for (int i = 0; i < 20; i++) {
+      FrequentPatternMaxHeap rs = new FrequentPatternMaxHeap(50, false);
+      for (int j = 0; j < 1000; j++) {
+        Pattern p = generateRandomPattern(gen);
+        rs.insert(p);
+      }
+      for (Pattern p : rs.getHeap()) {
+        pq.insert(p);
+      }
+    }
+  }
+
+  private static Pattern generateRandomPattern(Random gen) {
+    int length = 1 + Math.abs(gen.nextInt() % 6);
+    Pattern p = new Pattern();
+    Collection<Integer> set = Sets.newHashSet();
+    for (int i = 0; i < length; i++) {
+      int id = Math.abs(gen.nextInt() % 20);
+      while (set.contains(id)) {
+        id = Math.abs(gen.nextInt() % 20);
+      }
+      set.add(id);
+      int s = 5 + gen.nextInt() % 4;
+      p.add(id, s);
+    }
+    Arrays.sort(p.getPattern());
+    return p;
+  }
+}
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/DeliciousTagsExample.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/DeliciousTagsExample.java
index e69de29b..bee1e873 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/DeliciousTagsExample.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/DeliciousTagsExample.java
@@ -0,0 +1,94 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+
+import org.apache.commons.cli2.CommandLine;
+import org.apache.commons.cli2.Group;
+import org.apache.commons.cli2.Option;
+import org.apache.commons.cli2.OptionException;
+import org.apache.commons.cli2.builder.ArgumentBuilder;
+import org.apache.commons.cli2.builder.DefaultOptionBuilder;
+import org.apache.commons.cli2.builder.GroupBuilder;
+import org.apache.commons.cli2.commandline.Parser;
+import org.apache.mahout.common.CommandLineUtil;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.commandline.DefaultOptionCreator;
+import org.apache.mahout.fpm.pfpgrowth.dataset.KeyBasedStringTupleGrouper;
+
+public final class DeliciousTagsExample {
+  private DeliciousTagsExample() { }
+  
+  public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
+    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
+    ArgumentBuilder abuilder = new ArgumentBuilder();
+    GroupBuilder gbuilder = new GroupBuilder();
+    Option inputDirOpt = DefaultOptionCreator.inputOption().create();
+    
+    Option outputOpt = DefaultOptionCreator.outputOption().create();
+    
+    Option helpOpt = DefaultOptionCreator.helpOption();
+    Option recordSplitterOpt = obuilder.withLongName("splitterPattern").withArgument(
+      abuilder.withName("splitterPattern").withMinimum(1).withMaximum(1).create()).withDescription(
+      "Regular Expression pattern used to split given line into fields."
+          + " Default value splits comma or tab separated fields."
+          + " Default Value: \"[ ,\\t]*\\t[ ,\\t]*\" ").withShortName("regex").create();
+    Option encodingOpt = obuilder.withLongName("encoding").withArgument(
+      abuilder.withName("encoding").withMinimum(1).withMaximum(1).create()).withDescription(
+      "(Optional) The file encoding.  Default value: UTF-8").withShortName("e").create();
+    Group group = gbuilder.withName("Options").withOption(inputDirOpt).withOption(outputOpt).withOption(
+      helpOpt).withOption(recordSplitterOpt).withOption(encodingOpt).create();
+    
+    try {
+      Parser parser = new Parser();
+      parser.setGroup(group);
+      CommandLine cmdLine = parser.parse(args);
+      
+      if (cmdLine.hasOption(helpOpt)) {
+        CommandLineUtil.printHelp(group);
+        return;
+      }
+      Parameters params = new Parameters();
+      if (cmdLine.hasOption(recordSplitterOpt)) {
+        params.set("splitPattern", (String) cmdLine.getValue(recordSplitterOpt));
+      }
+      
+      String encoding = "UTF-8";
+      if (cmdLine.hasOption(encodingOpt)) {
+        encoding = (String) cmdLine.getValue(encodingOpt);
+      }
+      params.set("encoding", encoding);
+      String inputDir = (String) cmdLine.getValue(inputDirOpt);
+      String outputDir = (String) cmdLine.getValue(outputOpt);
+      params.set("input", inputDir);
+      params.set("output", outputDir);
+      params.set("groupingFieldCount", "2");
+      params.set("gfield0", "1");
+      params.set("gfield1", "2");
+      params.set("selectedFieldCount", "1");
+      params.set("field0", "3");
+      params.set("maxTransactionLength", "100");
+      KeyBasedStringTupleGrouper.startJob(params);
+      
+    } catch (OptionException ex) {
+      CommandLineUtil.printHelp(group);
+    }
+    
+  }
+}
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleCombiner.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleCombiner.java
index e69de29b..6b04f652 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleCombiner.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleCombiner.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.dataset;
+
+import java.io.IOException;
+import java.util.Set;
+
+import com.google.common.collect.Sets;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.mahout.common.StringTuple;
+
+public class KeyBasedStringTupleCombiner extends Reducer<Text,StringTuple,Text,StringTuple> {
+  
+  @Override
+  protected void reduce(Text key,
+                        Iterable<StringTuple> values,
+                        Context context) throws IOException, InterruptedException {
+    Set<String> outputValues = Sets.newHashSet();
+    for (StringTuple value : values) {
+      outputValues.addAll(value.getEntries());
+    }
+    context.write(key, new StringTuple(outputValues));
+  }
+}
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleGrouper.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleGrouper.java
index e69de29b..da3e4414 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleGrouper.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleGrouper.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.dataset;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
+import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.StringTuple;
+
+public final class KeyBasedStringTupleGrouper {
+  
+  private KeyBasedStringTupleGrouper() { }
+  
+  public static void startJob(Parameters params) throws IOException,
+                                                InterruptedException,
+                                                ClassNotFoundException {
+    Configuration conf = new Configuration();
+    
+    conf.set("job.parameters", params.toString());
+    conf.set("mapred.compress.map.output", "true");
+    conf.set("mapred.output.compression.type", "BLOCK");
+    conf.set("mapred.map.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec");
+    conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,"
+                                  + "org.apache.hadoop.io.serializer.WritableSerialization");
+    
+    String input = params.get("input");
+    Job job = new Job(conf, "Generating dataset based from input" + input);
+    job.setJarByClass(KeyBasedStringTupleGrouper.class);
+    
+    job.setMapOutputKeyClass(Text.class);
+    job.setMapOutputValueClass(StringTuple.class);
+    
+    job.setOutputKeyClass(Text.class);
+    job.setOutputValueClass(Text.class);
+    
+    FileInputFormat.addInputPath(job, new Path(input));
+    Path outPath = new Path(params.get("output"));
+    FileOutputFormat.setOutputPath(job, outPath);
+    
+    HadoopUtil.delete(conf, outPath);
+
+    job.setInputFormatClass(TextInputFormat.class);
+    job.setMapperClass(KeyBasedStringTupleMapper.class);
+    job.setCombinerClass(KeyBasedStringTupleCombiner.class);
+    job.setReducerClass(KeyBasedStringTupleReducer.class);
+    job.setOutputFormatClass(TextOutputFormat.class);
+    
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
+  }
+}
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleMapper.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleMapper.java
index e69de29b..a31c2ebc 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleMapper.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleMapper.java
@@ -0,0 +1,90 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.dataset;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.List;
+import java.util.regex.Pattern;
+
+import com.google.common.collect.Lists;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.StringTuple;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Splits the line using a {@link Pattern} and outputs key as given by the groupingFields
+ * 
+ */
+public class KeyBasedStringTupleMapper extends Mapper<LongWritable,Text,Text,StringTuple> {
+  
+  private static final Logger log = LoggerFactory.getLogger(KeyBasedStringTupleMapper.class);
+  
+  private Pattern splitter;
+  
+  private int[] selectedFields;
+  
+  private int[] groupingFields;
+  
+  @Override
+  protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
+    String[] fields = splitter.split(value.toString());
+    if (fields.length != 4) {
+      log.info("{} {}", fields.length, value.toString());
+      context.getCounter("Map", "ERROR").increment(1);
+      return;
+    }
+    Collection<String> oKey = Lists.newArrayList();
+    for (int groupingField : groupingFields) {
+      oKey.add(fields[groupingField]);
+      context.setStatus(fields[groupingField]);
+    }
+    
+    List<String> oValue = Lists.newArrayList();
+    for (int selectedField : selectedFields) {
+      oValue.add(fields[selectedField]);
+    }
+    
+    context.write(new Text(oKey.toString()), new StringTuple(oValue));
+    
+  }
+  
+  @Override
+  protected void setup(Context context) throws IOException, InterruptedException {
+    super.setup(context);
+    Parameters params = new Parameters(context.getConfiguration().get("job.parameters", ""));
+    splitter = Pattern.compile(params.get("splitPattern", "[ \t]*\t[ \t]*"));
+    
+    int selectedFieldCount = Integer.valueOf(params.get("selectedFieldCount", "0"));
+    selectedFields = new int[selectedFieldCount];
+    for (int i = 0; i < selectedFieldCount; i++) {
+      selectedFields[i] = Integer.valueOf(params.get("field" + i, "0"));
+    }
+    
+    int groupingFieldCount = Integer.valueOf(params.get("groupingFieldCount", "0"));
+    groupingFields = new int[groupingFieldCount];
+    for (int i = 0; i < groupingFieldCount; i++) {
+      groupingFields[i] = Integer.valueOf(params.get("gfield" + i, "0"));
+    }
+    
+  }
+}
diff --git a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleReducer.java b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleReducer.java
index e69de29b..3734739f 100644
--- a/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleReducer.java
+++ b/mahout/trunk/examples/src/main/java/org/apache/mahout/fpm/pfpgrowth/dataset/KeyBasedStringTupleReducer.java
@@ -0,0 +1,74 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.dataset;
+
+import java.io.IOException;
+import java.util.Collection;
+
+import com.google.common.collect.Sets;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.StringTuple;
+
+public class KeyBasedStringTupleReducer extends Reducer<Text,StringTuple,Text,Text> {
+  
+  private int maxTransactionLength = 100;
+  
+  @Override
+  protected void reduce(Text key, Iterable<StringTuple> values, Context context)
+    throws IOException, InterruptedException {
+    Collection<String> items = Sets.newHashSet();
+    
+    for (StringTuple value : values) {
+      for (String field : value.getEntries()) {
+        items.add(field);
+      }
+    }
+    if (items.size() > 1) {
+      int i = 0;
+      StringBuilder sb = new StringBuilder();
+      String sep = "";
+      for (String field : items) {
+        if (i % maxTransactionLength == 0) {
+          if (i != 0) {
+            context.write(null, new Text(sb.toString()));
+          }
+          sb.replace(0, sb.length(), "");
+          sep = "";
+        }
+        
+        sb.append(sep).append(field);
+        sep = "\t";
+        
+        i++;
+        
+      }
+      if (sb.length() > 0) {
+        context.write(null, new Text(sb.toString()));
+      }
+    }
+  }
+  
+  @Override
+  protected void setup(Context context) throws IOException, InterruptedException {
+    super.setup(context);
+    Parameters params = new Parameters(context.getConfiguration().get("job.parameters", ""));
+    maxTransactionLength = Integer.valueOf(params.get("maxTransactionLength", "100"));
+  }
+}
