diff --git a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
index 8335dc23..272e6f09 100644
--- a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
+++ b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
@@ -582,6 +582,9 @@ void sealFlushedSegment(FlushedSegment flushedSegment) throws IOException {
           infoStream.message("DWPT", "flush: write " + delCount + " deletes gen=" + flushedSegment.segmentInfo.getDelGen());
         }
 
+        // TODO: we should prune the segment if it's 100%
+        // deleted... but merge will also catch it.
+
         // TODO: in the NRT case it'd be better to hand
         // this del vector over to the
         // shortly-to-be-opened SegmentReader and let it
diff --git a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/index/TermsHash.java b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/index/TermsHash.java
index e8df788d..dcb9d353 100644
--- a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/index/TermsHash.java
+++ b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/index/TermsHash.java
@@ -90,8 +90,8 @@ public void abort() {
   // Clear all state
   void reset() {
     // we don't reuse so we drop everything and don't fill with 0
-    intPool.reset(); 
-    bytePool.reset();
+    intPool.reset(false, false); 
+    bytePool.reset(false, false);
   }
 
   @Override
diff --git a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/util/ByteBlockPool.java b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/util/ByteBlockPool.java
index b6f457f2..3b45f5c8 100644
--- a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/util/ByteBlockPool.java
+++ b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/util/ByteBlockPool.java
@@ -21,8 +21,6 @@
 import java.util.List;
 
 import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.IntBlockPool.SliceReader;
-import org.apache.lucene.util.IntBlockPool.SliceWriter;
 
 import static org.apache.lucene.util.RamUsageEstimator.NUM_BYTES_OBJECT_REF;
 
@@ -183,6 +181,7 @@ public void reset(boolean zeroFillBuffers, boolean reuseFirst) {
        buffer = buffers[0];
      } else {
        bufferUpto = -1;
+       buffers[0] = null;
        byteUpto = BYTE_BLOCK_SIZE;
        byteOffset = -BYTE_BLOCK_SIZE;
        buffer = null;
diff --git a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/util/IntBlockPool.java b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/util/IntBlockPool.java
index 398aaafb..772f9aae 100644
--- a/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/util/IntBlockPool.java
+++ b/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/util/IntBlockPool.java
@@ -133,6 +133,7 @@ public void reset(boolean zeroFillBuffers, boolean reuseFirst) {
        buffer = buffers[0];
      } else {
        bufferUpto = -1;
+        buffers[0] = null;
        intUpto = INT_BLOCK_SIZE;
        intOffset = -INT_BLOCK_SIZE;
        buffer = null;
diff --git a/lucene/dev/trunk/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java b/lucene/dev/trunk/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
index 363547fe..b3cf7b43 100644
--- a/lucene/dev/trunk/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
+++ b/lucene/dev/trunk/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
@@ -114,7 +114,7 @@ public void testMultiConfig() throws Throwable {
       Directory dir1 = newDirectory();
       Directory dir2 = newDirectory();
       if (VERBOSE) {
-        System.out.println("  nThreads=" + nThreads + " iter=" + iter + " range=" + range + " doPooling=" + doReaderPooling + " maxThreadStates=" + maxThreadStates + " sameFieldOrder=" + sameFieldOrder + " mergeFactor=" + mergeFactor);
+        System.out.println("  nThreads=" + nThreads + " iter=" + iter + " range=" + range + " doPooling=" + doReaderPooling + " maxThreadStates=" + maxThreadStates + " sameFieldOrder=" + sameFieldOrder + " mergeFactor=" + mergeFactor + " maxBufferedDocs=" + maxBufferedDocs);
       }
       Map<String,Document> docs = indexRandom(nThreads, iter, range, dir1, maxThreadStates, doReaderPooling);
       if (VERBOSE) {
@@ -334,9 +334,10 @@ public void verifyEquals(DirectoryReader r1, DirectoryReader r2, String idField)
     if (fields == null) {
       // make sure r1 is in fact empty (eg has only all
       // deleted docs):
+      Bits liveDocs = MultiFields.getLiveDocs(r1);
       DocsEnum docs = null;
       while(termsEnum.next() != null) {
-        docs = _TestUtil.docs(random(), termsEnum, null, docs, 0);
+        docs = _TestUtil.docs(random(), termsEnum, liveDocs, docs, 0);
         while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           fail("r1 is not empty but r2 is");
         }
