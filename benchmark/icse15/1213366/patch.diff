diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
index 55fa29b7..dca87299 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
@@ -71,6 +71,8 @@ public static Side getSide(String sideName) {
   private int curTermLength;
   private int curGramSize;
   private int tokStart;
+  private int tokEnd; // only used if the length changed before this filter
+  private boolean hasIllegalOffsets; // only if the length changed before this filter
   
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
@@ -126,6 +128,10 @@ public final boolean incrementToken() throws IOException {
           curTermLength = termAtt.length();
           curGramSize = minGram;
           tokStart = offsetAtt.startOffset();
+          tokEnd = offsetAtt.endOffset();
+          // if length by start + end offsets doesn't match the term text then assume
+          // this is a synonym and don't adjust the offsets.
+          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;
         }
       }
       if (curGramSize <= maxGram) {
@@ -135,7 +141,11 @@ public final boolean incrementToken() throws IOException {
           int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;
           int end = start + curGramSize;
           clearAttributes();
+          if (hasIllegalOffsets) {
+            offsetAtt.setOffset(tokStart, tokEnd);
+          } else {
           offsetAtt.setOffset(tokStart + start, tokStart + end);
+          }
           termAtt.copyBuffer(curTermBuffer, start, curGramSize);
           curGramSize++;
           return true;
diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
index c73208bf..d6cffb5a 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
@@ -38,6 +38,8 @@
   private int curGramSize;
   private int curPos;
   private int tokStart;
+  private int tokEnd; // only used if the length changed before this filter
+  private boolean hasIllegalOffsets; // only if the length changed before this filter
   
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
@@ -81,13 +83,21 @@ public final boolean incrementToken() throws IOException {
           curGramSize = minGram;
           curPos = 0;
           tokStart = offsetAtt.startOffset();
+          tokEnd = offsetAtt.endOffset();
+          // if length by start + end offsets doesn't match the term text then assume
+          // this is a synonym and don't adjust the offsets.
+          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;
         }
       }
       while (curGramSize <= maxGram) {
         while (curPos+curGramSize <= curTermLength) {     // while there is input
           clearAttributes();
           termAtt.copyBuffer(curTermBuffer, curPos, curGramSize);
+          if (hasIllegalOffsets) {
+            offsetAtt.setOffset(tokStart, tokEnd);
+          } else {
           offsetAtt.setOffset(tokStart + curPos, tokStart + curPos + curGramSize);
+          }
           curPos++;
           return true;
         }
diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
index c518ffe7..268563c8 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
@@ -17,11 +17,17 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.ASCIIFoldingFilter;
 
+import java.io.Reader;
 import java.io.StringReader;
 
 /**
@@ -104,4 +110,24 @@ public void testReset() throws Exception {
     tokenizer.reset(new StringReader("abcde"));
     assertTokenStreamContents(filter, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
+  
+  // LUCENE-3642
+  // EdgeNgram blindly adds term length to offset, but this can take things out of bounds
+  // wrt original text if a previous filter increases the length of the word (in this case æ -> ae)
+  // so in this case we behave like WDF, and preserve any modified offsets
+  public void testInvalidOffsets() throws Exception {
+    Analyzer analyzer = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
+        TokenFilter filters = new ASCIIFoldingFilter(tokenizer);
+        filters = new EdgeNGramTokenFilter(filters, EdgeNGramTokenFilter.Side.FRONT, 2, 15);
+        return new TokenStreamComponents(tokenizer, filters);
+      }
+    };
+    assertAnalyzesTo(analyzer, "mosfellsbær",
+        new String[] { "mo", "mos", "mosf", "mosfe", "mosfel", "mosfell", "mosfells", "mosfellsb", "mosfellsba", "mosfellsbae", "mosfellsbaer" },
+        new int[]    {    0,     0,      0,       0,        0,         0,          0,           0,            0,             0,              0 },
+        new int[]    {   11,    11,     11,      11,       11,        11,         11,          11,           11,            11,             11 });
+  }
 }
diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
index c18371aa..e332c9ec 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
@@ -17,11 +17,17 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.ASCIIFoldingFilter;
 
+import java.io.Reader;
 import java.io.StringReader;
 
 /**
@@ -93,4 +99,24 @@ public void testReset() throws Exception {
       tokenizer.reset(new StringReader("abcde"));
       assertTokenStreamContents(filter, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
+    
+    // LUCENE-3642
+    // EdgeNgram blindly adds term length to offset, but this can take things out of bounds
+    // wrt original text if a previous filter increases the length of the word (in this case æ -> ae)
+    // so in this case we behave like WDF, and preserve any modified offsets
+    public void testInvalidOffsets() throws Exception {
+      Analyzer analyzer = new ReusableAnalyzerBase() {
+        @Override
+        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
+          TokenFilter filters = new ASCIIFoldingFilter(tokenizer);
+          filters = new NGramTokenFilter(filters, 2, 2);
+          return new TokenStreamComponents(tokenizer, filters);
+        }
+      };
+      assertAnalyzesTo(analyzer, "mosfellsbær",
+          new String[] { "mo", "os", "sf", "fe", "el", "ll", "ls", "sb", "ba", "ae", "er" },
+          new int[]    {    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0 },
+          new int[]    {   11,   11,   11,   11,   11,   11,   11,   11,   11,   11,   11 });
+    }
 }
diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java
index 6f0ecea5..f33d56bc 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java
@@ -44,6 +44,10 @@
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
   private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
 
+  private int tokStart; // only used if the length changed before this filter
+  private int tokEnd; // only used if the length changed before this filter
+  private boolean hasIllegalOffsets; // only if the length changed before this filter
+
   /**
    * Construct a new WordTokenizer.
    * 
@@ -59,6 +63,11 @@ public boolean incrementToken() throws IOException {
     if (tokenIter == null || !tokenIter.hasNext()) {
       // there are no remaining tokens from the current sentence... are there more sentences?
       if (input.incrementToken()) {
+        tokStart = offsetAtt.startOffset();
+        tokEnd = offsetAtt.endOffset();
+        // if length by start + end offsets doesn't match the term text then assume
+        // this is a synonym and don't adjust the offsets.
+        hasIllegalOffsets = (tokStart + termAtt.length()) != tokEnd;
         // a new sentence is available: process it.
         tokenBuffer = wordSegmenter.segmentSentence(termAtt.toString(), offsetAtt.startOffset());
         tokenIter = tokenBuffer.iterator();
@@ -77,7 +86,11 @@ public boolean incrementToken() throws IOException {
     // There are remaining tokens from the current sentence, return the next one. 
     SegToken nextWord = tokenIter.next();
     termAtt.copyBuffer(nextWord.charArray, 0, nextWord.charArray.length);
+    if (hasIllegalOffsets) {
+      offsetAtt.setOffset(tokStart, tokEnd);
+    } else {
     offsetAtt.setOffset(nextWord.startOffset, nextWord.endOffset);
+    }
     typeAtt.setType("word");
     return true;
   }
diff --git a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java
index 3f7ad779..f244ddb1 100644
--- a/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java
+++ b/lucene/dev/branches/branch_3x/lucene/contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java
@@ -17,11 +17,17 @@
 
 package org.apache.lucene.analysis.cn.smart;
 
+import java.io.Reader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.ASCIIFoldingFilter;
 import org.apache.lucene.util.Version;
 
 public class TestSmartChineseAnalyzer extends BaseTokenStreamTestCase {
@@ -196,6 +202,24 @@ public void testLargeSentence() throws Exception {
     }
   }
   
+  // LUCENE-3642
+  public void testInvalidOffset() throws Exception {
+    Analyzer analyzer = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
+        TokenFilter filters = new ASCIIFoldingFilter(tokenizer);
+        filters = new WordTokenFilter(filters);
+        return new TokenStreamComponents(tokenizer, filters);
+      }
+    };
+    
+    assertAnalyzesTo(analyzer, "mosfellsbær", 
+        new String[] { "mosfellsbaer" },
+        new int[]    { 0 },
+        new int[]    { 11 });
+  }
+  
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
     checkRandomData(random, new SmartChineseAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
diff --git a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/analysis/CharTokenizer.java b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/analysis/CharTokenizer.java
index 530b42ec..926d794d 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/analysis/CharTokenizer.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/analysis/CharTokenizer.java
@@ -267,6 +267,7 @@ public final boolean incrementToken() throws IOException {
       return incrementTokenOld();
     int length = 0;
     int start = -1; // this variable is always initialized
+    int end = -1;
     char[] buffer = termAtt.buffer();
     while (true) {
       if (bufferIndex >= dataLen) {
@@ -285,15 +286,18 @@ public final boolean incrementToken() throws IOException {
       }
       // use CharacterUtils here to support < 3.1 UTF-16 code unit behavior if the char based methods are gone
       final int c = charUtils.codePointAt(ioBuffer.getBuffer(), bufferIndex);
-      bufferIndex += Character.charCount(c);
+      final int charCount = Character.charCount(c);
+      bufferIndex += charCount;
 
       if (isTokenChar(c)) {               // if it's a token char
         if (length == 0) {                // start of token
           assert start == -1;
-          start = offset + bufferIndex - 1;
+          start = offset + bufferIndex - charCount;
+          end = start;
         } else if (length >= buffer.length-1) { // check if a supplementary could run out of bounds
           buffer = termAtt.resizeBuffer(2+length); // make sure a supplementary fits in the buffer
         }
+        end += charCount;
         length += Character.toChars(normalize(c), buffer, length); // buffer it, normalized
         if (length >= MAX_WORD_LEN) // buffer overflow! make sure to check for >= surrogate pair could break == test
           break;
@@ -303,7 +307,7 @@ public final boolean incrementToken() throws IOException {
 
     termAtt.setLength(length);
     assert start != -1;
-    offsetAtt.setOffset(correctOffset(start), finalOffset = correctOffset(start+length));
+    offsetAtt.setOffset(correctOffset(start), finalOffset = correctOffset(end));
     return true;
     
   }
diff --git a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java
index ff6f9610..f1467439 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java
@@ -22,6 +22,12 @@
 import java.io.StringReader;
 
 import org.apache.lucene.util.Version;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.util._TestUtil;
 
 /**
  * Testcase for {@link CharTokenizer} subclasses
@@ -219,4 +225,80 @@ protected boolean isTokenChar(char c) {
       return Character.isLetter(c);
     }
   }
+  
+  // LUCENE-3642: normalize SMP->BMP and check that offsets are correct
+  public void testCrossPlaneNormalization() throws IOException {
+    Analyzer analyzer = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader) {
+          @Override
+          protected int normalize(int c) {
+            if (c > 0xffff) {
+              return 'δ';
+            } else {
+              return c;
+            }
+          }
+        };
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    int num = 10000 * RANDOM_MULTIPLIER;
+    for (int i = 0; i < num; i++) {
+      String s = _TestUtil.randomUnicodeString(random);
+      TokenStream ts = analyzer.tokenStream("foo", new StringReader(s));
+      ts.reset();
+      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
+      while (ts.incrementToken()) {
+        String highlightedText = s.substring(offsetAtt.startOffset(), offsetAtt.endOffset());
+        for (int j = 0, cp = 0; j < highlightedText.length(); j += Character.charCount(cp)) {
+          cp = highlightedText.codePointAt(j);
+          assertTrue("non-letter:" + Integer.toHexString(cp), Character.isLetter(cp));
+        }
+      }
+      ts.end();
+      ts.close();
+    }
+    // just for fun
+    checkRandomData(random, analyzer, num);
+  }
+  
+  // LUCENE-3642: normalize BMP->SMP and check that offsets are correct
+  public void testCrossPlaneNormalization2() throws IOException {
+    Analyzer analyzer = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader) {
+          @Override
+          protected int normalize(int c) {
+            if (c <= 0xffff) {
+              return 0x1043C;
+            } else {
+              return c;
+            }
+          }
+        };
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    int num = 10000 * RANDOM_MULTIPLIER;
+    for (int i = 0; i < num; i++) {
+      String s = _TestUtil.randomUnicodeString(random);
+      TokenStream ts = analyzer.tokenStream("foo", new StringReader(s));
+      ts.reset();
+      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
+      while (ts.incrementToken()) {
+        String highlightedText = s.substring(offsetAtt.startOffset(), offsetAtt.endOffset());
+        for (int j = 0, cp = 0; j < highlightedText.length(); j += Character.charCount(cp)) {
+          cp = highlightedText.codePointAt(j);
+          assertTrue("non-letter:" + Integer.toHexString(cp), Character.isLetter(cp));
+        }
+      }
+      ts.end();
+      ts.close();
+    }
+    // just for fun
+    checkRandomData(random, analyzer, num);
+  }
 }
diff --git a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestDuelingAnalyzers.java b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestDuelingAnalyzers.java
index 405ead8c..dd9cd4d8 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestDuelingAnalyzers.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/analysis/TestDuelingAnalyzers.java
@@ -11,3 +11,103 @@
   Merged /lucene/dev/branches/lucene_solr_3_2/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java:r1128223,1128247,1129418,1129472
   Merged /lucene/dev/branches/lucene_solr_3_3/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java:r1138390,1138979,1139775
   Merged /lucene/dev/trunk/lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java:r931298,931337,931502,932129-932131,932163,932304,932369,932374,932398,932417,932541,932576,932587,932698,932731-932749,932752,932773,932795,932828,932856-932857,932862,932864,932878,932963,932998-932999,933541-933575,933598,933613,933679,933879,934339,934954,935014-935048,935065,935186-935513,935521-935522,935553-935962,936522,936544,936605,936657-936726,937039,937360,938582-938646,938989,939111,939611,939649,940433,940447,940451-940452,940666,940699,940730,940878-940892,940994,941270,941363,941780,942166,942235,942288,942676,942719,943142,943493,943931,945057,945090,945130,945245,945343,945420,946139,946330,946338,946599,948011,948082,948429,949156,949288,949311,949318,949445,949976,949997,950008,950042,950458,950467,950613,950667,951126,951355,951397,951521,953628,955547,955613,955615,955796-955797,955809-955996,956097,956125,956173,956316,956715,957465,957481,957486,957520,957634,957707,960367,960371,960374,960719,962555,963372,963654,963720,963781,963873,963906,963909,963920,964019,964054,964430,964459,964720,964753,964832,964856,965103,965110,965222,965230,965299,965327,965330,965585,966354,966878,967080,979453,979809,980369,980428,980436,980501,980909,980911,980917,981265,981550,981598,981650,981661,981857,981936,982073,982084,982201,982323,982725,982824,983100,983212,983216,983313,983328,983495,983500,983530,983622,983632,983778,984187,984202,984232,984510,984968,985453,985455,985672,985875,986158,986173,986612,987122,988087,988206,988216,988259,988346,988478,988527,988543,988592,988613,988688,988710,988736,988739,989004,989010,989013,989030,989035,989315,989321,989334,989785,990160-990161,990180,990189,990281,990301,990451,990459,990766,990781,990854,991053,991191,991310,991497,992424,992469,992567,992571,992623,993106,993194,993199,993287,993408,994935,994976,994979,995247,995250,995376,995607,995772,996268,996357,996416,996511,996611,996623,996647-996653,996720,996942,996961,996978,997180,997230,998055,998505,998684,999016,999037,999137,999139,999152,999175,999223,999378,999409,999483,999545,999842,999984,1000000,1000424,1000428,1000581,1000597,1000675,1001006,1001010,1001129,1001318,1001420,1001661,1001796,1002002,1002739,1003107,1003291,1003614,1003631,1003645,1003841-1003852,1003873,1003877,1003906,1003938,1003954,1003978,1003990,1004038,1004082,1004179,1004200,1004215,1004241,1004335,1005310,1005356,1005363,1006146,1006280,1006290,1006324,1021340,1021357,1021360,1021439,1021449,1021969-1021971,1022165,1022191,1022632,1022708-1022710,1022730-1022735,1022748-1022755,1022762-1022793,1022798-1022802,1022805,1022826,1022927,1022939,1022956,1022989,1022998,1023006,1023009,1023022,1023040,1023106,1023235-1023246,1023250,1023264-1023265,1023312,1023329-1023330,1023346-1023347,1023355,1023493,1023509-1023511,1023518,1023520,1023535-1023536,1023562,1023579-1023588,1023594-1023595,1023600-1023602,1023606,1023621,1023635,1023637,1023711,1023845,1023870,1024196,1024219,1024233,1024238,1024256,1024292,1024305,1024338,1024395,1024402,1024408,1024475-1024476,1024486,1025545,1025547,1025570,1025579,1025597,1025669,1025929,1026044,1026058,1026129-1026130,1026167,1026336,1026431,1026446,1026456,1026460,1026592,1026606,1026610,1026738,1026841,1026868,1026882,1027743,1027788,1027998,1028039,1028386,1029096,1029325,1029333,1029345,1030012,1030019,1030073,1030078,1030754,1031076,1031219,1031460,1031467,1031474,1031480,1031496,1031686,1031689,1032570,1032776,1034007,1034011,1034017,1034342,1034361,1034763,1034921,1034975,1034977,1035096,1035103,1035194,1035205,1035214,1035395,1035397,1035420,1035535,1035651,1035996,1036088,1036970,1037077,1037154,1037223,1037406,1037429,1038562,1038785,1039068,1039314,1039688,1039737,1039759,1039773,1039778,1039868,1039911,1039917,1039962-1039967,1040064,1040290,1040390,1040447,1040463,1040608,1040815,1040935,1040940,1040982,1041844,1041914,1041954,1041963,1042008,1042185,1042213,1042315,1042359,1042373,1043071,1043114,1043148,1043277,1043693,1043749,1044066-1044069,1044098,1044257,1044315,1044328,1044505,1044561,1044635,1044660,1044854,1044867,1045212,1045266,1045310,1045315,1045322-1045323,1049094,1049107,1049117,1049131-1049132,1049144,1049187,1049413,1049502,1049693,1049918,1050063,1050084,1050687,1050697-1050725,1050728,1050733,1050737,1050813,1050827,1051041,1051058,1051305,1051715,1051872,1051891,1052898,1052926,1052974,1052980,1052991,1053236,1053405,1053509,1053896,1054015,1054164,1054172,1054405-1054406,1055285,1055408,1055435,1055595,1055877,1055892-1055906,1056014,1056428,1056702,1056821,1056955,1057010,1057149,1057221,1057340,1058284-1058288,1058324,1058393,1058939,1059426,1059719,1059866,1060023,1060324,1060437,1060608,1060779,1060807,1060846,1060872,1060997,1061050,1061065,1061078,1061350,1061424,1061499,1061622,1062070,1062123,1062153,1062319,1062451,1062454,1062509,1062604,1062633,1062876,1062879,1063323,1063333,1063478,1063493,1063498,1063501,1063513,1063702,1063762,1063837,1063842,1063868-1063869,1063877,1063897,1063908,1063920,1064330,1064379,1064735,1064781,1064844,1064942,1065059,1065095-1065096,1065102,1065261,1065265,1065272,1065286,1065302,1065304,1065327,1065337,1065410,1065416,1065465,1065474,1065572,1065601,1065621,1065719,1065853,1065855,1065891,1066008,1066691,1066764,1066819,1066850,1066889,1067119,1067131,1067160,1067163,1067165,1067299,1067427,1067551,1068387,1068979,1069316,1069341,1069656,1070183,1070185,1070206,1070240,1070321,1070691,1070760,1070879,1071074,1071417,1071435,1071569,1071594,1071654-1071655,1071658,1072127,1072250,1072567,1072591,1072607,1072683,1073336,1073806,1073850,1073957,1074009,1074017,1074226,1074326,1074357,1074726,1074750,1074952,1075023-1075024,1075069,1075072,1075079,1075089,1075103,1075184,1075190-1075191,1075196,1075287,1075443,1075505,1075850,1076032,1076237,1076279,1076311,1076315,1076319,1076325,1076433,1076884,1077908,1077916,1078058,1078117,1078127,1078398,1078448,1078451,1078463,1078471,1078500-1078501,1078512-1078515,1078529,1078540,1078553,1078563,1078570,1078580,1078599,1078614,1078639,1078659,1078670,1078681,1078770,1079707,1079786,1079949,1080038,1080258,1080424,1080443,1080445,1080647,1080665,1080691,1080762,1080970,1080979,1080985,1080988,1081012,1081017,1081777-1081778,1081790-1081791,1081795,1082186,1082514-1082516,1082601,1082687,1082720,1082730,1082776,1082865,1082919,1082926,1083010,1083213,1083447,1083459,1083991,1084045,1084210,1084247,1084273-1084274,1084327,1084544,1084549,1084566,1084929,1085004,1085089,1085224,1085241,1085423,1085515,1085530,1085689,1086276,1086584,1086629,1086821,1087319,1087426,1087722,1088021,1089335,1089813,1089815,1089906,1089918,1091132-1091159,1091499,1092105,1092136,1092328,1092396,1092812,1092848,1094014,1094214,1095120,1095260,1095432,1095861,1095937,1096073,1096077,1096178-1096183,1096194,1096249,1096301,1096315,1096334,1096339,1097187,1097216,1097627,1098303,1098357,1098367,1098375,1098532,1098633,1098730,1098740,1098800,1098860,1099041,1099340,1099529,1099582,1099745,1099999,1100435,1100437,1101047,1101056,1101072,1101088,1101539,1101572,1101574,1102058,1102120,1102290,1102377,1102658,1102718,1102785,1102817,1102827,1102907,1103024,1103048,1103077,1103102,1103120,1103155,1103979,1103983,1104421,1104432,1104452,1104519,1124160,1124266,1124293,1124307,1124316,1124330,1124366,1125006,1125150,1125165,1125376,1125932,1125972,1126022,1126091,1126280,1126284,1126487,1126573,1126642,1126645,1126761,1127156,1127247,1127301,1127436,1128105,1128246,1128253,1128549,1128830,1128844,1128854,1128856,1129398,1129403,1129413,1129427,1129450,1129453,1129456,1129459,1129465,1129645,1129656,1129694,1130039,1130042,1130052,1130063,1130150,1130439,1130527,1130547,1130648,1130852,1130858-1130859,1130861,1130954-1131005,1131150,1131158,1131371,1131395,1131401,1132391,1132517,1132620,1132729,1132806,1132855,1132969,1133021,1133136,1133187,1133330,1133383,1133385,1133486,1133553,1133565,1133599,1133616,1133631,1133646,1133839,1133937,1134163,1134328,1134515,1134592,1134685,1134763,1134781,1134895,1134995,1134998,1135009,1135011,1135154,1135204,1135300,1135369,1135509,1135525,1135527,1135537,1135650,1135658,1135670,1135764,1135801,1135818,1135822,1135825,1135954,1136027,1136080,1136357,1136467,1136568,1136605,1136644,1136789,1136792,1137054,1137060,1137064,1137162,1137211,1137330,1137357,1137477,1137480,1137529,1137533,1137665,1137733,1137882,1138030,1138069,1138319,1138405,1138446,1138450,1138821,1138890,1139054,1139173,1139178,1139188,1139199,1139285,1139513,1139789,1139995,1140004,1140119,1140243,1140252,1140498,1140574,1140720,1140827,1140836,1140851,1141167,1141170,1141295,1141400,1141593,1141629,1141999,1142179,1143122,1143189,1143238,1143420,1143558,1143766,1143783,1143878,1144294,1144415,1144513,1144792,1144841,1145158,1145163,1145182,1145198,1145233,1145239,1145255,1145263,1145292,1145442,1145479,1145502,1145518,1145594,1145657,1145701,1145730,1145885,1145925,1145957,1146638,1146984,1147023,1147578,1147586,1147671,1147691,1147807,1147881,1148596,1148602,1148681,1148728,1148763,1148968,1149028,1149050,1149108,1149256,1149740,1149746,1150091,1150362,1150384,1150389,1150394,1150404-1150405,1150415,1150478,1150480,1150486-1150489,1150671,1150840,1151081,1151146,1151720,1151782,1151984,1151997,1152024,1152055,1152089,1152288,1152456,1152525,1152530,1152653,1152669,1152892,1153399,1153408,1153844,1154005,1154926,1154936,1155278,1156053,1156590-1156591,1157437,1158342,1158697,1158730,1158819,1158832,1159291,1159418,1159627,1160832,1161488,1161505,1161964,1161966,1161972,1161974,1162135,1162156,1162158,1162166,1162375,1162394,1162401,1163370,1163568,1163576,1163589,1163625,1164287,1164311,1164620,1164956,1165902,1165995,1166106,1166457,1166530,1166541,1166582,1166656,1166702,1166715,1166728,1166784,1166850,1166866,1166954,1167008,1167199,1167467,1169612,1169816,1169820,1170157,1170203,1170586,1170616,1170699,1170716,1170725,1170908,1171556,1171570,1171597,1171691,1171704,1171739,1172227,1173139,1173423,1173430,1173720,1173778,1173961,1174377-1174407,1175300,1175376,1175385,1175397,1175413,1175425,1175475,1175529,1175579,1175650,1175696,1175699,1175956,1175975,1176097,1176114,1176478,1176772,1176774,1177048-1177049,1177723,1177940,1178612,1178923,1179315,1179677,1179762,1179956,1180124,1181265,1181268,1181299,1181659,1181664,1181760,1182982,1183458,1183464,1183582,1183738,1183753,1183756,1184753-1184754,1184761,1184822,1184851,1184877,1185120,1187900,1188597,1188777,1188975,1189039,1189160,1189186,1189655,1189903,1190029,1190107,1190410,1195082,1195101,1195275,1196228,1197469,1197690,1197742,1197879,1198009,1198024,1198039,1198089,1198134,1198332,1198371,1198636,1198777-1198778,1198911,1199405,1199832,1199837,1200007,1200051,1200080,1200274,1200440,1200480,1200854,1201036,1201165,1201191,1201329,1201375,1201855,1202152,1202657,1202754,1202969,1203114,1203206,1203756,1203966,1203970,1204416,1204453,1205021,1205152,1205342,1205360,1205366,1205430,1205774,1205954,1206017,1206033,1206070,1206143,1206229,1206436-1206437,1206452,1206707,1206767,1206789,1206996,1207070,1207103,1207291,1207577,1207718,1208032,1208118,1208509,1208525,1210020,1210054,1210469,1210714,1211710,1211827,1211887,1212894,1213013,1213016,1213020,1213033,1213044,1213106,1213329
+package org.apache.lucene.analysis;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+import java.io.StringReader;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Compares MockTokenizer (which is simple with no optimizations) with equivalent 
+ * core tokenizers (that have optimizations like buffering).
+ * 
+ * Any tests here need to probably consider unicode version of the JRE (it could
+ * cause false fails).
+ */
+public class TestDuelingAnalyzers extends LuceneTestCase {
+  
+  public void testLetterAscii() throws Exception {
+    Analyzer left = new MockAnalyzer(random, MockTokenizer.SIMPLE, false);
+    Analyzer right = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    for (int i = 0; i < 10000; i++) {
+      String s = _TestUtil.randomSimpleString(random);
+      assertEquals(s, left.tokenStream("foo", new StringReader(s)), 
+                   right.tokenStream("foo", new StringReader(s)));
+    }
+  }
+  
+  public void testLetterUnicode() throws Exception {
+    Analyzer left = new MockAnalyzer(random, MockTokenizer.SIMPLE, false);
+    Analyzer right = new ReusableAnalyzerBase() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    for (int i = 0; i < 10000; i++) {
+      String s = _TestUtil.randomUnicodeString(random);
+      assertEquals(s, left.tokenStream("foo", new StringReader(s)), 
+                   right.tokenStream("foo", new StringReader(s)));
+    }
+  }
+  
+  // we only check a few core attributes here.
+  // TODO: test other things
+  public void assertEquals(String s, TokenStream left, TokenStream right) throws Exception {
+    left.reset();
+    right.reset();
+    CharTermAttribute leftTerm = left.addAttribute(CharTermAttribute.class);
+    CharTermAttribute rightTerm = right.addAttribute(CharTermAttribute.class);
+    OffsetAttribute leftOffset = left.addAttribute(OffsetAttribute.class);
+    OffsetAttribute rightOffset = right.addAttribute(OffsetAttribute.class);
+    PositionIncrementAttribute leftPos = left.addAttribute(PositionIncrementAttribute.class);
+    PositionIncrementAttribute rightPos = right.addAttribute(PositionIncrementAttribute.class);
+    
+    while (left.incrementToken()) {
+      assertTrue("wrong number of tokens for input: " + s, right.incrementToken());
+      assertEquals("wrong term text for input: " + s, leftTerm.toString(), rightTerm.toString());
+      assertEquals("wrong position for input: " + s, leftPos.getPositionIncrement(), rightPos.getPositionIncrement());
+      assertEquals("wrong start offset for input: " + s, leftOffset.startOffset(), rightOffset.startOffset());
+      assertEquals("wrong end offset for input: " + s, leftOffset.endOffset(), rightOffset.endOffset());
+    };
+    assertFalse("wrong number of tokens for input: " + s, right.incrementToken());
+    left.end();
+    right.end();
+    assertEquals("wrong final offset for input: " + s, leftOffset.endOffset(), rightOffset.endOffset());
+    left.close();
+    right.close();
+  }
+}
diff --git a/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java b/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
index db82596e..93151a68 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
@@ -135,6 +135,10 @@ public static void assertTokenStreamContents(TokenStream ts, String[] output, in
         assertTrue("startOffset must be >= 0", offsetAtt.startOffset() >= 0);
         assertTrue("endOffset must be >= 0", offsetAtt.endOffset() >= 0);
         assertTrue("endOffset must be >= startOffset", offsetAtt.endOffset() >= offsetAtt.startOffset());
+        if (finalOffset != null) {
+          assertTrue("startOffset must be <= finalOffset", offsetAtt.startOffset() <= finalOffset.intValue());
+          assertTrue("endOffset must be <= finalOffset", offsetAtt.endOffset() <= finalOffset.intValue());
+        }
       }
       if (posIncrAtt != null) {
         assertTrue("posIncrement must be >= 0", posIncrAtt.getPositionIncrement() >= 0);
