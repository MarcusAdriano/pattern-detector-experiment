diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java
index 99d1db9e..dc4a25fe 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java
@@ -78,6 +78,8 @@ public int run(String[] args) throws Exception {
             + "Default Value:5 Recommended Values: [5-10]", "5");
     addOption("method", "method", "Method of processing: sequential|mapreduce", "sequential");
     addOption("encoding", "e", "(Optional) The file encoding.  Default value: UTF-8", "UTF-8");
+    addFlag("useFPG2", "2", "Use an alternate FPG implementation");
+
     if (parseArguments(args) == null) {
       return -1;
     }
@@ -112,6 +114,11 @@ public int run(String[] args) throws Exception {
       encoding = getOption("encoding");
     }
     params.set("encoding", encoding);
+
+    if (hasOption("useFPG2")) {
+      params.set(PFPGrowth.USE_FPG2, "true");
+    }
+
     Path inputDir = getInputPath();
     Path outputDir = getOutputPath();
 
@@ -148,7 +155,9 @@ private static void runFPGrowth(Parameters params) throws IOException {
 
     SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
 
-    FPGrowth<String> fp = new FPGrowth<String>();
+    if ("true".equals(params.get("useFPG2"))) {
+      org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj<String> fp 
+        = new org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj<String>();
     Collection<String> features = new HashSet<String>();
 
     try {
@@ -165,6 +174,25 @@ private static void runFPGrowth(Parameters params) throws IOException {
     } finally {
       Closeables.closeQuietly(writer);
     }
+    } else {
+      FPGrowth<String> fp = new FPGrowth<String>();
+      Collection<String> features = new HashSet<String>();
+      try {
+        fp.generateTopKFrequentPatterns(
+                new StringRecordIterator(new FileLineIterable(new File(input), encoding, false), pattern),
+                fp.generateFList(
+                        new StringRecordIterator(new FileLineIterable(new File(input), encoding, false), pattern),
+                        minSupport),
+                minSupport,
+                maxHeapSize,
+                features,
+                new StringOutputConverter(new SequenceFileOutputCollector<Text, TopKStringPatterns>(writer)),
+                new ContextStatusUpdater(null));
+      } finally {
+        Closeables.closeQuietly(writer);
+      }
+    } 
+
 
     List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowth.readFrequentPattern(conf, path);
     for (Pair<String, TopKStringPatterns> entry : frequentPatterns) {
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java
index ba9d207a..0d9a354e 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java
@@ -22,26 +22,27 @@
 
 import com.google.common.collect.AbstractIterator;
 import org.apache.mahout.common.Pair;
+import org.apache.mahout.math.list.IntArrayList;
 
 /**
  * Iterates over multiple transaction trees to produce a single iterator of transactions
  */
-public final class MultiTransactionTreeIterator extends AbstractIterator<List<Integer>> {
+public final class MultiTransactionTreeIterator extends AbstractIterator<IntArrayList> {
   
-  private final Iterator<Pair<List<Integer>,Long>> pIterator;
-  private List<Integer> current;
+  private final Iterator<Pair<IntArrayList,Long>> pIterator;
+  private IntArrayList current;
   private long currentMaxCount;
   private long currentCount;
   
-  public MultiTransactionTreeIterator(Iterator<Pair<List<Integer>,Long>> iterator) {
+  public MultiTransactionTreeIterator(Iterator<Pair<IntArrayList,Long>> iterator) {
     this.pIterator = iterator;
   }
 
   @Override
-  protected List<Integer> computeNext() {
+  protected IntArrayList computeNext() {
     if (currentCount >= currentMaxCount) {
       if (pIterator.hasNext()) {
-        Pair<List<Integer>,Long> nextValue = pIterator.next();
+        Pair<IntArrayList,Long> nextValue = pIterator.next();
         current = nextValue.getFirst();
         currentMaxCount = nextValue.getSecond();
         currentCount = 0;
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
index 7e329363..32ec80ab 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
@@ -35,6 +35,7 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Text;
@@ -52,6 +53,7 @@
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
 import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
 import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
+import org.apache.mahout.math.list.IntArrayList;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -68,6 +70,7 @@
   public static final String G_LIST = "gList";
   public static final String NUM_GROUPS = "numGroups";
   public static final int NUM_GROUPS_DEFAULT = 1000;
+  public static final String MAX_PER_GROUP = "maxPerGroup";
   public static final String OUTPUT = "output";
   public static final String MIN_SUPPORT = "minSupport";
   public static final String MAX_HEAPSIZE = "maxHeapSize";
@@ -77,8 +80,8 @@
   public static final String FPGROWTH = "fpgrowth";
   public static final String FREQUENT_PATTERNS = "frequentpatterns";
   public static final String PARALLEL_COUNTING = "parallelcounting";
-  public static final String SORTED_OUTPUT = "sortedoutput";
   public static final String SPLIT_PATTERN = "splitPattern";
+  public static final String USE_FPG2 = "use_fpg2";
 
   public static final Pattern SPLITTER = Pattern.compile("[ ,\t]*[,|\t][ ,\t]*");
   
@@ -94,8 +97,11 @@ private PFPGrowth() {}
   public static List<Pair<String,Long>> readFList(Configuration conf) throws IOException {
     List<Pair<String,Long>> list = new ArrayList<Pair<String,Long>>();
     URI[] files = DistributedCache.getCacheFiles(conf);
-    if (files == null || files.length < 2) {
-      throw new IOException("Cannot read Frequency list and Grouping list from Distributed Cache");
+    if (files == null) {
+      throw new IOException("Cannot read Frequency list from Distributed Cache");
+    }
+    if (files.length != 1) {
+      throw new IOException("Cannot read Frequency list from Distributed Cache ("+files.length+")");
     }
     for (Pair<Text,LongWritable> record :
          new SequenceFileIterable<Text,LongWritable>(new Path(files[0].getPath()), true, conf)) {
@@ -104,31 +110,12 @@ private PFPGrowth() {}
     return list;
   }
   
-  /**
-   * Generates the gList(Group ID Mapping of Various frequent Features) Map from the corresponding serialized
-   * representation
-   * 
-   * @return Deserialized Group List
-   */
-  public static Map<String,Long> readGList(Configuration conf) throws IOException {
-    Map<String,Long> map = new HashMap<String,Long>();
-    URI[] files = DistributedCache.getCacheFiles(conf);
-    if (files == null || files.length < 2) {
-      throw new IOException("Cannot read Frequency list and Grouping list from Distributed Cache");
-    }
-    for (Pair<Text,LongWritable> record :
-         new SequenceFileIterable<Text,LongWritable>(new Path(files[1].getPath()), true, conf)) {
-      map.put(record.getFirst().toString(), record.getSecond().get());
-    }
-    return map;
-  }
-  
   /**
    * Serializes the fList and returns the string representation of the List
    * 
    * @return Serialized String representation of List
    */
-  private static void saveFList(Iterable<Pair<String,Long>> flist, Parameters params, Configuration conf)
+  public static void saveFList(Iterable<Pair<String,Long>> flist, Parameters params, Configuration conf)
     throws IOException {
     Path flistPath = new Path(params.get(OUTPUT), F_LIST);
     FileSystem fs = FileSystem.get(conf);
@@ -145,27 +132,6 @@ private static void saveFList(Iterable<Pair<String,Long>> flist, Parameters para
     DistributedCache.addCacheFile(flistPath.toUri(), conf);
   }
   
-  /**
-   * Converts a given Map in to a String using DefaultStringifier of Hadoop
-   * 
-   * @return Serialized String representation of the GList Map
-   */
-  private static void saveGList(Map<String,Long> glist, Parameters params, Configuration conf) throws IOException {
-    Path flistPath = new Path(params.get(OUTPUT), G_LIST);
-    FileSystem fs = FileSystem.get(conf);
-    flistPath = fs.makeQualified(flistPath);
-    HadoopUtil.delete(conf, flistPath);
-    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, flistPath, Text.class, LongWritable.class);
-    try {
-      for (Entry<String,Long> pair : glist.entrySet()) {
-        writer.append(new Text(pair.getKey()), new LongWritable(pair.getValue()));
-      }
-    } finally {
-      writer.close();
-    }
-    DistributedCache.addCacheFile(flistPath.toUri(), conf);
-  }
-  
   /**
    * read the feature frequency List which is built at the end of the Parallel counting job
    * 
@@ -204,6 +170,24 @@ public int compare(Pair<String,Long> o1, Pair<String,Long> o2) {
     return fList;
   }
   
+  public static final int getGroup(int itemId, int maxPerGroup) {
+    return itemId / maxPerGroup;
+  }
+
+  public static final IntArrayList getGroupMembers(int groupId, 
+                                                   int maxPerGroup, 
+                                                   int numFeatures) {
+    IntArrayList ret = new IntArrayList();
+    int start = groupId * maxPerGroup;
+    int end = start + maxPerGroup;
+    if (end > numFeatures) 
+      end= numFeatures;
+    for (int i = start; i < end; i++) {
+      ret.add(i);
+    }
+    return ret;
+  }
+  
   /**
    * Read the Frequent Patterns generated from Text
    * 
@@ -237,8 +221,20 @@ public static void runPFPGrowth(Parameters params) throws IOException,
     conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,"
                                   + "org.apache.hadoop.io.serializer.WritableSerialization");
     startParallelCounting(params, conf);
-    startGroupingItems(params, conf);
-    startTransactionSorting(params, conf);
+
+    // save feature list to dcache
+    List<Pair<String,Long>> fList = readFList(params);
+    saveFList(fList, params, conf);
+
+    // set param to control group size in MR jobs
+    int numGroups = params.getInt(PFPGrowth.NUM_GROUPS, 
+                                  PFPGrowth.NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) 
+      maxPerGroup++;
+    params.set(MAX_PER_GROUP, Integer.toString(maxPerGroup));
+    fList = null;
+
     startParallelFPGrowth(params, conf);
     startAggregating(params, conf);
   }
@@ -275,41 +271,6 @@ public static void startAggregating(Parameters params, Configuration conf)
     job.waitForCompletion(true);
   }
   
-  /**
-   * Group the given Features into g groups as defined by the numGroups parameter in params
-   * 
-   * @param params
-   * @throws IOException
-   */
-  public static void startGroupingItems(Parameters params, Configuration conf) throws IOException {
-    List<Pair<String,Long>> fList = readFList(params);
-    int numGroups = params.getInt(NUM_GROUPS, NUM_GROUPS_DEFAULT);
-    
-    Map<String,Long> gList = Maps.newHashMap();
-    long maxPerGroup = fList.size() / numGroups;
-    if (fList.size() != maxPerGroup * numGroups) {
-      maxPerGroup++;
-    }
-    
-    long i = 0;
-    long groupID = 0;
-    for (Pair<String,Long> featureFreq : fList) {
-      String feature = featureFreq.getFirst();
-      if (i / maxPerGroup == groupID) {
-        gList.put(feature, groupID);
-      } else {
-        groupID++;
-        gList.put(feature, groupID);
-      }
-      i++;
-    }
-    
-    log.info("No of Features: {}", fList.size());
-    
-    saveFList(fList, params, conf);
-    saveGList(gList, params, conf);
-  }
-  
   /**
    * Count the frequencies of various features in parallel using Map/Reduce
    */
@@ -343,49 +304,6 @@ public static void startParallelCounting(Parameters params, Configuration conf)
     
   }
   
-  /**
-   * Run the Parallel FPGrowth Map/Reduce Job to calculate the Top K features of group dependent shards
-   */
-  public static void startTransactionSorting(Parameters params, Configuration conf)
-    throws IOException, InterruptedException, ClassNotFoundException {
-    conf.set(PFP_PARAMETERS, params.toString());
-    conf.set("mapred.compress.map.output", "true");
-    conf.set("mapred.output.compression.type", "BLOCK");
-    String input = params.get(INPUT);
-    Job job = new Job(conf, "PFP Transaction Sorting running over input" + input);
-    job.setJarByClass(PFPGrowth.class);
-
-    Integer numGroups = Integer.valueOf(params.get(NUM_GROUPS, "-1"));
-    int numRed = job.getNumReduceTasks();
-    if (numGroups < 0) {
-      if (NUM_GROUPS_DEFAULT < numRed) {
-        params.set(NUM_GROUPS, Integer.toString(numRed));
-      }
-    }
-    if (numRed > numGroups) {
-      log.info("Warning: running with less groups of work than  reducers!");
-    }
-
-    job.setMapOutputKeyClass(LongWritable.class);
-    job.setMapOutputValueClass(TransactionTree.class);
-    
-    job.setOutputKeyClass(LongWritable.class);
-    job.setOutputValueClass(TransactionTree.class);
-    
-    FileInputFormat.addInputPath(job, new Path(input));
-    Path outPath = new Path(params.get(OUTPUT), SORTED_OUTPUT);
-    FileOutputFormat.setOutputPath(job, outPath);
-    
-    HadoopUtil.delete(conf, outPath);
-    
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TransactionSortingMapper.class);
-    job.setReducerClass(TransactionSortingReducer.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    
-    job.waitForCompletion(true);
-  }
-  
   /**
    * Run the Parallel FPGrowth Map/Reduce Job to calculate the Top K features of group dependent shards
    */
@@ -394,11 +312,11 @@ public static void startParallelFPGrowth(Parameters params, Configuration conf)
     conf.set(PFP_PARAMETERS, params.toString());
     conf.set("mapred.compress.map.output", "true");
     conf.set("mapred.output.compression.type", "BLOCK");
-    Path input = new Path(params.get(OUTPUT), SORTED_OUTPUT);
+    Path input = new Path(params.get(INPUT));
     Job job = new Job(conf, "PFP Growth Driver running over input" + input);
     job.setJarByClass(PFPGrowth.class);
     
-    job.setMapOutputKeyClass(LongWritable.class);
+    job.setMapOutputKeyClass(IntWritable.class);
     job.setMapOutputValueClass(TransactionTree.class);
     
     job.setOutputKeyClass(Text.class);
@@ -410,7 +328,7 @@ public static void startParallelFPGrowth(Parameters params, Configuration conf)
     
     HadoopUtil.delete(conf, outPath);
     
-    job.setInputFormatClass(SequenceFileInputFormat.class);
+    job.setInputFormatClass(TextInputFormat.class);
     job.setMapperClass(ParallelFPGrowthMapper.class);
     job.setCombinerClass(ParallelFPGrowthCombiner.class);
     job.setReducerClass(ParallelFPGrowthReducer.class);
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java
index 3313c137..fe98976f 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java
@@ -20,7 +20,9 @@
 import java.io.IOException;
 import java.util.List;
 
-import org.apache.hadoop.io.LongWritable;
+import org.apache.mahout.math.list.IntArrayList;
+
+import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.mapreduce.Reducer;
 import org.apache.mahout.common.Pair;
 
@@ -28,14 +30,14 @@
  *  takes each group of dependent transactions and\ compacts it in a
  * TransactionTree structure
  */
-public class ParallelFPGrowthCombiner extends Reducer<LongWritable,TransactionTree,LongWritable,TransactionTree> {
+public class ParallelFPGrowthCombiner extends Reducer<IntWritable,TransactionTree,IntWritable,TransactionTree> {
   
   @Override
-  protected void reduce(LongWritable key, Iterable<TransactionTree> values, Context context)
+  protected void reduce(IntWritable key, Iterable<TransactionTree> values, Context context)
     throws IOException, InterruptedException {
     TransactionTree cTree = new TransactionTree();
     for (TransactionTree tr : values) {
-      for (Pair<List<Integer>,Long> p : tr) {
+      for (Pair<IntArrayList,Long> p : tr) {
         cTree.addPattern(p.getFirst(), p.getSecond());
       }
     }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java
index f5f5833d..54cc8dc8 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java
@@ -19,62 +19,96 @@
 
 import java.io.IOException;
 import java.util.Collection;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map.Entry;
+import java.util.regex.Pattern;
 
+import com.google.common.collect.Lists;
+
+import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.mahout.common.Pair;
+
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.math.list.IntArrayList;
 import org.apache.mahout.math.map.OpenIntLongHashMap;
 import org.apache.mahout.math.map.OpenObjectIntHashMap;
+import org.apache.mahout.math.set.OpenIntHashSet;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  *  maps each transaction to all unique items groups in the transaction. mapper
  * outputs the group id as key and the transaction as value
  * 
  */
-public class ParallelFPGrowthMapper extends Mapper<LongWritable,TransactionTree,LongWritable,TransactionTree> {
+public class ParallelFPGrowthMapper extends Mapper<LongWritable,Text,IntWritable,TransactionTree> {
+
+  private static final Logger log = LoggerFactory.getLogger(ParallelFPGrowthMapper.class);
   
-  private final OpenIntLongHashMap gListInt = new OpenIntLongHashMap();
+  private final OpenObjectIntHashMap<String> fMap = new OpenObjectIntHashMap<String>();
+  private Pattern splitter;
+  private int numGroups;
+  private int maxPerGroup;
+
+  private IntWritable wGroupID= new IntWritable();
   
   @Override
-  protected void map(LongWritable offset, TransactionTree input, Context context)
+    protected void map(LongWritable offset, Text input, Context context)
     throws IOException, InterruptedException {
 
-    for (Pair<List<Integer>,Long> pattern : input) {
-      Integer[] prunedItems = pattern.getFirst().toArray(new Integer[pattern.getFirst().size()]);
+    String[] items = splitter.split(input.toString());
+
+    OpenIntHashSet itemSet = new OpenIntHashSet();
+
+    for (String item : items) {
+      if (fMap.containsKey(item) && !item.trim().isEmpty()) {
+        itemSet.add(fMap.get(item));
+      }
+    }
 
-      Collection<Long> groups = new HashSet<Long>();
-      for (int j = prunedItems.length - 1; j >= 0; j--) {
+    IntArrayList itemArr = new IntArrayList(itemSet.size());
+    itemSet.keys(itemArr);
+    itemArr.sort();
+
+    OpenIntHashSet groups = new OpenIntHashSet();
+    for (int j = itemArr.size() - 1; j >= 0; j--) {
         // generate group dependent shards
-        Integer item = prunedItems[j];
-        Long groupID = gListInt.get(item);
+      int item = itemArr.get(j);
+      int groupID = PFPGrowth.getGroup(item, maxPerGroup);
 
         if (!groups.contains(groupID)) {
-          Integer[] tempItems = new Integer[j + 1];
-          System.arraycopy(prunedItems, 0, tempItems, 0, j + 1);
+        IntArrayList tempItems = new IntArrayList(j + 1);
+        tempItems.addAllOfFromTo(itemArr, 0, j);
           context.setStatus("Parallel FPGrowth: Generating Group Dependent transactions for: " + item);
-          context.write(new LongWritable(groupID), new TransactionTree(tempItems, pattern.getSecond()));
+        wGroupID.set(groupID);
+        context.write(wGroupID, new TransactionTree(tempItems, 1L));
         }
         groups.add(groupID);
       }
-    }
     
   }
   
   @Override
   protected void setup(Context context) throws IOException, InterruptedException {
     super.setup(context);
-    OpenObjectIntHashMap<String> fMap = new OpenObjectIntHashMap<String>();
+
     int i = 0;
     for (Pair<String,Long> e : PFPGrowth.readFList(context.getConfiguration())) {
       fMap.put(e.getFirst(), i++);
     }
     
-    for (Entry<String,Long> e : PFPGrowth.readGList(context.getConfiguration()).entrySet()) {
-      gListInt.put(fMap.get(e.getKey()), e.getValue());
-    }
+    Parameters params = 
+      new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
+
+    splitter = Pattern.compile(params.get(PFPGrowth.SPLIT_PATTERN,
+                                          PFPGrowth.SPLITTER.toString()));
     
+    numGroups = Integer.valueOf(params.getInt(PFPGrowth.NUM_GROUPS, 
+                                              PFPGrowth.NUM_GROUPS_DEFAULT));
+    maxPerGroup = Integer.valueOf(params.getInt(PFPGrowth.MAX_PER_GROUP, 0));
   }
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java
index 95236804..0e7b30f4 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java
@@ -18,15 +18,17 @@
 package org.apache.mahout.fpm.pfpgrowth;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
 
 import com.google.common.collect.Lists;
 import org.apache.commons.lang.mutable.MutableLong;
-import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Reducer;
 import org.apache.mahout.common.Pair;
@@ -37,31 +39,64 @@
 import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
 import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
 import org.apache.mahout.math.list.IntArrayList;
+import org.apache.mahout.math.list.LongArrayList;
 import org.apache.mahout.math.map.OpenLongObjectHashMap;
 import org.apache.mahout.math.map.OpenObjectIntHashMap;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 /**
  *  takes each group of transactions and runs Vanilla FPGrowth on it and
  * outputs the the Top K frequent Patterns for each group.
  * 
  */
-public class ParallelFPGrowthReducer extends Reducer<LongWritable,TransactionTree,Text,TopKStringPatterns> {
-  
-  private final List<String> featureReverseMap = Lists.newArrayList();
+public class ParallelFPGrowthReducer extends Reducer<IntWritable,TransactionTree,Text,TopKStringPatterns> {
   
-  private final OpenObjectIntHashMap<String> fMap = new OpenObjectIntHashMap<String>();
+  private static final Logger log = 
+      LoggerFactory.getLogger(ParallelFPGrowthReducer.class);
   
-  private final OpenLongObjectHashMap<IntArrayList> groupFeatures = new OpenLongObjectHashMap<IntArrayList>();
+  private final List<String> featureReverseMap = Lists.newArrayList();
+  private final LongArrayList freqList = new LongArrayList();
   
   private int maxHeapSize = 50;
   
   private int minSupport = 3;
   
+  private int numFeatures = 0;
+  private int maxPerGroup = 0;
+
+  private boolean useFP2 = false;
+
+  private class IteratorAdapter implements Iterator<Pair<List<Integer>,Long>> {
+    private Iterator<Pair<IntArrayList,Long>> innerIter;
+
+    private IteratorAdapter(Iterator<Pair<IntArrayList,Long>> transactionIter) {
+      innerIter = transactionIter;
+    }
+
+    @Override
+    public boolean hasNext() {
+      return innerIter.hasNext();
+    }
+
+    @Override
+    public Pair<List<Integer>,Long> next() {
+      Pair<IntArrayList,Long> innerNext = innerIter.next();
+      return new Pair(innerNext.getFirst().toList(), innerNext.getSecond());
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+  }
+
   @Override
-  protected void reduce(LongWritable key, Iterable<TransactionTree> values, Context context) throws IOException {
+  protected void reduce(IntWritable key, Iterable<TransactionTree> values, Context context) throws IOException {
     TransactionTree cTree = new TransactionTree();
     for (TransactionTree tr : values) {
-      for (Pair<List<Integer>,Long> p : tr) {
+      for (Pair<IntArrayList,Long> p : tr) {
         cTree.addPattern(p.getFirst(), p.getSecond());
       }
     }
@@ -69,22 +104,38 @@ protected void reduce(LongWritable key, Iterable<TransactionTree> values, Contex
     List<Pair<Integer,Long>> localFList = Lists.newArrayList();
     for (Entry<Integer,MutableLong> fItem : cTree.generateFList().entrySet()) {
       localFList.add(new Pair<Integer,Long>(fItem.getKey(), fItem.getValue().toLong()));
-      
     }
     
     Collections.sort(localFList, new CountDescendingPairComparator<Integer,Long>());
     
-    FPGrowth<Integer> fpGrowth = new FPGrowth<Integer>();
+    if (useFP2) {
+      org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthIds fpGrowth = 
+        new org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthIds();
     fpGrowth.generateTopKFrequentPatterns(
         cTree.iterator(),
+          freqList,
+          minSupport,
+          maxHeapSize,
+          PFPGrowth.getGroupMembers(key.get(), maxPerGroup, numFeatures),
+          new IntegerStringOutputConverter(
+              new ContextWriteOutputCollector<IntWritable,TransactionTree,Text,TopKStringPatterns>(context),
+              featureReverseMap),
+          new ContextStatusUpdater<IntWritable,TransactionTree,Text,TopKStringPatterns>(context));
+    } else {
+      FPGrowth<Integer> fpGrowth = new FPGrowth<Integer>();
+      fpGrowth.generateTopKFrequentPatterns(
+          new IteratorAdapter(cTree.iterator()),
         localFList,
         minSupport,
         maxHeapSize,
-        new HashSet<Integer>(groupFeatures.get(key.get()).toList()),
+          new HashSet<Integer>(PFPGrowth.getGroupMembers(key.get(), 
+                                                         maxPerGroup, 
+                                                         numFeatures).toList()),
         new IntegerStringOutputConverter(
-            new ContextWriteOutputCollector<LongWritable,TransactionTree,Text,TopKStringPatterns>(context),
+              new ContextWriteOutputCollector<IntWritable,TransactionTree,Text,TopKStringPatterns>(context),
             featureReverseMap),
-        new ContextStatusUpdater<LongWritable,TransactionTree,Text,TopKStringPatterns>(context));
+          new ContextStatusUpdater<IntWritable,TransactionTree,Text,TopKStringPatterns>(context));
+    }
   }
   
   @Override
@@ -96,25 +147,14 @@ protected void setup(Context context) throws IOException, InterruptedException {
     int i = 0;
     for (Pair<String,Long> e : PFPGrowth.readFList(context.getConfiguration())) {
       featureReverseMap.add(e.getFirst());
-      fMap.put(e.getFirst(), i++);
-      
+      freqList.add(e.getSecond());
     }
     
-    Map<String,Long> gList = PFPGrowth.readGList(context.getConfiguration());
-    
-    for (Entry<String,Long> entry : gList.entrySet()) {
-      IntArrayList groupList = groupFeatures.get(entry.getValue());
-      Integer itemInteger = fMap.get(entry.getKey());
-      if (groupList != null) {
-        groupList.add(itemInteger);
-      } else {
-        groupList = new IntArrayList();
-        groupList.add(itemInteger);
-        groupFeatures.put(entry.getValue(), groupList);
-      }
-      
-    }
     maxHeapSize = Integer.valueOf(params.get(PFPGrowth.MAX_HEAPSIZE, "50"));
     minSupport = Integer.valueOf(params.get(PFPGrowth.MIN_SUPPORT, "3"));
+
+    maxPerGroup= params.getInt(PFPGrowth.MAX_PER_GROUP, 0);
+    numFeatures= featureReverseMap.size();
+    useFP2 = "true".equals(params.get(PFPGrowth.USE_FPG2));
   }
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionSortingMapper.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionSortingMapper.java
index 2aac1ce8..e69de29b 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionSortingMapper.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionSortingMapper.java
@@ -1,83 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.fpm.pfpgrowth;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.regex.Pattern;
-
-import com.google.common.collect.Lists;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.common.Pair;
-import org.apache.mahout.common.Parameters;
-import org.apache.mahout.math.map.OpenObjectIntHashMap;
-
-/**
- *  maps each transaction to all unique items groups in the transaction.
- * mapper outputs the group id as key and the transaction as value
- * 
- */
-public class TransactionSortingMapper extends Mapper<LongWritable,Text,LongWritable,TransactionTree> {
-  
-  private final OpenObjectIntHashMap<String> fMap = new OpenObjectIntHashMap<String>();
-  
-  private Pattern splitter;
-  
-  @Override
-  protected void map(LongWritable offset, Text input, Context context) throws IOException,
-                                                                      InterruptedException {
-    
-    String[] items = splitter.split(input.toString());
-    Iterable<String> uniqueItems = new HashSet<String>(Arrays.asList(items));
-    
-    List<Integer> itemSet = Lists.newArrayList();
-    for (String item : uniqueItems) { // remove items not in the fList
-      if (fMap.containsKey(item) && !item.trim().isEmpty()) {
-        itemSet.add(fMap.get(item));
-      }
-    }
-    
-    Collections.sort(itemSet);
-    
-    Integer[] prunedItems = itemSet.toArray(new Integer[itemSet.size()]);
-    
-    if (prunedItems.length > 0) {
-      context.write(new LongWritable(prunedItems[0]), new TransactionTree(prunedItems, 1L));
-    }
-    
-  }
-  
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    Parameters params = new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
-
-    int i = 0;
-    for (Pair<String,Long> e : PFPGrowth.readFList(context.getConfiguration())) {
-      fMap.put(e.getFirst(), i++);
-    }
-    
-    splitter = Pattern.compile(params.get(PFPGrowth.SPLIT_PATTERN, PFPGrowth.SPLITTER.toString()));
-    
-  }
-}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionSortingReducer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionSortingReducer.java
index 6626c75c..e69de29b 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionSortingReducer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionSortingReducer.java
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.fpm.pfpgrowth;
-
-import java.io.IOException;
-
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.mapreduce.Reducer;
-
-/**
- *  takes each group of transactions and runs Vanilla FPGrowth on it and
- * outputs the the Top K frequent Patterns for each group.
- * 
- */
-
-public class TransactionSortingReducer extends
-    Reducer<LongWritable,TransactionTree,LongWritable,TransactionTree> {
-  
-  private static final LongWritable ONE = new LongWritable(1);
-  
-  @Override
-  protected void reduce(LongWritable key, Iterable<TransactionTree> values, Context context) throws IOException,
-                                                                                            InterruptedException {
-    for (TransactionTree tr : values) {
-      context.write(ONE, tr);
-    }
-  }
-}
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java
index b67c5935..acb94ca6 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java
@@ -27,6 +27,8 @@
 import java.util.List;
 import java.util.Map;
 
+import org.apache.mahout.math.list.IntArrayList;
+
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import org.apache.commons.lang.mutable.MutableLong;
@@ -43,7 +45,7 @@
  * Map/Reduce of {@link PFPGrowth} algorithm by reducing data size passed from the Mapper to the reducer where
  * {@link org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth} mining is done
  */
-public final class TransactionTree implements Writable, Iterable<Pair<List<Integer>,Long>> {
+public final class TransactionTree implements Writable, Iterable<Pair<IntArrayList,Long>> {
 
   private static final Logger log = LoggerFactory.getLogger(TransactionTree.class);
 
@@ -58,7 +60,7 @@
   private long[] nodeCount;
   private int nodes;
   private boolean representedAsList;
-  private List<Pair<List<Integer>,Long>> transactionSet;
+  private List<Pair<IntArrayList,Long>> transactionSet;
   
   public TransactionTree() {
     this(DEFAULT_INITIAL_SIZE);
@@ -76,13 +78,19 @@ public TransactionTree(int size) {
     representedAsList = false;
   }
   
-  public TransactionTree(Integer[] items, Long support) {
+  public TransactionTree(int[] items, Long support) {
+    representedAsList = true;
+    transactionSet = Lists.newArrayList();
+    transactionSet.add(new Pair<IntArrayList,Long>(new IntArrayList(items), support));
+  }
+
+  public TransactionTree(IntArrayList items, Long support) {
     representedAsList = true;
     transactionSet = Lists.newArrayList();
-    transactionSet.add(new Pair<List<Integer>,Long>(Arrays.asList(items), support));
+    transactionSet.add(new Pair<IntArrayList,Long>(items, support));
   }
   
-  public TransactionTree(List<Pair<List<Integer>,Long>> transactionSet) {
+  public TransactionTree(List<Pair<IntArrayList,Long>> transactionSet) {
     representedAsList = true;
     this.transactionSet = transactionSet;
   }
@@ -105,12 +113,12 @@ public boolean addCount(int nodeId, long nextNodeCount) {
     return false;
   }
   
-  public int addPattern(Iterable<Integer> myList, long addCount) {
+  public int addPattern(IntArrayList myList, long addCount) {
     int temp = ROOTNODEID;
     int ret = 0;
     boolean addCountMode = true;
-    for (int attributeValue : myList) {
-      
+    for (int idx = 0; idx < myList.size(); idx++) {
+      int attributeValue = myList.get(idx);
       int child;
       if (addCountMode) {
         child = childWithAttribute(temp, attributeValue);
@@ -169,18 +177,19 @@ public long count(int nodeId) {
   
   public Map<Integer,MutableLong> generateFList() {
     Map<Integer,MutableLong> frequencyList = Maps.newHashMap();
-    Iterator<Pair<List<Integer>,Long>> it = iterator();
+    Iterator<Pair<IntArrayList,Long>> it = iterator();
     //int items = 0;
     //int count = 0;
     while (it.hasNext()) {
-      Pair<List<Integer>,Long> p = it.next();
+      Pair<IntArrayList,Long> p = it.next();
       //items += p.getFirst().size();
       //count++;
-      for (Integer i : p.getFirst()) {
-        if (!frequencyList.containsKey(i)) {
-          frequencyList.put(i, new MutableLong(0));
+      IntArrayList items= p.getFirst();
+      for (int idx = 0; idx < items.size(); idx++) {
+        if (!frequencyList.containsKey(items.get(idx))) {
+          frequencyList.put(items.get(idx), new MutableLong(0));
         }
-        frequencyList.get(i).add(p.getSecond());
+        frequencyList.get(items.get(idx)).add(p.getSecond());
       }
     }
     return frequencyList;
@@ -188,7 +197,7 @@ public long count(int nodeId) {
   
   public TransactionTree getCompressedTree() {
     TransactionTree ctree = new TransactionTree();
-    Iterator<Pair<List<Integer>,Long>> it = iterator();
+    Iterator<Pair<IntArrayList,Long>> it = iterator();
     final Map<Integer,MutableLong> fList = generateFList();
     int node = 0;
     Comparator<Integer> comparator = new Comparator<Integer>() {
@@ -198,10 +207,10 @@ public int compare(Integer o1, Integer o2) {
       }
     };
     int size = 0;
-    List<Pair<List<Integer>,Long>> compressedTransactionSet = Lists.newArrayList();
+    List<Pair<IntArrayList,Long>> compressedTransactionSet = Lists.newArrayList();
     while (it.hasNext()) {
-      Pair<List<Integer>,Long> p = it.next();
-      Collections.sort(p.getFirst(), comparator);
+      Pair<IntArrayList,Long> p = it.next();
+      p.getFirst().sort();
       compressedTransactionSet.add(p);
       node += ctree.addPattern(p.getFirst(), p.getSecond());
       size += p.getFirst().size() + 2;
@@ -222,7 +231,7 @@ public int compare(Integer o1, Integer o2) {
   }
   
   @Override
-  public Iterator<Pair<List<Integer>,Long>> iterator() {
+  public Iterator<Pair<IntArrayList,Long>> iterator() {
     if (this.isTreeEmpty() && !representedAsList) {
       throw new IllegalStateException("This is a bug. Please report this to mahout-user list");
     } else if (representedAsList) {
@@ -254,12 +263,12 @@ public void readFields(DataInput in) throws IOException {
         vInt.readFields(in);
         int length = vInt.get();
         
-        Integer[] items = new Integer[length];
+        int[] items = new int[length];
         for (int j = 0; j < length; j++) {
           vInt.readFields(in);
           items[j] = vInt.get();
         }
-        Pair<List<Integer>,Long> transaction = new Pair<List<Integer>,Long>(Arrays.asList(items), support);
+        Pair<IntArrayList,Long> transaction = new Pair<IntArrayList,Long>(new IntArrayList(items), support);
         transactionSet.add(transaction);
       }
     } else {
@@ -295,14 +304,16 @@ public void write(DataOutput out) throws IOException {
       int transactionSetSize = transactionSet.size();
       vInt.set(transactionSetSize);
       vInt.write(out);
-      for (Pair<List<Integer>, Long> transaction : transactionSet) {
+      for (Pair<IntArrayList, Long> transaction : transactionSet) {
         vLong.set(transaction.getSecond());
         vLong.write(out);
 
         vInt.set(transaction.getFirst().size());
         vInt.write(out);
 
-        for (Integer item : transaction.getFirst()) {
+        IntArrayList items = transaction.getFirst();
+        for (int idx = 0; idx < items.size(); idx++) {
+          int item = items.get(idx);
           vInt.set(item);
           vInt.write(out);
         }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java
index e5085742..7a047629 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java
@@ -21,6 +21,8 @@
 import java.util.List;
 import java.util.Stack;
 
+import org.apache.mahout.math.list.IntArrayList;
+
 import com.google.common.collect.AbstractIterator;
 import com.google.common.collect.Lists;
 import org.apache.mahout.common.Pair;
@@ -29,7 +31,7 @@
  * Generates a List of transactions view of Transaction Tree by doing Depth First Traversal on the tree
  * structure
  */
-final class TransactionTreeIterator extends AbstractIterator<Pair<List<Integer>,Long>> {
+final class TransactionTreeIterator extends AbstractIterator<Pair<IntArrayList,Long>> {
 
   private final Stack<int[]> depth = new Stack<int[]>();
   private final TransactionTree transactionTree;
@@ -40,7 +42,7 @@
   }
 
   @Override
-  protected Pair<List<Integer>, Long> computeNext() {
+  protected Pair<IntArrayList, Long> computeNext() {
 
     if (depth.isEmpty()) {
       return endOfData();
@@ -67,14 +69,14 @@
       }
     } while (sum == transactionTree.count(childId));
 
-    List<Integer> data = Lists.newArrayList();
+    IntArrayList data = new IntArrayList();
     Iterator<int[]> it = depth.iterator();
     it.next();
     while (it.hasNext()) {
       data.add(transactionTree.attribute(it.next()[0]));
     }
 
-    Pair<List<Integer>,Long> returnable = new Pair<List<Integer>,Long>(data, transactionTree.count(childId) - sum);
+    Pair<IntArrayList,Long> returnable = new Pair<IntArrayList,Long>(data, transactionTree.count(childId) - sum);
 
     int[] top = depth.peek();
     while (top[1] + 1 == transactionTree.childCount(top[0])) {
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java
index 29281651..7e20f9b9 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java
@@ -162,4 +162,14 @@ private boolean addPattern(Pattern frequentPattern) {
     }
     return true;
   }
+
+  public String toString() { 
+    StringBuilder sb = new StringBuilder("FreqPatHeap{");
+    String sep = "";
+    for (Pattern p : getHeap()) {
+      sb.append(sep).append(p);
+      sep= ", ";
+    }
+    return sb.toString();
+  }
 }
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java
index 65bc0d2b..64938ad7 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java
@@ -42,8 +42,6 @@
   
   private long support = Long.MAX_VALUE;
   
-  private long[] supportValues;
-  
   public Pattern() {
     this(DEFAULT_INITIAL_SIZE);
   }
@@ -53,7 +51,6 @@ private Pattern(int size) {
       size = DEFAULT_INITIAL_SIZE;
     }
     this.pattern = new int[size];
-    this.supportValues = new long[size];
     dirty = true;
   }
   
@@ -62,8 +59,8 @@ public final void add(int id, long supportCount) {
     if (length >= pattern.length) {
       resize();
     }
-    this.pattern[length] = id;
-    this.supportValues[length++] = supportCount;
+    this.pattern[length++] = id;
+    Arrays.sort(this.pattern, 0, length);
     this.support = supportCount > this.support ? this.support : supportCount;
   }
 
@@ -71,10 +68,6 @@ public final void add(int id, long supportCount) {
     return this.pattern;
   }
 
-  public final Object[] getPatternWithSupport() {
-    return new Object[] {this.pattern, this.supportValues};
-  }
-
   public final boolean isSubPatternOf(Pattern frequentPattern) {
     int[] otherPattern = frequentPattern.getPattern();
     int otherLength = frequentPattern.length();
@@ -110,11 +103,8 @@ private void resize() {
       size = DEFAULT_INITIAL_SIZE;
     }
     int[] oldpattern = pattern;
-    long[] oldSupport = supportValues;
     this.pattern = new int[size];
-    this.supportValues = new long[size];
     System.arraycopy(oldpattern, 0, this.pattern, 0, length);
-    System.arraycopy(oldSupport, 0, this.supportValues, 0, length);
   }
   
   @Override
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java
index e69de29b..2d4f5c68 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java
@@ -0,0 +1,351 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth2;
+
+import java.io.IOException;
+import java.util.AbstractMap;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.commons.lang.mutable.MutableLong;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
+import org.apache.mahout.fpm.pfpgrowth.CountDescendingPairComparator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.convertors.TopKPatternsOutputConverter;
+import org.apache.mahout.fpm.pfpgrowth.convertors.TransactionIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.math.list.LongArrayList;
+import org.apache.mahout.math.list.IntArrayList;
+import org.apache.mahout.math.map.OpenIntIntHashMap;
+import org.apache.mahout.math.set.OpenIntHashSet;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import  org.apache.mahout.fpm.pfpgrowth.fpgrowth.Pattern;
+import  org.apache.mahout.fpm.pfpgrowth.fpgrowth.FrequentPatternMaxHeap;
+/**
+ * Implementation of PFGrowth Algorithm
+ */
+public class FPGrowthIds {
+
+  private static final Logger log = LoggerFactory.getLogger(FPGrowthIds.class);
+
+  public static List<Pair<String,TopKStringPatterns>> readFrequentPattern(Configuration conf, Path path) {
+    List<Pair<String,TopKStringPatterns>> ret = Lists.newArrayList();
+    // key is feature value is count
+    for (Pair<Writable,TopKStringPatterns> record
+         : new SequenceFileIterable<Writable,TopKStringPatterns>(path, true, conf)) {
+      ret.add(new Pair<String,TopKStringPatterns>(record.getFirst().toString(),
+                                                  new TopKStringPatterns(record.getSecond().getPatterns())));
+    }
+    return ret;
+  }
+ /**
+   * Generate Top K Frequent Patterns for every feature in returnableFeatures
+   * given a stream of transactions and the minimum support
+   *
+   * @param transactionStream
+   *          Iterator of transaction
+   * @param frequencyList
+   *          list of frequent features and their support value
+   * @param minSupport
+   *          minimum support of the transactions
+   * @param k
+   *          Number of top frequent patterns to keep
+   * @param returnableFeatures
+   *          set of features for which the frequent patterns are mined. If the
+   *          set is empty or null, then top K patterns for every frequent item (an item
+   *          whose support> minSupport) is generated
+   * @param output
+   *          The output collector to which the the generated patterns are
+   *          written
+   * @throws IOException
+   */
+  public final void generateTopKFrequentPatterns(Iterator<Pair<IntArrayList,Long>> transactionStream,
+                                                 LongArrayList attributeFrequency,
+                                                 long minSupport,
+                                                 int k,
+                                                 IntArrayList returnableFeatures,
+                                                 OutputCollector<Integer,List<Pair<List<Integer>,Long>>> output,
+                                                 StatusUpdater updater) throws IOException {
+
+    for (int i = 0; i < attributeFrequency.size(); i++) {
+      if (attributeFrequency.get(i) < minSupport) {
+        attributeFrequency.setSize(i);
+        attributeFrequency.trimToSize();
+        break;
+      }
+    }
+
+    log.info("Number of unique items {}", attributeFrequency.size());
+
+    OpenIntHashSet returnFeatures = new OpenIntHashSet();
+    if (returnableFeatures == null || returnableFeatures.isEmpty()) {
+      returnableFeatures = new IntArrayList();
+      for (int j = 0; j < attributeFrequency.size(); j++) {
+        returnableFeatures.add(j);
+      }
+    }
+
+    log.info("Number of unique pruned items {}", attributeFrequency.size());
+    generateTopKFrequentPatterns(transactionStream, attributeFrequency,
+        minSupport, k, attributeFrequency.size(), returnableFeatures,
+        new TopKPatternsOutputConverter<Integer>(output, new IdentityMapping()), updater);
+  }
+
+  private class IdentityMapping extends AbstractMap<Integer, Integer> {
+
+    public Set<Map.Entry<Integer,Integer>> entrySet() {
+      throw new IllegalStateException();
+    }
+
+    public Integer get(Object key) {
+      return (Integer) key;
+    }
+
+  }
+
+  /**
+   * Top K FpGrowth Algorithm
+   *
+   * @param tree
+   *          to be mined
+   * @param minSupportValue
+   *          minimum support of the pattern to keep
+   * @param k
+   *          Number of top frequent patterns to keep
+   * @param requiredFeatures
+   *          Set of integer id's of features to mine
+   * @param outputCollector
+   *          the Collector class which converts the given frequent pattern in
+   *          integer to A
+   * @return Top K Frequent Patterns for each feature and their support
+   */
+  private Map<Integer,FrequentPatternMaxHeap> fpGrowth(FPTree tree,
+                                                       long minSupportValue,
+                                                       int k,
+                                                       IntArrayList requiredFeatures,
+                                                       TopKPatternsOutputConverter<Integer> outputCollector,
+                                                       StatusUpdater updater) throws IOException {
+
+    Map<Integer,FrequentPatternMaxHeap> patterns = Maps.newHashMap();
+    requiredFeatures.sort();
+    for (int attribute : tree.attrIterableRev()) {
+      if (requiredFeatures.binarySearch(attribute) >= 0) {
+        log.info("Mining FTree Tree for all patterns with {}", attribute);
+        MutableLong minSupport = new MutableLong(minSupportValue);
+        FrequentPatternMaxHeap frequentPatterns = growth(tree, minSupport, k,
+                                                         attribute, updater);
+        patterns.put(attribute, frequentPatterns);
+        outputCollector.collect(attribute, frequentPatterns);
+
+        minSupportValue = Math.max(minSupportValue, minSupport.longValue() / 2);
+        log.info("Found {} Patterns with Least Support {}", patterns.get(
+            attribute).count(), patterns.get(attribute).leastSupport());
+      }
+    }
+    return patterns;
+  }
+
+      
+
+  /**
+   * Internal TopKFrequentPattern Generation algorithm, which represents the A's
+   * as integers and transforms features to use only integers
+   *
+   * @param transactions
+   *          Transaction database Iterator
+   * @param attributeFrequency
+   *          array representing the Frequency of the corresponding attribute id
+   * @param minSupport
+   *          minimum support of the pattern to be mined
+   * @param k
+   *          Max value of the Size of the Max-Heap in which Patterns are held
+   * @param featureSetSize
+   *          number of features
+   * @param returnFeatures
+   *          the id's of the features for which Top K patterns have to be mined
+   * @param topKPatternsOutputCollector
+   *          the outputCollector which transforms the given Pattern in integer
+   *          format to the corresponding A Format
+   * @return Top K frequent patterns for each attribute
+   */
+  private Map<Integer,FrequentPatternMaxHeap> generateTopKFrequentPatterns(
+    Iterator<Pair<IntArrayList,Long>> transactions,
+    LongArrayList attributeFrequency,
+    long minSupport,
+    int k,
+    int featureSetSize,
+    IntArrayList returnFeatures, 
+    TopKPatternsOutputConverter<Integer> topKPatternsOutputCollector,
+    StatusUpdater updater) throws IOException {
+
+    FPTree tree = new FPTree(attributeFrequency, minSupport);
+
+    // Constructing initial FPTree from the list of transactions
+    int nodecount = 0;
+    int i = 0;
+    while (transactions.hasNext()) {
+      Pair<IntArrayList,Long> transaction = transactions.next();
+      IntArrayList iArr = transaction.getFirst();
+      tree.accumulate(iArr, transaction.getSecond());
+      i++;
+      if (i % 10000 == 0) {
+        log.info("FPTree Building: Read {} Transactions", i);
+      }
+    }
+
+    log.info("Number of Nodes in the FP Tree: {}", nodecount);
+
+    return fpGrowth(tree, minSupport, k, returnFeatures, topKPatternsOutputCollector, updater);
+  }
+
+  /** 
+   * Run FP Growth recursively on tree, for the given target attribute
+   */
+  private static FrequentPatternMaxHeap growth(FPTree tree,
+                                               MutableLong minSupportMutable,
+                                               int k,
+                                               int currentAttribute,
+                                               StatusUpdater updater) {
+
+    long currentAttributeCount = tree.headerCount(currentAttribute);
+
+    if (currentAttributeCount < minSupportMutable.longValue()) {
+      return new FrequentPatternMaxHeap(k, true);
+    }
+ 
+    FPTree condTree = tree.createMoreFreqConditionalTree(currentAttribute);
+
+    Pair<FPTree, FPTree> pAndQ = condTree.splitSinglePrefix();
+    FPTree p = pAndQ.getFirst();
+    FPTree q = pAndQ.getSecond();
+
+    FrequentPatternMaxHeap prefixPats = null;
+    if (p != null) {
+      prefixPats = mineSinglePrefix(p, k);
+    }
+
+    FrequentPatternMaxHeap suffixPats = new FrequentPatternMaxHeap(k, true);
+
+    Pattern thisPat = new Pattern();
+    thisPat.add(currentAttribute, currentAttributeCount);
+    suffixPats.insert(thisPat);
+
+    for (int attr : q.attrIterableRev())  {
+      mergeHeap(suffixPats,
+                growth(q, minSupportMutable, k, attr, updater),
+                currentAttribute,
+                currentAttributeCount, true);
+    }
+
+    if (prefixPats != null) {
+      FrequentPatternMaxHeap toRet = cross(prefixPats, suffixPats, k);
+      return toRet;
+    }
+
+    return suffixPats;
+  }
+
+
+  /** 
+   * Return a set patterns which are the cross product of the patterns
+   * in pPats and qPats.  
+   */
+  private static FrequentPatternMaxHeap cross(FrequentPatternMaxHeap pPats, 
+                                              FrequentPatternMaxHeap qPats,
+                                              int k) {
+    FrequentPatternMaxHeap pats = new FrequentPatternMaxHeap(k, true);
+
+    for (Pattern p : pPats.getHeap()) {
+      int[] pints = p.getPattern();
+      for (Pattern q : qPats.getHeap()) {
+        int[] qints = q.getPattern();
+        
+        Pattern pq = new Pattern();
+        for (int pi = 0; pi < p.length(); pi++) 
+          pq.add(pints[pi], p.support());
+        for (int qi = 0; qi < q.length(); qi++) 
+          pq.add(qints[qi], q.support());
+        pats.insert(pq);
+      }
+    }
+
+    for (Pattern q : qPats.getHeap()) {
+      Pattern qq = new Pattern();
+      int[] qints = q.getPattern();
+      for (int qi = 0; qi < q.length(); qi++) 
+        qq.add(qints[qi], q.support());
+      pats.insert(qq);
+    }
+
+    return pats;
+  }
+
+  /**
+   * Mine all frequent patterns that can be created by following a prefix
+   * that is common to all sets in the given tree.
+   */
+  private static FrequentPatternMaxHeap mineSinglePrefix(FPTree tree, int k) {
+    FrequentPatternMaxHeap pats = new FrequentPatternMaxHeap(k, true);
+    FPTree.FPNode currNode = tree.root();
+    while (currNode.numChildren() == 1) {
+      currNode = currNode.children().iterator().next();
+      FrequentPatternMaxHeap singlePat = new FrequentPatternMaxHeap(k, true);
+      Pattern p = new Pattern();
+      p.add(currNode.attribute(), currNode.count());
+      singlePat.insert(p);
+      pats = cross(singlePat, pats, k);
+      pats.insert(p);
+    }
+
+    return pats;
+  }
+
+  private static FrequentPatternMaxHeap mergeHeap(FrequentPatternMaxHeap frequentPatterns,
+                                                  FrequentPatternMaxHeap returnedPatterns,
+                                                  int attribute,
+                                                  long count,
+                                                  boolean addAttribute) {
+    frequentPatterns.addAll(returnedPatterns, attribute, count);
+    if (frequentPatterns.addable(count) && addAttribute) {
+      Pattern p = new Pattern();
+      p.add(attribute, count);
+      frequentPatterns.insert(p);
+    }
+
+    return frequentPatterns;
+  }
+}
+
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java
index e69de29b..63bf4aa8 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java
@@ -0,0 +1,397 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth2;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.commons.lang.mutable.MutableLong;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
+import org.apache.mahout.fpm.pfpgrowth.CountDescendingPairComparator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.convertors.TopKPatternsOutputConverter;
+import org.apache.mahout.fpm.pfpgrowth.convertors.TransactionIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.math.map.OpenIntIntHashMap;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import  org.apache.mahout.fpm.pfpgrowth.fpgrowth.Pattern;
+import  org.apache.mahout.fpm.pfpgrowth.fpgrowth.FrequentPatternMaxHeap;
+/**
+ * Implementation of PFGrowth Algorithm
+ *
+ * @param <A> object type used as the cell items in a transaction list
+ */
+public class FPGrowthObj<A extends Comparable<? super A>> {
+
+  private static final Logger log = LoggerFactory.getLogger(FPGrowthObj.class);
+
+  public static List<Pair<String,TopKStringPatterns>> readFrequentPattern(Configuration conf, Path path) {
+    List<Pair<String,TopKStringPatterns>> ret = Lists.newArrayList();
+    // key is feature value is count
+    for (Pair<Writable,TopKStringPatterns> record
+         : new SequenceFileIterable<Writable,TopKStringPatterns>(path, true, conf)) {
+      ret.add(new Pair<String,TopKStringPatterns>(record.getFirst().toString(),
+                                                  new TopKStringPatterns(record.getSecond().getPatterns())));
+    }
+    return ret;
+  }
+
+  /**
+   * Generate the Feature Frequency list from the given transaction whose
+   * frequency > minSupport
+   *
+   * @param transactions
+   *          Iterator over the transaction database
+   * @param minSupport
+   *          minSupport of the feature to be included
+   * @return the List of features and their associated frequency as a Pair
+   */
+  public final List<Pair<A,Long>> generateFList(Iterator<Pair<List<A>,Long>> transactions, int minSupport) {
+
+    Map<A,MutableLong> attributeSupport = Maps.newHashMap();
+    while (transactions.hasNext()) {
+      Pair<List<A>,Long> transaction = transactions.next();
+      for (A attribute : transaction.getFirst()) {
+        if (attributeSupport.containsKey(attribute)) {
+          attributeSupport.get(attribute).add(transaction.getSecond().longValue());
+        } else {
+          attributeSupport.put(attribute, new MutableLong(transaction.getSecond()));
+        }
+      }
+    }
+    List<Pair<A,Long>> fList = Lists.newArrayList();
+    for (Entry<A,MutableLong> e : attributeSupport.entrySet()) {
+      long value = e.getValue().longValue();
+      if (value >= minSupport) {
+        fList.add(new Pair<A,Long>(e.getKey(), value));
+      }
+    }
+
+    Collections.sort(fList, new CountDescendingPairComparator<A,Long>());
+
+    return fList;
+  }
+
+ /**
+   * Generate Top K Frequent Patterns for every feature in returnableFeatures
+   * given a stream of transactions and the minimum support
+   *
+   * @param transactionStream
+   *          Iterator of transaction
+   * @param frequencyList
+   *          list of frequent features and their support value
+   * @param minSupport
+   *          minimum support of the transactions
+   * @param k
+   *          Number of top frequent patterns to keep
+   * @param returnableFeatures
+   *          set of features for which the frequent patterns are mined. If the
+   *          set is empty or null, then top K patterns for every frequent item (an item
+   *          whose support> minSupport) is generated
+   * @param output
+   *          The output collector to which the the generated patterns are
+   *          written
+   * @throws IOException
+   */
+  public final void generateTopKFrequentPatterns(Iterator<Pair<List<A>,Long>> transactionStream,
+                                                 Collection<Pair<A, Long>> frequencyList,
+                                                 long minSupport,
+                                                 int k,
+                                                 Collection<A> returnableFeatures,
+                                                 OutputCollector<A,List<Pair<List<A>,Long>>> output,
+                                                 StatusUpdater updater) throws IOException {
+
+    Map<Integer,A> reverseMapping = Maps.newHashMap();
+    Map<A,Integer> attributeIdMapping = Maps.newHashMap();
+
+    int id = 0;
+    for (Pair<A,Long> feature : frequencyList) {
+      A attrib = feature.getFirst();
+      Long frequency = feature.getSecond();
+      if (frequency >= minSupport) {
+        attributeIdMapping.put(attrib, id);
+        reverseMapping.put(id++, attrib);
+      }
+    }
+
+    long[] attributeFrequency = new long[attributeIdMapping.size()];
+    for (Pair<A,Long> feature : frequencyList) {
+      A attrib = feature.getFirst();
+      Long frequency = feature.getSecond();
+      if (frequency < minSupport) {
+        break;
+      }
+      attributeFrequency[attributeIdMapping.get(attrib)] = frequency;
+    }
+
+    log.info("Number of unique items {}", frequencyList.size());
+
+    Collection<Integer> returnFeatures = Sets.newHashSet();
+    if (returnableFeatures != null && !returnableFeatures.isEmpty()) {
+      for (A attrib : returnableFeatures) {
+        if (attributeIdMapping.containsKey(attrib)) {
+          returnFeatures.add(attributeIdMapping.get(attrib));
+          log.info("Adding Pattern {}=>{}", attrib, attributeIdMapping
+            .get(attrib));
+        }
+      }
+    } else {
+      for (int j = 0; j < attributeIdMapping.size(); j++) {
+        returnFeatures.add(j);
+      }
+    }
+
+    log.info("Number of unique pruned items {}", attributeIdMapping.size());
+    generateTopKFrequentPatterns(new TransactionIterator<A>(transactionStream,
+        attributeIdMapping), attributeFrequency, minSupport, k, reverseMapping
+        .size(), returnFeatures, new TopKPatternsOutputConverter<A>(output,
+            reverseMapping), updater);
+  }
+
+  /**
+   * Top K FpGrowth Algorithm
+   *
+   * @param tree
+   *          to be mined
+   * @param minSupportValue
+   *          minimum support of the pattern to keep
+   * @param k
+   *          Number of top frequent patterns to keep
+   * @param requiredFeatures
+   *          Set of integer id's of features to mine
+   * @param outputCollector
+   *          the Collector class which converts the given frequent pattern in
+   *          integer to A
+   * @return Top K Frequent Patterns for each feature and their support
+   */
+  private Map<Integer,FrequentPatternMaxHeap> fpGrowth(FPTree tree,
+                                                       long minSupportValue,
+                                                       int k,
+                                                       Collection<Integer> requiredFeatures,
+                                                       TopKPatternsOutputConverter<A> outputCollector,
+                                                       StatusUpdater updater) throws IOException {
+
+    Map<Integer,FrequentPatternMaxHeap> patterns = Maps.newHashMap();
+    for (int attribute : tree.attrIterableRev()) {
+      if (requiredFeatures.contains(attribute)) {
+        log.info("Mining FTree Tree for all patterns with {}", attribute);
+        MutableLong minSupport = new MutableLong(minSupportValue);
+        FrequentPatternMaxHeap frequentPatterns = growth(tree, minSupport, k,
+                                                         attribute, updater);
+        patterns.put(attribute, frequentPatterns);
+        outputCollector.collect(attribute, frequentPatterns);
+
+        minSupportValue = Math.max(minSupportValue, minSupport.longValue() / 2);
+        log.info("Found {} Patterns with Least Support {}", patterns.get(
+            attribute).count(), patterns.get(attribute).leastSupport());
+      }
+    }
+    return patterns;
+  }
+
+      
+
+  /**
+   * Internal TopKFrequentPattern Generation algorithm, which represents the A's
+   * as integers and transforms features to use only integers
+   *
+   * @param transactions
+   *          Transaction database Iterator
+   * @param attributeFrequency
+   *          array representing the Frequency of the corresponding attribute id
+   * @param minSupport
+   *          minimum support of the pattern to be mined
+   * @param k
+   *          Max value of the Size of the Max-Heap in which Patterns are held
+   * @param featureSetSize
+   *          number of features
+   * @param returnFeatures
+   *          the id's of the features for which Top K patterns have to be mined
+   * @param topKPatternsOutputCollector
+   *          the outputCollector which transforms the given Pattern in integer
+   *          format to the corresponding A Format
+   * @return Top K frequent patterns for each attribute
+   */
+  private Map<Integer,FrequentPatternMaxHeap> generateTopKFrequentPatterns(
+    Iterator<Pair<int[],Long>> transactions,
+    long[] attributeFrequency,
+    long minSupport,
+    int k,
+    int featureSetSize,
+    Collection<Integer> returnFeatures, TopKPatternsOutputConverter<A> topKPatternsOutputCollector,
+    StatusUpdater updater) throws IOException {
+
+    FPTree tree = new FPTree(attributeFrequency, minSupport);
+
+    // Constructing initial FPTree from the list of transactions
+    int nodecount = 0;
+    int i = 0;
+    while (transactions.hasNext()) {
+      Pair<int[],Long> transaction = transactions.next();
+      List<Integer> iLst = Lists.newArrayList();
+      int[] iArr = transaction.getFirst();
+      for (int j = 0; j < iArr.length; j++)
+        iLst.add(iArr[j]);
+      tree.accumulate(iLst, transaction.getSecond());
+      i++;
+      if (i % 10000 == 0) {
+        log.info("FPTree Building: Read {} Transactions", i);
+      }
+    }
+
+    log.info("Number of Nodes in the FP Tree: {}", nodecount);
+
+    return fpGrowth(tree, minSupport, k, returnFeatures, topKPatternsOutputCollector, updater);
+  }
+
+  /** 
+   * Run FP Growth recursively on tree, for the given target attribute
+   */
+  private static FrequentPatternMaxHeap growth(FPTree tree,
+                                               MutableLong minSupportMutable,
+                                               int k,
+                                               int currentAttribute,
+                                               StatusUpdater updater) {
+
+    long currentAttributeCount = tree.headerCount(currentAttribute);
+
+    if (currentAttributeCount < minSupportMutable.longValue()) {
+      return new FrequentPatternMaxHeap(k, true);
+    }
+ 
+    FPTree condTree = tree.createMoreFreqConditionalTree(currentAttribute);
+
+    Pair<FPTree, FPTree> pAndQ = condTree.splitSinglePrefix();
+    FPTree p = pAndQ.getFirst();
+    FPTree q = pAndQ.getSecond();
+
+    FrequentPatternMaxHeap prefixPats = null;
+    if (p != null) {
+      prefixPats = mineSinglePrefix(p, k);
+    }
+
+    FrequentPatternMaxHeap suffixPats = new FrequentPatternMaxHeap(k, true);
+
+    Pattern thisPat = new Pattern();
+    thisPat.add(currentAttribute, currentAttributeCount);
+    suffixPats.insert(thisPat);
+
+    for (int attr : q.attrIterableRev())  {
+      mergeHeap(suffixPats,
+                growth(q, minSupportMutable, k, attr, updater),
+                currentAttribute,
+                currentAttributeCount, true);
+    }
+
+    if (prefixPats != null) {
+      FrequentPatternMaxHeap toRet = cross(prefixPats, suffixPats, k);
+      return toRet;
+    }
+
+    return suffixPats;
+  }
+
+
+  /** 
+   * Return a set patterns which are the cross product of the patterns
+   * in pPats and qPats.  
+   */
+  private static FrequentPatternMaxHeap cross(FrequentPatternMaxHeap pPats, 
+                                              FrequentPatternMaxHeap qPats,
+                                              int k) {
+    FrequentPatternMaxHeap pats = new FrequentPatternMaxHeap(k, true);
+
+    for (Pattern p : pPats.getHeap()) {
+      int[] pints = p.getPattern();
+      for (Pattern q : qPats.getHeap()) {
+        int[] qints = q.getPattern();
+        
+        Pattern pq = new Pattern();
+        for (int pi = 0; pi < p.length(); pi++) 
+          pq.add(pints[pi], p.support());
+        for (int qi = 0; qi < q.length(); qi++) 
+          pq.add(qints[qi], q.support());
+        pats.insert(pq);
+      }
+    }
+
+    for (Pattern q : qPats.getHeap()) {
+      Pattern qq = new Pattern();
+      int[] qints = q.getPattern();
+      for (int qi = 0; qi < q.length(); qi++) 
+        qq.add(qints[qi], q.support());
+      pats.insert(qq);
+    }
+
+    return pats;
+  }
+
+  /**
+   * Mine all frequent patterns that can be created by following a prefix
+   * that is common to all sets in the given tree.
+   */
+  private static FrequentPatternMaxHeap mineSinglePrefix(FPTree tree, int k) {
+    FrequentPatternMaxHeap pats = new FrequentPatternMaxHeap(k, true);
+    FPTree.FPNode currNode = tree.root();
+    while (currNode.numChildren() == 1) {
+      currNode = currNode.children().iterator().next();
+      FrequentPatternMaxHeap singlePat = new FrequentPatternMaxHeap(k, true);
+      Pattern p = new Pattern();
+      p.add(currNode.attribute(), currNode.count());
+      singlePat.insert(p);
+      pats = cross(singlePat, pats, k);
+      pats.insert(p);
+    }
+
+    return pats;
+  }
+
+  private static FrequentPatternMaxHeap mergeHeap(FrequentPatternMaxHeap frequentPatterns,
+                                                  FrequentPatternMaxHeap returnedPatterns,
+                                                  int attribute,
+                                                  long count,
+                                                  boolean addAttribute) {
+    frequentPatterns.addAll(returnedPatterns, attribute, count);
+    if (frequentPatterns.addable(count) && addAttribute) {
+      Pattern p = new Pattern();
+      p.add(attribute, count);
+      frequentPatterns.insert(p);
+    }
+
+    return frequentPatterns;
+  }
+}
+
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java
index e69de29b..d456d834 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java
@@ -0,0 +1,382 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth.fpgrowth2;
+
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+import com.google.common.collect.Lists;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.math.list.IntArrayList;
+import org.apache.mahout.math.list.LongArrayList;
+import org.apache.mahout.math.map.OpenIntObjectHashMap;
+
+/**
+ * A straightforward implementation of FPTrees as described in Han et. al.
+ */
+public class FPTree {
+  private static final Logger log = LoggerFactory.getLogger(FPTree.class);
+
+  private final AttrComparator attrComparator = new AttrComparator();
+  private FPNode root;
+  private long minSupport;
+  private LongArrayList attrCountList;
+  private OpenIntObjectHashMap attrNodeLists; 
+
+  public class FPNode {
+    private FPNode parent;
+    private OpenIntObjectHashMap childMap;
+    private int attribute;
+    private long count;
+
+    private FPNode(FPNode parent, int attribute, long count) {
+      this.parent = parent;
+      this.attribute = attribute;
+      this.count = count;
+      this.childMap = new OpenIntObjectHashMap();
+    }
+
+    private void addChild(FPNode child) {
+      this.childMap.put(child.attribute(), child);
+    }
+
+    public Iterable<FPNode> children() {
+      return childMap.values();
+    }
+
+    public int numChildren() {
+      return childMap.size();
+    }
+
+    public FPNode parent() {
+      return parent;
+    }
+
+    public FPNode child(int attribute) {
+      return (FPNode) childMap.get(attribute);
+    }
+
+    public int attribute() {
+      return attribute;
+    }
+
+    public void accumulate(long incr) {
+      count = count + incr;
+    }
+
+    public long count() {
+      return count;
+    }
+
+  }
+
+  /**
+   * Creates an FPTree using the attribute counts in attrCountList.
+   *
+   * Note that the counts in attrCountList are assumed to be complete;
+   * they are not updated as the tree is modified.
+   */
+  public FPTree(LongArrayList attrCountList, long minSupport) {
+    this.root = new FPNode(null, -1, 0);
+    this.attrCountList = attrCountList;
+    this.attrNodeLists = new OpenIntObjectHashMap();
+    this.minSupport = minSupport;
+  }
+
+  /**
+   * Creates an FPTree using the attribute counts in attrCounts.
+   *
+   * Note that the counts in attrCounts are assumed to be complete;
+   * they are not updated as the tree is modified.
+   */
+  public FPTree(long[] attrCounts, long minSupport) {
+    this.root = new FPNode(null, -1, 0);
+    this.attrCountList = new LongArrayList();
+    for (int i = 0; i < attrCounts.length; i++) 
+      if (attrCounts[i] > 0) {
+        if (attrCountList.size() < (i+1)) {
+          attrCountList.setSize(i+1);
+        }
+        attrCountList.set(i, attrCounts[i]);
+      }
+    this.attrNodeLists = new OpenIntObjectHashMap();
+    this.minSupport = minSupport;
+  }
+
+
+  /**
+   * Returns the count of the given attribute, as supplied on construction.
+   */
+  public long headerCount(int attribute) {
+    return attrCountList.get(attribute);
+  }
+
+  /**
+   * Returns the root node of the tree.
+   */
+  public FPNode root() {
+    return root;
+  }
+
+  /**
+   * Adds an itemset with the given occurrance count.
+   */
+  public void accumulate(IntArrayList argItems, long count) {
+    // boxed primitive used so we can use custom comparitor in sort
+    List<Integer> items = Lists.newArrayList();
+    for (int i = 0; i < argItems.size(); i++) {
+      items.add(argItems.get(i));
+    }
+    Collections.sort(items, attrComparator);
+    
+    FPNode currNode = root;
+    for (int i = 0; i < items.size(); i++) {
+      int item = items.get(i);
+      long attrCount = 0;
+      if (item < attrCountList.size())
+        attrCount = attrCountList.get(item);
+      if (attrCount < minSupport)
+        continue;
+
+      FPNode next = currNode.child(item);
+      if (next == null) {
+        next = new FPNode(currNode, item, count);
+        currNode.addChild(next);
+        List<FPNode> nodeList = (List<FPNode>) attrNodeLists.get(item);
+        if (nodeList == null) {
+          nodeList = Lists.newArrayList();
+          attrNodeLists.put(item, nodeList);
+        }
+        nodeList.add(next);
+      } else {
+        next.accumulate(count);
+      }
+      currNode = next;
+    }
+  } 
+
+  /**
+   * Adds an itemset with the given occurrance count.
+   */
+  public void accumulate(List<Integer> argItems, long count) {
+    List<Integer> items = Lists.newArrayList();
+    items.addAll(argItems);
+    Collections.sort(items, attrComparator);
+    
+    FPNode currNode = root;
+    for (int i = 0; i < items.size(); i++) {
+      int item = items.get(i);
+      long attrCount = attrCountList.get(item);
+      if (attrCount < minSupport)
+        continue;
+
+      FPNode next = currNode.child(item);
+      if (next == null) {
+        next = new FPNode(currNode, item, count);
+        currNode.addChild(next);
+        List<FPNode> nodeList = (List<FPNode>) attrNodeLists.get(item);
+        if (nodeList == null) {
+          nodeList = Lists.newArrayList();
+          attrNodeLists.put(item, nodeList);
+        }
+        nodeList.add(next);
+      } else {
+        next.accumulate(count);
+      }
+      currNode = next;
+    }
+  } 
+
+
+  /**
+   * Returns an Iterable over the attributes in the tree, sorted by
+   * frequency (low to high).
+   */
+  public Iterable<Integer> attrIterable() {
+    List<Integer> attrs = Lists.newArrayList();
+    for (int i = 0; i < attrCountList.size(); i++) {
+      if (attrCountList.get(i) > 0)
+        attrs.add(i);
+    }
+    Collections.sort(attrs, attrComparator);
+    return attrs;
+  }
+
+  /**
+   * Returns an Iterable over the attributes in the tree, sorted by
+   * frequency (high to low).
+   */
+  public Iterable<Integer> attrIterableRev() {
+    List<Integer> attrs = Lists.newArrayList();
+    for (int i = 0; i < attrCountList.size(); i++) {
+      if (attrCountList.get(i) > 0)
+        attrs.add(i);
+    }
+    Collections.sort(attrs, Collections.reverseOrder(attrComparator));
+    return attrs;
+  }
+
+  /**
+   * Returns a conditional FP tree based on the targetAttr, containing
+   * only items that are more frequent.
+   */
+  public FPTree createMoreFreqConditionalTree(int targetAttr) {
+    LongArrayList counts = new LongArrayList();
+    List<FPNode> nodeList = (List<FPNode>) attrNodeLists.get(targetAttr);
+
+    for (FPNode currNode : nodeList) {
+      long pathCount = currNode.count();
+      while (currNode != root) {
+        int currAttr = currNode.attribute();
+        if (counts.size() <= currAttr) {
+          counts.setSize(currAttr+1);
+        }
+        long count = counts.get(currAttr);
+        counts.set(currNode.attribute(), count + pathCount);
+        currNode = currNode.parent();
+      }
+    }
+    if (counts.get(targetAttr) != attrCountList.get(targetAttr))
+      throw new IllegalStateException("mismatched counts for targetAttr="
+                                      +targetAttr+", ("+counts.get(targetAttr)
+                                      +" != "+attrCountList.get(targetAttr)+"); "
+                                      +"thisTree="+this+"\n");
+    counts.set(targetAttr, 0L);
+
+    FPTree toRet = new FPTree(counts, minSupport);
+    IntArrayList attrLst = new IntArrayList();
+    for (FPNode currNode : (List<FPNode>) attrNodeLists.get(targetAttr)) {
+      long count = currNode.count();
+      attrLst.clear();
+      while (currNode != root) {
+        if (currNode.count() < count) 
+          throw new IllegalStateException();
+        attrLst.add(currNode.attribute());
+        currNode = currNode.parent();
+      }
+
+      toRet.accumulate(attrLst, count);      
+    }    
+    return toRet;
+  }
+
+  // biggest count or smallest attr number goes first
+  private class AttrComparator implements Comparator<Integer>{
+    public int compare(Integer a, Integer b) {
+
+      long aCnt = 0;
+      if (a < attrCountList.size())
+        aCnt = attrCountList.get(a);
+      long bCnt = 0;
+      if (b < attrCountList.size())
+        bCnt = attrCountList.get(b);
+      if (aCnt == bCnt)
+        return a - b;
+      return (bCnt - aCnt) < 0 ? -1 : 1;
+    }
+  }
+
+  /**
+   *  Return a pair of trees that result from separating a common prefix
+   *  (if one exists) from the lower portion of this tree.
+   */
+  public Pair<FPTree, FPTree> splitSinglePrefix() {
+    if (root.numChildren() != 1) {
+      return new Pair<FPTree, FPTree>(null, this);
+    }
+    LongArrayList pAttrCountList = new LongArrayList();
+    LongArrayList qAttrCountList = attrCountList.copy();
+
+    FPNode currNode = root;
+    while (currNode.numChildren() == 1) {
+      currNode = currNode.children().iterator().next();
+      if (pAttrCountList.size() <= currNode.attribute())
+        pAttrCountList.setSize(currNode.attribute()+1);
+      pAttrCountList.set(currNode.attribute(), currNode.count());
+      qAttrCountList.set(currNode.attribute(), 0);
+    }
+
+    FPTree pTree = new FPTree(pAttrCountList, minSupport);
+    FPTree qTree = new FPTree(qAttrCountList, minSupport);
+    recursivelyAddPrefixPats(pTree, qTree, root, null);
+
+    return new Pair<FPTree, FPTree>(pTree, qTree);
+  }
+
+  private long recursivelyAddPrefixPats(FPTree pTree, FPTree qTree, FPNode node,
+                                        IntArrayList items) {
+    long added = 0;
+    long count = node.count();
+    int attribute = node.attribute();
+    if (items == null) {
+      // at root
+      if (!(node == root))
+        throw new IllegalStateException();
+      items = new IntArrayList();
+    } else {
+      items.add(attribute);
+    }
+    for (FPNode child : node.children()) {
+      added+= recursivelyAddPrefixPats(pTree, qTree, child, items);
+    }
+    if (added < count) {
+      long toAdd = count - added;
+      pTree.accumulate(items, toAdd);
+      qTree.accumulate(items, toAdd);
+      added+= toAdd;
+    }
+    if (!(node == root)) {
+      int lastIdx = items.size() - 1;
+      if (items.get(lastIdx) != attribute) {
+        throw new IllegalStateException();
+      }
+      items.remove(lastIdx);
+    }
+    return added;
+  }
+
+  private void toStringHelper(StringBuilder sb, FPNode currNode, String prefix) {
+    if (currNode.numChildren() == 0) {
+      sb.append(prefix).append("-{attr:").append(currNode.attribute())
+        .append(", cnt:").append(currNode.count()).append("}\n");
+    } else {
+      StringBuilder newPre = new StringBuilder(prefix);
+      newPre.append("-{attr:").append(currNode.attribute())
+        .append(", cnt:").append(currNode.count()).append('}');
+      StringBuilder fakePre = new StringBuilder();
+      while (fakePre.length() < newPre.length()) {
+        fakePre.append(' ');
+      }
+      int i = 0;
+      for (FPNode child : currNode.children()) 
+        toStringHelper(sb, child, (i++ == 0 ? newPre : fakePre).toString() + '-' + i + "->");
+    }
+  }
+
+  public String toString() {
+    StringBuilder sb = new StringBuilder("[FPTree\n");
+    toStringHelper(sb, root, "  ");
+    sb.append("]");
+    return sb.toString();
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java
index e69de29b..46315c37 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java
@@ -0,0 +1,113 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+
+import com.google.common.io.Resources;
+
+public final class FPGrowthRetailDataTest2 extends MahoutTestCase {
+
+  @Test
+  public void testSpecificCaseFromRetailDataMinSup500() throws IOException {
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+    
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail.dat").openStream()), "\\s+");
+    int pattern_41_36_39 = 0;
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = next.getFirst();
+      if (items.contains("41") && items.contains("36") && items.contains("39")) {
+        pattern_41_36_39++;
+      }
+    }
+    
+    final Map<Set<String>,Long> results = Maps.newHashMap();
+    
+    Set<String> returnableFeatures = new HashSet<String>();
+    returnableFeatures.add("41");
+    returnableFeatures.add("36");
+    returnableFeatures.add("39");
+    
+    fp.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource("retail.dat").openStream()), "\\s+"),
+
+      fp.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource("retail.dat")
+          .openStream()), "\\s+"), 500), 500, 1000, returnableFeatures,
+      new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+        @Override
+        public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+          for (Pair<List<String>,Long> v : value) {
+            List<String> l = v.getFirst();
+            results.put(new HashSet<String>(l), v.getSecond());
+          }
+        }
+        
+      }, new StatusUpdater() {
+        
+        @Override
+        public void update(String status) {}
+      });
+    
+    assertEquals(Long.valueOf(pattern_41_36_39), results.get(returnableFeatures));
+    
+  }
+
+  private long bestResults(Map<Set<String>, Long> res, Set<String> feats) {
+    Long best = res.get(feats);
+    if (best != null) 
+      return best;
+    else 
+      best = -1L;
+    for (Map.Entry<Set<String>, Long> ent : res.entrySet()) { 
+      Set<String> r = ent.getKey();
+      Long supp = ent.getValue();
+      if (supp <= best) 
+        continue;
+      boolean hasAll = true;
+      for (String f : feats) {
+        if (!r.contains(f)) {
+          hasAll = false;
+          break;
+        }
+      }
+      if (hasAll) 
+        best = supp;
+    }
+    return best;
+  }
+  
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java
index e69de29b..873b4743 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java
@@ -0,0 +1,181 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+
+import com.google.common.io.Resources;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public final class FPGrowthRetailDataTestVs extends MahoutTestCase {
+
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthRetailDataTestVs.class);
+
+  private long bestResults(Map<Set<String>, Long> res, Set<String> feats) {
+    Long best = res.get(feats);
+    if (best != null) 
+      return best;
+    else 
+      best = -1L;
+    for (Map.Entry<Set<String>, Long> ent : res.entrySet()) { 
+      Set<String> r = ent.getKey();
+      Long supp = ent.getValue();
+      if (supp <= best) 
+        continue;
+      boolean hasAll = true;
+      for (String f : feats) {
+        if (!r.contains(f)) {
+          hasAll = false;
+          break;
+        }
+      }
+      if (hasAll) 
+        best = supp;
+    }
+    return best;
+  }
+
+  @Test
+  public void testVsWithRetailData() throws IOException {
+    String inputFilename = "retail.dat";
+    int minSupport = 500;
+    Set<String> returnableFeatures = new HashSet<String>();
+    
+    org.apache.mahout.fpm.pfpgrowth.fpgrowth.
+      FPGrowth<String> fp1 = new org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth<String>();
+
+    final Map<Set<String>,Long> results1 = Maps.newHashMap();
+    
+    fp1.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+      fp1.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+           .openStream()), "\\s+"), minSupport), minSupport, 100000, 
+      returnableFeatures,
+      new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+        @Override
+        public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+          for (Pair<List<String>,Long> v : value) {
+            List<String> l = v.getFirst();
+            results1.put(new HashSet<String>(l), v.getSecond());
+            log.info("found pat ["+v.getSecond()+"]: "+ v.getFirst());
+          }
+        }
+        
+      }, new StatusUpdater() {
+        
+        @Override
+        public void update(String status) {}
+      });
+
+    FPGrowthObj<String> fp2 = new FPGrowthObj<String>();
+    final Map<Set<String>,Long> initialResults2 = Maps.newHashMap();
+    fp2.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+      fp2.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+           .openStream()), "\\s+"), minSupport), minSupport, 100000, 
+      new HashSet<String>(),
+      new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+        @Override
+        public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+          for (Pair<List<String>,Long> v : value) {
+            List<String> l = v.getFirst();
+            initialResults2.put(new HashSet<String>(l), v.getSecond());
+            log.info("found pat ["+v.getSecond()+"]: "+ v.getFirst());
+          }
+        }
+        
+      }, new StatusUpdater() {
+        
+        @Override
+        public void update(String status) {}
+      });
+
+    Map<Set<String>, Long> results2 = new HashMap<Set<String>, Long>();    
+    if (!returnableFeatures.isEmpty()) {
+      Map<Set<String>, Long> tmpResult = new HashMap<Set<String>, Long>();
+      for (Map.Entry<Set<String>, Long> result2 : initialResults2.entrySet()) {
+        Set<String> r2feats = result2.getKey();
+        boolean hasSome = false;
+        for (String rf : returnableFeatures) {
+          if (r2feats.contains(rf)) {
+            hasSome = true;
+            break;
+          }
+        }
+        if (hasSome) 
+          tmpResult.put(result2.getKey(), result2.getValue());
+      }
+      results2 = tmpResult;
+    } else {
+      results2 = initialResults2;
+    }
+
+    boolean allMatch = true;
+    int itemsetsChecked = 0;
+    for (Map.Entry<Set<String>, Long> result1 : results1.entrySet()) {
+      itemsetsChecked++;
+      Set<String> feats = result1.getKey();
+      long supp1 = result1.getValue();
+      long supp2 = bestResults(results2, feats);
+      if (supp1 != supp2) {
+        allMatch = false;
+        log.info("mismatch checking results1 ["+supp1+" vs "+supp2+"]: "+feats);
+      }
+    }
+    log.info("checked "+itemsetsChecked+" itemsets iterating through #1");
+
+    itemsetsChecked = 0;
+    for (Map.Entry<Set<String>, Long> result2 : results2.entrySet()) { 
+      itemsetsChecked++;
+      Set<String> feats = result2.getKey();
+      long supp2 = result2.getValue();
+      long supp1 = bestResults(results1, feats);
+      if (supp1 != supp2) {
+        allMatch = false;
+        log.info("mismatch checking results2 [ "+supp1+" vs "+supp2+"]: "+feats);
+      }
+    }
+    log.info("checked "+itemsetsChecked+" itemsets iterating through #2");
+
+    assertEquals( "Had mismatches!", allMatch, true);
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java
index e69de29b..b9abf9f6 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java
@@ -0,0 +1,249 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+
+import com.google.common.io.Resources;
+
+public final class FPGrowthSyntheticDataTest extends MahoutTestCase {
+
+  @Test
+    public void testSpecificCasesFromSynthData() throws IOException {
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+    
+    String inputFilename = "FPGsynth.dat";
+    int minSupport = 50;
+
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+                                                                                                  inputFilename).openStream()), "\\s+");
+    int patternCnt_10_13_1669 = 0;
+    int patternCnt_10_13 = 0;
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = next.getFirst();
+      if (items.contains("10") && items.contains("13")) {
+        patternCnt_10_13++;
+        if (items.contains("1669")) {
+          patternCnt_10_13_1669++;
+        }
+      }
+    }
+    
+    if (patternCnt_10_13_1669 < minSupport) 
+      throw new IllegalStateException("the test is broken or data is missing ("
+                                      + patternCnt_10_13_1669+", "
+                                      + patternCnt_10_13+")");
+
+    final Map<Set<String>,Long> results = Maps.newHashMap();
+    
+    Set<String> features_10_13 = new HashSet<String>();
+    features_10_13.add("10");
+    features_10_13.add("13");
+
+    Set<String> returnableFeatures = new HashSet<String>();
+    returnableFeatures.add("10");
+    returnableFeatures.add("13");
+    returnableFeatures.add("1669");
+    
+    fp.generateTopKFrequentPatterns(
+                                    new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+                                    fp.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+                                                                                                   .openStream()), "\\s+"), minSupport), minSupport, 100000, 
+                                    returnableFeatures,
+                                    new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+                                      @Override
+                                        public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+                                        for (Pair<List<String>,Long> v : value) {
+                                          List<String> l = v.getFirst();
+                                          results.put(new HashSet<String>(l), v.getSecond());
+                                          System.out.println("found pat ["+v.getSecond()+"]: "+ v.getFirst());
+                                        }
+                                      }
+        
+                                    }, new StatusUpdater() {
+        
+                                        @Override
+                                          public void update(String status) {}
+                                      });
+
+    assertEquals(patternCnt_10_13, highestSupport(results, features_10_13));
+    assertEquals(patternCnt_10_13_1669, highestSupport(results, returnableFeatures));
+    
+  }
+
+  private long highestSupport(Map<Set<String>, Long> res, Set<String> feats) {
+    Long best= res.get(feats);
+    if (best != null) 
+      return best;
+    else 
+      best= -1L;
+    for (Map.Entry<Set<String>, Long> ent : res.entrySet()) { 
+      Set<String> r= ent.getKey();
+      Long supp= ent.getValue();
+      if (supp <= best) 
+        continue;
+      boolean hasAll= true;
+      for (String f : feats) {
+        if (!r.contains(f)) {
+          hasAll= false;
+          break;
+        }
+      }
+      if (hasAll) 
+        best= supp;
+    }
+    return best;
+  }
+
+  @Test
+    public void testVsWithSynthData() throws IOException {
+    String inputFilename= "FPGsynth.dat";
+    int minSupport= 100;
+    Set<String> returnableFeatures = new HashSet<String>();
+
+    // not limiting features (or including too many) can cause
+    // the test to run a very long time
+    returnableFeatures.add("10");
+    returnableFeatures.add("13");
+    //    returnableFeatures.add("1669");
+    
+    FPGrowth<String> fp1 = new FPGrowth<String>();
+
+    final Map<Set<String>,Long> results1 = Maps.newHashMap();
+    
+    fp1.generateTopKFrequentPatterns(
+                                     new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+                                     fp1.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+                                                                                                     .openStream()), "\\s+"), minSupport), minSupport, 1000000, 
+                                     returnableFeatures,
+                                     new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+                                       @Override
+                                         public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+                                         for (Pair<List<String>,Long> v : value) {
+                                           List<String> l = v.getFirst();
+                                           results1.put(new HashSet<String>(l), v.getSecond());
+                                           System.out.println("found pat ["+v.getSecond()+"]: "+ v.getFirst());
+                                         }
+                                       }
+        
+                                     }, new StatusUpdater() {
+        
+                                         @Override
+                                           public void update(String status) {}
+                                       });
+
+    FPGrowthObj<String> fp2 = new FPGrowthObj<String>();
+    final Map<Set<String>,Long> initialResults2 = Maps.newHashMap();
+    fp2.generateTopKFrequentPatterns(
+                                     new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+                                     fp2.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+                                                                                                     .openStream()), "\\s+"), minSupport), minSupport, 1000000, 
+                                     new HashSet<String>(),
+                                     new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+                                       @Override
+                                         public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+                                         for (Pair<List<String>,Long> v : value) {
+                                           List<String> l = v.getFirst();
+                                           initialResults2.put(new HashSet<String>(l), v.getSecond());
+                                           System.out.println("found pat ["+v.getSecond()+"]: "+ v.getFirst());
+                                         }
+                                       }
+        
+                                     }, new StatusUpdater() {
+        
+                                         @Override
+                                           public void update(String status) {}
+                                       });
+
+    Map<Set<String>, Long> results2= new HashMap<Set<String>, Long>();    
+    if (!returnableFeatures.isEmpty()) {
+      Map<Set<String>, Long> tmpResult= new HashMap<Set<String>, Long>();
+      for (Map.Entry<Set<String>, Long> result2 : initialResults2.entrySet()) {
+        Set<String> r2feats= result2.getKey();
+        boolean hasSome= false;
+        for (String rf : returnableFeatures) {
+          if (r2feats.contains(rf)) {
+            hasSome= true;
+            break;
+          }
+        }
+        if (hasSome) 
+          tmpResult.put(result2.getKey(), result2.getValue());
+      }
+      results2= tmpResult;
+    } else {
+      results2= initialResults2;
+    }
+
+    boolean allMatch= true;
+    int itemsetsChecked= 0;
+    for (Map.Entry<Set<String>, Long> result1 : results1.entrySet()) {
+      itemsetsChecked++;
+      Set<String> feats= result1.getKey();
+      long supp1= result1.getValue();
+      long supp2= highestSupport(results2, feats);
+      if (supp1 != supp2) {
+        allMatch= false;
+        System.out.println("mismatch checking results1 [ "+supp1+" vs "+supp2+"]: "+feats);
+      }
+    }
+    System.out.println("checked "+itemsetsChecked+" itemsets iterating through #1");
+
+    itemsetsChecked= 0;
+    for (Map.Entry<Set<String>, Long> result2 : results2.entrySet()) { 
+      itemsetsChecked++;
+      Set<String> feats= result2.getKey();
+      long supp2= result2.getValue();
+      long supp1= highestSupport(results1, feats);
+      if (supp1 != supp2) {
+        allMatch= false;
+        System.out.println("mismatch checking results2 [ "+supp1+" vs "+supp2+"]: "+feats);
+      }
+    }
+    System.out.println("checked "+itemsetsChecked+" itemsets iterating through #2");
+
+    assertEquals("Had mismatches!", allMatch, true);
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java
index e69de29b..bdcd0589 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java
@@ -0,0 +1,192 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.List;
+
+import com.google.common.collect.Lists;
+import com.google.common.io.Closeables;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.fpm.pfpgrowth.convertors.ContextStatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.convertors.SequenceFileOutputCollector;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.StringOutputConverter;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+
+public final class FPGrowthTest2 extends MahoutTestCase {
+
+  @Test
+  public void testMaxHeapFPGrowth() throws Exception {
+
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("E", "A", "D", "B"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("D", "A", "C", "E", "B"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("C", "A", "B", "E"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("B", "A", "D"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("D"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("D", "B"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("A", "D", "E"), 1L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("B", "C"), 1L));
+
+    Path path = getTestTempFilePath("fpgrowthTest.dat");
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(conf);
+
+    SequenceFile.Writer writer =
+        new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
+    try {
+    fp.generateTopKFrequentPatterns(
+        transactions.iterator(),
+        fp.generateFList(transactions.iterator(), 3),
+        3,
+        100,
+        new HashSet<String>(),
+        new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)),
+        new ContextStatusUpdater(null));
+    } finally {
+      Closeables.closeQuietly(writer);
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowthObj.readFrequentPattern(conf, path);
+    assertEquals(
+      "[(C,([B, C],3)), "
+          + "(E,([A, E],4), ([A, B, E],3), ([A, D, E],3)), "
+          + "(A,([A],5), ([A, D],4), ([A, B],4), ([A, B, D],3)), "
+          + "(D,([D],6), ([B, D],4)), "
+          + "(B,([B],6))]",
+      frequentPatterns.toString());
+
+  }
+  
+  /**
+   * Trivial test for MAHOUT-617
+   */
+  @Test
+  public void testMaxHeapFPGrowthData1() throws Exception {
+
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X"), 12L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("Y"), 4L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y"), 10L));
+
+    Path path = getTestTempFilePath("fpgrowthTestData1.dat");
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(conf);
+    System.out.println(fp.generateFList(transactions.iterator(), 2));
+    SequenceFile.Writer writer =
+        new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
+    try {
+      fp.generateTopKFrequentPatterns(
+          transactions.iterator(),
+          fp.generateFList(transactions.iterator(), 2),
+          2,
+          100,
+          new HashSet<String>(),
+          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)),
+          new ContextStatusUpdater(null));
+    } finally {
+      Closeables.closeQuietly(writer);
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowthObj.readFrequentPattern(conf, path);
+    assertEquals(
+      "[(Y,([Y],14), ([X, Y],10)), (X,([X],22))]", frequentPatterns.toString());
+  }
+  
+  /**
+   * Trivial test for MAHOUT-617
+   */
+  @Test
+  public void testMaxHeapFPGrowthData2() throws Exception {
+
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X"), 12L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("Y"), 4L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y"), 10L));
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y", "Z"), 11L));
+
+    Path path = getTestTempFilePath("fpgrowthTestData2.dat");
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(conf);
+    System.out.println(fp.generateFList(transactions.iterator(), 2));
+    SequenceFile.Writer writer =
+        new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
+    try {
+      fp.generateTopKFrequentPatterns(
+          transactions.iterator(),
+          fp.generateFList(transactions.iterator(), 2),
+          2,
+          100,
+          new HashSet<String>(),
+          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)),
+          new ContextStatusUpdater(null));
+    } finally {
+      Closeables.closeQuietly(writer);
+    }
+
+    List<Pair<String, TopKStringPatterns>> frequentPatterns = FPGrowthObj.readFrequentPattern(conf, path);
+    assertEquals(
+      "[(Z,([X, Y, Z],11)), (Y,([Y],25), ([X, Y],21)), (X,([X],33))]",
+      frequentPatterns.toString());
+  }
+
+  /**
+   * Trivial test for MAHOUT-355
+   */
+  @Test
+  public void testNoNullPointerExceptionWhenReturnableFeaturesIsNull() throws Exception {
+
+    FPGrowthObj<String> fp = new FPGrowthObj<String>();
+
+    Collection<Pair<List<String>,Long>> transactions = Lists.newArrayList();
+    transactions.add(new Pair<List<String>,Long>(Arrays.asList("E", "A", "D", "B"), 1L));
+
+    OutputCollector<String, List<Pair<List<String>, Long>>> noOutput =
+        new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+      @Override
+      public void collect(String arg0, List<Pair<List<String>, Long>> arg1) { 
+      }
+    };
+
+    fp.generateTopKFrequentPatterns(
+        transactions.iterator(),
+        fp.generateFList(transactions.iterator(), 3),
+        3,
+        100,
+        null,
+        noOutput,
+        new ContextStatusUpdater(null));
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
index ade8557f..ee19bb0c 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
@@ -87,11 +87,67 @@ public void setUp() throws Exception {
     }
   }
   
+   
+  /**
+   * Test Parallel FPGrowth on retail data using top-level runPFPGrowth() method
+   */ 
   @Test
   public void testRetailDataMinSup100() throws Exception {
     StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
       "retail_results_with_min_sup_100.dat").openStream()), "\\s+");
+    Map<Set<String>,Long> expectedResults = Maps.newHashMap();
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = Lists.newArrayList(next.getFirst());
+      String supportString = items.remove(items.size() - 1);
+      Long support = Long.parseLong(supportString.substring(1, supportString.length() - 1));
+      expectedResults.put(new HashSet<String>(items), support);
+    }
+
+    PFPGrowth.runPFPGrowth(params);
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
     
+    Map<Set<String>,Long> results = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results.put(new HashSet<String>(entry.getFirst()), entry.getSecond());
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : results.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (expectedResults.get(key) == null) {
+        System.out.println("spurious (1): " + key+ " with " +entry.getValue());
+      } else {
+        if (!expectedResults.get(key).equals(results.get(entry.getKey()))) {
+          System.out.println("invalid (1): " + key + ", expected: " + expectedResults.get(key) + ", got: "
+                             +                             + results.get(entry.getKey()));
+        } else {
+          System.out.println("matched (1): " + key + ", with: " + expectedResults.get(key));
+        }
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : expectedResults.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results.get(key) == null) {
+        System.out.println("missing (1): " + key+ " with " +entry.getValue());
+      }
+    }
+    assertEquals(expectedResults.size(), results.size());
+  }
+  
+
+  /**
+   * Test Parallel FPG on retail data, running various stages individually
+   */ 
+  @Test
+  public void testRetailDataMinSup100InSteps() throws Exception {
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail_results_with_min_sup_100.dat").openStream()), "\\s+");   
     Map<Set<String>,Long> expectedResults = Maps.newHashMap();
     while (it.hasNext()) {
       Pair<List<String>,Long> next = it.next();
@@ -103,11 +159,16 @@ public void testRetailDataMinSup100() throws Exception {
     Configuration conf = new Configuration();
     log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
     PFPGrowth.startParallelCounting(params, conf);
-    log.info("Starting Grouping Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
-    PFPGrowth.startGroupingItems(params, conf);
-    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
-    PFPGrowth.startGroupingItems(params, conf);
-    PFPGrowth.startTransactionSorting(params, conf);
+
+    List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
+    PFPGrowth.saveFList(fList, params, conf);
+    int numGroups = params.getInt(PFPGrowth.NUM_GROUPS, 
+                                  PFPGrowth.NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) 
+      maxPerGroup++;
+    params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
+
     PFPGrowth.startParallelFPGrowth(params, conf);
     log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
     PFPGrowth.startAggregating(params, conf);
@@ -125,11 +186,13 @@ public void testRetailDataMinSup100() throws Exception {
     for (Entry<Set<String>,Long> entry : results.entrySet()) {
       Set<String> key = entry.getKey();
       if (expectedResults.get(key) == null) {
-        System.out.println("missing: " + key);
+        System.out.println("spurious (2): " + key);
       } else {
         if (!expectedResults.get(key).equals(results.get(entry.getKey()))) {
-          System.out.println("invalid: " + key + ", expected: " + expectedResults.get(key) + ", got: "
+          System.out.println("invalid (2): " + key + ", expected: " + expectedResults.get(key) + ", got: "
                              + results.get(entry.getKey()));
+        } else {
+          System.out.println("matched (2): " + key + ", with: " + expectedResults.get(key));
         }
       }
     }
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java
index e69de29b..dbb35ce3 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java
@@ -0,0 +1,210 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth2;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.fpm.pfpgrowth.PFPGrowth;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.io.Resources;
+
+public class PFPGrowthRetailDataTest2 extends MahoutTestCase {
+  
+  private final Parameters params = new Parameters();
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthRetailDataTest2.class);
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    params.set(PFPGrowth.MIN_SUPPORT, "100");
+    params.set(PFPGrowth.MAX_HEAPSIZE, "10000");
+    params.set(PFPGrowth.NUM_GROUPS, "50");
+    params.set(PFPGrowth.ENCODING, "UTF-8");
+    params.set(PFPGrowth.USE_FPG2, "true");
+    File inputDir = getTestTempDir("transactions");
+    File outputDir = getTestTempDir("frequentpatterns");
+    File input = new File(inputDir, "test.txt");
+    params.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    params.set(PFPGrowth.OUTPUT, outputDir.getAbsolutePath());
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+        "retail.dat").openStream()), "\\s+");
+      Collection<List<String>> transactions = Lists.newArrayList();
+      
+      while (it.hasNext()) {
+        Pair<List<String>,Long> next = it.next();
+        transactions.add(next.getFirst());
+      }
+      
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = ",";
+        }
+        writer.write("\n");
+      }
+      
+    } finally {
+      Closeables.closeQuietly(writer);
+    }
+  }
+  
+  /**
+   * Test Parallel FPGrowth on retail data using top-level runPFPGrowth() method
+   */ 
+  @Test
+  public void testRetailDataMinSup100() throws Exception {
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail_results_with_min_sup_100.dat").openStream()), "\\s+");
+    Map<Set<String>,Long> expectedResults = Maps.newHashMap();
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = Lists.newArrayList(next.getFirst());
+      String supportString = items.remove(items.size() - 1);
+      Long support = Long.parseLong(supportString.substring(1, supportString.length() - 1));
+      expectedResults.put(new HashSet<String>(items), support);
+    }
+
+    PFPGrowth.runPFPGrowth(params);
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+    
+    Map<Set<String>,Long> results = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results.put(new HashSet<String>(entry.getFirst()), entry.getSecond());
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : results.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (expectedResults.get(key) == null) {
+        System.out.println("spurious (1): " + key+ " with " +entry.getValue());
+      } else {
+        if (!expectedResults.get(key).equals(results.get(entry.getKey()))) {
+          System.out.println("invalid (1): " + key + ", expected: " + expectedResults.get(key) + ", got: "
+                             + results.get(entry.getKey()));
+        } else {
+          System.out.println("matched (1): " + key + ", with: " + expectedResults.get(key));
+        }
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : expectedResults.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results.get(key) == null) {
+        System.out.println("missing (1): " + key+ " with " +entry.getValue());
+      }
+    }
+    assertEquals(expectedResults.size(), results.size());
+  }
+
+  /**
+   * Test Parallel FPG on retail data, running various stages individually
+   */ 
+  @Test
+  public void testRetailDataMinSup100InSteps() throws Exception {
+    StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+      "retail_results_with_min_sup_100.dat").openStream()), "\\s+");    
+    Map<Set<String>,Long> expectedResults = Maps.newHashMap();
+    while (it.hasNext()) {
+      Pair<List<String>,Long> next = it.next();
+      List<String> items = Lists.newArrayList(next.getFirst());
+      String supportString = items.remove(items.size() - 1);
+      Long support = Long.parseLong(supportString.substring(1, supportString.length() - 1));
+      expectedResults.put(new HashSet<String>(items), support);
+    }
+    Configuration conf = new Configuration();
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    PFPGrowth.startParallelCounting(params, conf);
+
+    List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
+    PFPGrowth.saveFList(fList, params, conf);
+
+    int numGroups = params.getInt(PFPGrowth.NUM_GROUPS, 
+                                  PFPGrowth.NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) 
+      maxPerGroup++;
+    params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
+
+    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    PFPGrowth.startParallelFPGrowth(params, conf);
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    PFPGrowth.startAggregating(params, conf);
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+    
+    Map<Set<String>,Long> results = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results.put(new HashSet<String>(entry.getFirst()), entry.getSecond());
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : results.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (expectedResults.get(key) == null) {
+        System.out.println("spurious (2): " + key + ", " + entry.getValue());
+      } else {
+        if (!expectedResults.get(key).equals(results.get(entry.getKey()))) {
+          System.out.println("invalid (2): " + key + ", expected: " + expectedResults.get(key) + ", got: "
+                             + results.get(entry.getKey()));
+        } else {
+          System.out.println("matched (2): " + key + ", with: " + expectedResults.get(key));
+        }         
+      }
+    }
+    
+    for (Entry<Set<String>,Long> entry : expectedResults.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results.get(key) == null) {
+        System.out.println("missing2: " + key + ", " + entry.getValue());
+      }
+    }
+    assertEquals(expectedResults.size(), results.size());
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java
index e69de29b..a5ebd7dd 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java
@@ -0,0 +1,184 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Lists;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.junit.Test;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+import com.google.common.base.Charsets;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import com.google.common.io.Resources;
+
+public final class PFPGrowthRetailDataTestVs extends MahoutTestCase {
+
+  private final Parameters paramsImpl1 = new Parameters();
+  private final Parameters paramsImpl2 = new Parameters();
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthRetailDataTestVs.class);
+
+  private static long bestResults(Map<Set<String>, Long> res, Set<String> feats) {
+    Long best = res.get(feats);
+    if (best != null) 
+      return best;
+    else 
+      best = -1L;
+    for (Map.Entry<Set<String>, Long> ent : res.entrySet()) { 
+      Set<String> r = ent.getKey();
+      Long supp = ent.getValue();
+      if (supp <= best) 
+        continue;
+      boolean hasAll = true;
+      for (String f : feats) {
+        if (!r.contains(f)) {
+          hasAll = false;
+          break;
+        }
+      }
+      if (hasAll) 
+        best = supp;
+    }
+    return best;
+  }
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+
+    File inputDir = getTestTempDir("transactions");
+    File input = new File(inputDir, "test.txt");
+
+    paramsImpl1.set(PFPGrowth.MIN_SUPPORT, "100");
+    paramsImpl1.set(PFPGrowth.MAX_HEAPSIZE, "10000");
+    paramsImpl1.set(PFPGrowth.NUM_GROUPS, "50");
+    paramsImpl1.set(PFPGrowth.ENCODING, "UTF-8");
+    paramsImpl1.set(PFPGrowth.INPUT, input.getAbsolutePath());
+
+    paramsImpl2.set(PFPGrowth.MIN_SUPPORT, "100");
+    paramsImpl2.set(PFPGrowth.MAX_HEAPSIZE, "10000");
+    paramsImpl2.set(PFPGrowth.NUM_GROUPS, "50");
+    paramsImpl2.set(PFPGrowth.ENCODING, "UTF-8");
+    paramsImpl2.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    paramsImpl2.set(PFPGrowth.USE_FPG2, "true");
+
+    File outputDir1 = getTestTempDir("frequentpatterns1");
+    paramsImpl1.set(PFPGrowth.OUTPUT, outputDir1.getAbsolutePath());
+
+    File outputDir2 = getTestTempDir("frequentpatterns2");
+    paramsImpl2.set(PFPGrowth.OUTPUT, outputDir2.getAbsolutePath());
+
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+        "retail.dat").openStream()), "\\s+");
+      Collection<List<String>> transactions = Lists.newArrayList();
+      
+      while (it.hasNext()) {
+        Pair<List<String>,Long> next = it.next();
+        transactions.add(next.getFirst());
+      }
+      
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = ",";
+        }
+        writer.write("\n");
+      }
+      
+    } finally {
+      Closeables.closeQuietly(writer);
+    }
+  }
+  
+   
+  /**
+   * Test Parallel FPGrowth on retail data using top-level runPFPGrowth() method
+   */ 
+  @Test
+  public void testParallelRetailVs() throws Exception {
+
+    PFPGrowth.runPFPGrowth(paramsImpl1);
+    List<Pair<String,TopKStringPatterns>> frequentPatterns1 = PFPGrowth.readFrequentPattern(paramsImpl1);
+    
+    Map<Set<String>,Long> results1 = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns1) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results1.put(new HashSet<String>(entry.getFirst()), entry.getSecond());
+      }
+    }
+  
+    PFPGrowth.runPFPGrowth(paramsImpl2);
+    List<Pair<String,TopKStringPatterns>> frequentPatterns2 = PFPGrowth.readFrequentPattern(paramsImpl2);
+  
+    Map<Set<String>,Long> results2 = Maps.newHashMap();
+    for (Pair<String,TopKStringPatterns> topK : frequentPatterns2) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        results2.put(new HashSet<String>(entry.getFirst()), entry.getSecond());
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : results1.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results2.get(key) == null) {
+        System.out.println("spurious (1): " + key+ " with " +entry.getValue());
+      } else {
+        if (!results2.get(key).equals(results1.get(entry.getKey()))) {
+          System.out.println("invalid (1): " + key + ", expected: " + results2.get(key) + ", got: "
+                             +                             + results1.get(entry.getKey()));
+        } else {
+          System.out.println("matched (1): " + key + ", with: " + results2.get(key));
+        }
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : results2.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (results1.get(key) == null) {
+        System.out.println("missing (1): " + key+ " with " +entry.getValue());
+      }
+    }
+    assertEquals(results2.size(), results1.size());
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java
index e69de29b..03526a95 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java
@@ -0,0 +1,191 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth2;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.common.iterator.FileLineIterable;
+import org.apache.mahout.common.iterator.StringRecordIterator;
+import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.apache.mahout.fpm.pfpgrowth.PFPGrowth;
+import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.io.Resources;
+
+public class PFPGrowthSynthDataTest2 extends MahoutTestCase {
+  
+  private final Parameters params = new Parameters();
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthSynthDataTest2.class);
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    params.set(PFPGrowth.MIN_SUPPORT, "100");
+    params.set(PFPGrowth.MAX_HEAPSIZE, "10000");
+    params.set(PFPGrowth.NUM_GROUPS, "50");
+    params.set(PFPGrowth.ENCODING, "UTF-8");
+    params.set(PFPGrowth.USE_FPG2, "true");
+    params.set(PFPGrowth.SPLIT_PATTERN, " ");
+    File inputDir = getTestTempDir("transactions");
+    File outputDir = getTestTempDir("frequentpatterns");
+    File input = new File(inputDir, "synth_test.txt");
+    params.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    params.set(PFPGrowth.OUTPUT, outputDir.getAbsolutePath());
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
+        "FPGsynth.dat").openStream()), "\\s+");
+      Collection<List<String>> transactions = Lists.newArrayList();
+      
+      while (it.hasNext()) {
+        Pair<List<String>,Long> next = it.next();
+        transactions.add(next.getFirst());
+      }
+      
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = " ";
+        }
+        writer.write("\n");
+      }
+      
+    } finally {
+      Closeables.closeQuietly(writer);
+    }
+  }
+
+
+  private long highestSupport(Map<Set<String>, Long> res, Set<String> feats) {
+    Long best= res.get(feats);
+    if (best != null) 
+      return best;
+    else 
+      best= -1L;
+    for (Map.Entry<Set<String>, Long> ent : res.entrySet()) { 
+      Set<String> r= ent.getKey();
+      Long supp= ent.getValue();
+      if (supp <= best) 
+        continue;
+      boolean hasAll= true;
+      for (String f : feats) {
+        if (!r.contains(f)) {
+          hasAll= false;
+          break;
+        }
+      }
+      if (hasAll) 
+        best= supp;
+    }
+    return best;
+  }
+
+  @Test
+  public void testVsSequential() throws Exception {
+
+    final Map<Set<String>,Long> parallelResult = Maps.newHashMap();
+
+    PFPGrowth.runPFPGrowth(params);
+    List<Pair<String,TopKStringPatterns>> tmpParallel = PFPGrowth.readFrequentPattern(params);
+    
+    for (Pair<String,TopKStringPatterns> topK : tmpParallel) {
+      Iterator<Pair<List<String>,Long>> topKIt = topK.getSecond().iterator();
+      while (topKIt.hasNext()) {
+        Pair<List<String>,Long> entry = topKIt.next();
+        parallelResult.put(new HashSet<String>(entry.getFirst()), entry.getSecond());
+      }
+    }
+
+    //////
+
+    String inputFilename= "FPGsynth.dat";
+    int minSupport= 100;
+
+    final Map<Set<String>,Long> seqResult = Maps.newHashMap();
+    
+    FPGrowthObj<String> fpSeq = new FPGrowthObj<String>();
+    fpSeq.generateTopKFrequentPatterns(
+      new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename).openStream()), "\\s+"),
+
+      fpSeq.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
+           .openStream()), "\\s+"), minSupport), minSupport, 1000000, 
+      null,
+      new OutputCollector<String,List<Pair<List<String>,Long>>>() {
+        
+        @Override
+        public void collect(String key, List<Pair<List<String>,Long>> value) {
+          
+          for (Pair<List<String>,Long> v : value) {
+            List<String> l = v.getFirst();
+            seqResult.put(new HashSet<String>(l), v.getSecond());
+          }
+        }
+        
+      }, new StatusUpdater() {
+        
+        @Override
+        public void update(String status) {}
+      });
+
+    for (Entry<Set<String>,Long> entry : parallelResult.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (seqResult.get(key) == null) {
+        log.info("spurious (1): " + key+ " with " +entry.getValue());
+      } else {
+        if (!seqResult.get(key).equals(parallelResult.get(entry.getKey()))) {
+          log.info("invalid (1): " + key + ", expected: " + seqResult.get(key) + ", got: "
+                             +                             + parallelResult.get(entry.getKey()));
+        } else {
+          log.info("matched (1): " + key + ", with: " + seqResult.get(key));
+        }
+      }
+    }
+  
+    for (Entry<Set<String>,Long> entry : seqResult.entrySet()) {
+      Set<String> key = entry.getKey();
+      if (parallelResult.get(key) == null) {
+        log.info("missing (1): " + key+ " with " +entry.getValue());
+      }
+    }
+    assertEquals(seqResult.size(), parallelResult.size());
+  }
+
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
index cc1ed8a8..a448adf4 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
@@ -79,8 +79,30 @@ public void setUp() throws Exception {
     
   }
 
+  /**
+   * Test Parallel FPGrowth on small example data using top-level
+   * runPFPGrowth() method
+   */ 
   @Test
   public void testStartParallelFPGrowth() throws Exception {
+    Configuration conf = new Configuration();
+    PFPGrowth.runPFPGrowth(params);
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+
+    assertEquals("[(A,([A],5), ([D, A],4), ([B, A],4), ([A, E],4)), "
+                 + "(B,([B],6), ([B, D],4), ([B, A],4), ([B, D, A],3)), " 
+                 + "(C,([B, C],3)), "
+                 + "(D,([D],6), ([D, A],4), ([B, D],4), ([D, A, E],3)), "
+                 + "(E,([A, E],4), ([D, A, E],3), ([B, A, E],3))]", frequentPatterns.toString());
+  }
+
+  /**
+   * Test Parallel FPGrowth on small example data using top-level
+   * runPFPGrowth() method
+   */ 
+  @Test
+  public void testStartParallelFPGrowthInSteps() throws Exception {
     Configuration conf = new Configuration();
     log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
     PFPGrowth.startParallelCounting(params, conf);
@@ -89,12 +111,15 @@ public void testStartParallelFPGrowth() throws Exception {
     log.info("{}", fList);
     assertEquals("[(B,6), (D,6), (A,5), (E,4), (C,3)]", fList.toString());
  
-    log.info("Starting Grouping Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
-    PFPGrowth.startGroupingItems(params, conf);
+    PFPGrowth.saveFList(fList, params, conf);
+    int numGroups = params.getInt(PFPGrowth.NUM_GROUPS, 
+                                  PFPGrowth.NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) 
+      maxPerGroup++;
+    params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
  
     log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
-    PFPGrowth.startGroupingItems(params, conf);
-    PFPGrowth.startTransactionSorting(params, conf);
     PFPGrowth.startParallelFPGrowth(params, conf);
     log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
     PFPGrowth.startAggregating(params, conf);
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java
index e69de29b..8af292e0 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java
@@ -0,0 +1,136 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.fpm.pfpgrowth;
+
+import java.io.File;
+import java.io.Writer;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.Parameters;
+import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public final class PFPGrowthTest2 extends MahoutTestCase {
+  
+  private static final Logger log = LoggerFactory.getLogger(PFPGrowthTest.class);
+  
+  private final Parameters params = new Parameters();
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    params.set(PFPGrowth.MIN_SUPPORT, "3");
+    params.set(PFPGrowth.MAX_HEAPSIZE, "4");
+    params.set(PFPGrowth.NUM_GROUPS, "2");
+    params.set(PFPGrowth.ENCODING, "UTF-8");
+    params.set(PFPGrowth.USE_FPG2, "true");
+    File inputDir = getTestTempDir("transactions");
+    File outputDir = getTestTempDir("frequentpatterns");
+    File input = new File(inputDir, "test.txt");
+    params.set(PFPGrowth.INPUT, input.getAbsolutePath());
+    params.set(PFPGrowth.OUTPUT, outputDir.getAbsolutePath());
+    Writer writer = Files.newWriter(input, Charsets.UTF_8);
+    try {
+      Collection<List<String>> transactions = Lists.newArrayList();
+      transactions.add(Arrays.asList("E", "A", "D", "B"));
+      transactions.add(Arrays.asList("D", "A", "C", "E", "B"));
+      transactions.add(Arrays.asList("C", "A", "B", "E"));
+      transactions.add(Arrays.asList("B", "A", "D"));
+      transactions.add(Arrays.asList("D"));
+      transactions.add(Arrays.asList("D", "B"));
+      transactions.add(Arrays.asList("A", "D", "E"));
+      transactions.add(Arrays.asList("B", "C"));
+      for (List<String> transaction : transactions) {
+        String sep = "";
+        for (String item : transaction) {
+          writer.write(sep + item);
+          sep = ",";
+        }
+        writer.write("\n");
+      }
+    } finally {
+      Closeables.closeQuietly(writer);
+    }
+    
+  }
+
+  /**
+   * Test Parallel FPGrowth on small example data using top-level
+   * runPFPGrowth() method
+   */ 
+  @Test
+  public void testStartParallelFPGrowth() throws Exception {
+    Configuration conf = new Configuration();
+    PFPGrowth.runPFPGrowth(params);
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+
+    assertEquals("[(A,([A],5), ([D, A],4), ([B, A],4), ([A, E],4)), "
+                 + "(B,([B],6), ([B, D],4), ([B, A],4), ([B, D, A],3)), " 
+                 + "(C,([B, C],3)), "
+                 + "(D,([D],6), ([D, A],4), ([B, D],4), ([D, A, E],3)), "
+                 + "(E,([A, E],4), ([D, A, E],3), ([B, A, E],3))]", frequentPatterns.toString());                                                                 
+  }
+
+  /**
+   * Test Parallel FPG on small example data, running various stages
+   * individually
+   */ 
+  @Test
+  public void testStartParallelFPGrowthInSteps() throws Exception {
+    Configuration conf = new Configuration();
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    PFPGrowth.startParallelCounting(params, conf);
+    log.info("Reading fList Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
+    log.info("{}", fList);
+    assertEquals("[(B,6), (D,6), (A,5), (E,4), (C,3)]", fList.toString());
+ 
+    PFPGrowth.saveFList(fList, params, conf);
+    int numGroups = params.getInt(PFPGrowth.NUM_GROUPS, 
+                                  PFPGrowth.NUM_GROUPS_DEFAULT);
+    int maxPerGroup = fList.size() / numGroups;
+    if (fList.size() % numGroups != 0) 
+      maxPerGroup++;
+    params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
+
+    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    PFPGrowth.startParallelFPGrowth(params, conf);
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    PFPGrowth.startAggregating(params, conf);
+
+    List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
+
+    assertEquals("[(A,([A],5), ([D, A],4), ([B, A],4), ([A, E],4)), "
+                 + "(B,([B],6), ([B, D],4), ([B, A],4), ([B, D, A],3)), " 
+                 + "(C,([B, C],3)), "
+                 + "(D,([D],6), ([D, A],4), ([B, D],4), ([D, A, E],3)), "
+                 + "(E,([A, E],4), ([D, A, E],3), ([B, A, E],3))]", frequentPatterns.toString());                                                                 
+  }
+}
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java
index e727f97b..c0a656a7 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java
@@ -21,10 +21,10 @@
 import java.util.List;
 import java.util.Random;
 
-import com.google.common.collect.Lists;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.RandomUtils;
+import org.apache.mahout.math.list.IntArrayList;
 import org.junit.Before;
 import org.junit.Test;
 import org.slf4j.Logger;
@@ -52,8 +52,8 @@ public void setUp() throws Exception {
     gen = RandomUtils.getRandom();
   }
 
-  private List<Integer> generateRandomArray() {
-    List<Integer> list = Lists.newArrayList();
+  private IntArrayList generateRandomArray() {
+    IntArrayList list = new IntArrayList();
     for (int i = 0; i < MAX_FEATURES; i++) {
       if (gen.nextInt() % SKIP_RATE == 0) {
         list.add(i);
@@ -69,7 +69,7 @@ public void testTransactionTree() {
     int nodes = 0;
     int total = 0;
     for (int i = 0; i < MAX_TRANSACTIONS; i++) {
-      List<Integer> array = generateRandomArray();
+      IntArrayList array = generateRandomArray();
       total += array.size();
       nodes += tree.addPattern(array, 1 + gen.nextInt(MAX_DUPLICATION));
     }
@@ -84,9 +84,9 @@ public void testTransactionTree() {
     StringBuilder sb = new StringBuilder();
     int count = 0;
     int items = 0;
-    Iterator<Pair<List<Integer>,Long>> it = tree.iterator();
+    Iterator<Pair<IntArrayList,Long>> it = tree.iterator();
     while (it.hasNext()) {
-      Pair<List<Integer>,Long> p = it.next();
+      Pair<IntArrayList,Long> p = it.next();
       vtree.addPattern(p.getFirst(), p.getSecond());
       items += p.getFirst().size();
       count++;
