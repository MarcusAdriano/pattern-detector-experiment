diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/HighDFWordsPruner.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/HighDFWordsPruner.java
index a1ce3506..a0f4866e 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/HighDFWordsPruner.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/HighDFWordsPruner.java
@@ -43,12 +43,13 @@
 
   public static final String STD_CALC_DIR = "stdcalc";
   public static final String MAX_DF = "max.df";
+  public static final String MIN_DF = "min.df";
 
   private HighDFWordsPruner() {
   }
 
   public static void pruneVectors(Path tfDir, Path prunedTFDir, Path prunedPartialTFDir, long maxDF,
-                                  Configuration baseConf,
+                                  long minDF, Configuration baseConf,
                                   Pair<Long[], List<Path>> docFrequenciesFeatures,
                                   float normPower,
                                   boolean logNormalize,
@@ -59,7 +60,7 @@ public static void pruneVectors(Path tfDir, Path prunedTFDir, Path prunedPartial
     for (Path path : docFrequenciesFeatures.getSecond()) {
       Path partialVectorOutputPath = new Path(prunedPartialTFDir, "partial-" + partialVectorIndex++);
       partialVectorPaths.add(partialVectorOutputPath);
-      pruneVectorsPartial(tfDir, partialVectorOutputPath, path, maxDF, baseConf);
+      pruneVectorsPartial(tfDir, partialVectorOutputPath, path, maxDF, minDF, baseConf);
     }
 
     mergePartialVectors(partialVectorPaths, prunedTFDir, baseConf, normPower, logNormalize, numReducers);
@@ -67,7 +68,7 @@ public static void pruneVectors(Path tfDir, Path prunedTFDir, Path prunedPartial
   }
 
   private static void pruneVectorsPartial(Path input, Path output, Path dictionaryFilePath, long maxDF,
-                                          Configuration baseConf) throws IOException, InterruptedException,
+                                          long minDF, Configuration baseConf) throws IOException, InterruptedException,
           ClassNotFoundException {
 
     Configuration conf = new Configuration(baseConf);
@@ -77,6 +78,7 @@ private static void pruneVectorsPartial(Path input, Path output, Path dictionary
             "org.apache.hadoop.io.serializer.JavaSerialization,"
                     + "org.apache.hadoop.io.serializer.WritableSerialization");
     conf.setLong(MAX_DF, maxDF);
+    conf.setLong(MIN_DF, minDF);
     DistributedCache.setCacheFiles(
             new URI[]{dictionaryFilePath.toUri()}, conf);
 
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java
index 94e0da0e..1df4b185 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java
@@ -264,7 +264,7 @@ public int run(String[] args) throws Exception {
       if (cmdLine.hasOption(namedVectorOpt)) {
         namedVectors = true;
       }
-      boolean shouldPrune = maxDFSigma >= 0.0;
+      boolean shouldPrune = maxDFSigma >= 0.0  || maxDFPercent > 0.00;
       String tfDirName = shouldPrune
           ? DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER + "-toprune"
           : DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER;
@@ -308,13 +308,17 @@ public int run(String[] args) throws Exception {
 
       long maxDF = maxDFPercent; //if we are pruning by std dev, then this will get changed
       if (shouldPrune) {
+	long vectorCount = docFrequenciesFeatures.getFirst()[1];
+	if (maxDFSigma >= 0.0) {
         Path dfDir = new Path(outputDir, TFIDFConverter.WORDCOUNT_OUTPUT_FOLDER);
         Path stdCalcDir = new Path(outputDir, HighDFWordsPruner.STD_CALC_DIR);
 
         // Calculate the standard deviation
         double stdDev = BasicStats.stdDevForGivenMean(dfDir, stdCalcDir, 0.0, conf);
-        long vectorCount = docFrequenciesFeatures.getFirst()[1];
         maxDF = (int) (100.0 * maxDFSigma * stdDev / vectorCount);
+	}
+
+	long maxDFThreshold = (long) (vectorCount * ((float) maxDF / 100f));
 
         // Prune the term frequency vectors
         Path tfDir = new Path(outputDir, tfDirName);
@@ -326,7 +330,8 @@ public int run(String[] args) throws Exception {
           HighDFWordsPruner.pruneVectors(tfDir,
                                          prunedTFDir,
                                          prunedPartialTFDir,
-                                         maxDF,
+                                         maxDFThreshold,
+					 minDf,
                                          conf,
                                          docFrequenciesFeatures,
                                          -1.0f,
@@ -336,7 +341,8 @@ public int run(String[] args) throws Exception {
           HighDFWordsPruner.pruneVectors(tfDir,
                                          prunedTFDir,
                                          prunedPartialTFDir,
-                                         maxDF,
+                                         maxDFThreshold,
+					 minDf,
                                          conf,
                                          docFrequenciesFeatures,
                                          norm,
diff --git a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/pruner/WordsPrunerReducer.java b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/pruner/WordsPrunerReducer.java
index b52093d4..12102cab 100644
--- a/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/pruner/WordsPrunerReducer.java
+++ b/mahout/trunk/core/src/main/java/org/apache/mahout/vectorizer/pruner/WordsPrunerReducer.java
@@ -39,7 +39,8 @@
         Reducer<WritableComparable<?>, VectorWritable, WritableComparable<?>, VectorWritable> {
 
   private final OpenIntLongHashMap dictionary = new OpenIntLongHashMap();
-  private long maxDf = -1;
+  private long maxDf = Long.MAX_VALUE;
+  private long minDf = -1;
 
   @Override
   protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context context)
@@ -50,14 +51,14 @@ protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values
     }
     Vector value = it.next().get();
     Vector vector = value.clone();
-    if (maxDf > -1) {
+    if (maxDf != Long.MAX_VALUE || minDf > -1) {
       for (Vector.Element e : value.nonZeroes()) {
         if (!dictionary.containsKey(e.index())) {
           vector.setQuick(e.index(), 0.0);
           continue;
         }
         long df = dictionary.get(e.index());
-        if (df > maxDf) {
+        if (df > maxDf || df < minDf) {
           vector.setQuick(e.index(), 0.0);
         }
       }
@@ -75,7 +76,8 @@ protected void setup(Context context) throws IOException, InterruptedException {
     Preconditions.checkArgument(localFiles != null && localFiles.length >= 1,
             "missing paths from the DistributedCache");
 
-    maxDf = conf.getLong(HighDFWordsPruner.MAX_DF, -1);
+    maxDf = conf.getLong(HighDFWordsPruner.MAX_DF, Long.MAX_VALUE);
+    minDf = conf.getLong(HighDFWordsPruner.MIN_DF, -1);
 
     Path dictionaryFile = new Path(localFiles[0].getPath());
     // key is feature, value is the document frequency
