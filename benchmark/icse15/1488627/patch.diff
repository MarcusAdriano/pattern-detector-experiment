diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJobTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJobTest.java
index eeaf5776..2b5c0c73 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJobTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJobTest.java
@@ -724,7 +724,7 @@ public void testCompleteJob() throws Exception {
 
     RecommenderJob recommenderJob = new RecommenderJob();
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set("mapred.input.dir", inputFile.getAbsolutePath());
     conf.set("mapred.output.dir", outputDir.getAbsolutePath());
     conf.setBoolean("mapred.output.compress", false);
@@ -810,7 +810,7 @@ public void testCompleteJobBoolean() throws Exception {
 
     RecommenderJob recommenderJob = new RecommenderJob();
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set("mapred.input.dir", inputFile.getAbsolutePath());
     conf.set("mapred.output.dir", outputDir.getAbsolutePath());
     conf.setBoolean("mapred.output.compress", false);
@@ -871,7 +871,7 @@ public void testCompleteJobWithFiltering() throws Exception {
 
      RecommenderJob recommenderJob = new RecommenderJob();
 
-     Configuration conf = new Configuration();
+     Configuration conf = getConfiguration();
      conf.set("mapred.input.dir", inputFile.getAbsolutePath());
      conf.set("mapred.output.dir", outputDir.getAbsolutePath());
      conf.setBoolean("mapred.output.compress", false);
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJobTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJobTest.java
index 25f6fbf4..f61b5e64 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJobTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJobTest.java
@@ -127,16 +127,14 @@ public void testCompleteJob() throws Exception {
 
     ItemSimilarityJob similarityJob = new ItemSimilarityJob();
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set("mapred.input.dir", inputFile.getAbsolutePath());
     conf.set("mapred.output.dir", outputDir.getAbsolutePath());
     conf.setBoolean("mapred.output.compress", false);
 
     similarityJob.setConf(conf);
-
     similarityJob.run(new String[] { "--tempDir", tmpDir.getAbsolutePath(), "--similarityClassname",
        CosineSimilarity.class.getName() });
-
     File outPart = outputDir.listFiles(new FilenameFilter() {
       @Override
       public boolean accept(File dir, String name) {
@@ -223,16 +221,14 @@ public void testMaxSimilaritiesPerItem() throws Exception {
 
     ItemSimilarityJob similarityJob =  new ItemSimilarityJob();
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set("mapred.input.dir", inputFile.getAbsolutePath());
     conf.set("mapred.output.dir", outputDir.getAbsolutePath());
     conf.setBoolean("mapred.output.compress", false);
 
     similarityJob.setConf(conf);
-
     similarityJob.run(new String[] { "--tempDir", tmpDir.getAbsolutePath(), "--similarityClassname",
         TanimotoCoefficientSimilarity.class.getName(), "--maxSimilaritiesPerItem", "1" });
-
     File outPart = outputDir.listFiles(new FilenameFilter() {
       @Override
       public boolean accept(File dir, String name) {
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/classifier/naivebayes/NaiveBayesTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/classifier/naivebayes/NaiveBayesTest.java
index 7b52100c..af052883 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/classifier/naivebayes/NaiveBayesTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/classifier/naivebayes/NaiveBayesTest.java
@@ -58,7 +58,7 @@
   public void setUp() throws Exception {
     super.setUp();
 
-    conf = new Configuration();
+    conf = getConfiguration();
 
     inputFile = getTestTempFile("trainingInstances.seq");
     outputDir = getTestTempDir("output");
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java
index 37d0ce6b..91851da9 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java
@@ -106,7 +106,7 @@ private static void printCanopies(Iterable<Canopy> canopies) {
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    fs = FileSystem.get(new Configuration());
+    fs = FileSystem.get(getConfiguration());
     referenceManhattan = CanopyClusterer.createCanopies(getPoints(),
         manhattanDistanceMeasure, 3.1, 2.1);
     manhattanCentroids = CanopyClusterer.getCenters(referenceManhattan);
@@ -173,7 +173,7 @@ public void testReferenceEuclidean() throws Exception {
   @Test
   public void testCanopyMapperManhattan() throws Exception {
     CanopyMapper mapper = new CanopyMapper();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, manhattanDistanceMeasure
         .getClass().getName());
     conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));
@@ -209,7 +209,7 @@ public void testCanopyMapperManhattan() throws Exception {
   @Test
   public void testCanopyMapperEuclidean() throws Exception {
     CanopyMapper mapper = new CanopyMapper();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, euclideanDistanceMeasure
         .getClass().getName());
     conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));
@@ -245,7 +245,7 @@ public void testCanopyMapperEuclidean() throws Exception {
   @Test
   public void testCanopyReducerManhattan() throws Exception {
     CanopyReducer reducer = new CanopyReducer();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY,
         "org.apache.mahout.common.distance.ManhattanDistanceMeasure");
     conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));
@@ -281,7 +281,7 @@ public void testCanopyReducerManhattan() throws Exception {
   @Test
   public void testCanopyReducerEuclidean() throws Exception {
     CanopyReducer reducer = new CanopyReducer();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY,
         "org.apache.mahout.common.distance.EuclideanDistanceMeasure");
     conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));
@@ -316,7 +316,7 @@ public void testCanopyReducerEuclidean() throws Exception {
   @Test
   public void testCanopyGenManhattanMR() throws Exception {
     List<VectorWritable> points = getPointsWritable();
-    Configuration config = new Configuration();
+    Configuration config = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points,
         getTestTempFilePath("testdata/file1"), fs, config);
     ClusteringTestUtils.writePointsToFile(points,
@@ -373,7 +373,7 @@ static boolean findAndRemove(Pair<Double, Double> target,
   @Test
   public void testCanopyGenEuclideanMR() throws Exception {
     List<VectorWritable> points = getPointsWritable();
-    Configuration config = new Configuration();
+    Configuration config = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points,
         getTestTempFilePath("testdata/file1"), fs, config);
     ClusteringTestUtils.writePointsToFile(points,
@@ -414,7 +414,7 @@ public void testCanopyGenEuclideanMR() throws Exception {
   @Test
   public void testClusteringManhattanSeq() throws Exception {
     List<VectorWritable> points = getPointsWritable();
-    Configuration config = new Configuration();
+    Configuration config = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points,
         getTestTempFilePath("testdata/file1"), fs, config);
     // now run the Canopy Driver in sequential mode
@@ -441,7 +441,7 @@ public void testClusteringManhattanSeq() throws Exception {
   @Test
   public void testClusteringEuclideanSeq() throws Exception {
     List<VectorWritable> points = getPointsWritable();
-    Configuration config = new Configuration();
+    Configuration config = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points,
         getTestTempFilePath("testdata/file1"), fs, config);
     // now run the Canopy Driver in sequential mode
@@ -457,7 +457,7 @@ public void testClusteringEuclideanSeq() throws Exception {
         optKey(DefaultOptionCreator.OVERWRITE_OPTION),
         optKey(DefaultOptionCreator.METHOD_OPTION),
         DefaultOptionCreator.SEQUENTIAL_METHOD };
-    new CanopyDriver().run(args);
+    ToolRunner.run(config, new CanopyDriver(), args);
 
     // verify output from sequence file
     Path path = new Path(output, "clusters-0-final/part-r-00000");
@@ -479,7 +479,7 @@ public void testClusteringEuclideanSeq() throws Exception {
   @Test
   public void testClusteringEuclideanWithOutlierRemovalSeq() throws Exception {
     List<VectorWritable> points = getPointsWritable();
-    Configuration config = new Configuration();
+    Configuration config = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points,
         getTestTempFilePath("testdata/file1"), fs, config);
     // now run the Canopy Driver in sequential mode
@@ -496,7 +496,7 @@ public void testClusteringEuclideanWithOutlierRemovalSeq() throws Exception {
         optKey(DefaultOptionCreator.OVERWRITE_OPTION),
         optKey(DefaultOptionCreator.METHOD_OPTION),
         DefaultOptionCreator.SEQUENTIAL_METHOD };
-    new CanopyDriver().run(args);
+    ToolRunner.run(config, new CanopyDriver(), args);
 
     // verify output from sequence file
     Path path = new Path(output, "clusters-0-final/part-r-00000");
@@ -523,7 +523,7 @@ public void testClusteringEuclideanWithOutlierRemovalSeq() throws Exception {
   @Test
   public void testClusteringManhattanMR() throws Exception {
     List<VectorWritable> points = getPointsWritable();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, 
         getTestTempFilePath("testdata/file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(points, true, 
@@ -544,7 +544,7 @@ public void testClusteringManhattanMR() throws Exception {
   @Test
   public void testClusteringEuclideanMR() throws Exception {
     List<VectorWritable> points = getPointsWritable();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, 
         getTestTempFilePath("testdata/file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(points, true, 
@@ -560,7 +560,7 @@ public void testClusteringEuclideanMR() throws Exception {
         optKey(DefaultOptionCreator.T2_OPTION), "2.1",
         optKey(DefaultOptionCreator.CLUSTERING_OPTION),
         optKey(DefaultOptionCreator.OVERWRITE_OPTION) };
-    ToolRunner.run(new Configuration(), new CanopyDriver(), args);
+    ToolRunner.run(getConfiguration(), new CanopyDriver(), args);
     Path path = new Path(output, "clusteredPoints/part-m-00000");
     long count = HadoopUtil.countRecords(path, conf);
     assertEquals("number of points", points.size(), count);
@@ -573,7 +573,7 @@ public void testClusteringEuclideanMR() throws Exception {
   @Test
   public void testClusteringEuclideanWithOutlierRemovalMR() throws Exception {
     List<VectorWritable> points = getPointsWritable();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, 
         getTestTempFilePath("testdata/file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(points, true, 
@@ -590,7 +590,7 @@ public void testClusteringEuclideanWithOutlierRemovalMR() throws Exception {
         optKey(DefaultOptionCreator.OUTLIER_THRESHOLD), "0.7",
         optKey(DefaultOptionCreator.CLUSTERING_OPTION),
         optKey(DefaultOptionCreator.OVERWRITE_OPTION) };
-    ToolRunner.run(new Configuration(), new CanopyDriver(), args);
+    ToolRunner.run(getConfiguration(), new CanopyDriver(), args);
     Path path = new Path(output, "clusteredPoints/part-m-00000");
     long count = HadoopUtil.countRecords(path, conf);
     int expectedPointsAfterOutlierRemoval = 8;
@@ -605,7 +605,7 @@ public void testClusteringEuclideanWithOutlierRemovalMR() throws Exception {
   @Test
   public void testCanopyReducerT3T4Configuration() throws Exception {
     CanopyReducer reducer = new CanopyReducer();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY,
         "org.apache.mahout.common.distance.ManhattanDistanceMeasure");
     conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));
@@ -628,7 +628,7 @@ public void testCanopyReducerT3T4Configuration() throws Exception {
   @Test
   public void testCanopyMapperClusterFilter() throws Exception {
     CanopyMapper mapper = new CanopyMapper();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, manhattanDistanceMeasure
         .getClass().getName());
     conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));
@@ -658,7 +658,7 @@ public void testCanopyMapperClusterFilter() throws Exception {
   @Test
   public void testCanopyReducerClusterFilter() throws Exception {
     CanopyReducer reducer = new CanopyReducer();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY,
         "org.apache.mahout.common.distance.ManhattanDistanceMeasure");
     conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/classify/ClusterClassificationDriverTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/classify/ClusterClassificationDriverTest.java
index d345ab63..f88e02ea 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/classify/ClusterClassificationDriverTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/classify/ClusterClassificationDriverTest.java
@@ -30,6 +30,7 @@
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.clustering.ClusteringTestUtils;
 import org.apache.mahout.clustering.canopy.CanopyDriver;
 import org.apache.mahout.clustering.iterator.CanopyClusteringPolicy;
@@ -65,7 +66,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     fs = FileSystem.get(conf);
     firstCluster = Lists.newArrayList();
     secondCluster = Lists.newArrayList();
@@ -92,7 +93,7 @@ public void testVectorClassificationWithOutlierRemovalMR() throws Exception {
     classifiedOutputPath = getTestTempDirPath("classifiedClusters");
     HadoopUtil.delete(conf, classifiedOutputPath);
 
-    conf = new Configuration();
+    conf = getConfiguration();
 
     ClusteringTestUtils.writePointsToFile(points, true,
         new Path(pointsPath, "file1"), fs, conf);
@@ -110,7 +111,7 @@ public void testVectorClassificationWithoutOutlierRemoval() throws Exception {
     clusteringOutputPath = getTestTempDirPath("output");
     classifiedOutputPath = getTestTempDirPath("classify");
 
-    conf = new Configuration();
+    conf = getConfiguration();
 
     ClusteringTestUtils.writePointsToFile(points,
         new Path(pointsPath, "file1"), fs, conf);
@@ -150,12 +151,12 @@ private void runClustering(Path pointsPath, Configuration conf,
 
   private void runClassificationWithoutOutlierRemoval()
     throws IOException, InterruptedException, ClassNotFoundException {
-    ClusterClassificationDriver.run(pointsPath, clusteringOutputPath, classifiedOutputPath, 0.0, true, true);
+    ClusterClassificationDriver.run(getConfiguration(), pointsPath, clusteringOutputPath, classifiedOutputPath, 0.0, true, true);
   }
 
   private void runClassificationWithOutlierRemoval(boolean runSequential)
     throws IOException, InterruptedException, ClassNotFoundException {
-    ClusterClassificationDriver.run(pointsPath, clusteringOutputPath, classifiedOutputPath, 0.73, true, runSequential);
+    ClusterClassificationDriver.run(getConfiguration(), pointsPath, clusteringOutputPath, classifiedOutputPath, 0.73, true, runSequential);
   }
 
   private void collectVectorsForAssertion() throws IOException {
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDirichletClustering.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDirichletClustering.java
index b291172e..8f1b6600 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDirichletClustering.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDirichletClustering.java
@@ -81,8 +81,8 @@ private void generateSamples(int num, double mx, double my, double sd) {
   @Test
   public void testDirichletClusteringSeq() throws Exception {
     Path output = getTestTempDirPath("output");
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.get(new Configuration());
+    Configuration conf = getConfiguration();
+    FileSystem fs = FileSystem.get(getConfiguration());
     
     generateSamples(40, 1, 1, 3);
     generateSamples(30, 1, 0, 0.1);
@@ -109,8 +109,8 @@ public void testDirichletClusteringSeq() throws Exception {
   @Test
   public void testDirichletClusteringMR() throws Exception {
     Path output = getTestTempDirPath("output");
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.get(new Configuration());
+    Configuration conf = getConfiguration();
+    FileSystem fs = FileSystem.get(getConfiguration());
     
     generateSamples(40, 1, 1, 3);
     generateSamples(30, 1, 0, 0.1);
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestMapReduce.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestMapReduce.java
index 5cba101b..7d64b8fe 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestMapReduce.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestMapReduce.java
@@ -107,7 +107,7 @@ private void generateAsymmetricSamples(int num, double mx, double my, double sdx
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    conf = new Configuration();
+    conf = getConfiguration(); 
     fs = FileSystem.get(conf);
   }
   
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/fuzzykmeans/TestFuzzyKmeansClustering.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/fuzzykmeans/TestFuzzyKmeansClustering.java
index a73a8077..361b1aa0 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/fuzzykmeans/TestFuzzyKmeansClustering.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/fuzzykmeans/TestFuzzyKmeansClustering.java
@@ -48,7 +48,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     fs = FileSystem.get(conf);
   }
 
@@ -62,7 +62,7 @@ public void testFuzzyKMeansSeqJob() throws Exception {
 
     Path pointsPath = getTestTempDirPath("points");
     Path clustersPath = getTestTempDirPath("clusters");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);
 
     for (int k = 0; k < points.size(); k++) {
@@ -133,7 +133,7 @@ public void testFuzzyKMeansMRJob() throws Exception {
 
     Path pointsPath = getTestTempDirPath("points");
     Path clustersPath = getTestTempDirPath("clusters");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);
 
     for (int k = 0; k < points.size(); k++) {
@@ -192,7 +192,7 @@ public void testFuzzyKMeansMRJob() throws Exception {
           optKey(DefaultOptionCreator.EMIT_MOST_LIKELY_OPTION),
           optKey(DefaultOptionCreator.OVERWRITE_OPTION)
       };
-      ToolRunner.run(new Configuration(), new FuzzyKMeansDriver(), args);
+      ToolRunner.run(getConfiguration(), new FuzzyKMeansDriver(), args);
       long count = HadoopUtil.countRecords(new Path(output, "clusteredPoints/part-m-00000"), conf);
       assertTrue(count > 0);
     }
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/iterator/TestClusterClassifier.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/iterator/TestClusterClassifier.java
index c802b1ee..6e987c7e 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/iterator/TestClusterClassifier.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/iterator/TestClusterClassifier.java
@@ -93,7 +93,7 @@ private ClusterClassifier writeAndRead(ClusterClassifier classifier) throws IOEx
     Path path = new Path(getTestTempDirPath(), "output");
     classifier.writeToSeqFiles(path);
     ClusterClassifier newClassifier = new ClusterClassifier();
-    newClassifier.readFromSeqFiles(new Configuration(), path);
+    newClassifier.readFromSeqFiles(getConfiguration(), path);
     return newClassifier;
   }
   
@@ -221,7 +221,7 @@ public void testSeqFileClusterIteratorKMeans() throws IOException {
     Path pointsPath = getTestTempDirPath("points");
     Path priorPath = getTestTempDirPath("prior");
     Path outPath = getTestTempDirPath("output");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(pointsPath.toUri(), conf);
     List<VectorWritable> points = TestKmeansClustering.getPointsWritable(TestKmeansClustering.REFERENCE);
     ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);
@@ -253,7 +253,7 @@ public void testMRFileClusterIteratorKMeans() throws Exception {
     Path pointsPath = getTestTempDirPath("points");
     Path priorPath = getTestTempDirPath("prior");
     Path outPath = getTestTempDirPath("output");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(pointsPath.toUri(), conf);
     List<VectorWritable> points = TestKmeansClustering.getPointsWritable(TestKmeansClustering.REFERENCE);
     ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/kmeans/TestKmeansClustering.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/kmeans/TestKmeansClustering.java
index bb6f5eda..ac0c3020 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/kmeans/TestKmeansClustering.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/kmeans/TestKmeansClustering.java
@@ -62,7 +62,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     fs = FileSystem.get(conf);
   }
   
@@ -127,7 +127,7 @@ public void testKMeansSeqJob() throws Exception {
     
     Path pointsPath = getTestTempDirPath("points");
     Path clustersPath = getTestTempDirPath("clusters");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file2"), fs, conf);
     for (int k = 1; k < points.size(); k++) {
@@ -160,7 +160,7 @@ public void testKMeansSeqJob() throws Exception {
           optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "2", optKey(DefaultOptionCreator.CLUSTERING_OPTION),
           optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.METHOD_OPTION),
           DefaultOptionCreator.SEQUENTIAL_METHOD};
-      new KMeansDriver().run(args);
+      ToolRunner.run(conf, new KMeansDriver(), args);
       
       // now compare the expected clusters with actual
       Path clusteredPointsPath = new Path(outputPath, "clusteredPoints");
@@ -183,7 +183,7 @@ public void testKMeansMRJob() throws Exception {
     
     Path pointsPath = getTestTempDirPath("points");
     Path clustersPath = getTestTempDirPath("clusters");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file2"), fs, conf);
     for (int k = 1; k < points.size(); k += 3) {
@@ -216,7 +216,7 @@ public void testKMeansMRJob() throws Exception {
           optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.001",
           optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "2", optKey(DefaultOptionCreator.CLUSTERING_OPTION),
           optKey(DefaultOptionCreator.OVERWRITE_OPTION)};
-      ToolRunner.run(new Configuration(), new KMeansDriver(), args);
+      ToolRunner.run(getConfiguration(), new KMeansDriver(), args);
       
       // now compare the expected clusters with actual
       Path clusteredPointsPath = new Path(outputPath, "clusteredPoints");
@@ -241,7 +241,7 @@ public void testKMeansWithCanopyClusterInput() throws Exception {
     List<VectorWritable> points = getPointsWritable(REFERENCE);
     
     Path pointsPath = getTestTempDirPath("points");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file2"), fs, conf);
     
@@ -288,7 +288,7 @@ public void testKMeansWithCanopyClusterInput() throws Exception {
 
     // now run the KMeans job
     Path kmeansOutput = new Path(outputPath, "kmeans");
-	KMeansDriver.run(pointsPath, new Path(outputPath, "clusters-0-final"), kmeansOutput, new EuclideanDistanceMeasure(),
+	KMeansDriver.run(getConfiguration(), pointsPath, new Path(outputPath, "clusters-0-final"), kmeansOutput, new EuclideanDistanceMeasure(),
         0.001, 10, true, 0.0, false);
     
     // now compare the expected clusters with actual
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/lda/cvb/TestCVBModelTrainer.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/lda/cvb/TestCVBModelTrainer.java
index c629259b..58c62fd4 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/lda/cvb/TestCVBModelTrainer.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/lda/cvb/TestCVBModelTrainer.java
@@ -95,14 +95,15 @@ public double apply(double d) {
                                                              numDocs, numSamples, numTopicsPerDoc);
 
     Path sampleCorpusPath = getTestTempDirPath("corpus");
-    MatrixUtils.write(sampleCorpusPath, new Configuration(), sampledCorpus);
+    Configuration configuration = getConfiguration();
+    MatrixUtils.write(sampleCorpusPath, configuration, sampledCorpus);
     int numIterations = 5;
     List<Double> perplexities = Lists.newArrayList();
     int startTopic = numGeneratingTopics - 1;
     int numTestTopics = startTopic;
     while (numTestTopics < numGeneratingTopics + 2) {
       Path topicModelStateTempPath = getTestTempDirPath("topicTemp" + numTestTopics);
-      Configuration conf = new Configuration();
+      Configuration conf = getConfiguration();
       CVB0Driver.run(conf, sampleCorpusPath, null, numTestTopics, numTerms,
                      ALPHA, ETA, numIterations, 1, 0, null, null, topicModelStateTempPath, 1234, 0.2f, 2,
                      1, 3, 1, false);
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/meanshift/TestMeanShift.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/meanshift/TestMeanShift.java
index 4c8e82f7..e86f3dba 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/meanshift/TestMeanShift.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/meanshift/TestMeanShift.java
@@ -193,7 +193,7 @@ public void testCanopyMapperEuclidean() throws Exception {
           euclideanDistanceMeasure), refCanopies);
     }
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(MeanShiftCanopyConfigKeys.DISTANCE_MEASURE_KEY, EuclideanDistanceMeasure.class.getName());
     conf.set(MeanShiftCanopyConfigKeys.KERNEL_PROFILE_KEY, TriangularKernelProfile.class.getName());
     conf.set(MeanShiftCanopyConfigKeys.T1_KEY, "4");
@@ -278,7 +278,7 @@ public void testCanopyReducerEuclidean() throws Exception {
       clusterer.shiftToMean(canopy);
     }
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(MeanShiftCanopyConfigKeys.DISTANCE_MEASURE_KEY, EuclideanDistanceMeasure.class.getName());
     conf.set(MeanShiftCanopyConfigKeys.KERNEL_PROFILE_KEY, TriangularKernelProfile.class.getName());
     conf.set(MeanShiftCanopyConfigKeys.T1_KEY, "4");
@@ -353,7 +353,7 @@ public void testCanopyReducerEuclidean() throws Exception {
   @Test
   public void testCanopyEuclideanMRJob() throws Exception {
     Path input = getTestTempDirPath("testdata");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(input.toUri(), conf);
     Collection<VectorWritable> points = Lists.newArrayList();
     // TODO fix test so it doesn't need this random seed!
@@ -414,7 +414,7 @@ public void testCanopyEuclideanMRJob() throws Exception {
   @Test
   public void testCanopyEuclideanSeqJob() throws Exception {
     Path input = getTestTempDirPath("testdata");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(input.toUri(), conf);
     Collection<VectorWritable> points = Lists.newArrayList();
     for (Vector v : raw) {
@@ -445,7 +445,7 @@ public void testCanopyEuclideanSeqJob() throws Exception {
         optKey(DefaultOptionCreator.OVERWRITE_OPTION),
         optKey(DefaultOptionCreator.METHOD_OPTION),
         DefaultOptionCreator.SEQUENTIAL_METHOD };
-    ToolRunner.run(new Configuration(), new MeanShiftCanopyDriver(), args);
+    ToolRunner.run(getConfiguration(), new MeanShiftCanopyDriver(), args);
     Path outPart = new Path(output, "clusters-7-final/part-r-00000");
     long count = HadoopUtil.countRecords(outPart, conf);
     assertEquals("count", 3, count);
@@ -458,7 +458,7 @@ public void testCanopyEuclideanSeqJob() throws Exception {
   @Test
   public void testCanopyEuclideanMRJobNoClustering() throws Exception {
     Path input = getTestTempDirPath("testdata");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(input.toUri(), conf);
     Collection<VectorWritable> points = Lists.newArrayList();
     for (Vector v : raw) {
@@ -506,7 +506,7 @@ public void testCanopyEuclideanMRJobNoClustering() throws Exception {
   @Test
   public void testCanopyEuclideanSeqJobNoClustering() throws Exception {
     Path input = getTestTempDirPath("testdata");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(input.toUri(), conf);
     Collection<VectorWritable> points = Lists.newArrayList();
     for (Vector v : raw) {
@@ -536,7 +536,7 @@ public void testCanopyEuclideanSeqJobNoClustering() throws Exception {
         optKey(DefaultOptionCreator.OVERWRITE_OPTION),
         optKey(DefaultOptionCreator.METHOD_OPTION),
         DefaultOptionCreator.SEQUENTIAL_METHOD };
-    ToolRunner.run(new Configuration(), new MeanShiftCanopyDriver(), args);
+    ToolRunner.run(getConfiguration(), new MeanShiftCanopyDriver(), args);
     Path outPart = new Path(output, "clusters-7-final/part-r-00000");
     long count = HadoopUtil.countRecords(outPart, conf);
     assertEquals("count", 3, count);
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/minhash/TestMinHashClustering.java b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/minhash/TestMinHashClustering.java
index b349bcc2..e2cef8c1 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/minhash/TestMinHashClustering.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/clustering/minhash/TestMinHashClustering.java
@@ -36,6 +36,7 @@
 import org.apache.mahout.math.VectorWritable;
 import org.junit.Test;
 
+import java.io.IOException;
 import java.util.Collection;
 import java.util.List;
 import java.util.Set;
@@ -63,7 +64,7 @@
   @Override
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     List<VectorWritable> points = getPointsWritable(REFERENCE);
     input = getTestTempDirPath("points");
     output = new Path(getTestTempDirPath(), "output");
@@ -124,8 +125,8 @@ private static void runPairwiseSimilarity(List<Vector> clusteredItems, double si
     }
   }
   
-  private static void verify(Path output, double simThreshold, String msg) {
-    Configuration conf = new Configuration();
+  private void verify(Path output, double simThreshold, String msg) throws IOException {
+    Configuration conf = getConfiguration();
     Path outputFile = new Path(output, "part-r-00000");
     List<Vector> clusteredItems = Lists.newArrayList();
     String prevClusterId = "";
@@ -147,7 +148,7 @@ private static void verify(Path output, double simThreshold, String msg) {
   @Test
   public void testLinearMinHashMRJob() throws Exception {
     String[] args = makeArguments(2, 3, 20, 3, HashType.LINEAR.toString());
-    int ret = ToolRunner.run(new Configuration(), new MinHashDriver(), args);
+    int ret = ToolRunner.run(getConfiguration(), new MinHashDriver(), args);
     assertEquals("Minhash MR Job failed for " + HashType.LINEAR, 0, ret);
     verify(output, 0.2, "Hash Type: LINEAR");
   }
@@ -155,7 +156,7 @@ public void testLinearMinHashMRJob() throws Exception {
   @Test
   public void testPolynomialMinHashMRJob() throws Exception {
     String[] args = makeArguments(2, 3, 20, 3, HashType.POLYNOMIAL.toString());
-    int ret = ToolRunner.run(new Configuration(), new MinHashDriver(), args);
+    int ret = ToolRunner.run(getConfiguration(), new MinHashDriver(), args);
     assertEquals("Minhash MR Job failed for " + HashType.POLYNOMIAL, 0, ret);
     verify(output, 0.27, "Hash Type: POLYNOMIAL");
   }
@@ -163,7 +164,7 @@ public void testPolynomialMinHashMRJob() throws Exception {
   @Test
   public void testMurmurMinHashMRJob() throws Exception {
     String[] args = makeArguments(2, 3, 20, 4, HashType.MURMUR.toString());
-    int ret = ToolRunner.run(new Configuration(), new MinHashDriver(), args);
+    int ret = ToolRunner.run(getConfiguration(), new MinHashDriver(), args);
     assertEquals("Minhash MR Job failed for " + HashType.MURMUR, 0, ret);
     verify(output, 0.2, "Hash Type: MURMUR");
   }
@@ -171,7 +172,7 @@ public void testMurmurMinHashMRJob() throws Exception {
   @Test
   public void testMurmur3MinHashMRJob() throws Exception {
     String[] args = makeArguments(2, 3, 20, 4, HashType.MURMUR3.toString());
-    int ret = ToolRunner.run(new Configuration(), new MinHashDriver(), args);
+    int ret = ToolRunner.run(getConfiguration(), new MinHashDriver(), args);
     assertEquals("Minhash MR Job failed for " + HashType.MURMUR3, 0, ret);
     verify(output, 0.2, "Hash Type: MURMUR");
   }
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/common/MahoutTestCase.java b/mahout/trunk/core/src/test/java/org/apache/mahout/common/MahoutTestCase.java
index 7a9fa24b..06b3e49d 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/common/MahoutTestCase.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/common/MahoutTestCase.java
@@ -64,6 +64,12 @@ public void tearDown() throws Exception {
     super.tearDown();
   }
 
+  protected final Configuration getConfiguration() throws IOException {
+	Configuration conf = new Configuration();
+    conf.set("hadoop.tmp.dir", getTestTempDir("hadoop" + Math.random()).getAbsolutePath());
+    return conf;
+  }
+
   protected final Path getTestTempDirPath() throws IOException {
     if (testTempDirPath == null) {
       fs = FileSystem.get(new Configuration());
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java
index f8f5eebc..c93402a8 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java
@@ -57,7 +57,7 @@ public void testMaxHeapFPGrowth() throws Exception {
     transactions.add(new Pair<List<String>,Long>(Arrays.asList("B", "C"), 1L));
 
     Path path = getTestTempFilePath("fpgrowthTest.dat");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(path.toUri(), conf);
 
     SequenceFile.Writer writer =
@@ -100,7 +100,7 @@ public void testMaxHeapFPGrowthData1() throws Exception {
     transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y"), 10L));
 
     Path path = getTestTempFilePath("fpgrowthTestData1.dat");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(path.toUri(), conf);
     System.out.println(fp.generateFList(transactions.iterator(), 2));
     SequenceFile.Writer writer =
@@ -138,7 +138,7 @@ public void testMaxHeapFPGrowthData2() throws Exception {
     transactions.add(new Pair<List<String>,Long>(Arrays.asList("X", "Y", "Z"), 11L));
 
     Path path = getTestTempFilePath("fpgrowthTestData2.dat");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(path.toUri(), conf);
     System.out.println(fp.generateFList(transactions.iterator(), 2));
     SequenceFile.Writer writer =
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
index 800e45fe..75fe6123 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
@@ -62,6 +62,7 @@ public void setUp() throws Exception {
     File input = new File(inputDir, "test.txt");
     params.set(PFPGrowth.INPUT, input.getAbsolutePath());
     params.set(PFPGrowth.OUTPUT, outputDir.getAbsolutePath());
+
     Writer writer = Files.newWriter(input, Charsets.UTF_8);
     try {
       StringRecordIterator it = new StringRecordIterator(new FileLineIterable(Resources.getResource(
@@ -104,7 +105,7 @@ public void testRetailDataMinSup100() throws Exception {
       expectedResults.put(Sets.newHashSet(items), support);
     }
 
-    PFPGrowth.runPFPGrowth(params);
+    PFPGrowth.runPFPGrowth(params, getConfiguration());
 
     List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
   
@@ -156,7 +157,7 @@ public void testRetailDataMinSup100InSteps() throws Exception {
       Long support = Long.parseLong(supportString.substring(1, supportString.length() - 1));
       expectedResults.put(Sets.newHashSet(items), support);
     }
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
     PFPGrowth.startParallelCounting(params, conf);
 
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
index c7a6d33d..9f0e1d65 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
@@ -85,7 +85,7 @@ public void setUp() throws Exception {
    */ 
   @Test
   public void testStartParallelFPGrowth() throws Exception {
-    PFPGrowth.runPFPGrowth(params);
+    PFPGrowth.runPFPGrowth(params, getConfiguration());
 
     List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
 
@@ -102,7 +102,7 @@ public void testStartParallelFPGrowth() throws Exception {
    */ 
   @Test
   public void testStartParallelFPGrowthInSteps() throws Exception {
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
     PFPGrowth.startParallelCounting(params, conf);
     log.info("Reading fList Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/TestDistributedRowMatrix.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/TestDistributedRowMatrix.java
index 718807e6..31b7e0ca 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/TestDistributedRowMatrix.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/TestDistributedRowMatrix.java
@@ -75,13 +75,15 @@ private static void assertEquals(VectorIterable m, VectorIterable mtt, double er
   @Test
   public void testTranspose() throws Exception {
     DistributedRowMatrix m = randomDistributedMatrix(10, 9, 5, 4, 1.0, false);
+    m.setConf(getConfiguration());
     DistributedRowMatrix mt = m.transpose();
+    mt.setConf(getConfiguration());
 
     Path tmpPath = getTestTempDirPath();
     m.setOutputTempPathString(tmpPath.toString());
     Path tmpOutPath = new Path(tmpPath, "/tmpOutTranspose");
     mt.setOutputTempPathString(tmpOutPath.toString());
-    HadoopUtil.delete(new Configuration(), tmpOutPath);
+    HadoopUtil.delete(getConfiguration(), tmpOutPath);
     DistributedRowMatrix mtt = mt.transpose();
     assertEquals(m, mtt, EPSILON);
   }
@@ -92,6 +94,7 @@ public void testMatrixColumnMeansJob() throws Exception {
         SolverTest.randomSequentialAccessSparseMatrix(100, 90, 50, 20, 1.0);
     DistributedRowMatrix dm =
         randomDistributedMatrix(100, 90, 50, 20, 1.0, false);
+    dm.setConf(getConfiguration());
 
     Vector expected = new DenseVector(50);
     for (int i = 0; i < m.numRows(); i++) {
@@ -108,6 +111,7 @@ public void testNullMatrixColumnMeansJob() throws Exception {
         SolverTest.randomSequentialAccessSparseMatrix(100, 90, 0, 0, 1.0);
     DistributedRowMatrix dm =
         randomDistributedMatrix(100, 90, 0, 0, 1.0, false);
+    dm.setConf(getConfiguration());
 
     Vector expected = new DenseVector(0);
     for (int i = 0; i < m.numRows(); i++) {
@@ -124,6 +128,7 @@ public void testMatrixTimesVector() throws Exception {
     v.assign(1.0);
     Matrix m = SolverTest.randomSequentialAccessSparseMatrix(100, 90, 50, 20, 1.0);
     DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 50, 20, 1.0, false);
+    dm.setConf(getConfiguration());
 
     Vector expected = m.times(v);
     Vector actual = dm.times(v);
@@ -136,6 +141,7 @@ public void testMatrixTimesSquaredVector() throws Exception {
     v.assign(1.0);
     Matrix m = SolverTest.randomSequentialAccessSparseMatrix(100, 90, 50, 20, 1.0);
     DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 50, 20, 1.0, false);
+    dm.setConf(getConfiguration());
 
     Vector expected = m.timesSquared(v);
     Vector actual = dm.timesSquared(v);
@@ -149,7 +155,9 @@ public void testMatrixTimesMatrix() throws Exception {
     Matrix expected = inputA.transpose().times(inputB);
 
     DistributedRowMatrix distA = randomDistributedMatrix(20, 19, 15, 5, 10.0, false, "distA");
+    distA.setConf(getConfiguration());
     DistributedRowMatrix distB = randomDistributedMatrix(20, 13, 25, 10, 5.0, false, "distB");
+    distB.setConf(getConfiguration());
     DistributedRowMatrix product = distA.times(distB);
 
     assertEquals(expected, product, EPSILON);
@@ -247,10 +255,11 @@ public void testTransposeJobConfBuilder() throws Exception {
 
   @Test
   public void testTimesVectorTempDirDeletion() throws Exception {
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     Vector v = new RandomAccessSparseVector(50);
     v.assign(1.0);
     DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 50, 20, 1.0, false);
+    dm.setConf(conf);
 
     Path outputPath = dm.getOutputTempPath();
     FileSystem fs = outputPath.getFileSystem(conf);
@@ -284,10 +293,11 @@ public void testTimesVectorTempDirDeletion() throws Exception {
 
   @Test
   public void testTimesSquaredVectorTempDirDeletion() throws Exception {
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     Vector v = new RandomAccessSparseVector(50);
     v.assign(1.0);
     DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 50, 20, 1.0, false);
+    dm.setConf(getConfiguration());
 
     Path outputPath = dm.getOutputTempPath();
     FileSystem fs = outputPath.getFileSystem(conf);
@@ -319,8 +329,8 @@ public void testTimesSquaredVectorTempDirDeletion() throws Exception {
     assertEquals(0.0, result1.getDistanceSquared(result2), EPSILON);
   }
 
-  public static Configuration createInitialConf() {
-    Configuration initialConf = new Configuration();
+  public Configuration createInitialConf() throws IOException {
+    Configuration initialConf = getConfiguration();
     initialConf.set(TEST_PROPERTY_KEY, TEST_PROPERTY_VALUE);
     return initialConf;
   }
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/decomposer/TestDistributedLanczosSolver.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/decomposer/TestDistributedLanczosSolver.java
index d9ddb2f6..e6bbb32d 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/decomposer/TestDistributedLanczosSolver.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/decomposer/TestDistributedLanczosSolver.java
@@ -67,7 +67,7 @@ private LanczosState doTestDistributedLanczosSolver(boolean symmetric,
       int desiredRank, boolean hdfsBackedState)
       throws IOException {
     DistributedRowMatrix corpus = getCorpus(symmetric);
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     corpus.setConf(conf);
     DistributedLanczosSolver solver = new DistributedLanczosSolver();
     Vector intitialVector = DistributedLanczosSolver.getInitialVector(corpus);
@@ -92,7 +92,7 @@ private LanczosState doTestDistributedLanczosSolver(boolean symmetric,
 
   public void doTestResumeIteration(boolean symmetric) throws IOException {
     DistributedRowMatrix corpus = getCorpus(symmetric);
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     corpus.setConf(conf);
     DistributedLanczosSolver solver = new DistributedLanczosSolver();
     int rank = 10;
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/decomposer/TestDistributedLanczosSolverCLI.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/decomposer/TestDistributedLanczosSolverCLI.java
index 6f524fd1..e4badb74 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/decomposer/TestDistributedLanczosSolverCLI.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/decomposer/TestDistributedLanczosSolverCLI.java
@@ -20,6 +20,7 @@
 import com.google.common.collect.Lists;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable;
 import org.apache.mahout.math.DenseMatrix;
@@ -45,7 +46,7 @@ public void testDistributedLanczosSolverCLI() throws Exception {
     DistributedRowMatrix corpus =
         new TestDistributedRowMatrix().randomDenseHierarchicalDistributedMatrix(10, 9, false,
             testData.toString());
-    corpus.setConf(new Configuration());
+    corpus.setConf(getConfiguration());
     Path output = getTestTempDirPath("output");
     Path tmp = getTestTempDirPath("tmp");
     Path workingDir = getTestTempDirPath("working");
@@ -59,7 +60,7 @@ public void testDistributedLanczosSolverCLI() throws Exception {
         "--symmetric", "false",
         "--workingDir", workingDir.toString()
     };
-    new DistributedLanczosSolver().new DistributedLanczosSolverJob().run(args);
+    ToolRunner.run(getConfiguration(), new DistributedLanczosSolver().new DistributedLanczosSolverJob(), args);
 
     output = getTestTempDirPath("output2");
     tmp = getTestTempDirPath("tmp2");
@@ -73,11 +74,11 @@ public void testDistributedLanczosSolverCLI() throws Exception {
         "--symmetric", "false",
         "--workingDir", workingDir.toString()
     };
-    new DistributedLanczosSolver().new DistributedLanczosSolverJob().run(args);
+    ToolRunner.run(getConfiguration(), new DistributedLanczosSolver().new DistributedLanczosSolverJob(), args);
 
     Path rawEigenvectors = new Path(output, DistributedLanczosSolver.RAW_EIGENVECTORS);
     Matrix eigenVectors = new DenseMatrix(7, corpus.numCols());
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
 
     int i = 0;
     for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(rawEigenvectors, conf)) {
@@ -93,7 +94,7 @@ public void testDistributedLanczosSolverEVJCLI() throws Exception {
     Path testData = getTestTempDirPath("testdata");
     DistributedRowMatrix corpus = new TestDistributedRowMatrix()
         .randomDenseHierarchicalDistributedMatrix(10, 9, false, testData.toString());
-    corpus.setConf(new Configuration());
+    corpus.setConf(getConfiguration());
     Path output = getTestTempDirPath("output");
     Path tmp = getTestTempDirPath("tmp");
     String[] args = {
@@ -106,7 +107,7 @@ public void testDistributedLanczosSolverEVJCLI() throws Exception {
         "--symmetric", "false",
         "--cleansvd", "true"
     };
-    new DistributedLanczosSolver().new DistributedLanczosSolverJob().run(args);
+    ToolRunner.run(getConfiguration(), new DistributedLanczosSolver().new DistributedLanczosSolverJob(), args);
   
     Path cleanEigenvectors = new Path(output, EigenVerificationJob.CLEAN_EIGENVECTORS);
     Matrix eigenVectors = new DenseMatrix(6, corpus.numCols());
@@ -124,10 +125,10 @@ public void testDistributedLanczosSolverEVJCLI() throws Exception {
         "--symmetric", "false",
         "--cleansvd", "true"
     };
-    new DistributedLanczosSolver().new DistributedLanczosSolverJob().run(args);
+    ToolRunner.run(getConfiguration(), new DistributedLanczosSolver().new DistributedLanczosSolverJob(), args);
     Path cleanEigenvectors2 = new Path(output, EigenVerificationJob.CLEAN_EIGENVECTORS);
     Matrix eigenVectors2 = new DenseMatrix(7, corpus.numCols());
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     Collection<Double> newEigenValues = Lists.newArrayList();
 
     int i = 0;
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/similarity/TestVectorDistanceSimilarityJob.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/similarity/TestVectorDistanceSimilarityJob.java
index af37c107..a0145665 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/similarity/TestVectorDistanceSimilarityJob.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/similarity/TestVectorDistanceSimilarityJob.java
@@ -62,7 +62,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    fs = FileSystem.get(new Configuration());
+    fs = FileSystem.get(getConfiguration());
   }
 
   @Test
@@ -143,7 +143,7 @@ public void testRun() throws Exception {
     List<VectorWritable> points = getPointsWritable(REFERENCE);
     List<VectorWritable> seeds = getPointsWritable(SEEDS);
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, new Path(input, "file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(seeds, true, new Path(seedsPath, "part-seeds"), fs, conf);
 
@@ -152,7 +152,7 @@ public void testRun() throws Exception {
         output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION),
         EuclideanDistanceMeasure.class.getName() };
 
-    ToolRunner.run(new Configuration(), new VectorDistanceSimilarityJob(), args);
+    ToolRunner.run(getConfiguration(), new VectorDistanceSimilarityJob(), args);
 
     int expectedOutputSize = SEEDS.length * REFERENCE.length;
     int outputSize = Iterables.size(new SequenceFileIterable<StringTuple, DoubleWritable>(new Path(output,
@@ -182,7 +182,7 @@ public void testMaxDistance() throws Exception {
         EuclideanDistanceMeasure.class.getName(),
         optKey(VectorDistanceSimilarityJob.MAX_DISTANCE), String.valueOf(maxDistance) };
 
-    ToolRunner.run(new Configuration(), new VectorDistanceSimilarityJob(), args);
+    ToolRunner.run(getConfiguration(), new VectorDistanceSimilarityJob(), args);
 
     int outputSize = 0;
 
@@ -202,7 +202,7 @@ public void testRunInverted() throws Exception {
     Path seedsPath = getTestTempDirPath("seeds");
     List<VectorWritable> points = getPointsWritable(REFERENCE);
     List<VectorWritable> seeds = getPointsWritable(SEEDS);
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, new Path(input, "file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(seeds, true, new Path(seedsPath, "part-seeds"), fs, conf);
     String[] args = {optKey(DefaultOptionCreator.INPUT_OPTION), input.toString(),
@@ -211,7 +211,7 @@ public void testRunInverted() throws Exception {
         EuclideanDistanceMeasure.class.getName(),
         optKey(VectorDistanceSimilarityJob.OUT_TYPE_KEY), "v"
     };
-    ToolRunner.run(new Configuration(), new VectorDistanceSimilarityJob(), args);
+    ToolRunner.run(getConfiguration(), new VectorDistanceSimilarityJob(), args);
 
     DummyOutputCollector<Text, VectorWritable> collector = new DummyOutputCollector<Text, VectorWritable>();
 
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJobTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJobTest.java
index 2461c9ac..43ac6a8e 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJobTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJobTest.java
@@ -57,7 +57,7 @@ public void toyIntegration() throws Exception {
     outputDir.delete();
     File tmpDir = getTestTempDir("tmp");
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     Path inputPath = new Path(inputFile.getAbsolutePath());
     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
 
@@ -98,7 +98,7 @@ public void toyIntegrationMaxSimilaritiesPerRow() throws Exception {
     outputDir.delete();
     File tmpDir = getTestTempDir("tmp");
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     Path inputPath = new Path(inputFile.getAbsolutePath());
     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
 
@@ -143,7 +143,7 @@ public void toyIntegrationWithThreshold() throws Exception {
     outputDir.delete();
     File tmpDir = getTestTempDir("tmp");
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     Path inputPath = new Path(inputFile.getAbsolutePath());
     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
 
@@ -184,7 +184,7 @@ public void testVectorDimensions() throws Exception {
 
     File inputFile = getTestTempFile("rows");
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     Path inputPath = new Path(inputFile.getAbsolutePath());
     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
 
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolver.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolver.java
index f90f1eb8..634907e3 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolver.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolver.java
@@ -21,9 +21,9 @@
 import java.util.Random;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.MahoutTestCase;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.hadoop.DistributedRowMatrix;
 import org.apache.mahout.math.hadoop.TestDistributedRowMatrix;
@@ -47,7 +47,7 @@ public void testSolver() throws Exception {
     File testData = getTestTempDir("testdata");
     DistributedRowMatrix matrix = new TestDistributedRowMatrix().randomDistributedMatrix(
         10, 10, 10, 10, 10.0, true, testData.getAbsolutePath());
-    matrix.setConf(new Configuration());
+    matrix.setConf(getConfiguration());
     Vector vector = randomVector(matrix.numCols(), 10.0);
     
     DistributedConjugateGradientSolver solver = new DistributedConjugateGradientSolver();
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolverCLI.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolverCLI.java
index 2330e783..3ac9405a 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolverCLI.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/solver/TestDistributedConjugateGradientSolverCLI.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.math.DenseVector;
@@ -76,7 +77,7 @@ private static Vector loadVector(Configuration conf, Path path) throws IOExcepti
 
   @Test
   public void testSolver() throws Exception {
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     Path testData = getTestTempDirPath("testdata");
     DistributedRowMatrix matrix = new TestDistributedRowMatrix().randomDistributedMatrix(
         10, 10, 10, 10, 10.0, true, testData.toString());
@@ -99,7 +100,7 @@ public void testSolver() throws Exception {
     };
     
     DistributedConjugateGradientSolver solver = new DistributedConjugateGradientSolver();
-    solver.job().run(args);
+    ToolRunner.run(getConfiguration(), solver.job(), args);
     
     Vector x = loadVector(conf, output);
     
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stats/BasicStatsTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stats/BasicStatsTest.java
index c9be0faf..7e59eb42 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stats/BasicStatsTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/hadoop/stats/BasicStatsTest.java
@@ -39,7 +39,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    conf = new Configuration();
+    conf = getConfiguration();
   }
 
   @Test
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/ConditionalEntropyTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/ConditionalEntropyTest.java
index d537a02f..8e78d499 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/ConditionalEntropyTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/ConditionalEntropyTest.java
@@ -39,7 +39,7 @@
   @Test
   public void testConditionalEntropy() throws Exception {
 
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     Path input = getTestTempFilePath("input");
     Path output = getTestTempFilePath("output");
     FileSystem fileSystem = FileSystem.get(input.toUri(), configuration);
@@ -60,7 +60,7 @@ public void testConditionalEntropy() throws Exception {
     Tool job = new ConditionalEntropy();
     String[] args = { "-i", input.toString(), "-o", output.toString(),
         "--tempDir", getTestTempDirPath("tmp").toString() };
-    ToolRunner.run(job, args);
+    ToolRunner.run(configuration, job, args);
 
     // check the output
     Iterator<DoubleWritable> iteratorNodes =
@@ -69,7 +69,7 @@ public void testConditionalEntropy() throws Exception {
                                                          PathFilters.logsCRCFilter(),
                                                          null,
                                                          false,
-                                                         new Configuration());
+                                                         getConfiguration());
     while (iteratorNodes.hasNext()) {
       assertEquals(0.5, iteratorNodes.next().get(), EPSILON);
     }
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/EntropyTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/EntropyTest.java
index e39e27ca..933c9eac 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/EntropyTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/EntropyTest.java
@@ -50,7 +50,7 @@ public void testYN() throws Exception {
 
   private void calculateEntropy(String[] content, double expected, String source) throws Exception {
 
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     Path input = getTestTempFilePath("input");
     Path output = getTestTempFilePath("output");
     FileSystem fileSystem = FileSystem.get(input.toUri(), configuration);
@@ -75,7 +75,7 @@ private void calculateEntropy(String[] content, double expected, String source)
     String[] args = { "-i", input.toString(), "-o", output.toString(), "-s", source,
         "--tempDir", getTestTempDirPath("tmp").toString() };
     Entropy job = new Entropy();
-    ToolRunner.run(job, args);
+    ToolRunner.run(configuration, job, args);
 
     assertEquals(content.length, job.getNumberItems());
 
@@ -86,7 +86,7 @@ private void calculateEntropy(String[] content, double expected, String source)
                                                          PathFilters.logsCRCFilter(),
                                                          null,
                                                          false,
-                                                         new Configuration());
+                                                         getConfiguration());
     assertTrue(iteratorNodes.hasNext());
     assertEquals(expected, iteratorNodes.next().get(), EPSILON);
   }
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainRatioTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainRatioTest.java
index d75865ad..9bb329ff 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainRatioTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainRatioTest.java
@@ -32,7 +32,7 @@
   @Test
   public void testInformationGain() throws Exception {
 
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     Path input = getTestTempFilePath("input");
     FileSystem fileSystem = FileSystem.get(input.toUri(), configuration);
 
@@ -51,7 +51,7 @@ public void testInformationGain() throws Exception {
     // run the job
     InformationGainRatio job = new InformationGainRatio();
     String[] args = { "-i", input.toString(), "--tempDir", getTestTempDirPath("tmp").toString() };
-    ToolRunner.run(job, args);
+    ToolRunner.run(configuration, job, args);
 
     // check the output
     assertEquals(1.0, job.getEntropy(), EPSILON);
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainTest.java
index 51b2ce8a..55f6668e 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainTest.java
@@ -32,7 +32,7 @@
   @Test
   public void testInformationGain() throws Exception {
 
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     Path input = getTestTempFilePath("input");
     FileSystem fileSystem = FileSystem.get(input.toUri(), configuration);
 
@@ -51,7 +51,7 @@ public void testInformationGain() throws Exception {
     // run the job
     InformationGain job = new InformationGain();
     String[] args = { "-i", input.toString(), "--tempDir", getTestTempDirPath("tmp").toString() };
-    ToolRunner.run(job, args);
+    ToolRunner.run(configuration, job, args);
 
     // check the output
     assertEquals(1.0, job.getEntropy(), EPSILON);
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/DictionaryVectorizerTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/DictionaryVectorizerTest.java
index c0649f02..39a623fa 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/DictionaryVectorizerTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/DictionaryVectorizerTest.java
@@ -55,7 +55,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
 
     inputPath = getTestTempFilePath("documents/docs.file");
     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
@@ -104,7 +104,7 @@ private void runTest(boolean sequential, boolean named)
     Path tfidf = getTestTempDirPath("output/tfidf");
     Path tfidfVectors = new Path(tfidf, "tfidf-vectors");
     
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     DocumentProcessor.tokenizeDocuments(inputPath, analyzer, tokenizedDocuments, conf);
     
     DictionaryVectorizer.createTermFrequencyVectors(tokenizedDocuments,
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/DocumentProcessorTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/DocumentProcessorTest.java
index 3e136d86..8fdbc5dc 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/DocumentProcessorTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/DocumentProcessorTest.java
@@ -42,7 +42,7 @@
 
   @Test
   public void testTokenizeDocuments() throws Exception {
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     Path input = new Path(getTestTempDirPath(), "inputDir");
     Path output = new Path(getTestTempDirPath(), "outputDir");
     FileSystem fs = FileSystem.get(input.toUri(), configuration);
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/EncodedVectorsFromSequenceFilesTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/EncodedVectorsFromSequenceFilesTest.java
index ebd3c907..b017a2ca 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/EncodedVectorsFromSequenceFilesTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/EncodedVectorsFromSequenceFilesTest.java
@@ -23,6 +23,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.iterator.sequencefile.PathFilters;
@@ -48,7 +49,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    conf = new Configuration();
+    conf = getConfiguration();
 
     inputPath = getTestTempFilePath("documents/docs.file");
     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
@@ -107,7 +108,7 @@ private void runTest(boolean sequential, boolean named) throws Exception {
     
     String[] args = argList.toArray(new String[argList.size()]);
 
-    EncodedVectorsFromSequenceFiles.main(args);
+    ToolRunner.run(getConfiguration(), new EncodedVectorsFromSequenceFiles(), args);
 
     SequenceFileDirIterator<Text, VectorWritable> iter = new SequenceFileDirIterator<Text, VectorWritable>(outputPath, PathType.LIST, PathFilters.partFilter(), null, true, conf);
     int seen = 0;
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/HighDFWordsPrunerTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/HighDFWordsPrunerTest.java
index 7e52f8d7..69f60691 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/HighDFWordsPrunerTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/HighDFWordsPrunerTest.java
@@ -22,6 +22,7 @@
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.iterator.sequencefile.PathFilters;
@@ -50,7 +51,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    conf = new Configuration();
+    conf = getConfiguration();
 
     inputPath = getTestTempFilePath("documents/docs.file");
     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
@@ -92,6 +93,8 @@ private void runTest(boolean prune) throws Exception {
     argList.add(inputPath.toString());
     argList.add("-o");
     argList.add(outputPath.toString());
+    argList.add("--mapred");
+    argList.add(getTestTempDir("mapred" + Math.random()).getAbsolutePath());
     if (prune) {
       argList.add("-xs");
       argList.add("3"); // we prune all words that are outside 3*sigma
@@ -105,7 +108,7 @@ private void runTest(boolean prune) throws Exception {
 
     String[] args = argList.toArray(new String[argList.size()]);
 
-    SparseVectorsFromSequenceFiles.main(args);
+    ToolRunner.run(getConfiguration(), new SparseVectorsFromSequenceFiles(), args);
 
     Path dictionary = new Path(outputPath, "dictionary.file-0");
     Path tfVectors = new Path(outputPath, "tf-vectors");
diff --git a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFilesTest.java b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFilesTest.java
index 8cce9886..6f8552e1 100644
--- a/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFilesTest.java
+++ b/mahout/trunk/core/src/test/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFilesTest.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.iterator.sequencefile.PathFilters;
 import org.apache.mahout.common.iterator.sequencefile.PathType;
@@ -43,7 +44,7 @@
   private Path inputPath;
 
   private void setupDocs() throws IOException {
-    conf = new Configuration();
+    conf = getConfiguration();
 
     inputPath = getTestTempFilePath("documents/docs.file");
     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
@@ -88,8 +89,7 @@ public void testCreateTermFrequencyVectorsSeqNam() throws Exception {
 
   @Test
   public void testPruning() throws Exception {
-    conf = new Configuration();
-
+    conf = getConfiguration();
     inputPath = getTestTempFilePath("documents/docs.file");
     FileSystem fs = FileSystem.get(inputPath.toUri(), conf);
 
@@ -126,7 +126,7 @@ public void testPruning() throws Exception {
 
   @Test
   public void testPruningTF() throws Exception {
-    conf = new Configuration();
+    conf = getConfiguration();
     FileSystem fs = FileSystem.get(conf);
 
     inputPath = getTestTempFilePath("documents/docs.file");
@@ -164,7 +164,6 @@ public void testPruningTF() throws Exception {
   private Path runTest(boolean tfWeighting, boolean sequential, boolean named, double maxDFSigma, int numDocs) throws Exception {
     Path outputPath = getTestTempFilePath("output");
 
-    
     List<String> argList = new LinkedList<String>();
     argList.add("-i");
     argList.add(inputPath.toString());
@@ -188,7 +187,7 @@ private Path runTest(boolean tfWeighting, boolean sequential, boolean named, dou
     }
     String[] args = argList.toArray(new String[argList.size()]);
     
-    SparseVectorsFromSequenceFiles.main(args);
+    ToolRunner.run(getConfiguration(), new SparseVectorsFromSequenceFiles(), args);
 
     Path tfVectors = new Path(outputPath, "tf-vectors");
     Path tfidfVectors = new Path(outputPath, "tfidf-vectors");
diff --git a/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/SplitInput.java b/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/SplitInput.java
index 16d32cad..e2e11afb 100644
--- a/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/SplitInput.java
+++ b/mahout/trunk/integration/src/main/java/org/apache/mahout/utils/SplitInput.java
@@ -280,6 +280,13 @@ public void splitDirectory(Path inputDir) throws IOException, ClassNotFoundExcep
     if (conf == null) {
       conf = new Configuration();
     }
+    splitDirectory(conf, inputDir);
+  }
+
+  /*
+   * See also splitDirectory(Path inputDir)
+   * */
+  public void splitDirectory(Configuration conf, Path inputDir) throws IOException, ClassNotFoundException, InterruptedException {
     FileSystem fs = inputDir.getFileSystem(conf);
     if (fs.getFileStatus(inputDir) == null) {
       throw new IOException(inputDir + " does not exist");
@@ -289,7 +296,7 @@ public void splitDirectory(Path inputDir) throws IOException, ClassNotFoundExcep
     }
 
     if (useMapRed) {
-      SplitInputJob.run(new Configuration(), inputDir, mapRedOutputDirectory,
+      SplitInputJob.run(conf, inputDir, mapRedOutputDirectory,
               keepPct, testRandomSelectionPct);
     } else {
       // input dir contains one file per category.
@@ -302,7 +309,6 @@ public void splitDirectory(Path inputDir) throws IOException, ClassNotFoundExcep
     }
   }
 
-
   /**
    * Perform a split on the specified input file. Results will be written to files of the same name in the specified
    * training and test output directories. The {@link #validate()} method is called prior to executing the split.
diff --git a/mahout/trunk/integration/src/test/java/org/apache/mahout/clustering/TestClusterDumper.java b/mahout/trunk/integration/src/test/java/org/apache/mahout/clustering/TestClusterDumper.java
index f126df22..745591fb 100644
--- a/mahout/trunk/integration/src/test/java/org/apache/mahout/clustering/TestClusterDumper.java
+++ b/mahout/trunk/integration/src/test/java/org/apache/mahout/clustering/TestClusterDumper.java
@@ -24,6 +24,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -198,7 +199,7 @@ public void testKmeans() throws Exception {
     DistanceMeasure measure = new EuclideanDistanceMeasure();
     // now run the Canopy job to prime kMeans canopies
     Path output = getTestTempDirPath("output");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     CanopyDriver.run(conf, getTestTempDirPath("testdata"), output, measure, 8,
         4, false, 0.0, true);
     // now run the KMeans job
diff --git a/mahout/trunk/integration/src/test/java/org/apache/mahout/clustering/TestClusterEvaluator.java b/mahout/trunk/integration/src/test/java/org/apache/mahout/clustering/TestClusterEvaluator.java
index 6aaf4377..d9c9d30c 100644
--- a/mahout/trunk/integration/src/test/java/org/apache/mahout/clustering/TestClusterEvaluator.java
+++ b/mahout/trunk/integration/src/test/java/org/apache/mahout/clustering/TestClusterEvaluator.java
@@ -150,7 +150,7 @@ private void initData(double dC, double dP, DistanceMeasure measure) {
   public void testRepresentativePoints() throws Exception {
     ClusteringTestUtils.writePointsToFile(referenceData, new Path(testdata, "file1"), fs, conf);
     DistanceMeasure measure = new EuclideanDistanceMeasure();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     // run using MR reference point calculation
     CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, true, 0.0, true);
     int numIterations = 2;
diff --git a/mahout/trunk/integration/src/test/java/org/apache/mahout/utils/SplitInputTest.java b/mahout/trunk/integration/src/test/java/org/apache/mahout/utils/SplitInputTest.java
index 33f9d93b..b3f5232d 100644
--- a/mahout/trunk/integration/src/test/java/org/apache/mahout/utils/SplitInputTest.java
+++ b/mahout/trunk/integration/src/test/java/org/apache/mahout/utils/SplitInputTest.java
@@ -33,6 +33,7 @@
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.classifier.ClassifierData;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
@@ -60,7 +61,7 @@
   @Override
   @Before
   public void setUp() throws Exception {
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     fs = FileSystem.get(conf);
 
     super.setUp();
@@ -190,7 +191,7 @@ public void testSplitFileRandomSelectionPct() throws Exception {
   private void writeVectorSequenceFile(Path path, int testPoints)
       throws IOException {
     Path tempSequenceFile = new Path(path, "part-00000");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     IntWritable key = new IntWritable();
     VectorWritable value = new VectorWritable();
     SequenceFile.Writer writer = null;
@@ -219,7 +220,7 @@ private void writeVectorSequenceFile(Path path, int testPoints)
   private void writeTextSequenceFile(Path path, int testPoints)
       throws IOException {
     Path tempSequenceFile = new Path(path, "part-00000");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     Text key = new Text();
     Text value = new Text();
     SequenceFile.Writer writer = null;
@@ -241,8 +242,8 @@ private void writeTextSequenceFile(Path path, int testPoints)
    * Display contents of a SequenceFile
    * @param sequenceFilePath path to SequenceFile
    */
-  private static void displaySequenceFile(Path sequenceFilePath) {
-    for (Pair<?,?> record : new SequenceFileIterable<Writable,Writable>(sequenceFilePath, true, new Configuration())) {
+  private void displaySequenceFile(Path sequenceFilePath) throws IOException {
+    for (Pair<?,?> record : new SequenceFileIterable<Writable,Writable>(sequenceFilePath, true, getConfiguration())) {
       System.out.println(record.getFirst() + "\t" + record.getSecond());
     }
   }
@@ -252,9 +253,9 @@ private static void displaySequenceFile(Path sequenceFilePath) {
    * @param sequenceFilePath path to SequenceFile
    * @return number of records
    */
-  private static int getNumberRecords(Path sequenceFilePath) {
+  private int getNumberRecords(Path sequenceFilePath) throws IOException {
     int numberRecords = 0;
-    for (Object value : new SequenceFileValueIterable<Writable>(sequenceFilePath, true, new Configuration())) {
+    for (Object value : new SequenceFileValueIterable<Writable>(sequenceFilePath, true, getConfiguration())) {
       numberRecords++;
     }
     return numberRecords;
@@ -311,7 +312,7 @@ private void testSplitInputMapReduceCli(int numPoints) throws Exception {
             "--mapRedOutputDir", tempMapRedOutputDirectory.toString(),
             "--randomSelectionPct", Integer.toString(randomSelectionPct),
             "--keepPct", Integer.toString(keepPct), "-ow" };
-    SplitInput.main(args);
+    ToolRunner.run(getConfiguration(), new SplitInput(), args);
     validateSplitInputMapReduce(numPoints, randomSelectionPct, keepPct);
   }
 
@@ -325,7 +326,7 @@ private void testSplitInputMapReduce(int numPoints) throws Exception {
     si.setKeepPct(keepPct);
     si.setMapRedOutputDirectory(tempMapRedOutputDirectory);
     si.setUseMapRed(true);
-    si.splitDirectory(tempSequenceDirectory);
+    si.splitDirectory(getConfiguration(), tempSequenceDirectory);
 
     validateSplitInputMapReduce(numPoints, randomSelectionPct, keepPct);
   }
@@ -334,7 +335,7 @@ private void testSplitInputMapReduce(int numPoints) throws Exception {
    * Validate that number of test records and number of training records
    * are consistant with keepPct and randomSelectionPct
    */
-  private void validateSplitInputMapReduce(int numPoints, int randomSelectionPct, int keepPct) {
+  private void validateSplitInputMapReduce(int numPoints, int randomSelectionPct, int keepPct) throws IOException {
     Path testPath = new Path(tempMapRedOutputDirectory, "test-r-00000");
     Path trainingPath = new Path(tempMapRedOutputDirectory, "training-r-00000");
     int numberTestRecords = getNumberRecords(testPath);
diff --git a/mahout/trunk/math/src/test/java/org/apache/mahout/math/MahoutTestCase.java b/mahout/trunk/math/src/test/java/org/apache/mahout/math/MahoutTestCase.java
index b52b7382..ab15f1e8 100644
--- a/mahout/trunk/math/src/test/java/org/apache/mahout/math/MahoutTestCase.java
+++ b/mahout/trunk/math/src/test/java/org/apache/mahout/math/MahoutTestCase.java
@@ -51,10 +51,13 @@ public void tearDown() throws Exception {
 
   protected final File getTestTempDir() throws IOException {
     if (testTempDir == null) {
-      String systemTmpDir = System.getProperty("java.io.tmpdir");
+      String systemTmpDir = System.getProperty("mahout.test.directory");
+      if (systemTmpDir == null)
+    	systemTmpDir = "target/";
+      systemTmpDir += "test-data";
       long simpleRandomLong = (long) (Long.MAX_VALUE * Math.random());
       testTempDir = new File(systemTmpDir, "mahout-" + getClass().getSimpleName() + '-' + simpleRandomLong);
-      if (!testTempDir.mkdir()) {
+      if (!testTempDir.mkdirs()) {
         throw new IOException("Could not create " + testTempDir);
       }
       testTempDir.deleteOnExit();
