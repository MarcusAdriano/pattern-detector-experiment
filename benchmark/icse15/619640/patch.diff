diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java
index 3b2520d1..3cdae5ef 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -876,10 +876,12 @@ void init(Document doc, int docID) throws IOException, AbortException {
             tvf = directory.createOutput(docStoreSegment +  "." + IndexFileNames.VECTORS_FIELDS_EXTENSION);
             tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);
 
-            // We must "catch up" for all docIDs that had no
-            // vectors before this one
-            for(int i=0;i<docID;i++) {
-              tvx.writeLong(0);
+            // We must "catch up" for all docs before us
+            // that had no vectors:
+            final long tvdPos = tvd.getFilePointer();
+            tvd.writeVInt(0);
+            for(int i=0;i<numDocsInStore-1;i++) {
+              tvx.writeLong(tvdPos);
               tvx.writeLong(0);
             }
           } catch (Throwable t) {
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/FieldsReader.java b/lucene/java/trunk/src/java/org/apache/lucene/index/FieldsReader.java
index 295a6e86..c97f5fa0 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/FieldsReader.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/FieldsReader.java
@@ -201,7 +201,7 @@ final IndexInput rawDocs(int[] lengths, int startDocID, int numDocs) throws IOEx
     int count = 0;
     while (count < numDocs) {
       final long offset;
-      final int docID = startDocID + count + 1;
+      final int docID = docStoreOffset + startDocID + count + 1;
       assert docID <= numTotalDocs;
       if (docID < numTotalDocs) 
         offset = indexStream.readLong();
diff --git a/lucene/java/trunk/src/java/org/apache/lucene/index/TermVectorsReader.java b/lucene/java/trunk/src/java/org/apache/lucene/index/TermVectorsReader.java
index 27e5e290..c46d6090 100644
--- a/lucene/java/trunk/src/java/org/apache/lucene/index/TermVectorsReader.java
+++ b/lucene/java/trunk/src/java/org/apache/lucene/index/TermVectorsReader.java
@@ -168,7 +168,8 @@ final void rawDocs(int[] tvdLengths, int[] tvfLengths, int startDocID, int numDo
 
     int count = 0;
     while (count < numDocs) {
-      final int docID = startDocID + count + 1;
+      final int docID = docStoreOffset + startDocID + count + 1;
+      assert docID <= numTotalDocs;
       if (docID < numTotalDocs)  {
         tvdPosition = tvx.readLong();
         tvfPosition = tvx.readLong();
diff --git a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java
index fe887d68..e0eca91c 100644
--- a/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/java/trunk/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -2682,6 +2682,148 @@ public void testUnlimitedMaxFieldLength() throws IOException {
     dir.close();
   }
 
+  // LUCENE-1168
+  public void testTermVectorCorruption() throws IOException {
+
+    Directory dir = new MockRAMDirectory();
+    for(int iter=0;iter<4;iter++) {
+      final boolean autoCommit = 1==iter/2;
+      IndexWriter writer = new IndexWriter(dir,
+                                           autoCommit, new StandardAnalyzer(),
+                                           IndexWriter.MaxFieldLength.LIMITED);
+      writer.setMaxBufferedDocs(2);
+      writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
+      writer.setMergeScheduler(new SerialMergeScheduler());
+      writer.setMergePolicy(new LogDocMergePolicy());
+
+      Document document = new Document();
+
+      Field storedField = new Field("stored", "stored", Field.Store.YES,
+                                    Field.Index.NO);
+      document.add(storedField);
+      writer.addDocument(document);
+      writer.addDocument(document);
+
+      document = new Document();
+      document.add(storedField);
+      Field termVectorField = new Field("termVector", "termVector",
+                                        Field.Store.NO, Field.Index.UN_TOKENIZED,
+                                        Field.TermVector.WITH_POSITIONS_OFFSETS);
+
+      document.add(termVectorField);
+      writer.addDocument(document);
+      writer.optimize();
+      writer.close();
+
+      IndexReader reader = IndexReader.open(dir);
+      for(int i=0;i<reader.numDocs();i++) {
+        reader.document(i);
+        reader.getTermFreqVectors(i);
+      }
+      reader.close();
+
+      writer = new IndexWriter(dir,
+                               autoCommit, new StandardAnalyzer(),
+                               IndexWriter.MaxFieldLength.LIMITED);
+      writer.setMaxBufferedDocs(2);
+      writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
+      writer.setMergeScheduler(new SerialMergeScheduler());
+      writer.setMergePolicy(new LogDocMergePolicy());
+
+      Directory[] indexDirs = { dir};
+      writer.addIndexes(indexDirs);
+      writer.close();
+    }
+    dir.close();
+  }
+
+  // LUCENE-1168
+  public void testTermVectorCorruption2() throws IOException {
+    Directory dir = new MockRAMDirectory();
+    for(int iter=0;iter<4;iter++) {
+      final boolean autoCommit = 1==iter/2;
+      IndexWriter writer = new IndexWriter(dir,
+                                           autoCommit, new StandardAnalyzer(),
+                                           IndexWriter.MaxFieldLength.LIMITED);
+      writer.setMaxBufferedDocs(2);
+      writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
+      writer.setMergeScheduler(new SerialMergeScheduler());
+      writer.setMergePolicy(new LogDocMergePolicy());
+
+      Document document = new Document();
+
+      Field storedField = new Field("stored", "stored", Field.Store.YES,
+                                    Field.Index.NO);
+      document.add(storedField);
+      writer.addDocument(document);
+      writer.addDocument(document);
+
+      document = new Document();
+      document.add(storedField);
+      Field termVectorField = new Field("termVector", "termVector",
+                                        Field.Store.NO, Field.Index.UN_TOKENIZED,
+                                        Field.TermVector.WITH_POSITIONS_OFFSETS);
+      document.add(termVectorField);
+      writer.addDocument(document);
+      writer.optimize();
+      writer.close();
+
+      IndexReader reader = IndexReader.open(dir);
+      assertTrue(reader.getTermFreqVectors(0)==null);
+      assertTrue(reader.getTermFreqVectors(1)==null);
+      assertTrue(reader.getTermFreqVectors(2)!=null);
+      reader.close();
+    }
+    dir.close();
+  }
+
+  // LUCENE-1168
+  public void testTermVectorCorruption3() throws IOException {
+    Directory dir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(dir,
+                                         false, new StandardAnalyzer(),
+                                         IndexWriter.MaxFieldLength.LIMITED);
+    writer.setMaxBufferedDocs(2);
+    writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
+    writer.setMergeScheduler(new SerialMergeScheduler());
+    writer.setMergePolicy(new LogDocMergePolicy());
+
+    Document document = new Document();
+
+    document = new Document();
+    Field storedField = new Field("stored", "stored", Field.Store.YES,
+                                  Field.Index.NO);
+    document.add(storedField);
+    Field termVectorField = new Field("termVector", "termVector",
+                                      Field.Store.NO, Field.Index.UN_TOKENIZED,
+                                      Field.TermVector.WITH_POSITIONS_OFFSETS);
+    document.add(termVectorField);
+    for(int i=0;i<10;i++)
+      writer.addDocument(document);
+    writer.close();
+
+    writer = new IndexWriter(dir,
+                             false, new StandardAnalyzer(),
+                             IndexWriter.MaxFieldLength.LIMITED);
+    writer.setMaxBufferedDocs(2);
+    writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
+    writer.setMergeScheduler(new SerialMergeScheduler());
+    writer.setMergePolicy(new LogDocMergePolicy());
+    for(int i=0;i<6;i++)
+      writer.addDocument(document);
+
+    writer.optimize();
+    writer.close();
+
+    IndexReader reader = IndexReader.open(dir);
+    for(int i=0;i<10;i++) {
+      reader.getTermFreqVectors(i);
+      reader.document(i);
+    }
+    reader.close();
+    dir.close();
+  }
+
   // LUCENE-1084: test user-specified field length
   public void testUserSpecifiedMaxFieldLength() throws IOException {
     Directory dir = new MockRAMDirectory();
