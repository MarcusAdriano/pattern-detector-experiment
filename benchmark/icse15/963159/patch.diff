diff --git a/cassandra/trunk/src/java/org/apache/cassandra/config/Config.java b/cassandra/trunk/src/java/org/apache/cassandra/config/Config.java
index 315c9a49..daf41099 100644
--- a/cassandra/trunk/src/java/org/apache/cassandra/config/Config.java
+++ b/cassandra/trunk/src/java/org/apache/cassandra/config/Config.java
@@ -36,9 +36,6 @@
     
     public Integer memtable_flush_writers = null; // will get set to the length of data dirs in DatabaseDescriptor
     
-    public Double flush_data_buffer_size_in_mb = new Double(32);
-    public Double flush_index_buffer_size_in_mb = new Double(8);
-    
     public Integer sliced_buffer_size_in_kb = 64;
     
     public Integer storage_port = 7000;
diff --git a/cassandra/trunk/src/java/org/apache/cassandra/config/Converter.java b/cassandra/trunk/src/java/org/apache/cassandra/config/Converter.java
index 06b963fe..0ff1de46 100644
--- a/cassandra/trunk/src/java/org/apache/cassandra/config/Converter.java
+++ b/cassandra/trunk/src/java/org/apache/cassandra/config/Converter.java
@@ -41,7 +41,7 @@
             int size = tablesxml.getLength();
             for ( int i = 0; i < size; ++i )
             {
-                String value = null;
+                String value;
                 Keyspace ks = new Keyspace();
                 Node table = tablesxml.item(i);
                 /* parsing out the table ksName */
@@ -158,18 +158,6 @@ private static void loadPreviousConfig(String config) throws ConfigurationExcept
                 conf.concurrent_writes = Integer.parseInt(rawWriters);
             }
             
-            String rawFlushData = xmlUtils.getNodeValue("/Storage/FlushDataBufferSizeInMB");
-            if (rawFlushData != null)
-            {
-                conf.flush_data_buffer_size_in_mb = Double.parseDouble(rawFlushData);
-            }
-            
-            String rawFlushIndex = xmlUtils.getNodeValue("/Storage/FlushIndexBufferSizeInMB");
-            if (rawFlushIndex != null)
-            {
-                conf.flush_index_buffer_size_in_mb = Double.parseDouble(rawFlushIndex);
-            }
-
             String rawSlicedBuffer = xmlUtils.getNodeValue("/Storage/SlicedBufferSizeInKB");
             if (rawSlicedBuffer != null)
             {
@@ -287,7 +275,7 @@ public static void main (String[] args)
     {
         try
         {
-            String configname = null;
+            String configname;
             ClassLoader loader = Converter.class.getClassLoader();
             URL scpurl = loader.getResource(PREVIOUS_CONF_FILE);
             if (scpurl == null)
diff --git a/cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java b/cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
index b5f93028..33dae8c3 100644
--- a/cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
+++ b/cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
@@ -935,7 +935,7 @@ public static int getFlushWriters()
             return conf.memtable_flush_writers;
     }
 
-    public static long getInMemoryCompactionLimit()
+    public static int getInMemoryCompactionLimit()
     {
         return conf.in_memory_compaction_limit_in_mb * 1024 * 1024;
     }
@@ -1126,16 +1126,6 @@ public static int getCommitLogSyncPeriod() {
         return indexAccessMode;
     }
 
-    public static double getFlushDataBufferSizeInMB()
-    {
-        return conf.flush_data_buffer_size_in_mb;
-    }
-
-    public static double getFlushIndexBufferSizeInMB()
-    {
-        return conf.flush_index_buffer_size_in_mb;
-    }
-
     public static int getIndexedReadBufferSizeInKB()
     {
         return conf.column_index_size_in_kb;
diff --git a/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java b/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
index b6077896..11b02a9f 100644
--- a/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
+++ b/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
@@ -150,15 +150,8 @@ private SSTableReader writeSortedContents() throws IOException
         logger.info("Writing " + this);
         SSTableWriter writer = new SSTableWriter(cfs.getFlushPath(), columnFamilies.size(), partitioner);
 
-        DataOutputBuffer buffer = new DataOutputBuffer();
         for (Map.Entry<DecoratedKey, ColumnFamily> entry : columnFamilies.entrySet())
-        {
-            buffer.reset();
-            /* serialize the cf with column indexes */
-            ColumnFamily.serializer().serializeWithIndexes(entry.getValue(), buffer);
-            /* Now write the key and value to disk */
-            writer.append(entry.getKey(), buffer);
-        }
+            writer.append(entry.getKey(), entry.getValue());
 
         SSTableReader ssTable = writer.closeAndOpenReader();
         logger.info("Completed flushing " + ssTable.getFilename());
diff --git a/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java b/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
index 9c3fd541..a6fdfc12 100644
--- a/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
+++ b/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
@@ -43,11 +43,11 @@
 import org.slf4j.LoggerFactory;
 
 import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamily;
 import org.apache.cassandra.db.DecoratedKey;
 import org.apache.cassandra.dht.IPartitioner;
 import org.apache.cassandra.io.AbstractCompactedRow;
 import org.apache.cassandra.io.util.BufferedRandomAccessFile;
-import org.apache.cassandra.io.util.DataOutputBuffer;
 import org.apache.cassandra.io.util.SegmentedFile;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.BloomFilter;
@@ -67,7 +67,7 @@ public SSTableWriter(String filename, long keyCount, IPartitioner partitioner) t
         super(filename, partitioner);
         iwriter = new IndexWriter(desc, partitioner, keyCount);
         dbuilder = SegmentedFile.getBuilder();
-        dataFile = new BufferedRandomAccessFile(getFilename(), "rw", (int)(DatabaseDescriptor.getFlushDataBufferSizeInMB() * 1024 * 1024));
+        dataFile = new BufferedRandomAccessFile(getFilename(), "rw", DatabaseDescriptor.getInMemoryCompactionLimit());
     }
 
     private long beforeAppend(DecoratedKey decoratedKey) throws IOException
@@ -104,16 +104,22 @@ public void append(AbstractCompactedRow row) throws IOException
         afterAppend(row.key, currentPosition);
     }
 
-    // TODO make this take a DataOutputStream and wrap the byte[] version to combine them
-    public void append(DecoratedKey decoratedKey, DataOutputBuffer buffer) throws IOException
+    public void append(DecoratedKey decoratedKey, ColumnFamily cf) throws IOException
     {
-        long currentPosition = beforeAppend(decoratedKey);
+        long startPosition = beforeAppend(decoratedKey);
         FBUtilities.writeShortByteArray(partitioner.convertToDiskFormat(decoratedKey), dataFile);
-        int length = buffer.getLength();
-        assert length > 0;
-        dataFile.writeLong(length);
-        dataFile.write(buffer.getData(), 0, length);
-        afterAppend(decoratedKey, currentPosition);
+        // write placeholder for the row size, since we don't know it yet
+        long sizePosition = dataFile.getFilePointer();
+        dataFile.writeLong(-1);
+        // write out row data
+        ColumnFamily.serializer().serializeWithIndexes(cf, dataFile);
+        // seek back and write the row size (not including the size Long itself)
+        long endPosition = dataFile.getFilePointer();
+        dataFile.seek(sizePosition);
+        dataFile.writeLong(endPosition - (sizePosition + 8));
+        // finally, reset for next row
+        dataFile.seek(endPosition);
+        afterAppend(decoratedKey, startPosition);
     }
 
     public void append(DecoratedKey decoratedKey, byte[] value) throws IOException
@@ -209,7 +215,7 @@ private static void maybeRecover(Descriptor desc) throws IOException
         ffile.delete();
 
         // open the data file for input, and an IndexWriter for output
-        BufferedRandomAccessFile dfile = new BufferedRandomAccessFile(desc.filenameFor(SSTable.COMPONENT_DATA), "r", (int)(DatabaseDescriptor.getFlushDataBufferSizeInMB() * 1024 * 1024));
+        BufferedRandomAccessFile dfile = new BufferedRandomAccessFile(desc.filenameFor(SSTable.COMPONENT_DATA), "r", 8 * 1024 * 1024);
         IndexWriter iwriter;
         long estimatedRows;
         try
@@ -285,8 +291,7 @@ public static SSTableReader recoverAndOpen(Descriptor desc) throws IOException
         {
             this.desc = desc;
             this.partitioner = part;
-            int bufferbytes = (int)(DatabaseDescriptor.getFlushIndexBufferSizeInMB() * 1024 * 1024);
-            indexFile = new BufferedRandomAccessFile(desc.filenameFor(SSTable.COMPONENT_INDEX), "rw", bufferbytes);
+            indexFile = new BufferedRandomAccessFile(desc.filenameFor(SSTable.COMPONENT_INDEX), "rw", 8 * 1024 * 1024);
             builder = SegmentedFile.getBuilder();
             summary = new IndexSummary();
             bf = BloomFilter.getFilter(keyCount, 15);
diff --git a/cassandra/trunk/src/java/org/apache/cassandra/io/util/BufferedRandomAccessFile.java b/cassandra/trunk/src/java/org/apache/cassandra/io/util/BufferedRandomAccessFile.java
index cacd40c9..bb7452cb 100644
--- a/cassandra/trunk/src/java/org/apache/cassandra/io/util/BufferedRandomAccessFile.java
+++ b/cassandra/trunk/src/java/org/apache/cassandra/io/util/BufferedRandomAccessFile.java
@@ -40,7 +40,6 @@
 {
     static final int LogBuffSz_ = 16; // 64K buffer
     public static final int BuffSz_ = (1 << LogBuffSz_);
-    static final long BuffMask_ = ~(((long) BuffSz_) - 1L);
 
     private String path_;
     
@@ -84,22 +83,22 @@
     *
     * V3. Any (possibly) unflushed characters are stored in "f.buff":
     *
-    * (forall i in [f.lo, f.curr): c(f)[i] == f.buff[i - f.lo])
+    * (forall i in [f.lo, f.hi): c(f)[i] == f.buff[i - f.lo])
     *
     * V4. For all characters not covered by V3, c(f) and disk(f) agree:
     *
-    * (forall i in [f.lo, len(f)): i not in [f.lo, f.curr) => c(f)[i] ==
+    * (forall i in [f.lo, len(f)): i not in [f.lo, f.hi) => c(f)[i] ==
     * disk(f)[i])
     *
     * V5. "f.dirty" is true iff the buffer contains bytes that should be
     * flushed to the file; by V3 and V4, only part of the buffer can be dirty.
     *
-    * f.dirty == (exists i in [f.lo, f.curr): c(f)[i] != f.buff[i - f.lo])
+    * f.dirty == (exists i in [f.lo, f.hi): c(f)[i] != f.buff[i - f.lo])
     *
     * V6. this.maxHi == this.lo + this.buff.length
     *
     * Note that "f.buff" can be "null" in a valid file, since the range of
-    * characters in V3 is empty when "f.lo == f.curr".
+    * characters in V3 is empty when "f.lo == f.hi".
     *
     * A file is said to be *ready* if the buffer contains the current position,
     * i.e., when:
@@ -189,9 +188,9 @@ private void flushBuffer() throws IOException
         {
             if (this.diskPos_ != this.lo_)
                 super.seek(this.lo_);
-            int len = (int) (this.curr_ - this.lo_);
+            int len = (int) (this.hi_ - this.lo_);
             super.write(this.buff_, 0, len);
-            this.diskPos_ = this.curr_;             
+            this.diskPos_ = this.hi_;
             this.dirty_ = false;
         }
     }
@@ -203,41 +202,35 @@ private void flushBuffer() throws IOException
      */
     private int fillBuffer() throws IOException
     {
-        int cnt = 0;
-        int rem = this.buff_.length;
-        while (rem > 0)
+        int count = 0;
+        int remainder = this.buff_.length;
+        while (remainder > 0)
         {
-            int n = super.read(this.buff_, cnt, rem);
+            int n = super.read(this.buff_, count, remainder);
             if (n < 0)
                 break;
-            cnt += n;
-            rem -= n;
+            count += n;
+            remainder -= n;
         }
-        if ( (cnt < 0) && (this.hitEOF_ = (cnt < this.buff_.length)) )
-        {
-            // make sure buffer that wasn't read is initialized with -1
-            Arrays.fill(this.buff_, cnt, this.buff_.length, (byte) 0xff);
+        this.hitEOF_ = (count < this.buff_.length);
+        this.diskPos_ += count;
+        return count;
         }
-        this.diskPos_ += cnt;
-        return cnt;
+    
+    public void seek(long pos) throws IOException
+    {
+        this.curr_ = pos;
     }
     
     /*
-     * This method positions <code>this.curr</code> at position <code>pos</code>.
-     * If <code>pos</code> does not fall in the current buffer, it flushes the
-     * current buffer and loads the correct one.<p>
-     * 
      * On exit from this routine <code>this.curr == this.hi</code> iff <code>pos</code>
      * is at or past the end-of-file, which can only happen if the file was
      * opened in read-only mode.
      */
-    public void seek(long pos) throws IOException
+    private void reBuffer() throws IOException
     {
-        if (pos >= this.hi_ || pos < this.lo_)
-        {
-            // seeking outside of current buffer -- flush and read             
             this.flushBuffer();
-            this.lo_ = pos & BuffMask_; // start at BuffSz boundary
+        this.lo_ = this.curr_;
             this.maxHi_ = this.lo_ + (long) this.buff_.length;
             if (this.diskPos_ != this.lo_)
             {
@@ -247,17 +240,6 @@ public void seek(long pos) throws IOException
             int n = this.fillBuffer();
             this.hi_ = this.lo_ + (long) n;
         }
-        else
-        {
-            // seeking inside current buffer -- no read required
-            if (pos < this.curr_)
-            {
-                // if seeking backwards, we must flush to maintain V4
-                this.flushBuffer();
-            }
-        }
-        this.curr_ = pos;
-    }
 
     public long getFilePointer()
     {
@@ -280,16 +262,10 @@ public long length() throws IOException
 
     public int read() throws IOException
     {
-        if (this.curr_ >= this.hi_)
+        if (this.lo_ > this.curr_ || this.curr_ >= this.hi_)
         {
-            // test for EOF
-            // if (this.hi < this.maxHi) return -1;
-            if (this.hitEOF_)
-                return -1;
-            
-            // slow path -- read another buffer
-            this.seek(this.curr_);
-            if (this.curr_ == this.hi_)
+            this.reBuffer();
+            if (this.curr_ == this.hi_ && this.hitEOF_)
                 return -1;
         }
         byte res = this.buff_[(int) (this.curr_ - this.lo_)];
@@ -304,16 +280,10 @@ public int read(byte[] b) throws IOException
     
     public int read(byte[] b, int off, int len) throws IOException
     {
-        if (this.curr_ >= this.hi_)
+        if (this.lo_ > this.curr_ || this.curr_ >= this.hi_)
         {
-            // test for EOF
-            // if (this.hi < this.maxHi) return -1;
-            if (this.hitEOF_)
-                return -1;
-            
-            // slow path -- read another buffer
-            this.seek(this.curr_);
-            if (this.curr_ == this.hi_)
+            this.reBuffer();
+            if (this.curr_ == this.hi_ && this.hitEOF_)
                 return -1;
         }
         len = Math.min(len, (int) (this.hi_ - this.curr_));
@@ -325,26 +295,14 @@ public int read(byte[] b, int off, int len) throws IOException
     
     public void write(int b) throws IOException
     {
-        if (this.curr_ >= this.hi_)
-        {
-            if (this.hitEOF_ && this.hi_ < this.maxHi_)
-            {
-                // at EOF -- bump "hi"
-                this.hi_++;
-            }
-            else
+        if (this.lo_ > this.curr_ || this.curr_ > this.hi_ || this.curr_ >= maxHi_)
             {
-                // slow path -- write current buffer; read next one
-                this.seek(this.curr_);
-                if (this.curr_ == this.hi_)
-                {
-                    // appending to EOF -- bump "hi"
-                    this.hi_++;
-                }
-            }
+            this.reBuffer();
         }
         this.buff_[(int) (this.curr_ - this.lo_)] = (byte) b;
         this.curr_++;
+        if (this.curr_ > this.hi_)
+            this.hi_ = this.curr_;
         this.dirty_ = true;
         syncNeeded_ = true;
     }
@@ -368,32 +326,20 @@ public void write(byte[] b, int off, int len) throws IOException
     
     /*
      * Write at most "len" bytes to "b" starting at position "off", and return
-     * the number of bytes written.
+     * the number of bytes written. caller is responsible for setting dirty, syncNeeded.
      */
     private int writeAtMost(byte[] b, int off, int len) throws IOException
     {        
-        if (this.curr_ >= this.hi_)
+        if (this.lo_ > this.curr_ || this.curr_ > this.hi_ || this.curr_ >= maxHi_)
         {
-            if (this.hitEOF_ && this.hi_ < this.maxHi_)
-            {
-                // at EOF -- bump "hi"
-                this.hi_ = this.maxHi_;
+            this.reBuffer();
             }
-            else
-            {                                
-                // slow path -- write current buffer; read next one                
-                this.seek(this.curr_);
-                if (this.curr_ == this.hi_)
-                {
-                    // appending to EOF -- bump "hi"
-                    this.hi_ = this.maxHi_;
-                }
-            }
-        }
-        len = Math.min(len, (int) (this.hi_ - this.curr_));
+        len = Math.min(len, (int) (this.maxHi_ - this.curr_));
         int buffOff = (int) (this.curr_ - this.lo_);
         System.arraycopy(b, off, this.buff_, buffOff, len);
         this.curr_ += len;
+        if (this.curr_ > this.hi_)
+            this.hi_ = this.curr_;
         return len;
     }
 
diff --git a/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableImport.java b/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableImport.java
index 32d17a58..8b74deb2 100644
--- a/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableImport.java
+++ b/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableImport.java
@@ -154,7 +154,6 @@ public static void importJson(String jsonFile, String keyspace, String cf, Strin
         ColumnFamily cfamily = ColumnFamily.create(keyspace, cf);
         ColumnFamilyType cfType = cfamily.getColumnFamilyType();    // Super or Standard
         IPartitioner<?> partitioner = DatabaseDescriptor.getPartitioner();
-        DataOutputBuffer dob = new DataOutputBuffer();
         
         try
         {
@@ -174,9 +173,7 @@ public static void importJson(String jsonFile, String keyspace, String cf, Strin
                 else
                     addToStandardCF((JSONArray)json.get(rowKey.getValue()), cfamily);
                            
-                ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-                writer.append(rowKey.getKey(), dob);
-                dob.reset();
+                writer.append(rowKey.getKey(), cfamily);
                 cfamily.clear();
             }
             
diff --git a/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java b/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java
index eadfc228..b489d620 100644
--- a/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java
+++ b/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java
@@ -85,28 +85,16 @@ public static SSTableReader writeSSTable(Map<String, ColumnFamily> entries) thro
 
     public static SSTableReader writeRawSSTable(String tablename, String cfname, Map<byte[], byte[]> entries) throws IOException
     {
-        return writeRawSSTable(null, tablename, cfname, entries);
-    }
-
-    public static SSTableReader writeRawSSTable(File datafile, String tablename, String cfname, Map<byte[], byte[]> entries) throws IOException
-    {
-        boolean temporary = false;
-        if (datafile == null)
-        {
-            datafile = tempSSTableFile(tablename, cfname);
-            temporary = true;
-        }
+        File datafile = tempSSTableFile(tablename, cfname);
         SSTableWriter writer = new SSTableWriter(datafile.getAbsolutePath(), entries.size(), StorageService.getPartitioner());
         SortedMap<DecoratedKey, byte[]> sortedEntries = new TreeMap<DecoratedKey, byte[]>();
         for (Map.Entry<byte[], byte[]> entry : entries.entrySet())
             sortedEntries.put(writer.partitioner.decorateKey(entry.getKey()), entry.getValue());
         for (Map.Entry<DecoratedKey, byte[]> entry : sortedEntries.entrySet())
             writer.append(entry.getKey(), entry.getValue());
-        if (temporary)
-        {
             new File(writer.indexFilename()).deleteOnExit();
             new File(writer.filterFilename()).deleteOnExit();
-        }
         return writer.closeAndOpenReader();
     }
+
 }
diff --git a/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableExportTest.java b/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
index db306c32..9d9aa634 100644
--- a/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
+++ b/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
@@ -60,21 +60,16 @@ public void testEnumeratekeys() throws IOException
         File tempSS = tempSSTableFile("Keyspace1", "Standard1");
         ColumnFamily cfamily = ColumnFamily.create("Keyspace1", "Standard1");
         IPartitioner<?> partitioner = DatabaseDescriptor.getPartitioner();
-        DataOutputBuffer dob = new DataOutputBuffer();
         SSTableWriter writer = new SSTableWriter(tempSS.getPath(), 2, partitioner);
         
         // Add rowA
         cfamily.addColumn(new QueryPath("Standard1", null, "colA".getBytes()), "valA".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowA"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowA"), cfamily);
         cfamily.clear();
         
         // Add rowB
         cfamily.addColumn(new QueryPath("Standard1", null, "colB".getBytes()), "valB".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowB"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowB"), cfamily);
         cfamily.clear();
      
         writer.closeAndOpenReader();
@@ -98,28 +93,21 @@ public void testExportSimpleCf() throws IOException    {
         File tempSS = tempSSTableFile("Keyspace1", "Standard1");
         ColumnFamily cfamily = ColumnFamily.create("Keyspace1", "Standard1");
         IPartitioner<?> partitioner = DatabaseDescriptor.getPartitioner();
-        DataOutputBuffer dob = new DataOutputBuffer();
         SSTableWriter writer = new SSTableWriter(tempSS.getPath(), 2, partitioner);
         
         // Add rowA
         cfamily.addColumn(new QueryPath("Standard1", null, "colA".getBytes()), "valA".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowA"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowA"), cfamily);
         cfamily.clear();
         
         // Add rowB
         cfamily.addColumn(new QueryPath("Standard1", null, "colB".getBytes()), "valB".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowB"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowB"), cfamily);
         cfamily.clear();
 
         // Add rowExclude
         cfamily.addColumn(new QueryPath("Standard1", null, "colX".getBytes()), "valX".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowExclude"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowExclude"), cfamily);
         cfamily.clear();
 
         SSTableReader reader = writer.closeAndOpenReader();
@@ -148,28 +136,21 @@ public void testExportSuperCf() throws IOException
         File tempSS = tempSSTableFile("Keyspace1", "Super4");
         ColumnFamily cfamily = ColumnFamily.create("Keyspace1", "Super4");
         IPartitioner<?> partitioner = DatabaseDescriptor.getPartitioner();
-        DataOutputBuffer dob = new DataOutputBuffer();
         SSTableWriter writer = new SSTableWriter(tempSS.getPath(), 2, partitioner);
         
         // Add rowA
         cfamily.addColumn(new QueryPath("Super4", "superA".getBytes(), "colA".getBytes()), "valA".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowA"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowA"), cfamily);
         cfamily.clear();
         
         // Add rowB
         cfamily.addColumn(new QueryPath("Super4", "superB".getBytes(), "colB".getBytes()), "valB".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowB"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowB"), cfamily);
         cfamily.clear();
 
         // Add rowExclude
         cfamily.addColumn(new QueryPath("Super4", "superX".getBytes(), "colX".getBytes()), "valX".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowExclude"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowExclude"), cfamily);
         cfamily.clear();
 
         SSTableReader reader = writer.closeAndOpenReader();
@@ -196,21 +177,16 @@ public void testRoundTripStandardCf() throws IOException, ParseException
         File tempSS = tempSSTableFile("Keyspace1", "Standard1");
         ColumnFamily cfamily = ColumnFamily.create("Keyspace1", "Standard1");
         IPartitioner<?> partitioner = DatabaseDescriptor.getPartitioner();
-        DataOutputBuffer dob = new DataOutputBuffer();
         SSTableWriter writer = new SSTableWriter(tempSS.getPath(), 2, partitioner);
         
         // Add rowA
         cfamily.addColumn(new QueryPath("Standard1", null, "name".getBytes()), "val".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowA"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowA"), cfamily);
         cfamily.clear();
 
         // Add rowExclude
         cfamily.addColumn(new QueryPath("Standard1", null, "name".getBytes()), "val".getBytes(), new TimestampClock(1));
-        ColumnFamily.serializer().serializeWithIndexes(cfamily, dob);
-        writer.append(Util.dk("rowExclude"), dob);
-        dob.reset();
+        writer.append(Util.dk("rowExclude"), cfamily);
         cfamily.clear();
 
         SSTableReader reader = writer.closeAndOpenReader();
