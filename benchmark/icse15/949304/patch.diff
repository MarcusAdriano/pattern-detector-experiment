diff --git a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
index 96141dbb..33d0d2f9 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
@@ -285,8 +285,16 @@ public boolean isOptimized() {
    *  contents of the FieldCache, you must override this
    *  method to provide a different key */
   @Override
-  public Object getFieldCacheKey() {
-    return in.getFieldCacheKey();
+  public Object getCoreCacheKey() {
+    return in.getCoreCacheKey();
+  }
+
+  /** If the subclass of FilteredIndexReader modifies the
+   *  deleted docs, you must override this method to provide
+   *  a different key */
+  @Override
+  public Object getDeletesCacheKey() {
+    return in.getDeletesCacheKey();
   }
 
   /** {@inheritDoc} */
diff --git a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/IndexReader.java b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/IndexReader.java
index 2cd8a4e7..a8018afb 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/IndexReader.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/IndexReader.java
@@ -1170,7 +1170,13 @@ public static void main(String [] args) {
   }
 
   /** Expert */
-  public Object getFieldCacheKey() {
+  public Object getCoreCacheKey() {
+    return this;
+  }
+
+  /** Expert.  Warning: this returns null if the reader has
+   *  no deletions */
+  public Object getDeletesCacheKey() {
     return this;
   }
 
diff --git a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/SegmentReader.java
index 1ef4562f..74c068b9 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/index/SegmentReader.java
@@ -1265,10 +1265,15 @@ public Directory directory() {
   // share the underlying postings data) will map to the
   // same entry in the FieldCache.  See LUCENE-1579.
   @Override
-  public final Object getFieldCacheKey() {
+  public final Object getCoreCacheKey() {
     return core.freqStream;
   }
 
+  @Override
+  public Object getDeletesCacheKey() {
+    return deletedDocs;
+  }
+
   @Override
   public long getUniqueTermCount() {
     return core.getTermsReader().size();
diff --git a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/CachingSpanFilter.java b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/CachingSpanFilter.java
index d8284f9c..8c7d80b4 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/CachingSpanFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/CachingSpanFilter.java
@@ -19,9 +19,6 @@
 import org.apache.lucene.index.IndexReader;
 
 import java.io.IOException;
-import java.util.Map;
-import java.util.WeakHashMap;
-import java.util.concurrent.locks.ReentrantLock;
 
 /**
  * Wraps another SpanFilter's result and caches it.  The purpose is to allow
@@ -33,15 +30,32 @@
   /**
    * A transient Filter cache (package private because of test)
    */
-  private transient Map<IndexReader,SpanFilterResult> cache;
-
-  private final ReentrantLock lock = new ReentrantLock();
+  private final CachingWrapperFilter.FilterCache<SpanFilterResult> cache;
 
   /**
+   * New deletions always result in a cache miss, by default
+   * ({@link CachingWrapperFilter.DeletesMode#RECACHE}.
    * @param filter Filter to cache results of
    */
   public CachingSpanFilter(SpanFilter filter) {
+    this(filter, CachingWrapperFilter.DeletesMode.RECACHE);
+  }
+
+  /**
+   * @param filter Filter to cache results of
+   * @param deletesMode See {@link CachingWrapperFilter.DeletesMode}
+   */
+  public CachingSpanFilter(SpanFilter filter, CachingWrapperFilter.DeletesMode deletesMode) {
     this.filter = filter;
+    if (deletesMode == CachingWrapperFilter.DeletesMode.DYNAMIC) {
+      throw new IllegalArgumentException("DeletesMode.DYNAMIC is not supported");
+    }
+    this.cache = new CachingWrapperFilter.FilterCache<SpanFilterResult>(deletesMode) {
+      @Override
+      protected SpanFilterResult mergeDeletes(final IndexReader reader, final SpanFilterResult value) {
+        throw new IllegalStateException("DeletesMode.DYNAMIC is not supported");
+      }
+    };
   }
 
   @Override
@@ -50,26 +64,24 @@ public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
     return result != null ? result.getDocIdSet() : null;
   }
   
+  // for testing
+  int hitCount, missCount;
+
   private SpanFilterResult getCachedResult(IndexReader reader) throws IOException {
-    lock.lock();
-    try {
-      if (cache == null) {
-        cache = new WeakHashMap<IndexReader,SpanFilterResult>();
-      }
-      final SpanFilterResult cached = cache.get(reader);
-      if (cached != null) return cached;
-    } finally {
-      lock.unlock();
-    }
-    
-    final SpanFilterResult result = filter.bitSpans(reader);
-    lock.lock();
-    try {
-      cache.put(reader, result);
-    } finally {
-      lock.unlock();
+
+    final Object coreKey = reader.getCoreCacheKey();
+    final Object delCoreKey = reader.hasDeletions() ? reader.getDeletesCacheKey() : coreKey;
+
+    SpanFilterResult result = cache.get(reader, coreKey, delCoreKey);
+    if (result != null) {
+      hitCount++;
+      return result;
     }
     
+    missCount++;
+    result = filter.bitSpans(reader);
+
+    cache.put(coreKey, delCoreKey, result);
     return result;
   }
 
diff --git a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java
index 5e277e1b..37ad9b7d 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java
@@ -17,10 +17,10 @@
  * limitations under the License.
  */
 
+import java.io.Serializable;
 import java.io.IOException;
 import java.util.Map;
 import java.util.WeakHashMap;
-import java.util.concurrent.locks.ReentrantLock;
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.util.OpenBitSetDISI;
@@ -33,17 +33,126 @@
   Filter filter;
 
   /**
+   * Expert: Specifies how new deletions against a reopened
+   * reader should be handled.
+   *
+   * <p>The default is IGNORE, which means the cache entry
+   * will be re-used for a given segment, even when that
+   * segment has been reopened due to changes in deletions.
+   * This is a big performance gain, especially with
+   * near-real-timer readers, since you don't hit a cache
+   * miss on every reopened reader for prior segments.</p>
+   *
+   * <p>However, in some cases this can cause invalid query
+   * results, allowing deleted documents to be returned.
+   * This only happens if the main query does not rule out
+   * deleted documents on its own, such as a toplevel
+   * ConstantScoreQuery.  To fix this, use RECACHE to
+   * re-create the cached filter (at a higher per-reopen
+   * cost, but at faster subsequent search performance), or
+   * use DYNAMIC to dynamically intersect deleted docs (fast
+   * reopen time but some hit to search performance).</p>
+   */
+  public static enum DeletesMode {IGNORE, RECACHE, DYNAMIC};
+
+  protected final FilterCache<DocIdSet> cache;
+
+  static abstract class FilterCache<T> implements Serializable {
+
+    /**
    * A transient Filter cache (package private because of test)
    */
-  transient Map<IndexReader,DocIdSet> cache;
+    // NOTE: not final so that we can dynamically re-init
+    // after de-serialize
+    transient Map<Object,T> cache;
+
+    private final DeletesMode deletesMode;
+
+    public FilterCache(DeletesMode deletesMode) {
+      this.deletesMode = deletesMode;
+    }
+
+    public synchronized T get(IndexReader reader, Object coreKey, Object delCoreKey) throws IOException {
+      T value;
+
+      if (cache == null) {
+        cache = new WeakHashMap<Object,T>();
+      }
+
+      if (deletesMode == DeletesMode.IGNORE) {
+        // key on core
+        value = cache.get(coreKey);
+      } else if (deletesMode == DeletesMode.RECACHE) {
+        // key on deletes, if any, else core
+        value = cache.get(delCoreKey);
+      } else {
+        assert deletesMode == DeletesMode.DYNAMIC;
+
+        // first try for exact match
+        value = cache.get(delCoreKey);
+
+        if (value == null) {
+          // now for core match, but dynamically AND NOT
+          // deletions
+          value = cache.get(coreKey);
+          if (value != null && reader.hasDeletions()) {
+            value = mergeDeletes(reader, value);
+          }
+        }
+      }
   
-  private final ReentrantLock lock = new ReentrantLock();
+      return value;
+    }
+
+    protected abstract T mergeDeletes(IndexReader reader, T value);
+
+    public synchronized void put(Object coreKey, Object delCoreKey, T value) {
+      if (deletesMode == DeletesMode.IGNORE) {
+        cache.put(coreKey, value);
+      } else if (deletesMode == DeletesMode.RECACHE) {
+        cache.put(delCoreKey, value);
+      } else {
+        cache.put(coreKey, value);
+        cache.put(delCoreKey, value);
+      }
+    }
+  }
 
   /**
+   * New deletes are ignored by default, which gives higher
+   * cache hit rate on reopened readers.  Most of the time
+   * this is safe, because the filter will be AND'd with a
+   * Query that fully enforces deletions.  If instead you
+   * need this filter to always enforce deletions, pass
+   * either {@link DeletesMode#RECACHE} or {@link
+   * DeletesMode#DYNAMIC}.
    * @param filter Filter to cache results of
    */
   public CachingWrapperFilter(Filter filter) {
+    this(filter, DeletesMode.IGNORE);
+  }
+
+  /**
+   * Expert: by default, the cached filter will be shared
+   * across reopened segments that only had changes to their
+   * deletions.  
+   *
+   * @param filter Filter to cache results of
+   * @param deletesMode See {@link DeletesMode}
+   */
+  public CachingWrapperFilter(Filter filter, DeletesMode deletesMode) {
     this.filter = filter;
+    cache = new FilterCache<DocIdSet>(deletesMode) {
+      @Override
+      public DocIdSet mergeDeletes(final IndexReader r, final DocIdSet docIdSet) {
+        return new FilteredDocIdSet(docIdSet) {
+          @Override
+          protected boolean match(int docID) {
+            return !r.isDeleted(docID);
+          }
+        };
+      }
+    };
   }
 
   /** Provide the DocIdSet to be cached, using the DocIdSet provided
@@ -67,28 +176,28 @@ protected DocIdSet docIdSetToCache(DocIdSet docIdSet, IndexReader reader) throws
     }
   }
   
+  // for testing
+  int hitCount, missCount;
+
   @Override
   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
-    lock.lock();
-    try {
-      if (cache == null) {
-        cache = new WeakHashMap<IndexReader,DocIdSet>();
-      }
 
-      final DocIdSet cached = cache.get(reader);
-      if (cached != null) return cached;
-    } finally {
-      lock.unlock();
-    }
+    final Object coreKey = reader.getCoreCacheKey();
+    final Object delCoreKey = reader.hasDeletions() ? reader.getDeletesCacheKey() : coreKey;
 
-    final DocIdSet docIdSet = docIdSetToCache(filter.getDocIdSet(reader), reader);
+    DocIdSet docIdSet = cache.get(reader, coreKey, delCoreKey);
     if (docIdSet != null) {
-      lock.lock();
-      try {
-        cache.put(reader, docIdSet);
-      } finally {
-        lock.unlock();
+      hitCount++;
+      return docIdSet;
       }
+
+    missCount++;
+
+    // cache miss
+    docIdSet = docIdSetToCache(filter.getDocIdSet(reader), reader);
+
+    if (docIdSet != null) {
+      cache.put(coreKey, delCoreKey, docIdSet);
     }
     
     return docIdSet;
diff --git a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java
index 929c9195..06e95845 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java
@@ -151,7 +151,7 @@ protected abstract Object createValue(IndexReader reader, Entry key)
 
     /** Remove this reader from the cache, if present. */
     public void purge(IndexReader r) {
-      Object readerKey = r.getFieldCacheKey();
+      Object readerKey = r.getCoreCacheKey();
       synchronized(readerCache) {
         readerCache.remove(readerKey);
       }
@@ -160,7 +160,7 @@ public void purge(IndexReader r) {
     public Object get(IndexReader reader, Entry key) throws IOException {
       Map<Entry,Object> innerCache;
       Object value;
-      final Object readerKey = reader.getFieldCacheKey();
+      final Object readerKey = reader.getCoreCacheKey();
       synchronized (readerCache) {
         innerCache = readerCache.get(readerKey);
         if (innerCache == null) {
diff --git a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/IndexSearcher.java b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/IndexSearcher.java
index 0c0a3a91..c09801f5 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/IndexSearcher.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/search/IndexSearcher.java
@@ -165,7 +165,7 @@ public TopDocs search(Weight weight, Filter filter, int nDocs) throws IOExceptio
       throw new IllegalArgumentException("nDocs must be > 0");
     }
 
-    nDocs = Math.min(nDocs, reader.numDocs());
+    nDocs = Math.min(nDocs, reader.maxDoc());
 
     TopScoreDocCollector collector = TopScoreDocCollector.create(nDocs, !weight.scoresDocsOutOfOrder());
     search(weight, filter, collector);
@@ -193,7 +193,7 @@ public TopFieldDocs search(Weight weight, Filter filter, int nDocs,
                              Sort sort, boolean fillFields)
       throws IOException {
 
-    nDocs = Math.min(nDocs, reader.numDocs());
+    nDocs = Math.min(nDocs, reader.maxDoc());
 
     TopFieldCollector collector = TopFieldCollector.create(sort, nDocs,
         fillFields, fieldSortDoTrackScores, fieldSortDoMaxScore, !weight.scoresDocsOutOfOrder());
diff --git a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
index 7faf3686..6d53dcd2 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
@@ -274,7 +274,7 @@ public void setRamUsageEstimator(RamUsageEstimator r) {
       if (obj instanceof IndexReader) {
         IndexReader[] subs = ((IndexReader)obj).getSequentialSubReaders();
         for (int j = 0; (null != subs) && (j < subs.length); j++) {
-          all.add(subs[j].getFieldCacheKey());
+          all.add(subs[j].getCoreCacheKey());
         }
       }
       
diff --git a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/CachingWrapperFilterHelper.java b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/CachingWrapperFilterHelper.java
index 2d4100bb..571d7720 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/CachingWrapperFilterHelper.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/CachingWrapperFilterHelper.java
@@ -25,7 +25,6 @@
  */
 
 import java.io.IOException;
-import java.util.WeakHashMap;
 
 import junit.framework.Assert;
 import org.apache.lucene.index.IndexReader;
@@ -49,30 +48,18 @@ public void setShouldHaveCache(boolean shouldHaveCache) {
   }
   
   @Override
-  public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
-    if (cache == null) {
-      cache = new WeakHashMap<IndexReader,DocIdSet>();
-    }
+  public synchronized DocIdSet getDocIdSet(IndexReader reader) throws IOException {
+
+    final int saveMissCount = missCount;
+    DocIdSet docIdSet = super.getDocIdSet(reader);
     
-    synchronized (cache) {  // check cache
-      DocIdSet cached = cache.get(reader);
       if (shouldHaveCache) {
-        Assert.assertNotNull("Cache should have data ", cached);
+      Assert.assertEquals("Cache should have data ", saveMissCount, missCount);
       } else {
-        Assert.assertNull("Cache should be null " + cached , cached);
-      }
-      if (cached != null) {
-        return cached;
-      }
-    }
-
-    final DocIdSet bits = filter.getDocIdSet(reader);
-
-    synchronized (cache) {  // update cache
-      cache.put(reader, bits);
+      Assert.assertTrue("Cache should be null " + docIdSet, missCount > saveMissCount);
     }
 
-    return bits;
+    return docIdSet;
   }
 
   @Override
diff --git a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java
index e69de29b..e520a797 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java
@@ -0,0 +1,123 @@
+package org.apache.lucene.search;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.spans.SpanTermQuery;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockRAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TestCachingSpanFilter extends LuceneTestCase {
+
+  public void testEnforceDeletions() throws Exception {
+    Directory dir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer()));
+    IndexReader reader = writer.getReader();
+    IndexSearcher searcher = new IndexSearcher(reader);
+
+    // add a doc, refresh the reader, and check that its there
+    Document doc = new Document();
+    doc.add(new Field("id", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+    writer.addDocument(doc);
+
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+
+    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);
+    assertEquals("Should find a hit...", 1, docs.totalHits);
+
+    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term("id", "1")));
+
+    // ignore deletions
+    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);
+        
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
+    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+
+    // now delete the doc, refresh the reader, and see that it's not there
+    writer.deleteDocuments(new Term("id", "1"));
+
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
+
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+
+
+    // force cache to regenerate:
+    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);
+
+    writer.addDocument(doc);
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+        
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
+
+    constantScore = new ConstantScoreQuery(filter);
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+
+    // make sure we get a cache hit when we reopen readers
+    // that had no new deletions
+    IndexReader newReader = refreshReader(reader);
+    assertTrue(reader != newReader);
+    reader = newReader;
+    searcher = new IndexSearcher(reader);
+    int missCount = filter.missCount;
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+    assertEquals(missCount, filter.missCount);
+
+    // now delete the doc, refresh the reader, and see that it's not there
+    writer.deleteDocuments(new Term("id", "1"));
+
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
+
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should *not* find a hit...", 0, docs.totalHits);
+  }
+
+  private static IndexReader refreshReader(IndexReader reader) throws IOException {
+    IndexReader oldReader = reader;
+    reader = reader.reopen();
+    if (reader != oldReader) {
+      oldReader.close();
+    }
+    return reader;
+  }
+}
diff --git a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
index e1000504..903b7213 100644
--- a/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
+++ b/lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
@@ -20,12 +20,16 @@
 import java.io.IOException;
 
 import org.apache.lucene.analysis.KeywordAnalyzer;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.Term;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.store.MockRAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.OpenBitSet;
 import org.apache.lucene.util.OpenBitSetDISI;
@@ -143,4 +147,123 @@ public DocIdSet getDocIdSet(IndexReader reader) {
 
     reader.close();
   }
+
+  public void testEnforceDeletions() throws Exception {
+    Directory dir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer()));
+    IndexReader reader = writer.getReader();
+    IndexSearcher searcher = new IndexSearcher(reader);
+
+    // add a doc, refresh the reader, and check that its there
+    Document doc = new Document();
+    doc.add(new Field("id", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+    writer.addDocument(doc);
+
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+
+    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);
+    assertEquals("Should find a hit...", 1, docs.totalHits);
+
+    final Filter startFilter = new QueryWrapperFilter(new TermQuery(new Term("id", "1")));
+
+    // ignore deletions
+    CachingWrapperFilter filter = new CachingWrapperFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);
+        
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
+    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+
+    // now delete the doc, refresh the reader, and see that it's not there
+    writer.deleteDocuments(new Term("id", "1"));
+
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
+
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+
+
+    // force cache to regenerate:
+    filter = new CachingWrapperFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);
+
+    writer.addDocument(doc);
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+        
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
+
+    constantScore = new ConstantScoreQuery(filter);
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+
+    // make sure we get a cache hit when we reopen reader
+    // that had no change to deletions
+    IndexReader newReader = refreshReader(reader);
+    assertTrue(reader != newReader);
+    reader = newReader;
+    searcher = new IndexSearcher(reader);
+    int missCount = filter.missCount;
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+    assertEquals(missCount, filter.missCount);
+
+    // now delete the doc, refresh the reader, and see that it's not there
+    writer.deleteDocuments(new Term("id", "1"));
+
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+
+    missCount = filter.missCount;
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals(missCount+1, filter.missCount);
+    assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should *not* find a hit...", 0, docs.totalHits);
+
+
+    // apply deletions dynamically
+    filter = new CachingWrapperFilter(startFilter, CachingWrapperFilter.DeletesMode.DYNAMIC);
+
+    writer.addDocument(doc);
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+        
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
+    constantScore = new ConstantScoreQuery(filter);
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+
+    // now delete the doc, refresh the reader, and see that it's not there
+    writer.deleteDocuments(new Term("id", "1"));
+
+    reader = refreshReader(reader);
+    searcher = new IndexSearcher(reader);
+
+    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
+    assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
+
+    missCount = filter.missCount;
+    docs = searcher.search(constantScore, 1);
+    assertEquals("[just filter] Should *not* find a hit...", 0, docs.totalHits);
+
+    // doesn't count as a miss
+    assertEquals(missCount, filter.missCount);
+  }
+
+  private static IndexReader refreshReader(IndexReader reader) throws IOException {
+    IndexReader oldReader = reader;
+    reader = reader.reopen();
+    if (reader != oldReader) {
+      oldReader.close();
+    }
+    return reader;
+  }
 }
diff --git a/lucene/dev/branches/branch_3x/solr/src/java/org/apache/solr/search/SolrIndexReader.java b/lucene/dev/branches/branch_3x/solr/src/java/org/apache/solr/search/SolrIndexReader.java
index edf64c1a..ad6edacc 100644
--- a/lucene/dev/branches/branch_3x/solr/src/java/org/apache/solr/search/SolrIndexReader.java
+++ b/lucene/dev/branches/branch_3x/solr/src/java/org/apache/solr/search/SolrIndexReader.java
@@ -507,8 +507,8 @@ public void undeleteAll() throws StaleReaderException, CorruptIndexException, Lo
   }
 
   @Override
-  public Object getFieldCacheKey() {
-    return in.getFieldCacheKey();
+  public Object getCoreCacheKey() {
+    return in.getCoreCacheKey();
   }
 
   @Override
