diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java b/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
index 56c88ce4..063388fb 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
@@ -23,7 +23,7 @@
 /** Base implementation class for buffered {@link IndexInput}. */
 public abstract class BufferedIndexInput extends IndexInput {
 
-  /** Default buffer size set to 1024*/
+  /** Default buffer size set to {@value #BUFFER_SIZE}. */
   public static final int BUFFER_SIZE = 1024;
   
   // The normal read buffer size defaults to 1024, but
@@ -33,7 +33,7 @@
   // BufferedIndexInputs created during merging.  See
   // LUCENE-888 for details.
   /**
-   * A buffer size for merges set to 4096
+   * A buffer size for merges set to {@value #MERGE_BUFFER_SIZE}.
    */
   public static final int MERGE_BUFFER_SIZE = 4096;
 
@@ -115,15 +115,14 @@ public final void readBytes(byte[] b, int offset, int len) throws IOException {
 
   @Override
   public final void readBytes(byte[] b, int offset, int len, boolean useBuffer) throws IOException {
-
-    if(len <= (bufferLength-bufferPosition)){
+    int available = bufferLength - bufferPosition;
+    if(len <= available){
       // the buffer contains enough data to satisfy this request
       if(len>0) // to allow b to be null if len is 0...
         System.arraycopy(buffer, bufferPosition, b, offset, len);
       bufferPosition+=len;
     } else {
       // the buffer does not have enough data. First serve all we've got.
-      int available = bufferLength - bufferPosition;
       if(available > 0){
         System.arraycopy(buffer, bufferPosition, b, offset, available);
         offset += available;
diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java b/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
index 70ab4288..92d9a820 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
@@ -40,6 +40,7 @@
 
 import org.apache.lucene.util.ThreadInterruptedException;
 import org.apache.lucene.util.Constants;
+import org.apache.lucene.util.IOUtils;
 
 /**
  * Base class for Directory implementations that store index
@@ -121,13 +122,16 @@
 public abstract class FSDirectory extends Directory {
 
   /**
-   * Default read chunk size: 2*{@link BufferedIndexInput#MERGE_BUFFER_SIZE}.
+   * Default read chunk size: 8192 bytes (this is the size up to which the JDK
+     does not allocate additional arrays while reading/writing)
+     @deprecated This constant is no longer used since Lucene 4.5.
    */
-  public static final int DEFAULT_READ_CHUNK_SIZE = BufferedIndexInput.MERGE_BUFFER_SIZE * 2;
+  @Deprecated
+  public static final int DEFAULT_READ_CHUNK_SIZE = 8192;
 
   protected final File directory; // The underlying filesystem directory
   protected final Set<String> staleFiles = synchronizedSet(new HashSet<String>()); // Files written, but not yet sync'ed
-  private int chunkSize = DEFAULT_READ_CHUNK_SIZE; // LUCENE-1566
+  private int chunkSize = DEFAULT_READ_CHUNK_SIZE;
 
   // returns the canonical version of the directory, creating it if it doesn't exist.
   private static File getCanonicalPath(File file) throws IOException {
@@ -357,24 +361,11 @@ public String toString() {
   }
 
   /**
-   * Sets the maximum number of bytes read at once from the
-   * underlying file during {@link IndexInput#readBytes}.
-   * The default value is {@link #DEFAULT_READ_CHUNK_SIZE};
-   *
-   * <p> This was introduced due to <a
-   * href="http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6478546">Sun
-   * JVM Bug 6478546</a>, which throws an incorrect
-   * OutOfMemoryError when attempting to read too many bytes
-   * at once.  It only happens on 32bit JVMs with a large
-   * maximum heap size.</p>
-   *
-   * <p>Changes to this value will not impact any
-   * already-opened {@link IndexInput}s.  You should call
-   * this before attempting to open an index on the
-   * directory.</p>
+   * This setting has no effect anymore.
+   * @deprecated This is no longer used since Lucene 4.5.
    */
+  @Deprecated
   public final void setReadChunkSize(int chunkSize) {
-    // LUCENE-1566
     if (chunkSize <= 0) {
       throw new IllegalArgumentException("chunkSize must be positive");
     }
@@ -382,12 +373,11 @@ public final void setReadChunkSize(int chunkSize) {
   }
 
   /**
-   * The maximum number of bytes to read at once from the
-   * underlying file during {@link IndexInput#readBytes}.
-   * @see #setReadChunkSize
+   * This setting has no effect anymore.
+   * @deprecated This is no longer used since Lucene 4.5.
    */
+  @Deprecated
   public final int getReadChunkSize() {
-    // LUCENE-1566
     return chunkSize;
   }
 
@@ -396,27 +386,23 @@ public final int getReadChunkSize() {
     /** the underlying RandomAccessFile */
     protected final RandomAccessFile file;
     boolean isClone = false;
-    /** maximum read length on a 32bit JVM to prevent incorrect OOM, see LUCENE-1566 */ 
-    protected final int chunkSize;
     /** start offset: non-zero in the slice case */
     protected final long off;
     /** end offset (start+length) */
     protected final long end;
     
     /** Create a new FSIndexInput, reading the entire file from <code>path</code> */
-    protected FSIndexInput(String resourceDesc, File path, IOContext context, int chunkSize) throws IOException {
+    protected FSIndexInput(String resourceDesc, File path, IOContext context) throws IOException {
       super(resourceDesc, context);
       this.file = new RandomAccessFile(path, "r"); 
-      this.chunkSize = chunkSize;
       this.off = 0L;
       this.end = file.length();
     }
     
     /** Create a new FSIndexInput, representing a slice of an existing open <code>file</code> */
-    protected FSIndexInput(String resourceDesc, RandomAccessFile file, long off, long length, int bufferSize, int chunkSize) {
+    protected FSIndexInput(String resourceDesc, RandomAccessFile file, long off, long length, int bufferSize) {
       super(resourceDesc, bufferSize);
       this.file = file;
-      this.chunkSize = chunkSize;
       this.off = off;
       this.end = off + length;
       this.isClone = true; // well, we are sorta?
@@ -454,23 +440,35 @@ boolean isFDValid() throws IOException {
    * Writes output with {@link RandomAccessFile#write(byte[], int, int)}
    */
   protected static class FSIndexOutput extends BufferedIndexOutput {
+    /**
+     * The maximum chunk size is 8192 bytes, because {@link RandomAccessFile} mallocs
+     * a native buffer outside of stack if the write buffer size is larger.
+     */
+    private static final int CHUNK_SIZE = 8192;
+    
     private final FSDirectory parent;
     private final String name;
     private final RandomAccessFile file;
     private volatile boolean isOpen; // remember if the file is open, so that we don't try to close it more than once
     
     public FSIndexOutput(FSDirectory parent, String name) throws IOException {
+      super(CHUNK_SIZE);
       this.parent = parent;
       this.name = name;
       file = new RandomAccessFile(new File(parent.directory, name), "rw");
       isOpen = true;
     }
 
-    /** output methods: */
     @Override
-    public void flushBuffer(byte[] b, int offset, int size) throws IOException {
+    protected void flushBuffer(byte[] b, int offset, int size) throws IOException {
       assert isOpen;
-      file.write(b, offset, size);
+      while (size > 0) {
+        final int toWrite = Math.min(CHUNK_SIZE, size);
+        file.write(b, offset, toWrite);
+        offset += toWrite;
+        size -= toWrite;
+      }
+      assert size == 0;
     }
     
     @Override
@@ -478,21 +476,14 @@ public void close() throws IOException {
       parent.onIndexOutputClosed(this);
       // only close the file if it has not been closed yet
       if (isOpen) {
-        boolean success = false;
+        IOException priorE = null;
         try {
           super.close();
-          success = true;
+        } catch (IOException ioe) {
+          priorE = ioe;
         } finally {
           isOpen = false;
-          if (!success) {
-            try {
-              file.close();
-            } catch (Throwable t) {
-              // Suppress so we don't mask original exception
-            }
-          } else {
-            file.close();
-          }
+          IOUtils.closeWhileHandlingException(priorE, file);
         }
       }
     }
diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.java b/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.java
index f18797f6..17223151 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.java
@@ -77,7 +77,7 @@ public NIOFSDirectory(File path) throws IOException {
   @Override
   public IndexInput openInput(String name, IOContext context) throws IOException {
     ensureOpen();
-    return new NIOFSIndexInput(new File(getDirectory(), name), context, getReadChunkSize());
+    return new NIOFSIndexInput(new File(getDirectory(), name), context);
   }
   
   @Override
@@ -96,7 +96,7 @@ public void close() throws IOException {
       @Override
       public IndexInput openSlice(String sliceDescription, long offset, long length) {
         return new NIOFSIndexInput(sliceDescription, path, descriptor, descriptor.getChannel(), offset,
-            length, BufferedIndexInput.bufferSize(context), getReadChunkSize());
+            length, BufferedIndexInput.bufferSize(context));
       }
 
       @Override
@@ -114,18 +114,22 @@ public IndexInput openFullSlice() {
    * Reads bytes with {@link FileChannel#read(ByteBuffer, long)}
    */
   protected static class NIOFSIndexInput extends FSIndexInput {
+    /**
+     * The maximum chunk size for reads of 16384 bytes.
+     */
+    private static final int CHUNK_SIZE = 16384;
 
     private ByteBuffer byteBuf; // wraps the buffer for NIO
 
     final FileChannel channel;
 
-    public NIOFSIndexInput(File path, IOContext context, int chunkSize) throws IOException {
-      super("NIOFSIndexInput(path=\"" + path + "\")", path, context, chunkSize);
+    public NIOFSIndexInput(File path, IOContext context) throws IOException {
+      super("NIOFSIndexInput(path=\"" + path + "\")", path, context);
       channel = file.getChannel();
     }
     
-    public NIOFSIndexInput(String sliceDescription, File path, RandomAccessFile file, FileChannel fc, long off, long length, int bufferSize, int chunkSize) {
-      super("NIOFSIndexInput(" + sliceDescription + " in path=\"" + path + "\" slice=" + off + ":" + (off+length) + ")", file, off, length, bufferSize, chunkSize);
+    public NIOFSIndexInput(String sliceDescription, File path, RandomAccessFile file, FileChannel fc, long off, long length, int bufferSize) {
+      super("NIOFSIndexInput(" + sliceDescription + " in path=\"" + path + "\" slice=" + off + ":" + (off+length) + ")", file, off, length, bufferSize);
       channel = fc;
       isClone = true;
     }
@@ -138,24 +142,18 @@ protected void newBuffer(byte[] newBuffer) {
 
     @Override
     protected void readInternal(byte[] b, int offset, int len) throws IOException {
-
       final ByteBuffer bb;
 
       // Determine the ByteBuffer we should use
-      if (b == buffer && 0 == offset) {
+      if (b == buffer) {
         // Use our own pre-wrapped byteBuf:
         assert byteBuf != null;
-        byteBuf.clear();
-        byteBuf.limit(len);
         bb = byteBuf;
+        byteBuf.clear().position(offset);
       } else {
         bb = ByteBuffer.wrap(b, offset, len);
       }
 
-      int readOffset = bb.position();
-      int readLength = bb.limit() - readOffset;
-      assert readLength == len;
-
       long pos = getFilePointer() + off;
       
       if (pos + len > end) {
@@ -163,33 +161,20 @@ protected void readInternal(byte[] b, int offset, int len) throws IOException {
       }
 
       try {
+        int readLength = len;
         while (readLength > 0) {
-          final int limit;
-          if (readLength > chunkSize) {
-            // LUCENE-1566 - work around JVM Bug by breaking
-            // very large reads into chunks
-            limit = readOffset + chunkSize;
-          } else {
-            limit = readOffset + readLength;
-          }
-          bb.limit(limit);
-          int i = channel.read(bb, pos);
-          if (i < 0){//be defensive here, even though we checked before hand, something could have changed
-            throw new EOFException("read past EOF: " + this + " off: " + offset + " len: " + len + " pos: " + pos + " limit: " + limit + " end: " + end);
+          final int toRead = Math.min(CHUNK_SIZE, readLength);
+          bb.limit(bb.position() + toRead);
+          assert bb.remaining() == toRead;
+          final int i = channel.read(bb, pos);
+          if (i < 0) { // be defensive here, even though we checked before hand, something could have changed
+            throw new EOFException("read past EOF: " + this + " off: " + offset + " len: " + len + " pos: " + pos + " chunkLen: " + toRead + " end: " + end);
           }
+          assert i > 0 : "FileChannel.read with non zero-length bb.remaining() must always read at least one byte (FileChannel is in blocking mode, see spec of ReadableByteChannel)";
           pos += i;
-          readOffset += i;
           readLength -= i;
         }
-      } catch (OutOfMemoryError e) {
-        // propagate OOM up and add a hint for 32bit VM Users hitting the bug
-        // with a large chunk size in the fast path.
-        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
-              "OutOfMemoryError likely caused by the Sun VM Bug described in "
-              + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a value smaller than the current chunk size (" + chunkSize + ")");
-        outOfMemoryError.initCause(e);
-        throw outOfMemoryError;
+        assert readLength == 0;
       } catch (IOException ioe) {
         throw new IOException(ioe.getMessage() + ": " + this, ioe);
       }
diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.java b/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.java
index 082c90d8..6897df95 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.java
@@ -55,7 +55,7 @@ public SimpleFSDirectory(File path) throws IOException {
   public IndexInput openInput(String name, IOContext context) throws IOException {
     ensureOpen();
     final File path = new File(directory, name);
-    return new SimpleFSIndexInput("SimpleFSIndexInput(path=\"" + path.getPath() + "\")", path, context, getReadChunkSize());
+    return new SimpleFSIndexInput("SimpleFSIndexInput(path=\"" + path.getPath() + "\")", path, context);
   }
 
   @Override
@@ -74,7 +74,7 @@ public void close() throws IOException {
       @Override
       public IndexInput openSlice(String sliceDescription, long offset, long length) {
         return new SimpleFSIndexInput("SimpleFSIndexInput(" + sliceDescription + " in path=\"" + file.getPath() + "\" slice=" + offset + ":" + (offset+length) + ")", descriptor, offset,
-            length, BufferedIndexInput.bufferSize(context), getReadChunkSize());
+            length, BufferedIndexInput.bufferSize(context));
       }
 
       @Override
@@ -93,13 +93,18 @@ public IndexInput openFullSlice() {
    * {@link RandomAccessFile#read(byte[], int, int)}.  
    */
   protected static class SimpleFSIndexInput extends FSIndexInput {
+    /**
+     * The maximum chunk size is 8192 bytes, because {@link RandomAccessFile} mallocs
+     * a native buffer outside of stack if the read buffer size is larger.
+     */
+    private static final int CHUNK_SIZE = 8192;
   
-    public SimpleFSIndexInput(String resourceDesc, File path, IOContext context, int chunkSize) throws IOException {
-      super(resourceDesc, path, context, chunkSize);
+    public SimpleFSIndexInput(String resourceDesc, File path, IOContext context) throws IOException {
+      super(resourceDesc, path, context);
     }
     
-    public SimpleFSIndexInput(String resourceDesc, RandomAccessFile file, long off, long length, int bufferSize, int chunkSize) {
-      super(resourceDesc, file, off, length, bufferSize, chunkSize);
+    public SimpleFSIndexInput(String resourceDesc, RandomAccessFile file, long off, long length, int bufferSize) {
+      super(resourceDesc, file, off, length, bufferSize);
     }
   
     /** IndexInput methods */
@@ -116,29 +121,16 @@ protected void readInternal(byte[] b, int offset, int len)
         }
 
         try {
-          do {
-            final int readLength;
-            if (total + chunkSize > len) {
-              readLength = len - total;
-            } else {
-              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks
-              readLength = chunkSize;
-            }
-            final int i = file.read(b, offset + total, readLength);
-            if (i < 0){//be defensive here, even though we checked before hand, something could have changed
-             throw new EOFException("read past EOF: " + this + " off: " + offset + " len: " + len + " total: " + total + " readLen: " + readLength + " end: " + end);
+          while (total < len) {
+            final int toRead = Math.min(CHUNK_SIZE, len - total);
+            final int i = file.read(b, offset + total, toRead);
+            if (i < 0) { // be defensive here, even though we checked before hand, something could have changed
+             throw new EOFException("read past EOF: " + this + " off: " + offset + " len: " + len + " total: " + total + " chunkLen: " + toRead + " end: " + end);
             }
+            assert i > 0 : "RandomAccessFile.read with non zero-length toRead must always read at least one byte";
             total += i;
-          } while (total < len);
-        } catch (OutOfMemoryError e) {
-          // propagate OOM up and add a hint for 32bit VM Users hitting the bug
-          // with a large chunk size in the fast path.
-          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
-              "OutOfMemoryError likely caused by the Sun VM Bug described in "
-              + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a value smaller than the current chunk size (" + chunkSize + ")");
-          outOfMemoryError.initCause(e);
-          throw outOfMemoryError;
+          }
+          assert total == len;
         } catch (IOException ioe) {
           throw new IOException(ioe.getMessage() + ": " + this, ioe);
         }
diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
index 03a140f2..e83f9c51 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
@@ -91,24 +91,6 @@ public void testReadByte() throws Exception {
   public void testReadBytes() throws Exception {
     MyBufferedIndexInput input = new MyBufferedIndexInput();
     runReadBytes(input, BufferedIndexInput.BUFFER_SIZE, random());
-
-    // This tests the workaround code for LUCENE-1566 where readBytesInternal
-    // provides a workaround for a JVM Bug that incorrectly raises a OOM Error
-    // when a large byte buffer is passed to a file read.
-    // NOTE: this does only test the chunked reads and NOT if the Bug is triggered.
-    //final int tmpFileSize = 1024 * 1024 * 5;
-    final int inputBufferSize = 128;
-    File tmpInputFile = _TestUtil.createTempFile("IndexInput", "tmpFile", TEMP_DIR);
-    tmpInputFile.deleteOnExit();
-    writeBytes(tmpInputFile, TEST_FILE_LENGTH);
-
-    // run test with chunk size of 10 bytes
-    runReadBytesAndClose(new SimpleFSIndexInput("SimpleFSIndexInput(path=\"" + tmpInputFile + "\")", tmpInputFile,
-        newIOContext(random()), 10), inputBufferSize, random());
-
-    // run test with chunk size of 10 bytes
-    runReadBytesAndClose(new NIOFSIndexInput(tmpInputFile,
-        newIOContext(random()), 10), inputBufferSize, random());
   }
 
   private void runReadBytesAndClose(IndexInput input, int bufferSize, Random r)
@@ -218,6 +200,7 @@ public void testEOF() throws Exception {
     private static byte byten(long n){
       return (byte)(n*n%256);
     }
+    
     private static class MyBufferedIndexInput extends BufferedIndexInput {
       private long pos;
       private long len;
diff --git a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/store/TestDirectory.java b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/store/TestDirectory.java
index 540362df..e424ac6d 100644
--- a/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/store/TestDirectory.java
+++ b/lucene/dev/branches/branch_4x/lucene/core/src/test/org/apache/lucene/store/TestDirectory.java
@@ -133,52 +133,66 @@ public void run() {
   // Test that different instances of FSDirectory can coexist on the same
   // path, can read, write, and lock files.
   public void testDirectInstantiation() throws Exception {
-    File path = _TestUtil.getTempDir("testDirectInstantiation");
+    final File path = _TestUtil.getTempDir("testDirectInstantiation");
 
-    int sz = 3;
-    Directory[] dirs = new Directory[sz];
+    final byte[] largeBuffer = new byte[random().nextInt(256*1024)], largeReadBuffer = new byte[largeBuffer.length];
+    for (int i = 0; i < largeBuffer.length; i++) {
+      largeBuffer[i] = (byte) i; // automatically loops with modulo
+    }
 
-    dirs[0] = new SimpleFSDirectory(path, null);
-    dirs[1] = new NIOFSDirectory(path, null);
-    dirs[2] = new MMapDirectory(path, null);
+    final FSDirectory[] dirs = new FSDirectory[] {
+      new SimpleFSDirectory(path, null),
+      new NIOFSDirectory(path, null),
+      new MMapDirectory(path, null)
+    };
 
-    for (int i=0; i<sz; i++) {
-      Directory dir = dirs[i];
+    for (int i=0; i<dirs.length; i++) {
+      FSDirectory dir = dirs[i];
       dir.ensureOpen();
       String fname = "foo." + i;
       String lockname = "foo" + i + ".lck";
       IndexOutput out = dir.createOutput(fname, newIOContext(random()));
       out.writeByte((byte)i);
+      out.writeBytes(largeBuffer, largeBuffer.length);
       out.close();
 
-      for (int j=0; j<sz; j++) {
-        Directory d2 = dirs[j];
+      for (int j=0; j<dirs.length; j++) {
+        FSDirectory d2 = dirs[j];
         d2.ensureOpen();
         assertTrue(d2.fileExists(fname));
-        assertEquals(1, d2.fileLength(fname));
+        assertEquals(1 + largeBuffer.length, d2.fileLength(fname));
 
-        // don't test read on MMapDirectory, since it can't really be
-        // closed and will cause a failure to delete the file.
-        if (d2 instanceof MMapDirectory) continue;
+        // don't do read tests if unmapping is not supported!
+        if (d2 instanceof MMapDirectory && !((MMapDirectory) d2).getUseUnmap())
+          continue;
         
         IndexInput input = d2.openInput(fname, newIOContext(random()));
         assertEquals((byte)i, input.readByte());
+        // read array with buffering enabled
+        Arrays.fill(largeReadBuffer, (byte)0);
+        input.readBytes(largeReadBuffer, 0, largeReadBuffer.length, true);
+        assertArrayEquals(largeBuffer, largeReadBuffer);
+        // read again without using buffer
+        input.seek(1L);
+        Arrays.fill(largeReadBuffer, (byte)0);
+        input.readBytes(largeReadBuffer, 0, largeReadBuffer.length, false);
+        assertArrayEquals(largeBuffer, largeReadBuffer);        
         input.close();
       }
 
       // delete with a different dir
-      dirs[(i+1)%sz].deleteFile(fname);
+      dirs[(i+1)%dirs.length].deleteFile(fname);
 
-      for (int j=0; j<sz; j++) {
-        Directory d2 = dirs[j];
+      for (int j=0; j<dirs.length; j++) {
+        FSDirectory d2 = dirs[j];
         assertFalse(d2.fileExists(fname));
       }
 
       Lock lock = dir.makeLock(lockname);
       assertTrue(lock.obtain());
 
-      for (int j=0; j<sz; j++) {
-        Directory d2 = dirs[j];
+      for (int j=0; j<dirs.length; j++) {
+        FSDirectory d2 = dirs[j];
         Lock lock2 = d2.makeLock(lockname);
         try {
           assertFalse(lock2.obtain(1));
@@ -190,13 +204,13 @@ public void testDirectInstantiation() throws Exception {
       lock.release();
       
       // now lock with different dir
-      lock = dirs[(i+1)%sz].makeLock(lockname);
+      lock = dirs[(i+1)%dirs.length].makeLock(lockname);
       assertTrue(lock.obtain());
       lock.release();
     }
 
-    for (int i=0; i<sz; i++) {
-      Directory dir = dirs[i];
+    for (int i=0; i<dirs.length; i++) {
+      FSDirectory dir = dirs[i];
       dir.ensureOpen();
       dir.close();
       assertFalse(dir.isOpen);
diff --git a/lucene/dev/branches/branch_4x/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java b/lucene/dev/branches/branch_4x/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
index b0f34d49..de0e17b5 100644
--- a/lucene/dev/branches/branch_4x/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/dev/branches/branch_4x/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -1168,7 +1168,6 @@ private static Directory newFSDirectoryImpl(
     } catch (Exception e) {
       Rethrow.rethrow(e);
     }
-    d.setReadChunkSize(_TestUtil.nextInt(random(), 8, 32678));
     return d;
   }
 
