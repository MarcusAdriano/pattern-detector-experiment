diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java b/lucene/dev/branches/solrcloud/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java
index 2c318d51..8a9447bc 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java
@@ -28,13 +28,28 @@
  */
 public class SimpleFragListBuilder implements FragListBuilder {
   
-  public static final int MARGIN = 6;
-  public static final int MIN_FRAG_CHAR_SIZE = MARGIN * 3;
+  public static final int MARGIN_DEFAULT = 6;
+  public static final int MIN_FRAG_CHAR_SIZE_FACTOR = 3;
+
+  final int margin;
+  final int minFragCharSize;
+
+  public SimpleFragListBuilder( int margin ){
+    if( margin < 0 )
+      throw new IllegalArgumentException( "margin(" + margin + ") is too small. It must be 0 or higher." );
+
+    this.margin = margin;
+    this.minFragCharSize = Math.max( 1, margin * MIN_FRAG_CHAR_SIZE_FACTOR );
+  }
+
+  public SimpleFragListBuilder(){
+    this( MARGIN_DEFAULT );
+  }
 
   public FieldFragList createFieldFragList(FieldPhraseList fieldPhraseList, int fragCharSize) {
-    if( fragCharSize < MIN_FRAG_CHAR_SIZE )
+    if( fragCharSize < minFragCharSize )
       throw new IllegalArgumentException( "fragCharSize(" + fragCharSize + ") is too small. It must be " +
-          MIN_FRAG_CHAR_SIZE + " or higher." );
+          minFragCharSize + " or higher." );
 
     FieldFragList ffl = new FieldFragList( fragCharSize );
 
@@ -56,8 +71,8 @@ public FieldFragList createFieldFragList(FieldPhraseList fieldPhraseList, int fr
 
       wpil.clear();
       wpil.add( phraseInfo );
-      int st = phraseInfo.getStartOffset() - MARGIN < startOffset ?
-          startOffset : phraseInfo.getStartOffset() - MARGIN;
+      int st = phraseInfo.getStartOffset() - margin < startOffset ?
+          startOffset : phraseInfo.getStartOffset() - margin;
       int en = st + fragCharSize;
       if( phraseInfo.getEndOffset() > en )
         en = phraseInfo.getEndOffset();
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java b/lucene/dev/branches/solrcloud/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java
index 20d0949a..5f30c618 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java
@@ -31,7 +31,7 @@ public void testNullFieldFragList() throws Exception {
   public void testTooSmallFragSize() throws Exception {
     try{
       SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-      sflb.createFieldFragList( fpl(new TermQuery(new Term(F, "a")), "b c d" ), SimpleFragListBuilder.MIN_FRAG_CHAR_SIZE - 1 );
+      sflb.createFieldFragList( fpl(new TermQuery(new Term(F, "a")), "b c d" ), sflb.minFragCharSize - 1 );
       fail( "IllegalArgumentException must be thrown" );
     }
     catch ( IllegalArgumentException expected ) {
@@ -40,7 +40,7 @@ public void testTooSmallFragSize() throws Exception {
   
   public void testSmallerFragSizeThanTermQuery() throws Exception {
     SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl(new TermQuery(new Term(F, "abcdefghijklmnopqrs")), "abcdefghijklmnopqrs" ), SimpleFragListBuilder.MIN_FRAG_CHAR_SIZE );
+    FieldFragList ffl = sflb.createFieldFragList( fpl(new TermQuery(new Term(F, "abcdefghijklmnopqrs")), "abcdefghijklmnopqrs" ), sflb.minFragCharSize );
     assertEquals( 1, ffl.getFragInfos().size() );
     assertEquals( "subInfos=(abcdefghijklmnopqrs((0,19)))/1.0(0,19)", ffl.getFragInfos().get( 0 ).toString() );
   }
@@ -52,7 +52,7 @@ public void testSmallerFragSizeThanPhraseQuery() throws Exception {
     phraseQuery.add(new Term(F, "abcdefgh"));
     phraseQuery.add(new Term(F, "jklmnopqrs"));
 
-    FieldFragList ffl = sflb.createFieldFragList( fpl(phraseQuery, "abcdefgh   jklmnopqrs" ), SimpleFragListBuilder.MIN_FRAG_CHAR_SIZE );
+    FieldFragList ffl = sflb.createFieldFragList( fpl(phraseQuery, "abcdefgh   jklmnopqrs" ), sflb.minFragCharSize );
     assertEquals( 1, ffl.getFragInfos().size() );
     if (VERBOSE) System.out.println( ffl.getFragInfos().get( 0 ).toString() );
     assertEquals( "subInfos=(abcdefghjklmnopqrs((0,21)))/1.0(0,21)", ffl.getFragInfos().get( 0 ).toString() );
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java b/lucene/dev/branches/solrcloud/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
index 02e31190..514a74cc 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
@@ -407,6 +407,11 @@ public int getDocCount() throws IOException {
             return -1;
           }
 
+          @Override
+          public long getUniqueTermCount() throws IOException {
+            return -1;
+          }
+
           @Override
           public Comparator<BytesRef> getComparator() {
             return BytesRef.getUTF8SortedAsUnicodeComparator();
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/NRTManager.java b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/NRTManager.java
index 71528023..9dc69a6f 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/NRTManager.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/NRTManager.java
@@ -93,10 +93,10 @@ public NRTManager(IndexWriter writer, ExecutorService es,
       SearcherWarmer warmer, boolean alwaysApplyDeletes) throws IOException {
     this.writer = writer;
     if (alwaysApplyDeletes) {
-      withoutDeletes = withDeletes = new SearcherManagerRef(true, 0,  SearcherManager.open(writer, true, warmer, es));
+      withoutDeletes = withDeletes = new SearcherManagerRef(true, 0,  new SearcherManager(writer, true, warmer, es));
     } else {
-      withDeletes = new SearcherManagerRef(true, 0,  SearcherManager.open(writer, true, warmer, es));
-      withoutDeletes = new SearcherManagerRef(false, 0,  SearcherManager.open(writer, false, warmer, es));
+      withDeletes = new SearcherManagerRef(true, 0, new SearcherManager(writer, true, warmer, es));
+      withoutDeletes = new SearcherManagerRef(false, 0, new SearcherManager(writer, false, warmer, es));
     }
     indexingGen = new AtomicLong(1);
   }
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java
index 7f707b1e..c84a5fcf 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java
@@ -121,7 +121,8 @@ public DocumentFilteredIndexReader(IndexReader reader, Filter preserveFilter, bo
       
       final int maxDoc = in.maxDoc();
       final FixedBitSet bits = new FixedBitSet(maxDoc);
-      final DocIdSet docs = preserveFilter.getDocIdSet((AtomicReaderContext) in.getTopReaderContext());
+      // ignore livedocs here, as we filter them later:
+      final DocIdSet docs = preserveFilter.getDocIdSet((AtomicReaderContext) in.getTopReaderContext(), null);
       if (docs != null) {
         final DocIdSetIterator it = docs.iterator();
         if (it != null) {
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosReader.java b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosReader.java
index aac8be17..e69de29b 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosReader.java
@@ -1,42 +0,0 @@
-package org.apache.lucene.index.codecs.appending;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.codecs.DefaultSegmentInfosReader;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-
-public class AppendingSegmentInfosReader extends DefaultSegmentInfosReader {
-
-  @Override
-  public void finalizeInput(IndexInput input) throws IOException,
-          CorruptIndexException {
-    input.close();
-  }
-
-  @Override
-  public IndexInput openInput(Directory dir, String segmentsFileName, IOContext context)
-          throws IOException {
-    return dir.openInput(segmentsFileName, context);
-  }
-
-}
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosWriter.java b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosWriter.java
index 2850037a..1ecb3e26 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosWriter.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosWriter.java
@@ -26,17 +26,6 @@
 
 public class AppendingSegmentInfosWriter extends DefaultSegmentInfosWriter {
 
-  @Override
-  protected IndexOutput createOutput(Directory dir, String segmentsFileName, IOContext context)
-          throws IOException {
-    return dir.createOutput(segmentsFileName, context);
-  }
-
-  @Override
-  public void finishCommit(IndexOutput out) throws IOException {
-    out.close();
-  }
-
   @Override
   public void prepareCommit(IndexOutput segmentOutput) throws IOException {
     // noop
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/search/SearcherLifetimeManager.java b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/search/SearcherLifetimeManager.java
index f6907186..e088dbae 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/search/SearcherLifetimeManager.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/search/SearcherLifetimeManager.java
@@ -173,8 +173,8 @@ public long record(IndexSearcher searcher) throws IOException {
         // incRef done by SearcherTracker ctor:
         tracker.close();
       }
-    } else {
-      assert tracker.searcher == searcher;
+    } else if (tracker.searcher != searcher) {
+      throw new IllegalArgumentException("the provided searcher has the same underlying reader version yet the searcher instance differs from before (new=" + searcher + " vs old=" + tracker.searcher);
     }
 
     return version;
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/search/SearcherManager.java b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/search/SearcherManager.java
index 13ace53c..858b8994 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/search/SearcherManager.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/java/org/apache/lucene/search/SearcherManager.java
@@ -64,23 +64,76 @@
  * @lucene.experimental
  */
 
-public abstract class SearcherManager {
+public final class SearcherManager {
 
-  protected volatile IndexSearcher currentSearcher;
-  protected final ExecutorService es;
-  protected final SearcherWarmer warmer;
-  protected final Semaphore reopenLock = new Semaphore(1);
+  private volatile IndexSearcher currentSearcher;
+  private final ExecutorService es;
+  private final SearcherWarmer warmer;
+  private final Semaphore reopenLock = new Semaphore(1);
   
-  protected SearcherManager(IndexReader openedReader, SearcherWarmer warmer,
+  /**
+   * Creates and returns a new SearcherManager from the given {@link IndexWriter}. 
+   * @param writer the IndexWriter to open the IndexReader from.
+   * @param applyAllDeletes If <code>true</code>, all buffered deletes will
+   *        be applied (made visible) in the {@link IndexSearcher} / {@link IndexReader}.
+   *        If <code>false</code>, the deletes may or may not be applied, but remain buffered 
+   *        (in IndexWriter) so that they will be applied in the future.
+   *        Applying deletes can be costly, so if your app can tolerate deleted documents
+   *        being returned you might gain some performance by passing <code>false</code>.
+   *        See {@link IndexReader#openIfChanged(IndexReader, IndexWriter, boolean)}.
+   * @param warmer An optional {@link SearcherWarmer}. Pass
+   *        <code>null</code> if you don't require the searcher to warmed
+   *        before going live.  If this is  <code>non-null</code> then a
+   *        merged segment warmer is installed on the
+   *        provided IndexWriter's config.
+   * @param es An optional {@link ExecutorService} so different segments can
+   *        be searched concurrently (see {@link
+   *        IndexSearcher#IndexSearcher(IndexReader,ExecutorService)}.  Pass <code>null</code>
+   *        to search segments sequentially.
+   *        
+   * @throws IOException
+   */
+  public SearcherManager(IndexWriter writer, boolean applyAllDeletes,
+      final SearcherWarmer warmer, final ExecutorService es) throws IOException {
+    this.es = es;
+    this.warmer = warmer;
+    currentSearcher = new IndexSearcher(IndexReader.open(writer, applyAllDeletes));
+    if (warmer != null) {
+      writer.getConfig().setMergedSegmentWarmer(
+          new IndexWriter.IndexReaderWarmer() {
+            @Override
+            public void warm(IndexReader reader) throws IOException {
+              warmer.warm(new IndexSearcher(reader, es));
+            }
+          });
+    }
+  }
+
+  /**
+   * Creates and returns a new SearcherManager from the given {@link Directory}. 
+   * @param dir the directory to open the IndexReader on.
+   * @param warmer An optional {@link SearcherWarmer}.  Pass
+   *        <code>null</code> if you don't require the searcher to warmed
+   *        before going live.  If this is  <code>non-null</code> then a
+   *        merged segment warmer is installed on the
+   *        provided IndexWriter's config.
+   * @param es And optional {@link ExecutorService} so different segments can
+   *        be searched concurrently (see {@link
+   *        IndexSearcher#IndexSearcher(IndexReader,ExecutorService)}.  Pass <code>null</code>
+   *        to search segments sequentially.
+   *        
+   * @throws IOException
+   */
+  public SearcherManager(Directory dir, SearcherWarmer warmer,
       ExecutorService es) throws IOException {
     this.es = es;
     this.warmer = warmer;
-    currentSearcher = new IndexSearcher(openedReader, es);
+    currentSearcher = new IndexSearcher(IndexReader.open(dir, true), es);
   }
 
   /**
    * You must call this, periodically, to perform a reopen. This calls
-   * {@link #openIfChanged(IndexReader)} with the underlying reader, and if that returns a
+   * {@link IndexReader#openIfChanged(IndexReader)} with the underlying reader, and if that returns a
    * new reader, it's warmed (if you provided a {@link SearcherWarmer} and then
    * swapped into production.
    * 
@@ -103,7 +156,9 @@ public boolean maybeReopen() throws IOException {
     // threads just return immediately:
     if (reopenLock.tryAcquire()) {
       try {
-        final IndexReader newReader = openIfChanged(currentSearcher.getIndexReader());
+        // IR.openIfChanged preserves NRT and applyDeletes
+        // in the newly returned reader:
+        final IndexReader newReader = IndexReader.openIfChanged(currentSearcher.getIndexReader());
         if (newReader != null) {
           final IndexSearcher newSearcher = new IndexSearcher(newReader, es);
           boolean success = false;
@@ -190,122 +245,10 @@ private void ensureOpen() {
     }
   }
 
-  protected synchronized void swapSearcher(IndexSearcher newSearcher) throws IOException {
+  private synchronized void swapSearcher(IndexSearcher newSearcher) throws IOException {
     ensureOpen();
     final IndexSearcher oldSearcher = currentSearcher;
     currentSearcher = newSearcher;
     release(oldSearcher);
   }
-
-  protected abstract IndexReader openIfChanged(IndexReader oldReader)
-      throws IOException;
-
-  /**
-   * Creates and returns a new SearcherManager from the given {@link IndexWriter}. 
-   * @param writer the IndexWriter to open the IndexReader from.
-   * @param applyAllDeletes If <code>true</code>, all buffered deletes will
-   *        be applied (made visible) in the {@link IndexSearcher} / {@link IndexReader}.
-   *        If <code>false</code>, the deletes are not applied but remain buffered 
-   *        (in IndexWriter) so that they will be applied in the future.
-   *        Applying deletes can be costly, so if your app can tolerate deleted documents
-   *        being returned you might gain some performance by passing <code>false</code>.
-   * @param warmer An optional {@link SearcherWarmer}. Pass
-   *        <code>null</code> if you don't require the searcher to warmed
-   *        before going live.  If this is  <code>non-null</code> then a
-   *        merged segment warmer is installed on the
-   *        provided IndexWriter's config.
-   * @param es An optional {@link ExecutorService} so different segments can
-   *        be searched concurrently (see {@link
-   *        IndexSearcher#IndexSearcher(IndexReader,ExecutorService)}.  Pass <code>null</code>
-   *        to search segments sequentially.
-   *        
-   * @see IndexReader#openIfChanged(IndexReader, IndexWriter, boolean)
-   * @throws IOException
-   */
-  public static SearcherManager open(IndexWriter writer, boolean applyAllDeletes,
-      SearcherWarmer warmer, ExecutorService es) throws IOException {
-    final IndexReader open = IndexReader.open(writer, true);
-    boolean success = false;
-    try {
-      SearcherManager manager = new NRTSearcherManager(writer, applyAllDeletes,
-          open, warmer, es);
-      success = true;
-      return manager;
-    } finally {
-      if (!success) {
-        open.close();
-      }
-    }
-  }
-
-  /**
-   * Creates and returns a new SearcherManager from the given {@link Directory}. 
-   * @param dir the directory to open the IndexReader on.
-   * @param warmer An optional {@link SearcherWarmer}.  Pass
-   *        <code>null</code> if you don't require the searcher to warmed
-   *        before going live.  If this is  <code>non-null</code> then a
-   *        merged segment warmer is installed on the
-   *        provided IndexWriter's config.
-   * @param es And optional {@link ExecutorService} so different segments can
-   *        be searched concurrently (see {@link
-   *        IndexSearcher#IndexSearcher(IndexReader,ExecutorService)}.  Pass <code>null</code>
-   *        to search segments sequentially.
-   *        
-   * @throws IOException
-   */
-  public static SearcherManager open(Directory dir, SearcherWarmer warmer,
-      ExecutorService es) throws IOException {
-    final IndexReader open = IndexReader.open(dir, true);
-    boolean success = false;
-    try {
-      SearcherManager manager = new DirectorySearchManager(open, warmer, es);
-      success = true;
-      return manager;
-    } finally {
-      if (!success) {
-        open.close();
-      }
-    }
-  }
-
-  static final class NRTSearcherManager extends SearcherManager {
-    private final IndexWriter writer;
-    private final boolean applyDeletes;
-
-    NRTSearcherManager(final IndexWriter writer, final boolean applyDeletes,
-        final IndexReader openedReader, final SearcherWarmer warmer, final ExecutorService es)
-        throws IOException {
-      super(openedReader, warmer, es);
-      this.writer = writer;
-      this.applyDeletes = applyDeletes;
-      if (warmer != null) {
-        writer.getConfig().setMergedSegmentWarmer(
-            new IndexWriter.IndexReaderWarmer() {
-              @Override
-              public void warm(IndexReader reader) throws IOException {
-                warmer.warm(new IndexSearcher(reader, es));
-              }
-            });
-      }
-    }
-
-    @Override
-    protected IndexReader openIfChanged(IndexReader oldReader)
-        throws IOException {
-      return IndexReader.openIfChanged(oldReader, writer, applyDeletes);
-    }
-  }
-
-  static final class DirectorySearchManager extends SearcherManager {
-    DirectorySearchManager(IndexReader openedReader,
-        SearcherWarmer warmer, ExecutorService es) throws IOException {
-      super(openedReader, warmer, es);
-    }
-
-    @Override
-    protected IndexReader openIfChanged(IndexReader oldReader)
-        throws IOException {
-      return IndexReader.openIfChanged(oldReader, true);
-    }
-  }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
index 4c6c72f1..60463f71 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
@@ -36,6 +36,7 @@
 import org.apache.lucene.index.TermsEnum.SeekStatus;
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.index.codecs.DefaultSegmentInfosReader;
 import org.apache.lucene.index.codecs.SegmentInfosReader;
 import org.apache.lucene.index.codecs.SegmentInfosWriter;
 import org.apache.lucene.store.Directory;
@@ -52,7 +53,7 @@
   static class AppendingCodecProvider extends CodecProvider {
     Codec appending = new AppendingCodec();
     SegmentInfosWriter infosWriter = new AppendingSegmentInfosWriter();
-    SegmentInfosReader infosReader = new AppendingSegmentInfosReader();
+    SegmentInfosReader infosReader = new DefaultSegmentInfosReader();
     public AppendingCodecProvider() {
       setDefaultFieldCodec(appending.name);
     }
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/test/org/apache/lucene/search/TestSearcherManager.java b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/test/org/apache/lucene/search/TestSearcherManager.java
index 102c77f8..c06d8f18 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/test/org/apache/lucene/search/TestSearcherManager.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/misc/src/test/org/apache/lucene/search/TestSearcherManager.java
@@ -18,16 +18,17 @@
  */
 
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutorService;
-import java.util.List;
-import java.util.ArrayList;
 import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.ConcurrentMergeScheduler;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.ThreadedIndexingAndSearchingTestCase;
@@ -63,7 +64,6 @@ protected IndexSearcher getFinalSearcher() throws Exception  {
 
   @Override
   protected void doAfterWriter(ExecutorService es) throws Exception {
-    // SearcherManager needs to see empty commit:
     final SearcherWarmer warmer = new SearcherWarmer() {
       @Override
       public void warm(IndexSearcher s) throws IOException {
@@ -72,11 +72,12 @@ public void warm(IndexSearcher s) throws IOException {
       }
     };
     if (random.nextBoolean()) {
-      mgr = SearcherManager.open(writer, true, warmer, es);
+      mgr = new SearcherManager(writer, true, warmer, es);
       isNRT = true;
     } else {
+      // SearcherManager needs to see empty commit:
       writer.commit();
-      mgr = SearcherManager.open(dir, warmer, es);
+      mgr = new SearcherManager(dir, warmer, es);
       isNRT = false;
     }
     
@@ -178,8 +179,9 @@ protected void doClose() throws Exception {
   
   public void testIntermediateClose() throws IOException, InterruptedException {
     Directory dir = newDirectory();
+    // Test can deadlock if we use SMS:
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random)));
+                                                                   TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergeScheduler(new ConcurrentMergeScheduler()));
     writer.addDocument(new Document());
     writer.commit();
     final CountDownLatch awaitEnterWarm = new CountDownLatch(1);
@@ -196,8 +198,8 @@ public void warm(IndexSearcher s) throws IOException {
         }
       }
     };
-    final SearcherManager searcherManager = random.nextBoolean() ? SearcherManager.open(dir,
-        warmer, es) : SearcherManager.open(writer, random.nextBoolean(), warmer, es);
+    final SearcherManager searcherManager = random.nextBoolean() ? new SearcherManager(dir,
+        warmer, es) : new SearcherManager(writer, random.nextBoolean(), warmer, es);
     IndexSearcher searcher = searcherManager.acquire();
     try {
       assertEquals(1, searcher.getIndexReader().numDocs());
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java b/lucene/dev/branches/solrcloud/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
index 3704918f..2c8a031b 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
@@ -70,17 +70,16 @@ public DuplicateFilter(String fieldName, KeepMode keepMode, ProcessingMode proce
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
     if (processingMode == ProcessingMode.PM_FAST_INVALIDATION) {
-      return fastBits(context.reader);
+      return fastBits(context.reader, acceptDocs);
     } else {
-      return correctBits(context.reader);
+      return correctBits(context.reader, acceptDocs);
     }
   }
 
-  private FixedBitSet correctBits(IndexReader reader) throws IOException {
+  private FixedBitSet correctBits(IndexReader reader, Bits acceptDocs) throws IOException {
     FixedBitSet bits = new FixedBitSet(reader.maxDoc()); //assume all are INvalid
-    final Bits liveDocs = MultiFields.getLiveDocs(reader);
     Terms terms = reader.fields().terms(fieldName);
 
     if (terms == null) {
@@ -94,7 +93,7 @@ private FixedBitSet correctBits(IndexReader reader) throws IOException {
       if (currTerm == null) {
         break;
       } else {
-        docs = termsEnum.docs(liveDocs, docs);
+        docs = termsEnum.docs(acceptDocs, docs);
         int doc = docs.nextDoc();
         if (doc != DocsEnum.NO_MORE_DOCS) {
           if (keepMode == KeepMode.KM_USE_FIRST_OCCURRENCE) {
@@ -116,10 +115,9 @@ private FixedBitSet correctBits(IndexReader reader) throws IOException {
     return bits;
   }
 
-  private FixedBitSet fastBits(IndexReader reader) throws IOException {
+  private FixedBitSet fastBits(IndexReader reader, Bits acceptDocs) throws IOException {
     FixedBitSet bits = new FixedBitSet(reader.maxDoc());
     bits.set(0, reader.maxDoc()); //assume all are valid
-    final Bits liveDocs = MultiFields.getLiveDocs(reader);
     Terms terms = reader.fields().terms(fieldName);
 
     if (terms == null) {
@@ -135,7 +133,7 @@ private FixedBitSet fastBits(IndexReader reader) throws IOException {
       } else {
         if (termsEnum.docFreq() > 1) {
           // unset potential duplicates
-          docs = termsEnum.docs(liveDocs, docs);
+          docs = termsEnum.docs(acceptDocs, docs);
           int doc = docs.nextDoc();
           if (doc != DocsEnum.NO_MORE_DOCS) {
             if (keepMode == KeepMode.KM_USE_FIRST_OCCURRENCE) {
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java b/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java
index 875a56a0..d5b0c836 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java
@@ -26,6 +26,7 @@
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.FilteredDocIdSet;
 import org.apache.lucene.spatial.DistanceUtils;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.spatial.tier.DistanceFilter;
 
@@ -57,7 +58,7 @@ public GeoHashDistanceFilter(Filter startingFilter, double lat, double lng, doub
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
 
     final DocTerms geoHashValues = FieldCache.DEFAULT.getTerms(context.reader, geoHashField);
     final BytesRef br = new BytesRef();
@@ -65,7 +66,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
     final int docBase = nextDocBase;
     nextDocBase += context.reader.maxDoc();
 
-    return new FilteredDocIdSet(startingFilter.getDocIdSet(context)) {
+    return new FilteredDocIdSet(startingFilter.getDocIdSet(context, acceptDocs)) {
       @Override
       public boolean match(int doc) {
 
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/CartesianShapeFilter.java b/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/CartesianShapeFilter.java
index e38c1ec7..35c25f3c 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/CartesianShapeFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/CartesianShapeFilter.java
@@ -45,8 +45,7 @@
   }
   
   @Override
-  public DocIdSet getDocIdSet(final AtomicReaderContext context) throws IOException {
-    final Bits liveDocs = context.reader.getLiveDocs();
+  public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
     final List<Double> area = shape.getArea();
     final int sz = area.size();
     
@@ -58,7 +57,7 @@ public DocIdSet getDocIdSet(final AtomicReaderContext context) throws IOExceptio
       return new DocIdSet() {
         @Override
         public DocIdSetIterator iterator() throws IOException {
-          return context.reader.termDocsEnum(liveDocs, fieldName, bytesRef);
+          return context.reader.termDocsEnum(acceptDocs, fieldName, bytesRef);
         }
         
         @Override
@@ -71,7 +70,7 @@ public boolean isCacheable() {
       for (int i =0; i< sz; i++) {
         double boxId = area.get(i).doubleValue();
         NumericUtils.longToPrefixCoded(NumericUtils.doubleToSortableLong(boxId), 0, bytesRef);
-        final DocsEnum docsEnum = context.reader.termDocsEnum(liveDocs, fieldName, bytesRef);
+        final DocsEnum docsEnum = context.reader.termDocsEnum(acceptDocs, fieldName, bytesRef);
         if (docsEnum == null) continue;
         // iterate through all documents
         // which have this boxId
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java b/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java
index 4574e773..4ed41d69 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java
@@ -24,6 +24,7 @@
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.spatial.DistanceUtils;
 
 
@@ -60,7 +61,7 @@ public LatLongDistanceFilter(Filter startingFilter, double lat, double lng, doub
   }
   
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
 
     final double[] latIndex = FieldCache.DEFAULT.getDoubles(context.reader, latField);
     final double[] lngIndex = FieldCache.DEFAULT.getDoubles(context.reader, lngField);
@@ -68,7 +69,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
     final int docBase = nextDocBase;
     nextDocBase += context.reader.maxDoc();
 
-    return new FilteredDocIdSet(startingFilter.getDocIdSet(context)) {
+    return new FilteredDocIdSet(startingFilter.getDocIdSet(context, acceptDocs)) {
       @Override
       protected boolean match(int doc) {
         double x = latIndex[doc];
diff --git a/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java b/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
index 878784ba..43aff916 100644
--- a/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
+++ b/lucene/dev/branches/solrcloud/lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
@@ -105,7 +105,7 @@ public void testLatLongFilterOnDeletedDocs() throws Exception {
 
     AtomicReaderContext[] leaves = ReaderUtil.leaves(r.getTopReaderContext());
     for (int i = 0; i < leaves.length; i++) {
-      f.getDocIdSet(leaves[i]);
+      f.getDocIdSet(leaves[i], leaves[i].reader.getLiveDocs());
     }
     r.close();
   }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/document/Document.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/document/Document.java
index bb528aac..c8d2a33d 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/document/Document.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/document/Document.java
@@ -47,25 +47,7 @@ public Document() {}
 
   @Override
   public Iterator<IndexableField> iterator() {
-
-    return new Iterator<IndexableField>() {
-      private int fieldUpto = 0;
-      
-      @Override
-      public boolean hasNext() {
-        return fieldUpto < fields.size();
-      }
-
-      @Override
-      public void remove() {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public IndexableField next() {
-        return fields.get(fieldUpto++);
-      }
-    };
+    return fields.iterator();
   }
 
   /**
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/document/FieldType.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/document/FieldType.java
index a219f821..d47d2f9d 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/document/FieldType.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/document/FieldType.java
@@ -55,7 +55,7 @@ private void checkIfFrozen() {
 
   /**
    * Prevents future changes. Note, it is recommended that this is called once
-   * the FieldTypes's properties have been set, to prevent unintential state
+   * the FieldTypes's properties have been set, to prevent unintentional state
    * changes.
    */
   public void freeze() {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java
index 29522299..c4fbb0aa 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java
@@ -432,13 +432,13 @@ public QueryAndLimit(Query query, int limit) {
   }
 
   // Delete by query
-  private synchronized long applyQueryDeletes(Iterable<QueryAndLimit> queriesIter, SegmentReader reader) throws IOException {
+  private static long applyQueryDeletes(Iterable<QueryAndLimit> queriesIter, SegmentReader reader) throws IOException {
     long delCount = 0;
     final AtomicReaderContext readerContext = (AtomicReaderContext) reader.getTopReaderContext();
     for (QueryAndLimit ent : queriesIter) {
       Query query = ent.query;
       int limit = ent.limit;
-      final DocIdSet docs = new QueryWrapperFilter(query).getDocIdSet(readerContext);
+      final DocIdSet docs = new QueryWrapperFilter(query).getDocIdSet(readerContext, readerContext.reader.getLiveDocs());
       if (docs != null) {
         final DocIdSetIterator it = docs.iterator();
         if (it != null) {
@@ -448,11 +448,8 @@ private synchronized long applyQueryDeletes(Iterable<QueryAndLimit> queriesIter,
               break;
 
             reader.deleteDocument(doc);
-            // TODO: we could/should change
-            // reader.deleteDocument to return boolean
-            // true if it did in fact delete, because here
-            // we could be deleting an already-deleted doc
-            // which makes this an upper bound:
+            // as we use getLiveDocs() to filter out already deleted documents,
+            // we only delete live documents, so the counting is right:
             delCount++;
           }
         }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/CheckIndex.java
index dc642ad1..83dc7614 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/CheckIndex.java
@@ -726,6 +726,9 @@ public Status checkIndex(List<String> onlySegments, CodecProvider codecs) throws
           }
 
           final int docFreq = terms.docFreq();
+          if (docFreq <= 0) {
+            throw new RuntimeException("docfreq: " + docFreq + " is out of bounds");
+          }
           status.totFreq += docFreq;
           sumDocFreq += docFreq;
 
@@ -823,6 +826,9 @@ public Status checkIndex(List<String> onlySegments, CodecProvider codecs) throws
             throw new RuntimeException("term " + term + " docFreq=" + docFreq + " != tot docs w/o deletions " + docCount);
           }
           if (hasTotalTermFreq) {
+            if (totalTermFreq2 <= 0) {
+              throw new RuntimeException("totalTermFreq: " + totalTermFreq2 + " is out of bounds");
+            }
             sumTotalTermFreq += totalTermFreq;
             if (totalTermFreq != totalTermFreq2) {
               throw new RuntimeException("term " + term + " totalTermFreq=" + totalTermFreq2 + " != recomputed totalTermFreq=" + totalTermFreq);
@@ -940,19 +946,19 @@ public Status checkIndex(List<String> onlySegments, CodecProvider codecs) throws
             is.search(new TermQuery(new Term(field, lastTerm)), 1);
           }
 
-          // Test seeking by ord
-          if (hasOrd && status.termCount-termCountStart > 0) {
-            long termCount;
-            try {
+          // check unique term count
+          long termCount = -1;
+          
+          if (status.termCount-termCountStart > 0) {
               termCount = fields.terms(field).getUniqueTermCount();
-            } catch (UnsupportedOperationException uoe) {
-              termCount = -1;
-            }
 
             if (termCount != -1 && termCount != status.termCount - termCountStart) {
               throw new RuntimeException("termCount mismatch " + termCount + " vs " + (status.termCount - termCountStart));
             }
+          }
 
+          // Test seeking by ord
+          if (hasOrd && status.termCount-termCountStart > 0) {
             int seekCount = (int) Math.min(10000L, termCount);
             if (seekCount > 0) {
               BytesRef[] seekTerms = new BytesRef[seekCount];
@@ -995,6 +1001,21 @@ public Status checkIndex(List<String> onlySegments, CodecProvider codecs) throws
         }
       }
 
+      // for most implementations, this is boring (just the sum across all fields)
+      // but codecs that don't work per-field like preflex actually implement this,
+      // but don't implement it on Terms, so the check isn't redundant.
+      long uniqueTermCountAllFields = reader.getUniqueTermCount();
+      
+      // this means something is seriously screwed, e.g. we are somehow getting enclosed in PFCW!!!!!!
+      
+      if (uniqueTermCountAllFields == -1) {
+        throw new RuntimeException("invalid termCount: -1");
+     }
+
+      if (status.termCount != uniqueTermCountAllFields) {
+        throw new RuntimeException("termCount mismatch " + uniqueTermCountAllFields + " vs " + (status.termCount));
+      }
+
       msg("OK [" + status.termCount + " terms; " + status.totFreq + " terms/docs pairs; " + status.totPos + " tokens]");
 
       if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DirectoryReader.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
index 6c931349..190ebd86 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
@@ -406,8 +406,15 @@ protected final IndexReader doOpenIfChanged(final IndexCommit commit) throws Cor
     return doOpenIfChanged(true, commit);
   }
 
-  // NOTE: always returns a non-null result (ie new reader)
-  // but that could change someday
+  @Override
+  protected final IndexReader doOpenIfChanged(IndexWriter writer, boolean applyAllDeletes) throws CorruptIndexException, IOException {
+    if (writer == this.writer && applyAllDeletes == this.applyAllDeletes) {
+      return doOpenIfChanged();
+    } else {    
+      return super.doOpenIfChanged(writer, applyAllDeletes);
+    }
+  }
+
   private final IndexReader doOpenFromWriter(boolean openReadOnly, IndexCommit commit) throws CorruptIndexException, IOException {
     assert readOnly;
 
@@ -419,10 +426,18 @@ private final IndexReader doOpenFromWriter(boolean openReadOnly, IndexCommit com
       throw new IllegalArgumentException("a reader obtained from IndexWriter.getReader() cannot currently accept a commit");
     }
 
-    // TODO: right now we *always* make a new reader; in
-    // the future we could have write make some effort to
-    // detect that no changes have occurred
+    if (writer.nrtIsCurrent(segmentInfos)) {
+      return null;
+    }
+
     IndexReader reader = writer.getReader(applyAllDeletes);
+
+    // If in fact no changes took place, return null:
+    if (reader.getVersion() == segmentInfos.getVersion()) {
+      reader.decRef();
+      return null;
+    }
+
     reader.readerFinishedListeners = readerFinishedListeners;
     return reader;
   }
@@ -803,7 +818,7 @@ void rollbackCommit() {
 
   @Override
   public long getUniqueTermCount() throws IOException {
-    throw new UnsupportedOperationException("");
+    return -1;
   }
 
   @Override
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocTermOrds.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocTermOrds.java
index 0b834ad4..42d0b915 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocTermOrds.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocTermOrds.java
@@ -101,7 +101,7 @@
 
 public class DocTermOrds {
 
-  // Term ords are shifted by this, internally, to reseve
+  // Term ords are shifted by this, internally, to reserve
   // values 0 (end term) and 1 (index is a pointer into byte array)
   private final static int TNUM_OFFSET = 2;
 
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
index a63b430e..e414fe12 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -411,7 +411,7 @@ private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOExcepti
          */
         try {
           synchronized (ticketQueue) {
-            // Each flush is assigned a ticket in the order they accquire the ticketQueue lock
+            // Each flush is assigned a ticket in the order they acquire the ticketQueue lock
             ticket =  new FlushTicket(flushingDWPT.prepareFlush(), true);
             ticketQueue.add(ticket);
           }
@@ -578,6 +578,7 @@ final boolean flushAllThreads()
         }
         applyFlushTickets();
       }
+      assert !flushingDeleteQueue.anyChanges();
     } finally {
       assert flushingDeleteQueue == currentFullFlushDelQueue;
     }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocumentsWriterDeleteQueue.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocumentsWriterDeleteQueue.java
index 3489fd27..29b0655f 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocumentsWriterDeleteQueue.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/DocumentsWriterDeleteQueue.java
@@ -169,7 +169,13 @@ void add(Node<?> item) {
   boolean anyChanges() {
     globalBufferLock.lock();
     try {
-      return !globalSlice.isEmpty() || globalBufferedDeletes.any();
+      /*
+       * check if all items in the global slice were applied 
+       * and if the global slice is up-to-date
+       * and if globalBufferedDeletes has changes
+       */
+      return globalBufferedDeletes.any() || !globalSlice.isEmpty() || globalSlice.sliceTail != tail
+          || tail.next != null;
     } finally {
       globalBufferLock.unlock();
     }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/Fields.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/Fields.java
index 01b7f0d5..43050fb7 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/Fields.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/Fields.java
@@ -32,5 +32,31 @@
    *  null if the field does not exist. */
   public abstract Terms terms(String field) throws IOException;
   
+  /** Returns the number of terms for all fields, or -1 if this 
+   *  measure isn't stored by the codec. Note that, just like 
+   *  other term measures, this measure does not take deleted 
+   *  documents into account. */
+  // TODO: deprecate?
+  public long getUniqueTermCount() throws IOException {
+    long numTerms = 0;
+    FieldsEnum it = iterator();
+    while(true) {
+      String field = it.next();
+      if (field == null) {
+        break;
+      }
+      Terms terms = terms(field);
+      if (terms != null) {
+        final long termCount = terms.getUniqueTermCount();
+        if (termCount == -1) {
+          return -1;
+        }
+          
+        numTerms += termCount;
+      }
+    }
+    return numTerms;
+  }
+  
   public final static Fields[] EMPTY_ARRAY = new Fields[0];
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexReader.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexReader.java
index 62d93008..0961853f 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexReader.java
@@ -561,10 +561,6 @@ private static IndexReader open(final Directory directory, final IndexDeletionPo
    * with the old reader uses "copy on write" semantics to
    * ensure the changes are not seen by other readers.
    *
-   * <p><b>NOTE</b>: If the provided reader is a near real-time
-   * reader, this method will return another near-real-time
-   * reader.
-   * 
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
    * @return null if there are no changes; else, a new
@@ -672,18 +668,42 @@ public static IndexReader openIfChanged(IndexReader oldReader, IndexWriter write
     return newReader;
   }
 
+  /**
+   * If the index has changed since it was opened, open and return a new reader;
+   * else, return {@code null}.
+   * 
+   * @see #openIfChanged(IndexReader)
+   */
   protected IndexReader doOpenIfChanged() throws CorruptIndexException, IOException {
     throw new UnsupportedOperationException("This reader does not support reopen().");
   }
   
+  /**
+   * If the index has changed since it was opened, open and return a new reader;
+   * else, return {@code null}.
+   * 
+   * @see #openIfChanged(IndexReader, boolean)
+   */
   protected IndexReader doOpenIfChanged(boolean openReadOnly) throws CorruptIndexException, IOException {
     throw new UnsupportedOperationException("This reader does not support reopen().");
   }
 
+  /**
+   * If the index has changed since it was opened, open and return a new reader;
+   * else, return {@code null}.
+   * 
+   * @see #openIfChanged(IndexReader, IndexCommit)
+   */
   protected IndexReader doOpenIfChanged(final IndexCommit commit) throws CorruptIndexException, IOException {
     throw new UnsupportedOperationException("This reader does not support reopen(IndexCommit).");
   }
 
+  /**
+   * If the index has changed since it was opened, open and return a new reader;
+   * else, return {@code null}.
+   * 
+   * @see #openIfChanged(IndexReader, IndexWriter, boolean)
+   */
   protected IndexReader doOpenIfChanged(IndexWriter writer, boolean applyAllDeletes) throws CorruptIndexException, IOException {
     return writer.getReader(applyAllDeletes);
   }
@@ -870,7 +890,7 @@ public long getVersion() {
    * (ie, obtained by a call to {@link
    * IndexWriter#getReader}, or by calling {@link #openIfChanged}
    * on a near real-time reader), then this method checks if
-   * either a new commmit has occurred, or any new
+   * either a new commit has occurred, or any new
    * uncommitted changes have taken place via the writer.
    * Note that even if the writer has only performed
    * merging, this method will still return false.</p>
@@ -1593,26 +1613,17 @@ public Object getCoreCacheKey() {
   /** Returns the number of unique terms (across all fields)
    *  in this reader.
    *
-   *  @throws UnsupportedOperationException if this count
+   *  @return number of unique terms or -1 if this count
    *  cannot be easily determined (eg Multi*Readers).
    *  Instead, you should call {@link
    *  #getSequentialSubReaders} and ask each sub reader for
    *  its unique term count. */
   public long getUniqueTermCount() throws IOException {
-    long numTerms = 0;
     final Fields fields = fields();
     if (fields == null) {
       return 0;
     }
-    FieldsEnum it = fields.iterator();
-    while(true) {
-      String field = it.next();
-      if (field == null) {
-        break;
-      }
-      numTerms += fields.terms(field).getUniqueTermCount();
-    }
-    return numTerms;
+    return fields.getUniqueTermCount();
   }
 
   /** For IndexReader implementations that use
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexWriter.java
index e8f7bdb8..0f5b2849 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexWriter.java
@@ -764,7 +764,7 @@ public int numDeletedDocs(SegmentInfo info) throws IOException {
       if (reader != null) {
         // the pulled reader could be from an in-flight merge 
         // while the info we see has already new applied deletes after a commit
-        // we max out the delets since deletes never shrink
+        // we max out the deletes since deletes never shrink
         return Math.max(info.getDelCount(), reader.numDeletedDocs());
       } else {
         return info.getDelCount();
@@ -4073,6 +4073,7 @@ boolean testPoint(String name) {
 
   synchronized boolean nrtIsCurrent(SegmentInfos infos) {
     //System.out.println("IW.nrtIsCurrent " + (infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any()));
+    ensureOpen();
     return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();
   }
 
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexableField.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexableField.java
index 1ee3f138..22782a55 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexableField.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexableField.java
@@ -40,7 +40,7 @@
 
   // TODO: add attrs to this API?
 
-  /* Field name */
+  /** Field name */
   public String name();
 
   // NOTE: if doc/field impl has the notion of "doc level boost"
@@ -49,24 +49,24 @@
   /** Field boost (you must pre-multiply in any doc boost). */
   public float boost();
   
-  /* Non-null if this field has a binary value */
+  /** Non-null if this field has a binary value */
   public BytesRef binaryValue();
 
-  /* Non-null if this field has a string value */
+  /** Non-null if this field has a string value */
   public String stringValue();
 
-  /* Non-null if this field has a Reader value */
+  /** Non-null if this field has a Reader value */
   public Reader readerValue();
 
   // Numeric field:
-  /* True if this field is numeric */
+  /** True if this field is numeric */
   public boolean numeric();
 
-  /* Numeric {@link NumericField.DataType}; only used if
+  /** Numeric {@link org.apache.lucene.document.NumericField.DataType}; only used if
    * the field is numeric */
   public NumericField.DataType numericDataType();
 
-  /* Numeric value; only used if the field is numeric */
+  /** Numeric value; only used if the field is numeric */
   public Number numericValue();
 
   /**
@@ -76,10 +76,10 @@
    */
   public IndexableFieldType fieldType();
   
-  /* Non-null if doc values should be indexed */
+  /** Non-null if doc values should be indexed */
   public PerDocFieldValues docValues();
 
-  /* DocValues type; only used if docValues is non-null */
+  /** DocValues type; only used if docValues is non-null */
   public ValueType docValuesType();
 
   /**
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexableFieldType.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexableFieldType.java
index 2a8b51c1..8821bee6 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexableFieldType.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/IndexableFieldType.java
@@ -21,28 +21,28 @@
 
 public interface IndexableFieldType {
 
-  /* True if this field should be indexed (inverted) */
+  /** True if this field should be indexed (inverted) */
   public boolean indexed();
 
-  /* True if the field's value should be stored */
+  /** True if the field's value should be stored */
   public boolean stored();
 
-  /* True if this field's value should be analyzed */
+  /** True if this field's value should be analyzed */
   public boolean tokenized();
 
-  /* True if term vectors should be indexed */
+  /** True if term vectors should be indexed */
   public boolean storeTermVectors();
 
-  /* True if term vector offsets should be indexed */
+  /** True if term vector offsets should be indexed */
   public boolean storeTermVectorOffsets();
 
-  /* True if term vector positions should be indexed */
+  /** True if term vector positions should be indexed */
   public boolean storeTermVectorPositions();
 
-  /* True if norms should not be indexed */
+  /** True if norms should not be indexed */
   public boolean omitNorms();
 
-  /* {@link IndexOptions}, describing what should be
+  /** {@link IndexOptions}, describing what should be
    * recorded into the inverted index */
   public IndexOptions indexOptions();
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiDocsAndPositionsEnum.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiDocsAndPositionsEnum.java
index e1830184..392a5452 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiDocsAndPositionsEnum.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiDocsAndPositionsEnum.java
@@ -29,6 +29,8 @@
  */
 
 public final class MultiDocsAndPositionsEnum extends DocsAndPositionsEnum {
+  private final MultiTermsEnum parent;
+  final DocsAndPositionsEnum[] subDocsAndPositionsEnum;
   private EnumWithSlice[] subs;
   int numSubs;
   int upto;
@@ -36,7 +38,16 @@
   int currentBase;
   int doc = -1;
 
-  MultiDocsAndPositionsEnum reset(final EnumWithSlice[] subs, final int numSubs) throws IOException {
+  public MultiDocsAndPositionsEnum(MultiTermsEnum parent, int subReaderCount) {
+    this.parent = parent;
+    subDocsAndPositionsEnum = new DocsAndPositionsEnum[subReaderCount];
+  }
+
+  public boolean canReuse(MultiTermsEnum parent) {
+    return this.parent == parent;
+  }
+
+  public MultiDocsAndPositionsEnum reset(final EnumWithSlice[] subs, final int numSubs) throws IOException {
     this.numSubs = numSubs;
     this.subs = new EnumWithSlice[subs.length];
     for(int i=0;i<subs.length;i++) {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiDocsEnum.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiDocsEnum.java
index 7932f6d4..43871591 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiDocsEnum.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiDocsEnum.java
@@ -27,6 +27,8 @@
  */
 
 public final class MultiDocsEnum extends DocsEnum {
+  private final MultiTermsEnum parent;
+  final DocsEnum[] subDocsEnum;
   private EnumWithSlice[] subs;
   int numSubs;
   int upto;
@@ -34,6 +36,11 @@
   int currentBase;
   int doc = -1;
 
+  public MultiDocsEnum(MultiTermsEnum parent, int subReaderCount) {
+    this.parent = parent;
+    subDocsEnum = new DocsEnum[subReaderCount];
+  }
+
   MultiDocsEnum reset(final EnumWithSlice[] subs, final int numSubs) throws IOException {
     this.numSubs = numSubs;
 
@@ -48,6 +55,10 @@ MultiDocsEnum reset(final EnumWithSlice[] subs, final int numSubs) throws IOExce
     return this;
   }
 
+  public boolean canReuse(MultiTermsEnum parent) {
+    return this.parent == parent;
+  }
+
   public int getNumSubs() {
     return numSubs;
   }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiTerms.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiTerms.java
index 3d296f1c..cabfc251 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiTerms.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiTerms.java
@@ -95,6 +95,11 @@ public TermsEnum iterator() throws IOException {
     }
   }
 
+  @Override
+  public long getUniqueTermCount() throws IOException {
+    return -1;
+  }
+
   @Override
   public long getSumTotalTermFreq() throws IOException {
     long sum = 0;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiTermsEnum.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiTermsEnum.java
index b0771cb7..ed0963d8 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiTermsEnum.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/MultiTermsEnum.java
@@ -77,7 +77,7 @@ public MultiTermsEnum(ReaderUtil.Slice[] slices) {
     subDocs = new MultiDocsEnum.EnumWithSlice[slices.length];
     subDocsAndPositions = new MultiDocsAndPositionsEnum.EnumWithSlice[slices.length];
     for(int i=0;i<slices.length;i++) {
-      subs[i] = new TermsEnumWithSlice(slices[i]);
+      subs[i] = new TermsEnumWithSlice(i, slices[i]);
       subDocs[i] = new MultiDocsEnum.EnumWithSlice();
       subDocs[i].slice = slices[i];
       subDocsAndPositions[i] = new MultiDocsAndPositionsEnum.EnumWithSlice();
@@ -347,11 +347,16 @@ public long totalTermFreq() throws IOException {
 
   @Override
   public DocsEnum docs(Bits liveDocs, DocsEnum reuse) throws IOException {
-    final MultiDocsEnum docsEnum;
-    if (reuse != null) {
+    MultiDocsEnum docsEnum;
+    // Can only reuse if incoming enum is also a MultiDocsEnum
+    if (reuse != null && reuse instanceof MultiDocsEnum) {
       docsEnum = (MultiDocsEnum) reuse;
+      // ... and was previously created w/ this MultiTermsEnum:
+      if (!docsEnum.canReuse(this)) {
+        docsEnum = new MultiDocsEnum(this, subs.length);
+      }
     } else {
-      docsEnum = new MultiDocsEnum();
+      docsEnum = new MultiDocsEnum(this, subs.length);
     }
     
     final MultiBits multiLiveDocs;
@@ -390,8 +395,11 @@ public DocsEnum docs(Bits liveDocs, DocsEnum reuse) throws IOException {
         b = null;
       }
 
-      final DocsEnum subDocsEnum = entry.terms.docs(b, null);
+      assert entry.index < docsEnum.subDocsEnum.length: entry.index + " vs " + docsEnum.subDocsEnum.length + "; " + subs.length;
+      final DocsEnum subDocsEnum = entry.terms.docs(b, docsEnum.subDocsEnum[entry.index]);
+
       if (subDocsEnum != null) {
+        docsEnum.subDocsEnum[entry.index] = subDocsEnum;
         subDocs[upto].docsEnum = subDocsEnum;
         subDocs[upto].slice = entry.subSlice;
 
@@ -408,11 +416,16 @@ public DocsEnum docs(Bits liveDocs, DocsEnum reuse) throws IOException {
 
   @Override
   public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-    final MultiDocsAndPositionsEnum docsAndPositionsEnum;
-    if (reuse != null) {
+    MultiDocsAndPositionsEnum docsAndPositionsEnum;
+    // Can only reuse if incoming enum is also a MultiDocsAndPositionsEnum
+    if (reuse != null && reuse instanceof MultiDocsAndPositionsEnum) {
       docsAndPositionsEnum = (MultiDocsAndPositionsEnum) reuse;
+      // ... and was previously created w/ this MultiTermsEnum:
+      if (!docsAndPositionsEnum.canReuse(this)) {
+        docsAndPositionsEnum = new MultiDocsAndPositionsEnum(this, subs.length);
+      }
     } else {
-      docsAndPositionsEnum = new MultiDocsAndPositionsEnum();
+      docsAndPositionsEnum = new MultiDocsAndPositionsEnum(this, subs.length);
     }
     
     final MultiBits multiLiveDocs;
@@ -452,9 +465,11 @@ public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum
         b = null;
       }
 
-      final DocsAndPositionsEnum subPostings = entry.terms.docsAndPositions(b, null);
+      assert entry.index < docsAndPositionsEnum.subDocsAndPositionsEnum.length: entry.index + " vs " + docsAndPositionsEnum.subDocsAndPositionsEnum.length + "; " + subs.length;
+      final DocsAndPositionsEnum subPostings = entry.terms.docsAndPositions(b, docsAndPositionsEnum.subDocsAndPositionsEnum[entry.index]);
 
       if (subPostings != null) {
+        docsAndPositionsEnum.subDocsAndPositionsEnum[entry.index] = subPostings;
         subDocsAndPositions[upto].docsAndPositionsEnum = subPostings;
         subDocsAndPositions[upto].slice = entry.subSlice;
         upto++;
@@ -479,9 +494,11 @@ public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum
     private final ReaderUtil.Slice subSlice;
     private TermsEnum terms;
     public BytesRef current;
+    final int index;
 
-    public TermsEnumWithSlice(ReaderUtil.Slice subSlice) {
+    public TermsEnumWithSlice(int index, ReaderUtil.Slice subSlice) {
       this.subSlice = subSlice;
+      this.index = index;
       assert subSlice.length >= 0: "length=" + subSlice.length;
     }
 
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/NoMergeScheduler.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/NoMergeScheduler.java
index e98723b5..1f6fce7a 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/NoMergeScheduler.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/NoMergeScheduler.java
@@ -23,7 +23,7 @@
  * A {@link MergeScheduler} which never executes any merges. It is also a
  * singleton and can be accessed through {@link NoMergeScheduler#INSTANCE}. Use
  * it if you want to prevent an {@link IndexWriter} from ever executing merges,
- * irregardless of the {@link MergePolicy} used. Note that you can achieve the
+ * regardless of the {@link MergePolicy} used. Note that you can achieve the
  * same thing by using {@link NoMergePolicy}, however with
  * {@link NoMergeScheduler} you also ensure that no unnecessary code of any
  * {@link MergeScheduler} implementation is ever executed. Hence it is
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/ParallelReader.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/ParallelReader.java
index 04ce475d..fd58be6e 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/ParallelReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/ParallelReader.java
@@ -569,8 +569,10 @@ public PerDocValues perDocValues() throws IOException {
 
     void addField(String field, IndexReader r) throws IOException {
       PerDocValues perDocs = MultiPerDocValues.getPerDocs(r);
+      if (perDocs != null) {
       fields.put(field, perDocs.docValues(field));
     }
+    }
 
     @Override
     public void close() throws IOException {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java
index e7d594aa..28336551 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java
@@ -41,10 +41,10 @@
  * a Lucene index. It is highly recommended to use a dedicated directory (and on
  * stable storage as well) for persisting the snapshots' information, and not
  * reuse the content index directory, or otherwise conflicts and index
- * corruptions will occur.
+ * corruption will occur.
  * <p>
  * <b>NOTE:</b> you should call {@link #close()} when you're done using this
- * class for safetyness (it will close the {@link IndexWriter} instance used).
+ * class for safety (it will close the {@link IndexWriter} instance used).
  */
 public class PersistentSnapshotDeletionPolicy extends SnapshotDeletionPolicy {
 
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentCodecs.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentCodecs.java
index c31083bf..e5ebd969 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentCodecs.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentCodecs.java
@@ -25,6 +25,7 @@
 
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.index.codecs.preflex.PreFlexCodec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
@@ -65,7 +66,7 @@
    */
   final Codec[] codecs;
   final CodecProvider provider;
-  private final Codec codec = new PerFieldCodecWrapper(this);
+  private final Codec codec;
   
   SegmentCodecs(CodecProvider provider, IndexInput input) throws IOException {
     this(provider, read(input, provider));
@@ -74,6 +75,11 @@
   SegmentCodecs(CodecProvider provider, Codec... codecs) {
     this.provider = provider;
     this.codecs = codecs;
+    if (codecs.length == 1 && codecs[0] instanceof PreFlexCodec) {
+      this.codec = codecs[0]; // hack for backwards break... don't wrap the codec in preflex
+    } else {
+      this.codec = new PerFieldCodecWrapper(this);
+    }
   }
 
   Codec codec() {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentInfo.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
index ad87a91c..005672ee 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -695,7 +695,7 @@ public String toString() {
    *  <p>Current format looks like
    *  <code>_a(3.1):c45/4->_1</code>, which means the segment's
    *  name is <code>_a</code>; it was created with Lucene 3.1 (or
-   *  '?' if it's unkown); it's using compound file
+   *  '?' if it's unknown); it's using compound file
    *  format (would be <code>C</code> if not compound); it
    *  has 45 documents; it has 4 deletions (this part is
    *  left off when there are no deletions); it's using the
@@ -718,7 +718,7 @@ public String toString(Directory dir, int pendingDelCount) {
       }
     } catch (Throwable e) {
       // Messy: because getHasVectors may be used in an
-      // un-thread-safe way, and may attempt to open an fnm
+      // thread-unsafe way, and may attempt to open an fnm
       // file that has since (legitimately) been deleted by
       // IndexWriter, instead of throwing these exceptions
       // up, just add v? to indicate we don't know if this
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
index a5d4daaf..a53d4b36 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -586,6 +586,11 @@ private final void mergeTerms() throws CorruptIndexException, IOException {
   private void mergePerDoc() throws IOException {
       final PerDocConsumer docsConsumer = codec
           .docsConsumer(new PerDocWriteState(segmentWriteState));
+      // TODO: remove this check when 3.x indexes are no longer supported
+      // (3.x indexes don't have docvalues)
+      if (docsConsumer == null) {
+        return;
+      }
       boolean success = false;
       try {
         docsConsumer.merge(mergeState);
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/Terms.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/Terms.java
index 96c5c8f5..c5de4301 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/Terms.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/Terms.java
@@ -155,9 +155,11 @@ public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, BytesRef term, TermS
     return termsEnum.docsAndPositions(liveDocs, reuse);
   }
 
-  public long getUniqueTermCount() throws IOException {
-    throw new UnsupportedOperationException("this reader does not implement getUniqueTermCount()");
-  }
+  /** Returns the number of terms for this field, or -1 if this 
+   *  measure isn't stored by the codec. Note that, just like 
+   *  other term measures, this measure does not take deleted 
+   *  documents into account. */
+  public abstract long getUniqueTermCount() throws IOException;
 
   /** Returns the sum of {@link TermsEnum#totalTermFreq} for
    *  all terms in this field, or -1 if this measure isn't
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/TermsEnum.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/TermsEnum.java
index e1bd0e31..d96ec0dc 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/TermsEnum.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/TermsEnum.java
@@ -147,7 +147,7 @@ public void seekExact(BytesRef term, TermState state) throws IOException {
    *  call this when the enum is unpositioned.  This method
    *  will not return null.
    *  
-   * @param liveDocs set bits are documents that should not
+   * @param liveDocs unset bits are documents that should not
    * be returned
    * @param reuse pass a prior DocsEnum for possible reuse */
   public abstract DocsEnum docs(Bits liveDocs, DocsEnum reuse) throws IOException;
@@ -155,7 +155,10 @@ public void seekExact(BytesRef term, TermState state) throws IOException {
   /** Get {@link DocsAndPositionsEnum} for the current term.
    *  Do not call this when the enum is unpositioned.
    *  This method will only return null if positions were
-   *  not indexed into the postings by this codec. */
+   *  not indexed into the postings by this codec.
+   *  @param liveDocs unset bits are documents that should not
+   *  be returned
+   *  @param reuse pass a prior DocsAndPositionsEnum for possible reuse */
   public abstract DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException;
 
   /**
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/TieredMergePolicy.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/TieredMergePolicy.java
index 6d85fac3..8a47a665 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/TieredMergePolicy.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/TieredMergePolicy.java
@@ -39,7 +39,7 @@
  *  <p>For normal merging, this policy first computes a
  *  "budget" of how many segments are allowed by be in the
  *  index.  If the index is over-budget, then the policy
- *  sorts segments by decresing size (pro-rating by percent
+ *  sorts segments by decreasing size (pro-rating by percent
  *  deletes), and then finds the least-cost merge.  Merge
  *  cost is measured by a combination of the "skew" of the
  *  merge (size of largest seg divided by smallest seg),
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java
index fc66d1ee..aa04ae2f 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java
@@ -766,7 +766,7 @@ public IntersectEnum(CompiledAutomaton compiled, BytesRef startTerm) throws IOEx
           arcs[arcIdx] = new FST.Arc<BytesRef>();
         }
 
-        // TODO: if the automaon is "smallish" we really
+        // TODO: if the automaton is "smallish" we really
         // should use the terms index to seek at least to
         // the initial term and likely to subsequent terms
         // (or, maybe just fallback to ATE for such cases).
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryCodec.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryCodec.java
index 8f603781..3eaa8ee0 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryCodec.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryCodec.java
@@ -75,7 +75,7 @@
  * queries that rely on advance will (AND BooleanQuery,
  * PhraseQuery) will be relatively slow!
  *
- * <p><b>NOTE</b>: this codec cannot adress more than ~2.1 GB
+ * <p><b>NOTE</b>: this codec cannot address more than ~2.1 GB
  * of postings, because the underlying FST uses an int
  * to address the underlying byte[].
  *
@@ -684,11 +684,13 @@ public long ord() {
     private final long sumTotalTermFreq;
     private final long sumDocFreq;
     private final int docCount;
+    private final int termCount;
     private FST<BytesRef> fst;
     private final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
     private final FieldInfo field;
 
-    public TermsReader(FieldInfos fieldInfos, IndexInput in) throws IOException {
+    public TermsReader(FieldInfos fieldInfos, IndexInput in, int termCount) throws IOException {
+      this.termCount = termCount;
       final int fieldNumber = in.readVInt();
       field = fieldInfos.fieldInfo(fieldNumber);
       if (field.indexOptions != IndexOptions.DOCS_ONLY) {
@@ -717,6 +719,11 @@ public int getDocCount() throws IOException {
       return docCount;
     }
 
+    @Override
+    public long getUniqueTermCount() throws IOException {
+      return termCount;
+    }
+
     @Override
     public TermsEnum iterator() {
       return new FSTTermsEnum(field, fst);
@@ -741,7 +748,7 @@ public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException
         if (termCount == 0) {
           break;
         }
-        final TermsReader termsReader = new TermsReader(state.fieldInfos, in);
+        final TermsReader termsReader = new TermsReader(state.fieldInfos, in, termCount);
         fields.put(termsReader.field.name, termsReader);
       }
     } finally {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexCodec.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexCodec.java
index e215e753..8c9e3634 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexCodec.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexCodec.java
@@ -85,11 +85,11 @@ public void getExtensions(Set<String> extensions) {
 
   @Override
   public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    throw new UnsupportedOperationException("PerDocConsumer is not supported by Preflex codec");
+    return null;
   }
 
   @Override
   public PerDocValues docsProducer(SegmentReadState state) throws IOException {
-    throw new UnsupportedOperationException("PerDocValues is not supported by Preflex codec");
+    return null;
   }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexFields.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexFields.java
index 1b16915a..e68555e9 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexFields.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexFields.java
@@ -162,6 +162,11 @@ public Terms terms(String field) {
     return preTerms.get(field);
   }
 
+  @Override
+  public long getUniqueTermCount() throws IOException {
+    return getTermsDict().size();
+  }
+
   synchronized private TermInfosReader getTermsDict() {
     if (tis != null) {
       return tis;
@@ -240,6 +245,11 @@ public TermsEnum iterator() throws IOException {
       }
     }
 
+    @Override
+    public long getUniqueTermCount() throws IOException {
+      return -1;
+    }
+
     @Override
     public long getSumTotalTermFreq() {
       return -1;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/SegmentTermEnum.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/SegmentTermEnum.java
index 6c6681d6..179e9462 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/SegmentTermEnum.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/SegmentTermEnum.java
@@ -61,6 +61,7 @@
   int skipInterval;
   int newSuffixStart;
   int maxSkipLevels;
+  private boolean first = true;
 
   SegmentTermEnum(IndexInput i, FieldInfos fis, boolean isi)
           throws CorruptIndexException, IOException {
@@ -123,6 +124,7 @@ final void seek(long pointer, long p, Term t, TermInfo ti)
     prevBuffer.reset();
     //System.out.println("  ste doSeek prev=" + prevBuffer.toTerm() + " this=" + this);
     termInfo.set(ti);
+    first = p == -1;
   }
 
   /** Increments the enumeration to the next element.  True if one exists.*/
@@ -162,6 +164,13 @@ public final boolean next() throws IOException {
   final int scanTo(Term term) throws IOException {
     scanBuffer.set(term);
     int count = 0;
+    if (first) {
+      // Always force initial next() in case term is
+      // Term("", "")
+      next();
+      first = false;
+      count++;
+    }
     while (scanBuffer.compareTo(termBuffer) > 0 && next()) {
       count++;
     }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/TermInfosReader.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/TermInfosReader.java
index 3ca8ca61..511d8776 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/TermInfosReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/TermInfosReader.java
@@ -47,9 +47,8 @@
   private final SegmentTermEnum origEnum;
   private final long size;
 
-  private final Term[] indexTerms;
-  private final TermInfo[] indexInfos;
-  private final long[] indexPointers;
+  private final TermInfosReaderIndex index;
+  private final int indexLength;
   
   private final int totalIndexInterval;
 
@@ -118,37 +117,22 @@ public Object clone() {
       if (indexDivisor != -1) {
         // Load terms index
         totalIndexInterval = origEnum.indexInterval * indexDivisor;
-        final SegmentTermEnum indexEnum = new SegmentTermEnum(directory.openInput(IndexFileNames.segmentFileName(segment, "", PreFlexCodec.TERMS_INDEX_EXTENSION),
+
+        final String indexFileName = IndexFileNames.segmentFileName(segment, "", PreFlexCodec.TERMS_INDEX_EXTENSION);
+        final SegmentTermEnum indexEnum = new SegmentTermEnum(directory.openInput(indexFileName,
                                                                                   context), fieldInfos, true);
 
         try {
-          int indexSize = 1+((int)indexEnum.size-1)/indexDivisor;  // otherwise read index
-
-          indexTerms = new Term[indexSize];
-          indexInfos = new TermInfo[indexSize];
-          indexPointers = new long[indexSize];
-
-          for (int i=0;indexEnum.next(); i++) {
-            indexTerms[i] = indexEnum.term();
-            assert indexTerms[i] != null;
-            assert indexTerms[i].text() != null;
-            assert indexTerms[i].field() != null;
-            indexInfos[i] = indexEnum.termInfo();
-            indexPointers[i] = indexEnum.indexPointer;
-        
-            for (int j = 1; j < indexDivisor; j++)
-              if (!indexEnum.next())
-                break;
-          }
+          index = new TermInfosReaderIndex(indexEnum, indexDivisor, dir.fileLength(indexFileName), totalIndexInterval);
+          indexLength = index.length();
         } finally {
           indexEnum.close();
         }
       } else {
         // Do not load terms index:
         totalIndexInterval = -1;
-        indexTerms = null;
-        indexInfos = null;
-        indexPointers = null;
+        index = null;
+        indexLength = -1;
       }
       success = true;
     } finally {
@@ -203,31 +187,6 @@ private final int compareAsUTF16(Term term1, Term term2) {
     }
   }
 
-  /** Returns the offset of the greatest index entry which is less than or equal to term.*/
-  private int getIndexOffset(Term term) {
-    int lo = 0;					  // binary search indexTerms[]
-    int hi = indexTerms.length - 1;
-
-    while (hi >= lo) {
-      int mid = (lo + hi) >>> 1;
-      assert indexTerms[mid] != null : "indexTerms = " + indexTerms.length + " mid=" + mid;
-      int delta = compareAsUTF16(term, indexTerms[mid]);
-      if (delta < 0)
-	hi = mid - 1;
-      else if (delta > 0)
-	lo = mid + 1;
-      else
-	return mid;
-    }
-    return hi;
-  }
-
-  private void seekEnum(SegmentTermEnum enumerator, int indexOffset) throws IOException {
-    enumerator.seek(indexPointers[indexOffset],
-                    ((long) indexOffset * totalIndexInterval) - 1,
-                    indexTerms[indexOffset], indexInfos[indexOffset]);
-  }
-
   /** Returns the TermInfo for a Term in the set, or null. */
   TermInfo get(Term term) throws IOException {
     return get(term, false);
@@ -272,8 +231,8 @@ TermInfo seekEnum(SegmentTermEnum enumerator, Term term, TermInfoAndOrd tiOrd, b
 	&& ((enumerator.prev() != null && compareAsUTF16(term, enumerator.prev())> 0)
 	    || compareAsUTF16(term, enumerator.term()) >= 0)) {
       int enumOffset = (int)(enumerator.position/totalIndexInterval)+1;
-      if (indexTerms.length == enumOffset	  // but before end of block
-          || compareAsUTF16(term, indexTerms[enumOffset]) < 0) {
+      if (indexLength == enumOffset    // but before end of block
+    || index.compareTo(term, enumOffset) < 0) {
        // no need to seek
 
         final TermInfo ti;
@@ -309,10 +268,10 @@ TermInfo seekEnum(SegmentTermEnum enumerator, Term term, TermInfoAndOrd tiOrd, b
       indexPos = (int) (tiOrd.termOrd / totalIndexInterval);
     } else {
       // Must do binary search:
-      indexPos = getIndexOffset(term);
+      indexPos = index.getIndexOffset(term);
     }
 
-    seekEnum(enumerator, indexPos);
+    index.seekEnum(enumerator, indexPos);
     enumerator.scanTo(term);
     final TermInfo ti;
 
@@ -320,14 +279,8 @@ TermInfo seekEnum(SegmentTermEnum enumerator, Term term, TermInfoAndOrd tiOrd, b
       ti = enumerator.termInfo;
       if (tiOrd == null) {
         if (useCache) {
-          // LUCENE-3183: it's possible, if term is Term("",
-          // ""), for the STE to be incorrectly un-positioned
-          // after scan-to; work around this by not caching in
-          // this case:
-          if (enumerator.position >= 0) {
             termsCache.put(new CloneableTerm(term), new TermInfoAndOrd(ti, enumerator.position));
           }
-        }
       } else {
         assert sameTermInfo(ti, tiOrd, enumerator);
         assert enumerator.position == tiOrd.termOrd;
@@ -358,7 +311,7 @@ private boolean sameTermInfo(TermInfo ti1, TermInfo ti2, SegmentTermEnum enumera
   }
 
   private void ensureIndexIsRead() {
-    if (indexTerms == null) {
+    if (index == null) {
       throw new IllegalStateException("terms index was not loaded when this reader was created");
     }
   }
@@ -368,10 +321,10 @@ long getPosition(Term term) throws IOException {
     if (size == 0) return -1;
 
     ensureIndexIsRead();
-    int indexOffset = getIndexOffset(term);
+    int indexOffset = index.getIndexOffset(term);
     
     SegmentTermEnum enumerator = getThreadResources().termEnum;
-    seekEnum(enumerator, indexOffset);
+    index.seekEnum(enumerator, indexOffset);
 
     while(compareAsUTF16(term, enumerator.term()) > 0 && enumerator.next()) {}
 
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/TermInfosReaderIndex.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/TermInfosReaderIndex.java
index e69de29b..d384ff9e 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/TermInfosReaderIndex.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/preflex/TermInfosReaderIndex.java
@@ -0,0 +1,252 @@
+package org.apache.lucene.index.codecs.preflex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.util.BitUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.PagedBytes.PagedBytesDataInput;
+import org.apache.lucene.util.PagedBytes.PagedBytesDataOutput;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.GrowableWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * This stores a monotonically increasing set of <Term, TermInfo> pairs in an
+ * index segment. Pairs are accessed either by Term or by ordinal position the
+ * set. The Terms and TermInfo are actually serialized and stored into a byte
+ * array and pointers to the position of each are stored in a int array.
+ */
+class TermInfosReaderIndex {
+
+  private static final int MAX_PAGE_BITS = 18; // 256 KB block
+  private Term[] fields;
+  private int totalIndexInterval;
+  private Comparator<BytesRef> comparator = BytesRef.getUTF8SortedAsUTF16Comparator();
+  private final PagedBytesDataInput dataInput;
+  private final PackedInts.Reader indexToDataOffset;
+  private final int indexSize;
+  private final int skipInterval;
+
+  /**
+   * Loads the segment information at segment load time.
+   * 
+   * @param indexEnum
+   *          the term enum.
+   * @param indexDivisor
+   *          the index divisor.
+   * @param tiiFileLength
+   *          the size of the tii file, used to approximate the size of the
+   *          buffer.
+   * @param totalIndexInterval
+   *          the total index interval.
+   */
+  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {
+    this.totalIndexInterval = totalIndexInterval;
+    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;
+    skipInterval = indexEnum.skipInterval;
+    // this is only an inital size, it will be GCed once the build is complete
+    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;
+    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));
+    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();
+
+    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);
+    String currentField = null;
+    List<String> fieldStrs = new ArrayList<String>();
+    int fieldCounter = -1;
+    for (int i = 0; indexEnum.next(); i++) {
+      Term term = indexEnum.term();
+      if (currentField == null || !currentField.equals(term.field())) {
+        currentField = term.field();
+        fieldStrs.add(currentField);
+        fieldCounter++;
+      }
+      TermInfo termInfo = indexEnum.termInfo();
+      indexToTerms.set(i, dataOutput.getPosition());
+      dataOutput.writeVInt(fieldCounter);
+      dataOutput.writeString(term.text());
+      dataOutput.writeVInt(termInfo.docFreq);
+      if (termInfo.docFreq >= skipInterval) {
+        dataOutput.writeVInt(termInfo.skipOffset);
+      }
+      dataOutput.writeVLong(termInfo.freqPointer);
+      dataOutput.writeVLong(termInfo.proxPointer);
+      dataOutput.writeVLong(indexEnum.indexPointer);
+      for (int j = 1; j < indexDivisor; j++) {
+        if (!indexEnum.next()) {
+          break;
+        }
+      }
+    }
+
+    fields = new Term[fieldStrs.size()];
+    for (int i = 0; i < fields.length; i++) {
+      fields[i] = new Term(fieldStrs.get(i));
+    }
+    
+    dataPagedBytes.freeze(true);
+    dataInput = dataPagedBytes.getDataInput();
+    indexToDataOffset = indexToTerms.getMutable();
+  }
+
+  private static int estimatePageBits(long estSize) {
+    return Math.max(Math.min(64 - BitUtil.nlz(estSize), MAX_PAGE_BITS), 4);
+  }
+
+  void seekEnum(SegmentTermEnum enumerator, int indexOffset) throws IOException {
+    PagedBytesDataInput input = (PagedBytesDataInput) dataInput.clone();
+    
+    input.setPosition(indexToDataOffset.get(indexOffset));
+
+    // read the term
+    int fieldId = input.readVInt();
+    Term field = fields[fieldId];
+    Term term = new Term(field.field(), input.readString());
+
+    // read the terminfo
+    TermInfo termInfo = new TermInfo();
+    termInfo.docFreq = input.readVInt();
+    if (termInfo.docFreq >= skipInterval) {
+      termInfo.skipOffset = input.readVInt();
+    } else {
+      termInfo.skipOffset = 0;
+    }
+    termInfo.freqPointer = input.readVLong();
+    termInfo.proxPointer = input.readVLong();
+
+    long pointer = input.readVLong();
+
+    // perform the seek
+    enumerator.seek(pointer, ((long) indexOffset * totalIndexInterval) - 1, term, termInfo);
+  }
+
+  /**
+   * Binary search for the given term.
+   * 
+   * @param term
+   *          the term to locate.
+   * @throws IOException 
+   */
+  int getIndexOffset(Term term) throws IOException {
+    int lo = 0;
+    int hi = indexSize - 1;
+    PagedBytesDataInput input = (PagedBytesDataInput) dataInput.clone();
+    BytesRef scratch = new BytesRef();
+    while (hi >= lo) {
+      int mid = (lo + hi) >>> 1;
+      int delta = compareTo(term, mid, input, scratch);
+      if (delta < 0)
+        hi = mid - 1;
+      else if (delta > 0)
+        lo = mid + 1;
+      else
+        return mid;
+    }
+    return hi;
+  }
+
+  /**
+   * Gets the term at the given position.  For testing.
+   * 
+   * @param termIndex
+   *          the position to read the term from the index.
+   * @return the term.
+   * @throws IOException
+   */
+  Term getTerm(int termIndex) throws IOException {
+    PagedBytesDataInput input = (PagedBytesDataInput) dataInput.clone();
+    input.setPosition(indexToDataOffset.get(termIndex));
+
+    // read the term
+    int fieldId = input.readVInt();
+    Term field = fields[fieldId];
+    return new Term(field.field(), input.readString());
+  }
+
+  /**
+   * Returns the number of terms.
+   * 
+   * @return int.
+   */
+  int length() {
+    return indexSize;
+  }
+
+  /**
+   * The compares the given term against the term in the index specified by the
+   * term index. ie It returns negative N when term is less than index term;
+   * 
+   * @param term
+   *          the given term.
+   * @param termIndex
+   *          the index of the of term to compare.
+   * @return int.
+   * @throws IOException 
+   */
+  int compareTo(Term term, int termIndex) throws IOException {
+    return compareTo(term, termIndex, (PagedBytesDataInput) dataInput.clone(), new BytesRef());
+  }
+
+  /**
+   * Compare the fields of the terms first, and if not equals return from
+   * compare. If equal compare terms.
+   * 
+   * @param term
+   *          the term to compare.
+   * @param termIndex
+   *          the position of the term in the input to compare
+   * @param input
+   *          the input buffer.
+   * @return int.
+   * @throws IOException 
+   */
+  private int compareTo(Term term, int termIndex, PagedBytesDataInput input, BytesRef reuse) throws IOException {
+    // if term field does not equal mid's field index, then compare fields
+    // else if they are equal, compare term's string values...
+    int c = compareField(term, termIndex, input);
+    if (c == 0) {
+      reuse.length = input.readVInt();
+      reuse.grow(reuse.length);
+      input.readBytes(reuse.bytes, 0, reuse.length);
+      return comparator.compare(term.bytes(), reuse);
+    }
+    return c;
+  }
+
+  /**
+   * Compares the fields before checking the text of the terms.
+   * 
+   * @param term
+   *          the given term.
+   * @param termIndex
+   *          the term that exists in the data block.
+   * @param input
+   *          the data block.
+   * @return int.
+   * @throws IOException 
+   */
+  private int compareField(Term term, int termIndex, PagedBytesDataInput input) throws IOException {
+    input.setPosition(indexToDataOffset.get(termIndex));
+    return term.field().compareTo(fields[input.readVInt()].field());
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReader.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReader.java
index aefad107..34b0dc7f 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReader.java
@@ -18,6 +18,8 @@
  */
 
 import java.io.IOException;
+import java.util.IdentityHashMap;
+import java.util.Map;
 
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
@@ -29,6 +31,9 @@
 import org.apache.lucene.store.ByteArrayDataInput;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeImpl;
+import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CodecUtil;
@@ -172,8 +177,6 @@ public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState) throws IOEx
     }
   }
 
-  // TODO: we could actually reuse, by having TL that
-  // holds the last wrapped reuse, and vice-versa
   @Override
   public DocsEnum docs(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsEnum reuse) throws IOException {
     PulsingTermState termState = (PulsingTermState) _termState;
@@ -185,20 +188,29 @@ public DocsEnum docs(FieldInfo field, BlockTermState _termState, Bits liveDocs,
           postings = new PulsingDocsEnum(field);
         }
       } else {
+        // the 'reuse' is actually the wrapped enum
+        PulsingDocsEnum previous = (PulsingDocsEnum) getOther(reuse);
+        if (previous != null && previous.canReuse(field)) {
+          postings = previous;
+        } else {
         postings = new PulsingDocsEnum(field);
       }
+      }
+      if (reuse != postings) {
+        setOther(postings, reuse); // postings.other = reuse
+      }
       return postings.reset(liveDocs, termState);
     } else {
-      // TODO: not great that we lose reuse of PulsingDocsEnum in this case:
       if (reuse instanceof PulsingDocsEnum) {
-        return wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, null);
+        DocsEnum wrapped = wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, getOther(reuse));
+        setOther(wrapped, reuse); // wrapped.other = reuse
+        return wrapped;
       } else {
         return wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, reuse);
       }
     }
   }
 
-  // TODO: -- not great that we can't always reuse
   @Override
   public DocsAndPositionsEnum docsAndPositions(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
     if (field.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
@@ -216,13 +228,23 @@ public DocsAndPositionsEnum docsAndPositions(FieldInfo field, BlockTermState _te
           postings = new PulsingDocsAndPositionsEnum(field);
         }
       } else {
+        // the 'reuse' is actually the wrapped enum
+        PulsingDocsAndPositionsEnum previous = (PulsingDocsAndPositionsEnum) getOther(reuse);
+        if (previous != null && previous.canReuse(field)) {
+          postings = previous;
+        } else {
         postings = new PulsingDocsAndPositionsEnum(field);
       }
-
+      }
+      if (reuse != postings) {
+        setOther(postings, reuse); // postings.other = reuse 
+      }
       return postings.reset(liveDocs, termState);
     } else {
       if (reuse instanceof PulsingDocsAndPositionsEnum) {
-        return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, null);
+        DocsAndPositionsEnum wrapped = wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, (DocsAndPositionsEnum) getOther(reuse));
+        setOther(wrapped, reuse); // wrapped.other = reuse
+        return wrapped;
       } else {
         return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, reuse);
       }
@@ -499,4 +521,69 @@ public BytesRef getPayload() throws IOException {
   public void close() throws IOException {
     wrappedPostingsReader.close();
   }
+  
+  /** for a docsenum, gets the 'other' reused enum.
+   * Example: Pulsing(Standard).
+   * when doing a term range query you are switching back and forth
+   * between Pulsing and Standard
+   * 
+   * The way the reuse works is that Pulsing.other = Standard and
+   * Standard.other = Pulsing.
+   */
+  private DocsEnum getOther(DocsEnum de) {
+    if (de == null) {
+      return null;
+    } else {
+      final AttributeSource atts = de.attributes();
+      return atts.addAttribute(PulsingEnumAttribute.class).enums().get(this);
+    }
+  }
+  
+  /** 
+   * for a docsenum, sets the 'other' reused enum.
+   * see getOther for an example.
+   */
+  private DocsEnum setOther(DocsEnum de, DocsEnum other) {
+    final AttributeSource atts = de.attributes();
+    return atts.addAttribute(PulsingEnumAttribute.class).enums().put(this, other);
+  }
+
+  /** 
+   * A per-docsenum attribute that stores additional reuse information
+   * so that pulsing enums can keep a reference to their wrapped enums,
+   * and vice versa. this way we can always reuse.
+   * 
+   * @lucene.internal */
+  public static interface PulsingEnumAttribute extends Attribute {
+    public Map<PulsingPostingsReader,DocsEnum> enums();
+  }
+    
+  /** @lucene.internal */
+  public static final class PulsingEnumAttributeImpl extends AttributeImpl implements PulsingEnumAttribute {
+    // we could store 'other', but what if someone 'chained' multiple postings readers,
+    // this could cause problems?
+    // TODO: we should consider nuking this map and just making it so if you do this,
+    // you don't reuse? and maybe pulsingPostingsReader should throw an exc if it wraps
+    // another pulsing, because this is just stupid and wasteful. 
+    // we still have to be careful in case someone does Pulsing(Stomping(Pulsing(...
+    private final Map<PulsingPostingsReader,DocsEnum> enums = 
+      new IdentityHashMap<PulsingPostingsReader,DocsEnum>();
+      
+    public Map<PulsingPostingsReader,DocsEnum> enums() {
+      return enums;
+    }
+
+    @Override
+    public void clear() {
+      // our state is per-docsenum, so this makes no sense.
+      // its best not to clear, in case a wrapped enum has a per-doc attribute or something
+      // and is calling clearAttributes(), so they don't nuke the reuse information!
+    }
+
+    @Override
+    public void copyTo(AttributeImpl target) {
+      // this makes no sense for us, because our state is per-docsenum.
+      // we don't want to copy any stuff over to another docsenum ever!
+    }
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
index 7919c4ea..7db26cb8 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
@@ -216,7 +216,7 @@ public long totalTermFreq() {
     @Override
     public DocsEnum docs(Bits liveDocs, DocsEnum reuse) throws IOException {
       SimpleTextDocsEnum docsEnum;
-      if (reuse != null && reuse instanceof SimpleTextDocsEnum && ((SimpleTextDocsEnum) reuse).canReuse(in)) {
+      if (reuse != null && reuse instanceof SimpleTextDocsEnum && ((SimpleTextDocsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
         docsEnum = (SimpleTextDocsEnum) reuse;
       } else {
         docsEnum = new SimpleTextDocsEnum();
@@ -231,7 +231,7 @@ public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum
       }
 
       SimpleTextDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse != null && reuse instanceof SimpleTextDocsAndPositionsEnum && ((SimpleTextDocsAndPositionsEnum) reuse).canReuse(in)) {
+      if (reuse != null && reuse instanceof SimpleTextDocsAndPositionsEnum && ((SimpleTextDocsAndPositionsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
         docsAndPositionsEnum = (SimpleTextDocsAndPositionsEnum) reuse;
       } else {
         docsAndPositionsEnum = new SimpleTextDocsAndPositionsEnum();
@@ -249,7 +249,7 @@ public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum
     private final IndexInput inStart;
     private final IndexInput in;
     private boolean omitTF;
-    private int docID;
+    private int docID = -1;
     private int tf;
     private Bits liveDocs;
     private final BytesRef scratch = new BytesRef(10);
@@ -268,6 +268,7 @@ public SimpleTextDocsEnum reset(long fp, Bits liveDocs, boolean omitTF) throws I
       this.liveDocs = liveDocs;
       in.seek(fp);
       this.omitTF = omitTF;
+      docID = -1;
       if (omitTF) {
         tf = 1;
       }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/Bytes.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/Bytes.java
index 4ab1a2a3..39e26ed3 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/Bytes.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/Bytes.java
@@ -107,7 +107,7 @@ private Bytes() { /* don't instantiate! */
    * @param bytesUsed
    *          an {@link AtomicLong} instance to track the used bytes within the
    *          {@link Writer}. A call to {@link Writer#finish(int)} will release
-   *          all internally used resources and frees the memeory tracking
+   *          all internally used resources and frees the memory tracking
    *          reference.
    * @param context 
    * @return a new {@link Writer} instance
@@ -393,6 +393,7 @@ public ValueType type() {
     protected int lastDocId = -1;
     protected int[] docToEntry;
     protected final BytesRefHash hash;
+    protected long maxBytes = 0;
     
     protected DerefBytesWriterBase(Directory dir, String id, String codecName,
         int codecVersion, Counter bytesUsed, IOContext context)
@@ -433,8 +434,11 @@ public void add(int docID, BytesRef bytes) throws IOException {
       int ord = hash.add(bytes);
       if (ord < 0) {
         ord = (-ord) - 1;
+      } else {
+        maxBytes += bytes.length;
       }
       
+      
       docToEntry[docID] = ord;
       lastDocId = docID;
     }
@@ -554,6 +558,8 @@ protected void writeIndex(IndexOutput idxOut, int docCount,
     private final PagedBytes pagedBytes;
     
     protected final PackedInts.Reader docToOrdIndex;
+    protected final PackedInts.Reader ordToOffsetIndex;
+
     protected final IndexInput datIn;
     protected final IndexInput idxIn;
     protected final BytesRef defaultValue = new BytesRef();
@@ -561,12 +567,12 @@ protected void writeIndex(IndexOutput idxOut, int docCount,
     protected final PagedBytes.Reader data;
 
     protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp, long bytesToRead, ValueType type) throws IOException {
-      this(datIn, idxIn, comp, new PagedBytes(PAGED_BYTES_BITS), bytesToRead, type);
+        Comparator<BytesRef> comp, long bytesToRead, ValueType type, boolean hasOffsets) throws IOException {
+      this(datIn, idxIn, comp, new PagedBytes(PAGED_BYTES_BITS), bytesToRead, type, hasOffsets);
     }
     
     protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp, PagedBytes pagedBytes, long bytesToRead,ValueType type)
+        Comparator<BytesRef> comp, PagedBytes pagedBytes, long bytesToRead, ValueType type, boolean hasOffsets)
         throws IOException {
       super(type, comp);
       assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
@@ -576,24 +582,19 @@ protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
       this.pagedBytes.copy(datIn, bytesToRead);
       data = pagedBytes.freeze(true);
       this.idxIn = idxIn;
+      ordToOffsetIndex = hasOffsets ? PackedInts.getReader(idxIn) : null; 
       docToOrdIndex = PackedInts.getReader(idxIn);
 
     }
     
     @Override
     public int ord(int docID) {
+      assert docToOrdIndex.get(docID) < getValueCount();
       return (int) docToOrdIndex.get(docID);
     }
 
     protected void closeIndexInput() throws IOException {
       IOUtils.close(datIn, idxIn);
     }
-    
-    /**
-     * Returns the largest doc id + 1 in this doc values source
-     */
-    public int maxDoc() {
-      return docToOrdIndex.size();
-    }
   }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java
index 45b2aed8..91d502a6 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java
@@ -19,17 +19,22 @@
 
 import java.io.IOException;
 import java.util.Comparator;
+import java.util.List;
 
-import org.apache.lucene.index.values.Bytes.BytesSortedSourceBase;
+import org.apache.lucene.index.codecs.MergeState;
 import org.apache.lucene.index.values.Bytes.BytesReaderBase;
+import org.apache.lucene.index.values.Bytes.BytesSortedSourceBase;
 import org.apache.lucene.index.values.Bytes.DerefBytesWriterBase;
 import org.apache.lucene.index.values.IndexDocValues.SortedSource;
+import org.apache.lucene.index.values.SortedBytesMergeUtils.MergeContext;
+import org.apache.lucene.index.values.SortedBytesMergeUtils.SortedSourceSlice;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.packed.PackedInts;
 
 // Stores fixed-length byte[] by deref, ie when two docs
@@ -53,6 +58,37 @@ public Writer(Directory dir, String id, Comparator<BytesRef> comp,
       this.comp = comp;
     }
 
+    @Override
+    public void merge(MergeState mergeState, IndexDocValues[] docValues)
+        throws IOException {
+      boolean success = false;
+      try {
+        final MergeContext ctx = SortedBytesMergeUtils.init(ValueType.BYTES_FIXED_SORTED, docValues, comp, mergeState);
+        List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
+        final IndexOutput datOut = getOrCreateDataOut();
+        datOut.writeInt(ctx.sizePerValues);
+        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
+        
+        final IndexOutput idxOut = getOrCreateIndexOut();
+        idxOut.writeInt(maxOrd);
+        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
+            PackedInts.bitsRequired(maxOrd));
+        for (SortedSourceSlice slice : slices) {
+          slice.writeOrds(ordsWriter);
+        }
+        ordsWriter.finish();
+        success = true;
+      } finally {
+        releaseResources();
+        if (success) {
+          IOUtils.close(getIndexOut(), getDataOut());
+        } else {
+          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
+        }
+
+      }
+    }
+
     // Important that we get docCount, in case there were
     // some last docs that we didn't see
     @Override
@@ -60,15 +96,15 @@ public void finishInternal(int docCount) throws IOException {
       fillDefault(docCount);
       final IndexOutput datOut = getOrCreateDataOut();
       final int count = hash.size();
-      final int[] address = new int[count]; // addr 0 is default values
+      final int[] address = new int[count];
       datOut.writeInt(size);
       if (size != -1) {
         final int[] sortedEntries = hash.sort(comp);
         // first dump bytes data, recording address as we go
-        final BytesRef bytesRef = new BytesRef(size);
+        final BytesRef spare = new BytesRef(size);
         for (int i = 0; i < count; i++) {
           final int e = sortedEntries[i];
-          final BytesRef bytes = hash.get(e, bytesRef);
+          final BytesRef bytes = hash.get(e, spare);
           assert bytes.length == size;
           datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
           address[e] = i;
@@ -95,8 +131,8 @@ public Reader(Directory dir, String id, int maxDoc, IOContext context,
 
     @Override
     public Source load() throws IOException {
-      return new FixedSortedSource(cloneData(), cloneIndex(), size,
-          valueCount, comparator);
+      return new FixedSortedSource(cloneData(), cloneIndex(), size, valueCount,
+          comparator);
     }
 
     @Override
@@ -117,7 +153,8 @@ public int getValueSize() {
 
     FixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
         int numValues, Comparator<BytesRef> comp) throws IOException {
-      super(datIn, idxIn, comp, size * numValues, ValueType.BYTES_FIXED_SORTED);
+      super(datIn, idxIn, comp, size * numValues, ValueType.BYTES_FIXED_SORTED,
+          false);
       this.size = size;
       this.valueCount = numValues;
       closeIndexInput();
@@ -165,9 +202,7 @@ public int ord(int docID) {
     public BytesRef getByOrd(int ord, BytesRef bytesRef) {
       try {
         datIn.seek(basePointer + size * ord);
-        if (bytesRef.bytes.length < size) {
           bytesRef.grow(size);
-        }
         datIn.readBytes(bytesRef.bytes, 0, size);
         bytesRef.length = size;
         bytesRef.offset = 0;
@@ -182,4 +217,5 @@ public int getValueCount() {
       return valueCount;
     }
   }
+
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/IndexDocValues.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/IndexDocValues.java
index 41bca30a..1e1b9d51 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/IndexDocValues.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/IndexDocValues.java
@@ -278,7 +278,7 @@ public int getByValue(BytesRef value, BytesRef spare) {
       return binarySearch(value, spare, 0, getValueCount() - 1);
     }    
 
-    protected int binarySearch(BytesRef b, BytesRef bytesRef, int low,
+    private int binarySearch(BytesRef b, BytesRef bytesRef, int low,
         int high) {
       int mid = 0;
       while (low <= high) {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/SortedBytesMergeUtils.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/SortedBytesMergeUtils.java
index e69de29b..549c79b2 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/SortedBytesMergeUtils.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/SortedBytesMergeUtils.java
@@ -0,0 +1,332 @@
+package org.apache.lucene.index.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.index.codecs.MergeState;
+import org.apache.lucene.index.codecs.MergeState.IndexReaderAndLiveDocs;
+import org.apache.lucene.index.values.IndexDocValues.SortedSource;
+import org.apache.lucene.index.values.IndexDocValues.Source;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.PriorityQueue;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * @lucene.internal
+ */
+final class SortedBytesMergeUtils {
+
+  private SortedBytesMergeUtils() {
+    // no instance
+  }
+
+  static MergeContext init(ValueType type, IndexDocValues[] docValues,
+      Comparator<BytesRef> comp, MergeState mergeState) {
+    int size = -1;
+    if (type == ValueType.BYTES_FIXED_SORTED) {
+      for (IndexDocValues indexDocValues : docValues) {
+        if (indexDocValues != null) {
+          size = indexDocValues.getValueSize();
+          break;
+        }
+      }
+      assert size >= 0;
+    }
+    return new MergeContext(comp, mergeState, size, type);
+  }
+
+  public static final class MergeContext {
+    private final Comparator<BytesRef> comp;
+    private final BytesRef missingValue = new BytesRef();
+    final int sizePerValues; // -1 if var length
+    final ValueType type;
+    final int[] docToEntry;
+    long[] offsets; // if non-null #mergeRecords collects byte offsets here
+
+    public MergeContext(Comparator<BytesRef> comp, MergeState mergeState,
+        int size, ValueType type) {
+      assert type == ValueType.BYTES_FIXED_SORTED || type == ValueType.BYTES_VAR_SORTED;
+      this.comp = comp;
+      this.sizePerValues = size;
+      this.type = type;
+      if (size > 0) {
+        missingValue.grow(size);
+        missingValue.length = size;
+      }
+      docToEntry = new int[mergeState.mergedDocCount];
+    }
+  }
+
+  static List<SortedSourceSlice> buildSlices(MergeState mergeState,
+      IndexDocValues[] docValues, MergeContext ctx) throws IOException {
+    final List<SortedSourceSlice> slices = new ArrayList<SortedSourceSlice>();
+    for (int i = 0; i < docValues.length; i++) {
+      final SortedSourceSlice nextSlice;
+      final Source directSource;
+      if (docValues[i] != null
+          && (directSource = docValues[i].getDirectSource()) != null) {
+        final SortedSourceSlice slice = new SortedSourceSlice(i, directSource
+            .asSortedSource(), mergeState, ctx.docToEntry);
+        nextSlice = slice;
+      } else {
+        nextSlice = new SortedSourceSlice(i, new MissingValueSource(ctx),
+            mergeState, ctx.docToEntry);
+      }
+      createOrdMapping(mergeState, nextSlice);
+      slices.add(nextSlice);
+    }
+    return Collections.unmodifiableList(slices);
+  }
+
+  /*
+   * In order to merge we need to map the ords used in each segment to the new
+   * global ords in the new segment. Additionally we need to drop values that
+   * are not referenced anymore due to deleted documents. This method walks all
+   * live documents and fetches their current ordinal. We store this ordinal per
+   * slice and (SortedSourceSlice#ordMapping) and remember the doc to ord
+   * mapping in docIDToRelativeOrd. After the merge SortedSourceSlice#ordMapping
+   * contains the new global ordinals for the relative index.
+   */
+  private static void createOrdMapping(MergeState mergeState,
+      SortedSourceSlice currentSlice) {
+    final int readerIdx = currentSlice.readerIdx;
+    final int[] currentDocMap = mergeState.docMaps[readerIdx];
+    final int docBase = currentSlice.docToOrdStart;
+    assert docBase == mergeState.docBase[readerIdx];
+    if (currentDocMap != null) { // we have deletes
+      for (int i = 0; i < currentDocMap.length; i++) {
+        final int doc = currentDocMap[i];
+        if (doc != -1) { // not deleted
+          final int ord = currentSlice.source.ord(i); // collect ords strictly
+                                                      // increasing
+          currentSlice.docIDToRelativeOrd[docBase + doc] = ord;
+          // use ord + 1 to identify unreferenced values (ie. == 0)
+          currentSlice.ordMapping[ord] = ord + 1;
+        }
+      }
+    } else { // no deletes
+      final IndexReaderAndLiveDocs indexReaderAndLiveDocs = mergeState.readers
+          .get(readerIdx);
+      final int numDocs = indexReaderAndLiveDocs.reader.numDocs();
+      assert indexReaderAndLiveDocs.liveDocs == null;
+      assert currentSlice.docToOrdEnd - currentSlice.docToOrdStart == numDocs;
+      for (int doc = 0; doc < numDocs; doc++) {
+        final int ord = currentSlice.source.ord(doc);
+        currentSlice.docIDToRelativeOrd[docBase + doc] = ord;
+        // use ord + 1 to identify unreferenced values (ie. == 0)
+        currentSlice.ordMapping[ord] = ord + 1;
+      }
+    }
+  }
+
+  static int mergeRecords(MergeContext ctx, IndexOutput datOut,
+      List<SortedSourceSlice> slices) throws IOException {
+    final RecordMerger merger = new RecordMerger(new MergeQueue(slices.size(),
+        ctx.comp), slices.toArray(new SortedSourceSlice[0]));
+    long[] offsets = ctx.offsets;
+    final boolean recordOffsets = offsets != null;
+    long offset = 0;
+    BytesRef currentMergedBytes;
+    merger.pushTop();
+    while (merger.queue.size() > 0) {
+      merger.pullTop();
+      currentMergedBytes = merger.current;
+      assert ctx.sizePerValues == -1 || ctx.sizePerValues == currentMergedBytes.length : "size: "
+          + ctx.sizePerValues + " spare: " + currentMergedBytes.length;
+
+      if (recordOffsets) {
+        offset += currentMergedBytes.length;
+        if (merger.currentOrd >= offsets.length) {
+          offsets = ArrayUtil.grow(offsets, merger.currentOrd + 1);
+        }
+        offsets[merger.currentOrd] = offset;
+      }
+      datOut.writeBytes(currentMergedBytes.bytes, currentMergedBytes.offset,
+          currentMergedBytes.length);
+      merger.pushTop();
+    }
+    ctx.offsets = offsets;
+    assert offsets == null || offsets[merger.currentOrd - 1] == offset;
+    return merger.currentOrd;
+  }
+
+  private static final class RecordMerger {
+    private final MergeQueue queue;
+    private final SortedSourceSlice[] top;
+    private int numTop;
+    BytesRef current;
+    int currentOrd = -1;
+
+    RecordMerger(MergeQueue queue, SortedSourceSlice[] top) {
+      super();
+      this.queue = queue;
+      this.top = top;
+      this.numTop = top.length;
+    }
+
+    private void pullTop() {
+      // extract all subs from the queue that have the same
+      // top record
+      assert numTop == 0;
+      assert currentOrd >= 0;
+      while (true) {
+        final SortedSourceSlice popped = top[numTop++] = queue.pop();
+        // use ord + 1 to identify unreferenced values (ie. == 0)
+        popped.ordMapping[popped.relativeOrd] = currentOrd + 1;
+        if (queue.size() == 0
+            || !(queue.top()).current.bytesEquals(top[0].current)) {
+          break;
+        }
+      }
+      current = top[0].current;
+    }
+
+    private void pushTop() throws IOException {
+      // call next() on each top, and put back into queue
+      for (int i = 0; i < numTop; i++) {
+        top[i].current = top[i].next();
+        if (top[i].current != null) {
+          queue.add(top[i]);
+        }
+      }
+      currentOrd++;
+      numTop = 0;
+    }
+  }
+
+  static class SortedSourceSlice {
+    final SortedSource source;
+    final int readerIdx;
+    /* global array indexed by docID containg the relative ord for the doc */
+    final int[] docIDToRelativeOrd;
+    /*
+     * maps relative ords to merged global ords - index is relative ord value
+     * new global ord this map gets updates as we merge ords. later we use the
+     * docIDtoRelativeOrd to get the previous relative ord to get the new ord
+     * from the relative ord map.
+     */
+    final int[] ordMapping;
+
+    /* start index into docIDToRelativeOrd */
+    final int docToOrdStart;
+    /* end index into docIDToRelativeOrd */
+    final int docToOrdEnd;
+    BytesRef current = new BytesRef();
+    /* the currently merged relative ordinal */
+    int relativeOrd = -1;
+
+    SortedSourceSlice(int readerIdx, SortedSource source, MergeState state,
+        int[] docToOrd) {
+      super();
+      this.readerIdx = readerIdx;
+      this.source = source;
+      this.docIDToRelativeOrd = docToOrd;
+      this.ordMapping = new int[source.getValueCount()];
+      this.docToOrdStart = state.docBase[readerIdx];
+      this.docToOrdEnd = this.docToOrdStart + numDocs(state, readerIdx);
+    }
+
+    private static int numDocs(MergeState state, int readerIndex) {
+      if (readerIndex == state.docBase.length - 1) {
+        return state.mergedDocCount - state.docBase[readerIndex];
+      }
+      return state.docBase[readerIndex + 1] - state.docBase[readerIndex];
+    }
+
+    BytesRef next() {
+      for (int i = relativeOrd + 1; i < ordMapping.length; i++) {
+        if (ordMapping[i] != 0) { // skip ords that are not referenced anymore
+          source.getByOrd(i, current);
+          relativeOrd = i;
+          return current;
+        }
+      }
+      return null;
+    }
+
+    void writeOrds(PackedInts.Writer writer) throws IOException {
+      for (int i = docToOrdStart; i < docToOrdEnd; i++) {
+        final int mappedOrd = docIDToRelativeOrd[i];
+        assert mappedOrd < ordMapping.length;
+        assert ordMapping[mappedOrd] > 0 : "illegal mapping ord maps to an unreferenced value";
+        writer.add(ordMapping[mappedOrd] - 1);
+      }
+    }
+  }
+
+  /*
+   * if a segment has no values at all we use this source to fill in the missing
+   * value in the right place (depending on the comparator used)
+   */
+  private static final class MissingValueSource extends SortedSource {
+
+    private BytesRef missingValue;
+
+    public MissingValueSource(MergeContext ctx) {
+      super(ctx.type, ctx.comp);
+      this.missingValue = ctx.missingValue;
+    }
+
+    @Override
+    public int ord(int docID) {
+      return 0;
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      bytesRef.copy(missingValue);
+      return bytesRef;
+    }
+
+    @Override
+    public int getValueCount() {
+      return 1;
+    }
+
+  }
+
+  /*
+   * merge queue
+   */
+  private static final class MergeQueue extends
+      PriorityQueue<SortedSourceSlice> {
+    final Comparator<BytesRef> comp;
+
+    public MergeQueue(int maxSize, Comparator<BytesRef> comp) {
+      super(maxSize);
+      this.comp = comp;
+    }
+
+    @Override
+    protected boolean lessThan(SortedSourceSlice a, SortedSourceSlice b) {
+      int cmp = comp.compare(a.current, b.current);
+      if (cmp != 0) {
+        return cmp < 0;
+      } else { // just a tie-breaker
+        return a.docToOrdStart < b.docToOrdStart;
+      }
+    }
+
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java
index bd039bd6..ab2fa183 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java
@@ -19,17 +19,22 @@
 
 import java.io.IOException;
 import java.util.Comparator;
+import java.util.List;
 
+import org.apache.lucene.index.codecs.MergeState;
 import org.apache.lucene.index.values.Bytes.BytesSortedSourceBase;
 import org.apache.lucene.index.values.Bytes.BytesReaderBase;
 import org.apache.lucene.index.values.Bytes.DerefBytesWriterBase;
 import org.apache.lucene.index.values.IndexDocValues.SortedSource;
+import org.apache.lucene.index.values.SortedBytesMergeUtils.MergeContext;
+import org.apache.lucene.index.values.SortedBytesMergeUtils.SortedSourceSlice;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.packed.PackedInts;
 
 // Stores variable-length byte[] by deref, ie when two docs
@@ -54,6 +59,47 @@ public Writer(Directory dir, String id, Comparator<BytesRef> comp,
       this.comp = comp;
       size = 0;
     }
+    @Override
+    public void merge(MergeState mergeState, IndexDocValues[] docValues)
+        throws IOException {
+      boolean success = false;
+      try {
+        MergeContext ctx = SortedBytesMergeUtils.init(ValueType.BYTES_VAR_SORTED, docValues, comp, mergeState);
+        final List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
+        IndexOutput datOut = getOrCreateDataOut();
+        
+        ctx.offsets = new long[1];
+        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
+        final long[] offsets = ctx.offsets;
+        maxBytes = offsets[maxOrd-1];
+        final IndexOutput idxOut = getOrCreateIndexOut();
+        
+        idxOut.writeLong(maxBytes);
+        final PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, maxOrd+1,
+            PackedInts.bitsRequired(maxBytes));
+        offsetWriter.add(0);
+        for (int i = 0; i < maxOrd; i++) {
+          offsetWriter.add(offsets[i]);
+        }
+        offsetWriter.finish();
+        
+        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
+            PackedInts.bitsRequired(maxOrd-1));
+        for (SortedSourceSlice slice : slices) {
+          slice.writeOrds(ordsWriter);
+        }
+        ordsWriter.finish();
+        success = true;
+      } finally {
+        releaseResources();
+        if (success) {
+          IOUtils.close(getIndexOut(), getDataOut());
+        } else {
+          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
+        }
+
+      }
+    }
 
     @Override
     protected void checkSize(BytesRef bytes) {
@@ -67,35 +113,31 @@ public void finishInternal(int docCount) throws IOException {
       fillDefault(docCount);
       final int count = hash.size();
       final IndexOutput datOut = getOrCreateDataOut();
+      final IndexOutput idxOut = getOrCreateIndexOut();
       long offset = 0;
       final int[] index = new int[count];
-      final long[] offsets = new long[count];
       final int[] sortedEntries = hash.sort(comp);
-      // first dump bytes data, recording index & offset as
+      // total bytes of data
+      idxOut.writeLong(maxBytes);
+      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,
+          PackedInts.bitsRequired(maxBytes));
+      // first dump bytes data, recording index & write offset as
       // we go
+      final BytesRef spare = new BytesRef();
       for (int i = 0; i < count; i++) {
         final int e = sortedEntries[i];
-        offsets[i] = offset;
+        offsetWriter.add(offset);
         index[e] = i;
-
-        final BytesRef bytes = hash.get(e, new BytesRef());
+        final BytesRef bytes = hash.get(e, spare);
         // TODO: we could prefix code...
         datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
         offset += bytes.length;
       }
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      // total bytes of data
-      idxOut.writeLong(offset);
-      // write index
-      writeIndex(idxOut, docCount, count, index, docToEntry);
-      // next ord (0-based) -> offset
-      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,
-          PackedInts.bitsRequired(offset));
-      for (int i = 0; i < count; i++) {
-        offsetWriter.add(offsets[i]);
-      }
       offsetWriter.add(offset);
       offsetWriter.finish();
+      // write index
+      writeIndex(idxOut, docCount, count, index, docToEntry);
+
     }
   }
 
@@ -123,13 +165,11 @@ public Source getDirectSource() throws IOException {
     
   }
   private static final class VarSortedSource extends BytesSortedSourceBase {
-    private final PackedInts.Reader ordToOffsetIndex; // 0-based
     private final int valueCount;
 
     VarSortedSource(IndexInput datIn, IndexInput idxIn,
         Comparator<BytesRef> comp) throws IOException {
-      super(datIn, idxIn, comp, idxIn.readLong(), ValueType.BYTES_VAR_SORTED);
-      ordToOffsetIndex = PackedInts.getReader(idxIn);
+      super(datIn, idxIn, comp, idxIn.readLong(), ValueType.BYTES_VAR_SORTED, true);
       valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
       closeIndexInput();
     }
@@ -149,7 +189,7 @@ public int getValueCount() {
   }
 
   private static final class DirectSortedSource extends SortedSource {
-    private final PackedInts.Reader docToOrdIndex;
+    private final PackedInts.RandomAccessReaderIterator docToOrdIndex;
     private final PackedInts.RandomAccessReaderIterator ordToOffsetIndex;
     private final IndexInput datIn;
     private final long basePointer;
@@ -159,16 +199,22 @@ public int getValueCount() {
         Comparator<BytesRef> comparator, ValueType type) throws IOException {
       super(type, comparator);
       idxIn.readLong();
-      docToOrdIndex = PackedInts.getReader(idxIn); // read the ords in to prevent too many random disk seeks
       ordToOffsetIndex = PackedInts.getRandomAccessReaderIterator(idxIn);
       valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
+      // advance this iterator to the end and clone the stream once it points to the docToOrdIndex header
+      ordToOffsetIndex.advance(valueCount);
+      docToOrdIndex = PackedInts.getRandomAccessReaderIterator((IndexInput) idxIn.clone()); // read the ords in to prevent too many random disk seeks
       basePointer = datIn.getFilePointer();
       this.datIn = datIn;
     }
 
     @Override
     public int ord(int docID) {
+      try {
       return (int) docToOrdIndex.get(docID);
+      } catch (IOException ex) {
+        throw new IllegalStateException("failed", ex);
+      }
     }
 
     @Override
@@ -178,9 +224,7 @@ public BytesRef getByOrd(int ord, BytesRef bytesRef) {
         final long nextOffset = ordToOffsetIndex.next();
         datIn.seek(basePointer + offset);
         final int length = (int) (nextOffset - offset);
-        if (bytesRef.bytes.length < length) {
           bytesRef.grow(length);
-        }
         datIn.readBytes(bytesRef.bytes, 0, length);
         bytesRef.length = length;
         bytesRef.offset = 0;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BitsFilteredDocIdSet.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BitsFilteredDocIdSet.java
index e69de29b..f196ba98 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BitsFilteredDocIdSet.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BitsFilteredDocIdSet.java
@@ -0,0 +1,63 @@
+package org.apache.lucene.search;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.Bits;
+
+/**
+ * This implementation supplies a filtered DocIdSet, that excludes all
+ * docids which are not in a Bits instance. This is especially useful in
+ * {@link org.apache.lucene.search.Filter} to apply the {@code acceptDocs}
+ * passed to {@code getDocIdSet()} before returning the final DocIdSet.
+ *
+ * @see DocIdSet
+ * @see org.apache.lucene.search.Filter
+ */
+
+public final class BitsFilteredDocIdSet extends FilteredDocIdSet {
+
+  private final Bits acceptDocs;
+  
+  /**
+   * Convenience wrapper method: If {@code acceptDocs == null} it returns the original set without wrapping.
+   * @param set Underlying DocIdSet. If {@code null}, this method returns {@code null}
+   * @param acceptDocs Allowed docs, all docids not in this set will not be returned by this DocIdSet.
+   * If {@code null}, this method returns the original set without wrapping.
+   */
+  public static DocIdSet wrap(DocIdSet set, Bits acceptDocs) {
+    return (set == null || acceptDocs == null) ? set : new BitsFilteredDocIdSet(set, acceptDocs);
+  }
+  
+  /**
+   * Constructor.
+   * @param innerSet Underlying DocIdSet
+   * @param acceptDocs Allowed docs, all docids not in this set will not be returned by this DocIdSet
+   */
+  public BitsFilteredDocIdSet(DocIdSet innerSet, Bits acceptDocs) {
+    super(innerSet);
+    if (acceptDocs == null)
+      throw new NullPointerException("acceptDocs is null");
+    this.acceptDocs = acceptDocs;
+  }
+
+  @Override
+  protected boolean match(int docid) {
+    return acceptDocs.get(docid);
+  }
+
+}
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BooleanQuery.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BooleanQuery.java
index 77a474eb..18821984 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BooleanQuery.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BooleanQuery.java
@@ -329,7 +329,7 @@ public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
       }
       
       // Check if we can return a BooleanScorer
-      if (!scoreDocsInOrder && topScorer && required.size() == 0 && prohibited.size() < 32) {
+      if (!scoreDocsInOrder && topScorer && required.size() == 0) {
         return new BooleanScorer(this, disableCoord, minNrShouldMatch, optional, prohibited, maxCoord);
       }
       
@@ -366,17 +366,10 @@ private Scorer createConjunctionTermScorer(AtomicReaderContext context, Bits acc
     
     @Override
     public boolean scoresDocsOutOfOrder() {
-      int numProhibited = 0;
       for (BooleanClause c : clauses) {
         if (c.isRequired()) {
           return false; // BS2 (in-order) will be used by scorer()
-        } else if (c.isProhibited()) {
-          ++numProhibited;
-        }
       }
-      
-      if (numProhibited > 32) { // cannot use BS
-        return false;
       }
       
       // scorer() will return an out-of-order scorer if requested.
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BooleanScorer.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BooleanScorer.java
index 07c17f2f..e7413351 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BooleanScorer.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/BooleanScorer.java
@@ -29,10 +29,10 @@
 /* Description from Doug Cutting (excerpted from
  * LUCENE-1483):
  *
- * BooleanScorer uses a ~16k array to score windows of
- * docs. So it scores docs 0-16k first, then docs 16-32k,
+ * BooleanScorer uses an array to score windows of
+ * 2K docs. So it scores docs 0-2K first, then docs 2K-4K,
  * etc. For each window it iterates through all query terms
- * and accumulates a score in table[doc%16k]. It also stores
+ * and accumulates a score in table[doc%2K]. It also stores
  * in the table a bitmask representing which terms
  * contributed to the score. Non-zero scores are chained in
  * a linked list. At the end of scoring each window it then
@@ -75,9 +75,7 @@ public BooleanScorerCollector(int mask, BucketTable bucketTable) {
     public void collect(final int doc) throws IOException {
       final BucketTable table = bucketTable;
       final int i = doc & BucketTable.MASK;
-      Bucket bucket = table.buckets[i];
-      if (bucket == null)
-        table.buckets[i] = bucket = new Bucket();
+      final Bucket bucket = table.buckets[i];
       
       if (bucket.doc != doc) {                    // invalid bucket
         bucket.doc = doc;                         // set doc
@@ -143,6 +141,9 @@ public boolean acceptsDocsOutOfOrder() {
   static final class Bucket {
     int doc = -1;            // tells if bucket is valid
     float score;             // incremental score
+    // TODO: break out bool anyProhibited, int
+    // numRequiredMatched; then we can remove 32 limit on
+    // required clauses
     int bits;                // used for bool constraints
     int coord;               // count of terms in score
     Bucket next;             // next valid bucket
@@ -156,7 +157,13 @@ public boolean acceptsDocsOutOfOrder() {
     final Bucket[] buckets = new Bucket[SIZE];
     Bucket first = null;                          // head of valid list
   
-    public BucketTable() {}
+    public BucketTable() {
+      // Pre-fill to save the lazy init when collecting
+      // each sub:
+      for(int idx=0;idx<SIZE;idx++) {
+        buckets[idx] = new Bucket();
+      }
+    }
 
     public Collector newCollector(int mask) {
       return new BooleanScorerCollector(mask, this);
@@ -169,7 +176,7 @@ public Collector newCollector(int mask) {
     public Scorer scorer;
     // TODO: re-enable this if BQ ever sends us required clauses
     //public boolean required = false;
-    public boolean prohibited = false;
+    public boolean prohibited;
     public Collector collector;
     public SubScorer next;
 
@@ -193,13 +200,14 @@ public SubScorer(Scorer scorer, boolean required, boolean prohibited,
   private final float[] coordFactors;
   // TODO: re-enable this if BQ ever sends us required clauses
   //private int requiredMask = 0;
-  private int prohibitedMask = 0;
-  private int nextMask = 1;
   private final int minNrShouldMatch;
   private int end;
   private Bucket current;
   private int doc = -1;
   
+  // Any time a prohibited clause matches we set bit 0:
+  private static final int PROHIBITED_MASK = 1;
+  
   BooleanScorer(BooleanWeight weight, boolean disableCoord, int minNrShouldMatch,
       List<Scorer> optionalScorers, List<Scorer> prohibitedScorers, int maxCoord) throws IOException {
     super(weight);
@@ -215,11 +223,8 @@ public SubScorer(Scorer scorer, boolean required, boolean prohibited,
     
     if (prohibitedScorers != null && prohibitedScorers.size() > 0) {
       for (Scorer scorer : prohibitedScorers) {
-        int mask = nextMask;
-        nextMask = nextMask << 1;
-        prohibitedMask |= mask;                     // update prohibited mask
         if (scorer.nextDoc() != NO_MORE_DOCS) {
-          scorers = new SubScorer(scorer, false, true, bucketTable.newCollector(mask), scorers);
+          scorers = new SubScorer(scorer, false, true, bucketTable.newCollector(PROHIBITED_MASK), scorers);
         }
       }
     }
@@ -233,9 +238,12 @@ public SubScorer(Scorer scorer, boolean required, boolean prohibited,
   // firstDocID is ignored since nextDoc() initializes 'current'
   @Override
   public boolean score(Collector collector, int max, int firstDocID) throws IOException {
+    // Make sure it's only BooleanScorer that calls us:
+    assert firstDocID == -1;
     boolean more;
     Bucket tmp;
     BucketScorer bs = new BucketScorer(weight);
+
     // The internal loop will set the score and doc before calling collect.
     collector.setScorer(bs);
     do {
@@ -244,12 +252,13 @@ public boolean score(Collector collector, int max, int firstDocID) throws IOExce
       while (current != null) {         // more queued 
 
         // check prohibited & required
-        if ((current.bits & prohibitedMask) == 0) {
+        if ((current.bits & PROHIBITED_MASK) == 0) {
 
             // TODO: re-enable this if BQ ever sends us required
             // clauses
             //&& (current.bits & requiredMask) == requiredMask) {
           
+          // TODO: can we remove this?  
           if (current.doc >= max){
             tmp = current;
             current = current.next;
@@ -298,48 +307,22 @@ public int advance(int target) throws IOException {
 
   @Override
   public int docID() {
-    return doc;
+    throw new UnsupportedOperationException();
   }
 
   @Override
   public int nextDoc() throws IOException {
-    boolean more;
-    do {
-      while (bucketTable.first != null) {         // more queued
-        current = bucketTable.first;
-        bucketTable.first = current.next;         // pop the queue
-
-        // check prohibited & required, and minNrShouldMatch
-        if ((current.bits & prohibitedMask) == 0 &&
-            current.coord >= minNrShouldMatch) {
-          // TODO: re-enable this if BQ ever sends us required clauses
-          // (current.bits & requiredMask) == requiredMask &&
-          return doc = current.doc;
-        }
-      }
-
-      // refill the queue
-      more = false;
-      end += BucketTable.SIZE;
-      for (SubScorer sub = scorers; sub != null; sub = sub.next) {
-        int subScorerDocID = sub.scorer.docID();
-        if (subScorerDocID != NO_MORE_DOCS) {
-          more |= sub.scorer.score(sub.collector, end, subScorerDocID);
-        }
-      }
-    } while (bucketTable.first != null || more);
-
-    return doc = NO_MORE_DOCS;
+    throw new UnsupportedOperationException();
   }
 
   @Override
   public float score() {
-    return current.score * coordFactors[current.coord];
+    throw new UnsupportedOperationException();
   }
 
   @Override
   public void score(Collector collector) throws IOException {
-    score(collector, Integer.MAX_VALUE, nextDoc());
+    score(collector, Integer.MAX_VALUE, -1);
   }
   
   @Override
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/CachingSpanFilter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/CachingSpanFilter.java
index 3fe79d71..61053fb8 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/CachingSpanFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/CachingSpanFilter.java
@@ -19,8 +19,11 @@
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.FixedBitSet;
 
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
 
 /**
  * Wraps another SpanFilter's result and caches it.  The purpose is to allow
@@ -34,36 +37,63 @@
    */
   private final CachingWrapperFilter.FilterCache<SpanFilterResult> cache;
 
-  /**
-   * New deletions always result in a cache miss, by default
-   * ({@link CachingWrapperFilter.DeletesMode#RECACHE}.
+  /** Wraps another SpanFilter's result and caches it.
    * @param filter Filter to cache results of
    */
   public CachingSpanFilter(SpanFilter filter) {
-    this(filter, CachingWrapperFilter.DeletesMode.RECACHE);
+    this.filter = filter;
+    this.cache = new CachingWrapperFilter.FilterCache<SpanFilterResult>();
   }
 
-  /**
-   * @param filter Filter to cache results of
-   * @param deletesMode See {@link CachingWrapperFilter.DeletesMode}
-   */
-  public CachingSpanFilter(SpanFilter filter, CachingWrapperFilter.DeletesMode deletesMode) {
-    this.filter = filter;
-    if (deletesMode == CachingWrapperFilter.DeletesMode.DYNAMIC) {
-      throw new IllegalArgumentException("DeletesMode.DYNAMIC is not supported");
+  @Override
+  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+    final SpanFilterResult result = getCachedResult(context);
+    return BitsFilteredDocIdSet.wrap(result.getDocIdSet(), acceptDocs);
     }
-    this.cache = new CachingWrapperFilter.FilterCache<SpanFilterResult>(deletesMode) {
+
       @Override
-      protected SpanFilterResult mergeLiveDocs(final Bits liveDocs, final SpanFilterResult value) {
-        throw new IllegalStateException("DeletesMode.DYNAMIC is not supported");
+  public SpanFilterResult bitSpans(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+    final SpanFilterResult result = getCachedResult(context);
+    if (acceptDocs == null) {
+      return result;
+    } else {
+      // TODO: filter positions more efficient
+      List<SpanFilterResult.PositionInfo> allPositions = result.getPositions();
+      List<SpanFilterResult.PositionInfo> positions = new ArrayList<SpanFilterResult.PositionInfo>(allPositions.size() / 2 + 1);
+      for (SpanFilterResult.PositionInfo p : allPositions) {
+        if (acceptDocs.get(p.getDoc())) {
+          positions.add(p);
+        }        
+      }
+      return new SpanFilterResult(BitsFilteredDocIdSet.wrap(result.getDocIdSet(), acceptDocs), positions);
       }
-    };
   }
 
-  @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
-    SpanFilterResult result = getCachedResult(context);
-    return result != null ? result.getDocIdSet() : null;
+  /** Provide the DocIdSet to be cached, using the DocIdSet provided
+   *  by the wrapped Filter.
+   *  <p>This implementation returns the given {@link DocIdSet}, if {@link DocIdSet#isCacheable}
+   *  returns <code>true</code>, else it copies the {@link DocIdSetIterator} into
+   *  an {@link FixedBitSet}.
+   */
+  protected SpanFilterResult spanFilterResultToCache(SpanFilterResult result, IndexReader reader) throws IOException {
+    if (result == null || result.getDocIdSet() == null) {
+      // this is better than returning null, as the nonnull result can be cached
+      return SpanFilterResult.EMPTY_SPAN_FILTER_RESULT;
+    } else if (result.getDocIdSet().isCacheable()) {
+      return result;
+    } else {
+      final DocIdSetIterator it = result.getDocIdSet().iterator();
+      // null is allowed to be returned by iterator(),
+      // in this case we wrap with the empty set,
+      // which is cacheable.
+      if (it == null) {
+        return SpanFilterResult.EMPTY_SPAN_FILTER_RESULT;
+      } else {
+        final FixedBitSet bits = new FixedBitSet(reader.maxDoc());
+        bits.or(it);
+        return new SpanFilterResult(bits, result.getPositions());
+      }
+    }
   }
   
   // for testing
@@ -71,27 +101,21 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
 
   private SpanFilterResult getCachedResult(AtomicReaderContext context) throws IOException {
     final IndexReader reader = context.reader;
-
     final Object coreKey = reader.getCoreCacheKey();
-    final Object delCoreKey = reader.hasDeletions() ? reader.getLiveDocs() : coreKey;
 
-    SpanFilterResult result = cache.get(reader, coreKey, delCoreKey);
+    SpanFilterResult result = cache.get(reader, coreKey);
     if (result != null) {
       hitCount++;
       return result;
-    }
-
+    } else {
     missCount++;
-    result = filter.bitSpans(context);
-
-    cache.put(coreKey, delCoreKey, result);
-    return result;
+      // cache miss: we use no acceptDocs here
+      // (this saves time on building SpanFilterResult, the acceptDocs will be applied on the cached set)
+      result = spanFilterResultToCache(filter.bitSpans(context, null/**!!!*/), reader);
+      cache.put(coreKey, result);
   }
 
-
-  @Override
-  public SpanFilterResult bitSpans(AtomicReaderContext context) throws IOException {
-    return getCachedResult(context);
+    return result;
   }
 
   @Override
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java
index 1cb10bfa..c18266e8 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java
@@ -32,11 +32,6 @@
  * Wraps another filter's result and caches it.  The purpose is to allow
  * filters to simply filter, and then wrap with this class
  * to add caching.
- *
- * <p><b>NOTE</b>: if you wrap this filter as a query (eg,
- * using ConstantScoreQuery), you'll likely want to enforce
- * deletions (using either {@link DeletesMode#RECACHE} or
- * {@link DeletesMode#DYNAMIC}).
  */
 public class CachingWrapperFilter extends Filter {
   // TODO: make this filter aware of ReaderContext. a cached filter could 
@@ -44,32 +39,9 @@
   // level of the readers hierarchy it should be cached.
   Filter filter;
 
-  /**
-   * Expert: Specifies how new deletions against a reopened
-   * reader should be handled.
-   *
-   * <p>The default is IGNORE, which means the cache entry
-   * will be re-used for a given segment, even when that
-   * segment has been reopened due to changes in deletions.
-   * This is a big performance gain, especially with
-   * near-real-timer readers, since you don't hit a cache
-   * miss on every reopened reader for prior segments.</p>
-   *
-   * <p>However, in some cases this can cause invalid query
-   * results, allowing deleted documents to be returned.
-   * This only happens if the main query does not rule out
-   * deleted documents on its own, such as a toplevel
-   * ConstantScoreQuery.  To fix this, use RECACHE to
-   * re-create the cached filter (at a higher per-reopen
-   * cost, but at faster subsequent search performance), or
-   * use DYNAMIC to dynamically intersect deleted docs (fast
-   * reopen time but some hit to search performance).</p>
-   */
-  public static enum DeletesMode {IGNORE, RECACHE, DYNAMIC};
-
   protected final FilterCache<DocIdSet> cache;
 
-  static abstract class FilterCache<T> {
+  static class FilterCache<T> {
 
     /**
      * A transient Filter cache (package private because of test)
@@ -78,97 +50,27 @@
     // after de-serialize
     transient Map<Object,T> cache;
 
-    private final DeletesMode deletesMode;
-
-    public FilterCache(DeletesMode deletesMode) {
-      this.deletesMode = deletesMode;
-    }
-
-    public synchronized T get(IndexReader reader, Object coreKey, Object delCoreKey) throws IOException {
+    public synchronized T get(IndexReader reader, Object coreKey) throws IOException {
       T value;
 
       if (cache == null) {
         cache = new WeakHashMap<Object,T>();
       }
 
-      if (deletesMode == DeletesMode.IGNORE) {
-        // key on core
-        value = cache.get(coreKey);
-      } else if (deletesMode == DeletesMode.RECACHE) {
-        // key on deletes, if any, else core
-        value = cache.get(delCoreKey);
-      } else {
-
-        assert deletesMode == DeletesMode.DYNAMIC;
-
-        // first try for exact match
-        value = cache.get(delCoreKey);
-
-        if (value == null) {
-          // now for core match, but dynamically AND
-          // live docs
-          value = cache.get(coreKey);
-          if (value != null) {
-            final Bits liveDocs = reader.getLiveDocs();
-            if (liveDocs != null) {
-              value = mergeLiveDocs(liveDocs, value);
-            }
-          }
-        }
+      return cache.get(coreKey);
       }
 
-      return value;
-    }
-
-    protected abstract T mergeLiveDocs(Bits liveDocs, T value);
-
-    public synchronized void put(Object coreKey, Object delCoreKey, T value) {
-      if (deletesMode == DeletesMode.IGNORE) {
-        cache.put(coreKey, value);
-      } else if (deletesMode == DeletesMode.RECACHE) {
-        cache.put(delCoreKey, value);
-      } else {
+    public synchronized void put(Object coreKey, T value) {
         cache.put(coreKey, value);
-        cache.put(delCoreKey, value);
-      }
     }
   }
 
-  /**
-   * New deletes are ignored by default, which gives higher
-   * cache hit rate on reopened readers.  Most of the time
-   * this is safe, because the filter will be AND'd with a
-   * Query that fully enforces deletions.  If instead you
-   * need this filter to always enforce deletions, pass
-   * either {@link DeletesMode#RECACHE} or {@link
-   * DeletesMode#DYNAMIC}.
+  /** Wraps another filter's result and caches it.
    * @param filter Filter to cache results of
    */
   public CachingWrapperFilter(Filter filter) {
-    this(filter, DeletesMode.IGNORE);
-  }
-
-  /**
-   * Expert: by default, the cached filter will be shared
-   * across reopened segments that only had changes to their
-   * deletions.  
-   *
-   * @param filter Filter to cache results of
-   * @param deletesMode See {@link DeletesMode}
-   */
-  public CachingWrapperFilter(Filter filter, DeletesMode deletesMode) {
     this.filter = filter;
-    cache = new FilterCache<DocIdSet>(deletesMode) {
-      @Override
-      public DocIdSet mergeLiveDocs(final Bits liveDocs, final DocIdSet docIdSet) {
-        return new FilteredDocIdSet(docIdSet) {
-          @Override
-          protected boolean match(int docID) {
-            return liveDocs.get(docID);
-          }
-        };
-      }
-    };
+    cache = new FilterCache<DocIdSet>();
   }
 
   /** Provide the DocIdSet to be cached, using the DocIdSet provided
@@ -202,27 +104,22 @@ protected DocIdSet docIdSetToCache(DocIdSet docIdSet, IndexReader reader) throws
   int hitCount, missCount;
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
     final IndexReader reader = context.reader;
     final Object coreKey = reader.getCoreCacheKey();
-    final Object delCoreKey = reader.hasDeletions() ? reader.getLiveDocs() : coreKey;
 
-    DocIdSet docIdSet = cache.get(reader, coreKey, delCoreKey);
+    DocIdSet docIdSet = cache.get(reader, coreKey);
     if (docIdSet != null) {
       hitCount++;
-      return docIdSet;
-    }
-
+    } else {
     missCount++;
-
-    // cache miss
-    docIdSet = docIdSetToCache(filter.getDocIdSet(context), reader);
-
-    if (docIdSet != null) {
-      cache.put(coreKey, delCoreKey, docIdSet);
+      // cache miss: we use no acceptDocs here
+      // (this saves time on building DocIdSet, the acceptDocs will be applied on the cached set)
+      docIdSet = docIdSetToCache(filter.getDocIdSet(context, null/**!!!*/), reader);
+      cache.put(coreKey, docIdSet);
     }
     
-    return docIdSet;
+    return BitsFilteredDocIdSet.wrap(docIdSet, acceptDocs);
   }
 
   @Override
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/ConstantScoreQuery.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/ConstantScoreQuery.java
index 7a8eba36..18b9ee04 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/ConstantScoreQuery.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/ConstantScoreQuery.java
@@ -30,12 +30,6 @@
  * A query that wraps another query or a filter and simply returns a constant score equal to the
  * query boost for every document that matches the filter or query.
  * For queries it therefore simply strips of all scores and returns a constant one.
- *
- * <p><b>NOTE</b>: if the wrapped filter is an instance of
- * {@link CachingWrapperFilter}, you'll likely want to
- * enforce deletions in the filter (using either {@link
- * CachingWrapperFilter.DeletesMode#RECACHE} or {@link
- * CachingWrapperFilter.DeletesMode#DYNAMIC}).
  */
 public class ConstantScoreQuery extends Query {
   protected final Filter filter;
@@ -128,11 +122,11 @@ public void normalize(float norm, float topLevelBoost) {
 
     @Override
     public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
-        boolean topScorer, Bits acceptDocs) throws IOException {
+        boolean topScorer, final Bits acceptDocs) throws IOException {
       final DocIdSetIterator disi;
       if (filter != null) {
         assert query == null;
-        final DocIdSet dis = filter.getDocIdSet(context);
+        final DocIdSet dis = filter.getDocIdSet(context, acceptDocs);
         if (dis == null) {
           return null;
         }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/DocIdSet.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/DocIdSet.java
index 03f6c642..e9b788c2 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/DocIdSet.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/DocIdSet.java
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import org.apache.lucene.util.Bits;
 
 /**
  * A DocIdSet contains a set of doc ids. Implementing classes must
@@ -46,6 +47,12 @@ public DocIdSetIterator iterator() {
     public boolean isCacheable() {
       return true;
     }
+    
+    // we explicitely provide no random access, as this filter is 100% sparse and iterator exits faster
+    @Override
+    public Bits bits() throws IOException {
+      return null;
+    }
   };
     
   /** Provides a {@link DocIdSetIterator} to access the set.
@@ -54,6 +61,23 @@ public boolean isCacheable() {
    * are no docs that match. */
   public abstract DocIdSetIterator iterator() throws IOException;
 
+  /** Optionally provides a {@link Bits} interface for random access
+   * to matching documents.
+   * @return {@code null}, if this {@code DocIdSet} does not support random access.
+   * In contrast to {@link #iterator()}, a return value of {@code null}
+   * <b>does not</b> imply that no documents match the filter!
+   * The default implementation does not provide random access, so you
+   * only need to implement this method if your DocIdSet can
+   * guarantee random access to every docid in O(1) time without
+   * external disk access (as {@link Bits} interface cannot throw
+   * {@link IOException}). This is generally true for bit sets
+   * like {@link org.apache.lucene.util.FixedBitSet}, which return
+   * itsself if they are used as {@code DocIdSet}.
+   */
+  public Bits bits() throws IOException {
+    return null;
+  }
+
   /**
    * This method is a hint for {@link CachingWrapperFilter}, if this <code>DocIdSet</code>
    * should be cached without copying it into a BitSet. The default is to return
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
index e14ded87..5755c9ea 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
@@ -73,7 +73,7 @@ private FieldCacheRangeFilter(String field, FieldCache.Parser parser, T lowerVal
   
   /** This method is implemented for each data type */
   @Override
-  public abstract DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException;
+  public abstract DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException;
 
   /**
    * Creates a string range filter using {@link FieldCache#getTermsIndex}. This works with all
@@ -83,7 +83,7 @@ private FieldCacheRangeFilter(String field, FieldCache.Parser parser, T lowerVal
   public static FieldCacheRangeFilter<String> newStringRange(String field, String lowerVal, String upperVal, boolean includeLower, boolean includeUpper) {
     return new FieldCacheRangeFilter<String>(field, null, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
         final FieldCache.DocTermsIndex fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader, field);
         final BytesRef spare = new BytesRef();
         final int lowerPoint = fcsi.binarySearchLookup(lowerVal == null ? null : new BytesRef(lowerVal), spare);
@@ -122,9 +122,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
         
         assert inclusiveLowerPoint > 0 && inclusiveUpperPoint > 0;
         
-        // for this DocIdSet, we can ignore deleted docs
-        // because deleted docs have an order of 0 (null entry in StringIndex)
-        return new FieldCacheDocIdSet(context.reader, true) {
+        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
           @Override
           final boolean matchDoc(int doc) {
             final int docOrd = fcsi.getOrd(doc);
@@ -152,7 +150,7 @@ final boolean matchDoc(int doc) {
   public static FieldCacheRangeFilter<Byte> newByteRange(String field, FieldCache.ByteParser parser, Byte lowerVal, Byte upperVal, boolean includeLower, boolean includeUpper) {
     return new FieldCacheRangeFilter<Byte>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
         final byte inclusiveLowerPoint, inclusiveUpperPoint;
         if (lowerVal != null) {
           final byte i = lowerVal.byteValue();
@@ -175,8 +173,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
           return DocIdSet.EMPTY_DOCIDSET;
         
         final byte[] values = FieldCache.DEFAULT.getBytes(context.reader, field, (FieldCache.ByteParser) parser);
-        // we only respect deleted docs if the range contains 0
-        return new FieldCacheDocIdSet(context.reader, !(inclusiveLowerPoint <= 0 && inclusiveUpperPoint >= 0)) {
+        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
           @Override
           boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -203,7 +200,7 @@ boolean matchDoc(int doc) {
   public static FieldCacheRangeFilter<Short> newShortRange(String field, FieldCache.ShortParser parser, Short lowerVal, Short upperVal, boolean includeLower, boolean includeUpper) {
     return new FieldCacheRangeFilter<Short>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
         final short inclusiveLowerPoint, inclusiveUpperPoint;
         if (lowerVal != null) {
           short i = lowerVal.shortValue();
@@ -226,8 +223,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
           return DocIdSet.EMPTY_DOCIDSET;
         
         final short[] values = FieldCache.DEFAULT.getShorts(context.reader, field, (FieldCache.ShortParser) parser);
-        // ignore deleted docs if range doesn't contain 0
-        return new FieldCacheDocIdSet(context.reader, !(inclusiveLowerPoint <= 0 && inclusiveUpperPoint >= 0)) {
+        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
           @Override
           boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -254,7 +250,7 @@ boolean matchDoc(int doc) {
   public static FieldCacheRangeFilter<Integer> newIntRange(String field, FieldCache.IntParser parser, Integer lowerVal, Integer upperVal, boolean includeLower, boolean includeUpper) {
     return new FieldCacheRangeFilter<Integer>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
         final int inclusiveLowerPoint, inclusiveUpperPoint;
         if (lowerVal != null) {
           int i = lowerVal.intValue();
@@ -277,8 +273,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
           return DocIdSet.EMPTY_DOCIDSET;
         
         final int[] values = FieldCache.DEFAULT.getInts(context.reader, field, (FieldCache.IntParser) parser);
-        // ignore deleted docs if range doesn't contain 0
-        return new FieldCacheDocIdSet(context.reader, !(inclusiveLowerPoint <= 0 && inclusiveUpperPoint >= 0)) {
+        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
           @Override
           boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -305,7 +300,7 @@ boolean matchDoc(int doc) {
   public static FieldCacheRangeFilter<Long> newLongRange(String field, FieldCache.LongParser parser, Long lowerVal, Long upperVal, boolean includeLower, boolean includeUpper) {
     return new FieldCacheRangeFilter<Long>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
         final long inclusiveLowerPoint, inclusiveUpperPoint;
         if (lowerVal != null) {
           long i = lowerVal.longValue();
@@ -328,8 +323,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
           return DocIdSet.EMPTY_DOCIDSET;
         
         final long[] values = FieldCache.DEFAULT.getLongs(context.reader, field, (FieldCache.LongParser) parser);
-        // ignore deleted docs if range doesn't contain 0
-        return new FieldCacheDocIdSet(context.reader, !(inclusiveLowerPoint <= 0L && inclusiveUpperPoint >= 0L)) {
+        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
           @Override
           boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -356,7 +350,7 @@ boolean matchDoc(int doc) {
   public static FieldCacheRangeFilter<Float> newFloatRange(String field, FieldCache.FloatParser parser, Float lowerVal, Float upperVal, boolean includeLower, boolean includeUpper) {
     return new FieldCacheRangeFilter<Float>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
         // we transform the floating point numbers to sortable integers
         // using NumericUtils to easier find the next bigger/lower value
         final float inclusiveLowerPoint, inclusiveUpperPoint;
@@ -383,8 +377,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
           return DocIdSet.EMPTY_DOCIDSET;
         
         final float[] values = FieldCache.DEFAULT.getFloats(context.reader, field, (FieldCache.FloatParser) parser);
-        // ignore deleted docs if range doesn't contain 0
-        return new FieldCacheDocIdSet(context.reader, !(inclusiveLowerPoint <= 0.0f && inclusiveUpperPoint >= 0.0f)) {
+        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
           @Override
           boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -411,7 +404,7 @@ boolean matchDoc(int doc) {
   public static FieldCacheRangeFilter<Double> newDoubleRange(String field, FieldCache.DoubleParser parser, Double lowerVal, Double upperVal, boolean includeLower, boolean includeUpper) {
     return new FieldCacheRangeFilter<Double>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
         // we transform the floating point numbers to sortable integers
         // using NumericUtils to easier find the next bigger/lower value
         final double inclusiveLowerPoint, inclusiveUpperPoint;
@@ -439,7 +432,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
         
         final double[] values = FieldCache.DEFAULT.getDoubles(context.reader, field, (FieldCache.DoubleParser) parser);
         // ignore deleted docs if range doesn't contain 0
-        return new FieldCacheDocIdSet(context.reader, !(inclusiveLowerPoint <= 0.0 && inclusiveUpperPoint >= 0.0)) {
+        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
           @Override
           boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -506,12 +499,12 @@ public final int hashCode() {
   public FieldCache.Parser getParser() { return parser; }
   
   static abstract class FieldCacheDocIdSet extends DocIdSet {
-    private final IndexReader reader;
-    private final boolean canIgnoreDeletedDocs;
+    private final int maxDoc;
+    private final Bits acceptDocs;
 
-    FieldCacheDocIdSet(IndexReader reader, boolean canIgnoreDeletedDocs) {
-      this.reader = reader;
-      this.canIgnoreDeletedDocs = canIgnoreDeletedDocs;
+    FieldCacheDocIdSet(int maxDoc, Bits acceptDocs) {
+      this.maxDoc = maxDoc;
+      this.acceptDocs = acceptDocs;
     }
 
     /**
@@ -530,11 +523,29 @@ public boolean isCacheable() {
     }
 
     @Override
-    public DocIdSetIterator iterator() throws IOException {
+    public Bits bits() {
+      return (acceptDocs == null) ? new Bits() {
+        public boolean get(int docid) {
+          return FieldCacheDocIdSet.this.matchDoc(docid);
+        }
+
+        public int length() {
+          return FieldCacheDocIdSet.this.maxDoc;
+        }
+      } : new Bits() {
+        public boolean get(int docid) {
+          return acceptDocs.get(docid) && FieldCacheDocIdSet.this.matchDoc(docid);
+        }
 
-      final Bits liveDocs = canIgnoreDeletedDocs ? null : reader.getLiveDocs();
+        public int length() {
+          return FieldCacheDocIdSet.this.maxDoc;
+        }
+      };
+    }
 
-      if (liveDocs == null) {
+    @Override
+    public DocIdSetIterator iterator() throws IOException {
+      if (acceptDocs == null) {
         // Specialization optimization disregard deletions
         return new DocIdSetIterator() {
           private int doc = -1;
@@ -569,12 +580,10 @@ public int advance(int target) {
           }
         };
       } else {
-        // Must consult deletions
-
-        final int maxDoc = reader.maxDoc();
+        // Must consult acceptDocs
 
         // a DocIdSetIterator generating docIds by
-        // incrementing a variable & checking liveDocs -
+        // incrementing a variable & checking acceptDocs -
         return new DocIdSetIterator() {
           private int doc = -1;
           @Override
@@ -589,14 +598,14 @@ public int nextDoc() {
               if (doc >= maxDoc) {
                 return doc = NO_MORE_DOCS;
               }
-            } while (!liveDocs.get(doc) || !matchDoc(doc));
+            } while (!acceptDocs.get(doc) || !matchDoc(doc));
             return doc;
           }
         
           @Override
           public int advance(int target) {
             for(doc=target;doc<maxDoc;doc++) {
-              if (liveDocs.get(doc) && matchDoc(doc)) {
+              if (acceptDocs.get(doc) && matchDoc(doc)) {
                 return doc;
               }
             }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java
index 49e933d7..374dfad7 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java
@@ -23,6 +23,7 @@
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 
 /**
@@ -116,68 +117,22 @@ public FieldCache getFieldCache() {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
-    return new FieldCacheTermsFilterDocIdSet(getFieldCache().getTermsIndex(context.reader, field));
-  }
-
-  protected class FieldCacheTermsFilterDocIdSet extends DocIdSet {
-    private FieldCache.DocTermsIndex fcsi;
-
-    private FixedBitSet bits;
-
-    public FieldCacheTermsFilterDocIdSet(FieldCache.DocTermsIndex fcsi) {
-      this.fcsi = fcsi;
-      bits = new FixedBitSet(this.fcsi.numOrd());
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    final FieldCache.DocTermsIndex fcsi = getFieldCache().getTermsIndex(context.reader, field);
+    final FixedBitSet bits = new FixedBitSet(fcsi.numOrd());
       final BytesRef spare = new BytesRef();
       for (int i=0;i<terms.length;i++) {
-        int termNumber = this.fcsi.binarySearchLookup(terms[i], spare);
+      int termNumber = fcsi.binarySearchLookup(terms[i], spare);
         if (termNumber > 0) {
           bits.set(termNumber);
         }
       }
-    }
-
+    final int maxDoc = context.reader.maxDoc();
+    return new FieldCacheRangeFilter.FieldCacheDocIdSet(maxDoc, acceptDocs) {
     @Override
-    public DocIdSetIterator iterator() {
-      return new FieldCacheTermsFilterDocIdSetIterator();
-    }
-
-    /** This DocIdSet implementation is cacheable. */
-    @Override
-    public boolean isCacheable() {
-      return true;
-    }
-
-    protected class FieldCacheTermsFilterDocIdSetIterator extends DocIdSetIterator {
-      private int doc = -1;
-
-      @Override
-      public int docID() {
-        return doc;
-      }
-
-      @Override
-      public int nextDoc() {
-        try {
-          while (!bits.get(fcsi.getOrd(++doc))) {}
-        } catch (ArrayIndexOutOfBoundsException e) {
-          doc = NO_MORE_DOCS;
-        }
-        return doc;
-      }
-
-      @Override
-      public int advance(int target) {
-        try {
-          doc = target;
-          while (!bits.get(fcsi.getOrd(doc))) {
-            doc++;
-          }
-        } catch (ArrayIndexOutOfBoundsException e) {
-          doc = NO_MORE_DOCS;
-        }
-        return doc;
-      }
+      boolean matchDoc(int doc) {
+        return bits.get(fcsi.getOrd(doc));
     }
+    };
   }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/Filter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/Filter.java
index 31fb8b54..f53085e9 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/Filter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/Filter.java
@@ -21,7 +21,7 @@
 
 import org.apache.lucene.index.IndexReader; // javadocs
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.util.DocIdBitSet;
+import org.apache.lucene.util.Bits;
 
 /** 
  *  Abstract base class for restricting which documents may
@@ -44,14 +44,16 @@
    *         represent the whole underlying index i.e. if the index has more than
    *         one segment the given reader only represents a single segment.
    *         The provided context is always an atomic context, so you can call 
-   *         {@link IndexReader#fields()} or {@link IndexReader#getLiveDocs()}
+   *         {@link IndexReader#fields()}
    *         on the context's reader, for example.
    *          
+   * @param acceptDocs
+   *          Bits that represent the allowable docs to match (typically deleted docs
+   *          but possibly filtering other documents)
+   *          
    * @return a DocIdSet that provides the documents which should be permitted or
    *         prohibited in search results. <b>NOTE:</b> null can be returned if
    *         no documents will be accepted by this Filter.
-   * 
-   * @see DocIdBitSet
    */
-  public abstract DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException;
+  public abstract DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException;
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredDocIdSet.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredDocIdSet.java
index bddd4ee7..f6c68950 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredDocIdSet.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredDocIdSet.java
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import org.apache.lucene.util.Bits;
 
 /**
  * Abstract decorator class for a DocIdSet implementation
@@ -55,12 +56,26 @@ public boolean isCacheable() {
     return _innerSet.isCacheable();
   }
 
+  @Override
+  public Bits bits() throws IOException {
+    final Bits bits = _innerSet.bits();
+    return (bits == null) ? null : new Bits() {
+      public boolean get(int docid) {
+        return bits.get(docid) && FilteredDocIdSet.this.match(docid);
+      }
+
+      public int length() {
+        return bits.length();
+      }
+    };
+  }
+
   /**
    * Validation method to determine whether a docid should be in the result set.
    * @param docid docid to be tested
    * @return true if input docid should be in the result set, false otherwise.
    */
-  protected abstract boolean match(int docid) throws IOException;
+  protected abstract boolean match(int docid);
 	
   /**
    * Implementation of the contract to build a DocIdSetIterator.
@@ -71,7 +86,7 @@ public boolean isCacheable() {
   public DocIdSetIterator iterator() throws IOException {
     return new FilteredDocIdSetIterator(_innerSet.iterator()) {
       @Override
-      protected boolean match(int docid) throws IOException {
+      protected boolean match(int docid) {
         return FilteredDocIdSet.this.match(docid);
       }
     };
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java
index afd26572..519bdeb2 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java
@@ -47,7 +47,7 @@ public FilteredDocIdSetIterator(DocIdSetIterator innerIter) {
    * @return true if input docid should be in the result set, false otherwise.
    * @see #FilteredDocIdSetIterator(DocIdSetIterator)
    */
-  abstract protected boolean match(int doc) throws IOException;
+  protected abstract boolean match(int doc);
 	
   @Override
   public int docID() {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredQuery.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredQuery.java
index 2a007c00..ec8e34a2 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredQuery.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/FilteredQuery.java
@@ -56,6 +56,22 @@ public FilteredQuery (Query query, Filter filter) {
     this.filter = filter;
   }
 
+  /**
+   * Expert: decides if a filter should be executed as "random-access" or not.
+   * random-access means the filter "filters" in a similar way as deleted docs are filtered
+   * in lucene. This is faster when the filter accepts many documents.
+   * However, when the filter is very sparse, it can be faster to execute the query+filter
+   * as a conjunction in some cases.
+   * 
+   * The default implementation returns true if the first document accepted by the
+   * filter is < 100.
+   * 
+   * @lucene.internal
+   */
+  protected boolean useRandomAccess(Bits bits, int firstFilterDoc) {
+    return firstFilterDoc < 100;
+  }
+
   /**
    * Returns a Weight that applies the filter to the enclosed query's Weight.
    * This is accomplished by overriding the Scorer returned by the Weight.
@@ -65,6 +81,13 @@ public Weight createWeight(final IndexSearcher searcher) throws IOException {
     final Weight weight = query.createWeight (searcher);
     return new Weight() {
       
+      @Override
+      public boolean scoresDocsOutOfOrder() {
+        // TODO: Support out-of-order scoring!
+        // For now we return false here, as we always get the scorer in order
+        return false;
+      }
+
       @Override
       public float getValueForNormalization() throws IOException { 
         return weight.getValueForNormalization() * getBoost() * getBoost(); // boost sub-weight
@@ -79,7 +102,7 @@ public void normalize (float norm, float topLevelBoost) {
       public Explanation explain (AtomicReaderContext ir, int i) throws IOException {
         Explanation inner = weight.explain (ir, i);
         Filter f = FilteredQuery.this.filter;
-        DocIdSet docIdSet = f.getDocIdSet(ir);
+        DocIdSet docIdSet = f.getDocIdSet(ir, ir.reader.getLiveDocs());
         DocIdSetIterator docIdSetIterator = docIdSet == null ? DocIdSet.EMPTY_DOCIDSET.iterator() : docIdSet.iterator();
         if (docIdSetIterator == null) {
           docIdSetIterator = DocIdSet.EMPTY_DOCIDSET.iterator();
@@ -100,61 +123,109 @@ public Explanation explain (AtomicReaderContext ir, int i) throws IOException {
 
       // return a filtering scorer
       @Override
-      public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
-          boolean topScorer, Bits acceptDocs)
-          throws IOException {
-        // we will advance() the subscorer
-        final Scorer scorer = weight.scorer(context, true, false, acceptDocs);
-        if (scorer == null) {
+      public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder, boolean topScorer, Bits acceptDocs) throws IOException {
+        assert filter != null;
+
+        final DocIdSet filterDocIdSet = filter.getDocIdSet(context, acceptDocs);
+        if (filterDocIdSet == null) {
+          // this means the filter does not accept any documents.
           return null;
         }
-        DocIdSet docIdSet = filter.getDocIdSet(context);
-        if (docIdSet == null) {
+        
+        final DocIdSetIterator filterIter = filterDocIdSet.iterator();
+        if (filterIter == null) {
+          // this means the filter does not accept any documents.
           return null;
         }
-        final DocIdSetIterator docIdSetIterator = docIdSet.iterator();
-        if (docIdSetIterator == null) {
+
+        final int firstFilterDoc = filterIter.nextDoc();
+        if (firstFilterDoc == DocIdSetIterator.NO_MORE_DOCS) {
           return null;
         }
 
-        return new Scorer(this) {
-
-          private int doc = -1;
+        final Bits filterAcceptDocs = filterDocIdSet.bits();
+        final boolean useRandomAccess = (filterAcceptDocs != null && FilteredQuery.this.useRandomAccess(filterAcceptDocs, firstFilterDoc));
           
-          private int advanceToCommon(int scorerDoc, int disiDoc) throws IOException {
-            while (scorerDoc != disiDoc) {
-              if (scorerDoc < disiDoc) {
-                scorerDoc = scorer.advance(disiDoc);
+        if (useRandomAccess) {
+          // if we are using random access, we return the inner scorer, just with other acceptDocs
+          // TODO, replace this by when BooleanWeight is fixed to be consistent with its scorer implementations:
+          // return weight.scorer(context, scoreDocsInOrder, topScorer, filterAcceptDocs);
+          return weight.scorer(context, true, topScorer, filterAcceptDocs);
               } else {
-                disiDoc = docIdSetIterator.advance(scorerDoc);
+          assert firstFilterDoc > -1;
+          // we are gonna advance() this scorer, so we set inorder=true/toplevel=false
+          // we pass null as acceptDocs, as our filter has already respected acceptDocs, no need to do twice
+          final Scorer scorer = weight.scorer(context, true, false, null);
+          return (scorer == null) ? null : new Scorer(this) {
+            private int scorerDoc = -1, filterDoc = firstFilterDoc;
+            
+            // optimization: we are topScorer and collect directly using short-circuited algo
+            @Override
+            public void score(Collector collector) throws IOException {
+              int filterDoc = firstFilterDoc;
+              int scorerDoc = scorer.advance(filterDoc);
+              // the normalization trick already applies the boost of this query,
+              // so we can use the wrapped scorer directly:
+              collector.setScorer(scorer);
+              for (;;) {
+                if (scorerDoc == filterDoc) {
+                  // Check if scorer has exhausted, only before collecting.
+                  if (scorerDoc == DocIdSetIterator.NO_MORE_DOCS) {
+                    break;
+                  }
+                  collector.collect(scorerDoc);
+                  filterDoc = filterIter.nextDoc();
+                  scorerDoc = scorer.advance(filterDoc);
+                } else if (scorerDoc > filterDoc) {
+                  filterDoc = filterIter.advance(scorerDoc);
+                } else {
+                  scorerDoc = scorer.advance(filterDoc);
+                }
               }
             }
+            
+            private int advanceToNextCommonDoc() throws IOException {
+              for (;;) {
+                if (scorerDoc < filterDoc) {
+                  scorerDoc = scorer.advance(filterDoc);
+                } else if (scorerDoc == filterDoc) {
             return scorerDoc;
+                } else {
+                  filterDoc = filterIter.advance(scorerDoc);
+                }
+              }
           }
 
           @Override
           public int nextDoc() throws IOException {
-            int scorerDoc, disiDoc;
-            return doc = (disiDoc = docIdSetIterator.nextDoc()) != NO_MORE_DOCS
-                && (scorerDoc = scorer.nextDoc()) != NO_MORE_DOCS
-                && advanceToCommon(scorerDoc, disiDoc) != NO_MORE_DOCS ? scorer.docID() : NO_MORE_DOCS;
+              // don't go to next doc on first call
+              // (because filterIter is already on first doc):
+              if (scorerDoc != -1) {
+                filterDoc = filterIter.nextDoc();
+              }
+              return advanceToNextCommonDoc();
           }
           
           @Override
-          public int docID() { return doc; }
+            public int advance(int target) throws IOException {
+              if (target > filterDoc) {
+                filterDoc = filterIter.advance(target);
+              }
+              return advanceToNextCommonDoc();
+            }
           
           @Override
-          public int advance(int target) throws IOException {
-            int disiDoc, scorerDoc;
-            return doc = (disiDoc = docIdSetIterator.advance(target)) != NO_MORE_DOCS
-                && (scorerDoc = scorer.advance(disiDoc)) != NO_MORE_DOCS 
-                && advanceToCommon(scorerDoc, disiDoc) != NO_MORE_DOCS ? scorer.docID() : NO_MORE_DOCS;
+            public int docID() {
+              return scorerDoc;
           }
 
           @Override
-          public float score() throws IOException { return scorer.score(); }
+            public float score() throws IOException {
+              return scorer.score();
+            }
         };
       }
+      }
     };
   }
 
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/IndexSearcher.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/IndexSearcher.java
index b70f3021..c145d404 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/IndexSearcher.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/IndexSearcher.java
@@ -41,6 +41,7 @@
 import org.apache.lucene.search.similarities.SimilarityProvider;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.NIOFSDirectory;    // javadoc
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.ThreadInterruptedException;
 
@@ -274,6 +275,11 @@ public void close() throws IOException {
     }
   }
 
+  /** @lucene.internal */
+  protected Query wrapFilter(Query query, Filter filter) {
+    return (filter == null) ? query : new FilteredQuery(query, filter);
+  }
+
   /** Finds the top <code>n</code>
    * hits for <code>query</code> where all results are after a previous 
    * result (<code>after</code>).
@@ -285,7 +291,7 @@ public void close() throws IOException {
    * @throws BooleanQuery.TooManyClauses
    */
   public TopDocs searchAfter(ScoreDoc after, Query query, int n) throws IOException {
-    return searchAfter(after, query, null, n);
+    return search(createNormalizedWeight(query), after, n);
   }
   
   /** Finds the top <code>n</code>
@@ -299,7 +305,7 @@ public TopDocs searchAfter(ScoreDoc after, Query query, int n) throws IOExceptio
    * @throws BooleanQuery.TooManyClauses
    */
   public TopDocs searchAfter(ScoreDoc after, Query query, Filter filter, int n) throws IOException {
-    return search(createNormalizedWeight(query), filter, after, n);
+    return search(createNormalizedWeight(wrapFilter(query, filter)), after, n);
   }
   
   /** Finds the top <code>n</code>
@@ -320,7 +326,7 @@ public TopDocs search(Query query, int n)
    */
   public TopDocs search(Query query, Filter filter, int n)
     throws IOException {
-    return search(createNormalizedWeight(query), filter, null, n);
+    return search(createNormalizedWeight(wrapFilter(query, filter)), null, n);
   }
 
   /** Lower-level search API.
@@ -341,7 +347,7 @@ public TopDocs search(Query query, Filter filter, int n)
    */
   public void search(Query query, Filter filter, Collector results)
     throws IOException {
-    search(leafContexts, createNormalizedWeight(query), filter, results);
+    search(leafContexts, createNormalizedWeight(wrapFilter(query, filter)), results);
   }
 
   /** Lower-level search API.
@@ -359,7 +365,7 @@ public void search(Query query, Filter filter, Collector results)
   */
   public void search(Query query, Collector results)
     throws IOException {
-    search(leafContexts, createNormalizedWeight(query), null, results);
+    search(leafContexts, createNormalizedWeight(query), results);
   }
   
   /** Search implementation with arbitrary sorting.  Finds
@@ -375,7 +381,7 @@ public void search(Query query, Collector results)
    */
   public TopFieldDocs search(Query query, Filter filter, int n,
                              Sort sort) throws IOException {
-    return search(createNormalizedWeight(query), filter, n, sort);
+    return search(createNormalizedWeight(wrapFilter(query, filter)), n, sort);
   }
 
   /**
@@ -388,7 +394,7 @@ public TopFieldDocs search(Query query, Filter filter, int n,
    */
   public TopFieldDocs search(Query query, int n,
                              Sort sort) throws IOException {
-    return search(createNormalizedWeight(query), null, n, sort);
+    return search(createNormalizedWeight(query), n, sort);
   }
 
   /** Expert: Low-level search implementation.  Finds the top <code>n</code>
@@ -398,9 +404,9 @@ public TopFieldDocs search(Query query, int n,
    * {@link IndexSearcher#search(Query,Filter,int)} instead.
    * @throws BooleanQuery.TooManyClauses
    */
-  protected TopDocs search(Weight weight, Filter filter, ScoreDoc after, int nDocs) throws IOException {
+  protected TopDocs search(Weight weight, ScoreDoc after, int nDocs) throws IOException {
     if (executor == null) {
-      return search(leafContexts, weight, filter, after, nDocs);
+      return search(leafContexts, weight, after, nDocs);
     } else {
       final HitQueue hq = new HitQueue(nDocs, false);
       final Lock lock = new ReentrantLock();
@@ -408,7 +414,7 @@ protected TopDocs search(Weight weight, Filter filter, ScoreDoc after, int nDocs
     
       for (int i = 0; i < leafSlices.length; i++) { // search each sub
         runner.submit(
-                      new SearcherCallableNoSort(lock, this, leafSlices[i], weight, filter, after, nDocs, hq));
+                      new SearcherCallableNoSort(lock, this, leafSlices[i], weight, after, nDocs, hq));
       }
 
       int totalHits = 0;
@@ -429,13 +435,13 @@ protected TopDocs search(Weight weight, Filter filter, ScoreDoc after, int nDocs
   }
 
   /** Expert: Low-level search implementation.  Finds the top <code>n</code>
-   * hits for <code>query</code>, using the given leaf readers applying <code>filter</code> if non-null.
+   * hits for <code>query</code>.
    *
    * <p>Applications should usually call {@link IndexSearcher#search(Query,int)} or
    * {@link IndexSearcher#search(Query,Filter,int)} instead.
    * @throws BooleanQuery.TooManyClauses
    */
-  protected TopDocs search(AtomicReaderContext[] leaves, Weight weight, Filter filter, ScoreDoc after, int nDocs) throws IOException {
+  protected TopDocs search(AtomicReaderContext[] leaves, Weight weight, ScoreDoc after, int nDocs) throws IOException {
     // single thread
     int limit = reader.maxDoc();
     if (limit == 0) {
@@ -443,37 +449,36 @@ protected TopDocs search(AtomicReaderContext[] leaves, Weight weight, Filter fil
     }
     nDocs = Math.min(nDocs, limit);
     TopScoreDocCollector collector = TopScoreDocCollector.create(nDocs, after, !weight.scoresDocsOutOfOrder());
-    search(leaves, weight, filter, collector);
+    search(leaves, weight, collector);
     return collector.topDocs();
   }
 
   /** Expert: Low-level search implementation with arbitrary sorting.  Finds
-   * the top <code>n</code> hits for <code>query</code>, applying
-   * <code>filter</code> if non-null, and sorting the hits by the criteria in
-   * <code>sort</code>.
+   * the top <code>n</code> hits for <code>query</code> and sorting the hits
+   * by the criteria in <code>sort</code>.
    *
    * <p>Applications should usually call {@link
    * IndexSearcher#search(Query,Filter,int,Sort)} instead.
    * 
    * @throws BooleanQuery.TooManyClauses
    */
-  protected TopFieldDocs search(Weight weight, Filter filter,
+  protected TopFieldDocs search(Weight weight,
       final int nDocs, Sort sort) throws IOException {
-    return search(weight, filter, nDocs, sort, true);
+    return search(weight, nDocs, sort, true);
   }
 
   /**
-   * Just like {@link #search(Weight, Filter, int, Sort)}, but you choose
+   * Just like {@link #search(Weight, int, Sort)}, but you choose
    * whether or not the fields in the returned {@link FieldDoc} instances should
    * be set by specifying fillFields.
    *
    * <p>NOTE: this does not compute scores by default.  If you
    * need scores, create a {@link TopFieldCollector}
    * instance by calling {@link TopFieldCollector#create} and
-   * then pass that to {@link #search(IndexReader.AtomicReaderContext[], Weight, Filter,
+   * then pass that to {@link #search(IndexReader.AtomicReaderContext[], Weight,
    * Collector)}.</p>
    */
-  protected TopFieldDocs search(Weight weight, Filter filter, int nDocs,
+  protected TopFieldDocs search(Weight weight, int nDocs,
                                 Sort sort, boolean fillFields)
       throws IOException {
 
@@ -481,7 +486,7 @@ protected TopFieldDocs search(Weight weight, Filter filter, int nDocs,
     
     if (executor == null) {
       // use all leaves here!
-      return search (leafContexts, weight, filter, nDocs, sort, fillFields);
+      return search (leafContexts, weight, nDocs, sort, fillFields);
     } else {
       final TopFieldCollector topCollector = TopFieldCollector.create(sort, nDocs,
                                                                       fillFields,
@@ -493,7 +498,7 @@ protected TopFieldDocs search(Weight weight, Filter filter, int nDocs,
       final ExecutionHelper<TopFieldDocs> runner = new ExecutionHelper<TopFieldDocs>(executor);
       for (int i = 0; i < leafSlices.length; i++) { // search each leaf slice
         runner.submit(
-                      new SearcherCallableWithSort(lock, this, leafSlices[i], weight, filter, nDocs, topCollector, sort));
+                      new SearcherCallableWithSort(lock, this, leafSlices[i], weight, nDocs, topCollector, sort));
       }
       int totalHits = 0;
       float maxScore = Float.NEGATIVE_INFINITY;
@@ -512,17 +517,17 @@ protected TopFieldDocs search(Weight weight, Filter filter, int nDocs,
   
   
   /**
-   * Just like {@link #search(Weight, Filter, int, Sort)}, but you choose
+   * Just like {@link #search(Weight, int, Sort)}, but you choose
    * whether or not the fields in the returned {@link FieldDoc} instances should
    * be set by specifying fillFields.
    *
    * <p>NOTE: this does not compute scores by default.  If you
    * need scores, create a {@link TopFieldCollector}
    * instance by calling {@link TopFieldCollector#create} and
-   * then pass that to {@link #search(IndexReader.AtomicReaderContext[], Weight, Filter,
+   * then pass that to {@link #search(IndexReader.AtomicReaderContext[], Weight, 
    * Collector)}.</p>
    */
-  protected TopFieldDocs search(AtomicReaderContext[] leaves, Weight weight, Filter filter, int nDocs,
+  protected TopFieldDocs search(AtomicReaderContext[] leaves, Weight weight, int nDocs,
       Sort sort, boolean fillFields) throws IOException {
     // single thread
     int limit = reader.maxDoc();
@@ -533,7 +538,7 @@ protected TopFieldDocs search(AtomicReaderContext[] leaves, Weight weight, Filte
 
     TopFieldCollector collector = TopFieldCollector.create(sort, nDocs,
                                                            fillFields, fieldSortDoTrackScores, fieldSortDoMaxScore, !weight.scoresDocsOutOfOrder());
-    search(leaves, weight, filter, collector);
+    search(leaves, weight, collector);
     return (TopFieldDocs) collector.topDocs();
   }
 
@@ -557,19 +562,16 @@ protected TopFieldDocs search(AtomicReaderContext[] leaves, Weight weight, Filte
    *          the searchers leaves to execute the searches on
    * @param weight
    *          to match documents
-   * @param filter
-   *          if non-null, used to permit documents to be collected.
    * @param collector
    *          to receive hits
    * @throws BooleanQuery.TooManyClauses
    */
-  protected void search(AtomicReaderContext[] leaves, Weight weight, Filter filter, Collector collector)
+  protected void search(AtomicReaderContext[] leaves, Weight weight, Collector collector)
       throws IOException {
 
     // TODO: should we make this
     // threaded...?  the Collector could be sync'd?
     // always use single thread:
-    if (filter == null) {
       for (int i = 0; i < leaves.length; i++) { // search each subreader
         collector.setNextReader(leaves[i]);
         Scorer scorer = weight.scorer(leaves[i], !collector.acceptsDocsOutOfOrder(), true, leaves[i].reader.getLiveDocs());
@@ -577,59 +579,6 @@ protected void search(AtomicReaderContext[] leaves, Weight weight, Filter filter
           scorer.score(collector);
         }
       }
-    } else {
-      for (int i = 0; i < leaves.length; i++) { // search each subreader
-        collector.setNextReader(leaves[i]);
-        searchWithFilter(leaves[i], weight, filter, collector);
-      }
-    }
-  }
-
-  private void searchWithFilter(AtomicReaderContext context, Weight weight,
-      final Filter filter, final Collector collector) throws IOException {
-
-    assert filter != null;
-    
-    // we are gonna advance() this scorer, so we set inorder=true/toplevel=false 
-    Scorer scorer = weight.scorer(context, true, false, context.reader.getLiveDocs());
-    if (scorer == null) {
-      return;
-    }
-
-    int docID = scorer.docID();
-    assert docID == -1 || docID == DocIdSetIterator.NO_MORE_DOCS;
-
-    // CHECKME: use ConjunctionScorer here?
-    DocIdSet filterDocIdSet = filter.getDocIdSet(context);
-    if (filterDocIdSet == null) {
-      // this means the filter does not accept any documents.
-      return;
-    }
-    
-    DocIdSetIterator filterIter = filterDocIdSet.iterator();
-    if (filterIter == null) {
-      // this means the filter does not accept any documents.
-      return;
-    }
-    int filterDoc = filterIter.nextDoc();
-    int scorerDoc = scorer.advance(filterDoc);
-    
-    collector.setScorer(scorer);
-    while (true) {
-      if (scorerDoc == filterDoc) {
-        // Check if scorer has exhausted, only before collecting.
-        if (scorerDoc == DocIdSetIterator.NO_MORE_DOCS) {
-          break;
-        }
-        collector.collect(scorerDoc);
-        filterDoc = filterIter.nextDoc();
-        scorerDoc = scorer.advance(filterDoc);
-      } else if (scorerDoc > filterDoc) {
-        filterDoc = filterIter.advance(scorerDoc);
-      } else {
-        scorerDoc = scorer.advance(filterDoc);
-      }
-    }
   }
 
   /** Expert: called to re-write queries into primitive queries.
@@ -729,18 +678,16 @@ public ReaderContext getTopReaderContext() {
     private final Lock lock;
     private final IndexSearcher searcher;
     private final Weight weight;
-    private final Filter filter;
     private final ScoreDoc after;
     private final int nDocs;
     private final HitQueue hq;
     private final LeafSlice slice;
 
     public SearcherCallableNoSort(Lock lock, IndexSearcher searcher, LeafSlice slice,  Weight weight,
-        Filter filter, ScoreDoc after, int nDocs, HitQueue hq) {
+        ScoreDoc after, int nDocs, HitQueue hq) {
       this.lock = lock;
       this.searcher = searcher;
       this.weight = weight;
-      this.filter = filter;
       this.after = after;
       this.nDocs = nDocs;
       this.hq = hq;
@@ -748,7 +695,7 @@ public SearcherCallableNoSort(Lock lock, IndexSearcher searcher, LeafSlice slice
     }
 
     public TopDocs call() throws IOException {
-      final TopDocs docs = searcher.search (slice.leaves, weight, filter, after, nDocs);
+      final TopDocs docs = searcher.search (slice.leaves, weight, after, nDocs);
       final ScoreDoc[] scoreDocs = docs.scoreDocs;
       //it would be so nice if we had a thread-safe insert 
       lock.lock();
@@ -775,18 +722,16 @@ public TopDocs call() throws IOException {
     private final Lock lock;
     private final IndexSearcher searcher;
     private final Weight weight;
-    private final Filter filter;
     private final int nDocs;
     private final TopFieldCollector hq;
     private final Sort sort;
     private final LeafSlice slice;
 
     public SearcherCallableWithSort(Lock lock, IndexSearcher searcher, LeafSlice slice, Weight weight,
-        Filter filter, int nDocs, TopFieldCollector hq, Sort sort) {
+        int nDocs, TopFieldCollector hq, Sort sort) {
       this.lock = lock;
       this.searcher = searcher;
       this.weight = weight;
-      this.filter = filter;
       this.nDocs = nDocs;
       this.hq = hq;
       this.sort = sort;
@@ -831,7 +776,7 @@ public float score() {
 
     public TopFieldDocs call() throws IOException {
       assert slice.leaves.length == 1;
-      final TopFieldDocs docs = searcher.search (slice.leaves, weight, filter, nDocs, sort, true);
+      final TopFieldDocs docs = searcher.search (slice.leaves, weight, nDocs, sort, true);
       lock.lock();
       try {
         final int base = slice.leaves[0].docBase;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
index 3f480008..3cd39287 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
@@ -105,7 +105,7 @@ public void clearTotalNumberOfTerms() {
    * results.
    */
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
     final IndexReader reader = context.reader;
     final Fields fields = reader.fields();
     if (fields == null) {
@@ -125,13 +125,12 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
       // fill into a FixedBitSet
       final FixedBitSet bitSet = new FixedBitSet(context.reader.maxDoc());
       int termCount = 0;
-      final Bits liveDocs = reader.getLiveDocs();
       DocsEnum docsEnum = null;
       do {
         termCount++;
         // System.out.println("  iter termCount=" + termCount + " term=" +
         // enumerator.term().toBytesString());
-        docsEnum = termsEnum.docs(liveDocs, docsEnum);
+        docsEnum = termsEnum.docs(acceptDocs, docsEnum);
         final DocsEnum.BulkReadResult result = docsEnum.getBulkResult();
         while (true) {
           final int count = docsEnum.read();
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/QueryWrapperFilter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/QueryWrapperFilter.java
index 24dab641..8f6b4064 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/QueryWrapperFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/QueryWrapperFilter.java
@@ -20,6 +20,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.util.Bits;
 
 /** 
  * Constrains search results to only match those which also match a provided
@@ -47,7 +48,7 @@ public final Query getQuery() {
   }
 
   @Override
-  public DocIdSet getDocIdSet(final AtomicReaderContext context) throws IOException {
+  public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
     // get a private context that is used to rewrite, createWeight and score eventually
     assert context.reader.getTopReaderContext().isAtomic;
     final AtomicReaderContext privateContext = (AtomicReaderContext) context.reader.getTopReaderContext();
@@ -55,7 +56,7 @@ public DocIdSet getDocIdSet(final AtomicReaderContext context) throws IOExceptio
     return new DocIdSet() {
       @Override
       public DocIdSetIterator iterator() throws IOException {
-        return weight.scorer(privateContext, true, false, privateContext.reader.getLiveDocs());
+        return weight.scorer(privateContext, true, false, acceptDocs);
       }
       @Override
       public boolean isCacheable() { return false; }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanFilter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanFilter.java
index e46ff1e8..b19968af 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanFilter.java
@@ -16,6 +16,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.util.Bits;
 
 import java.io.IOException;
 
@@ -34,5 +35,5 @@
    * @return A {@link SpanFilterResult}
    * @throws java.io.IOException if there was an issue accessing the necessary information
    * */
-  public abstract SpanFilterResult bitSpans(AtomicReaderContext context) throws IOException;
+  public abstract SpanFilterResult bitSpans(AtomicReaderContext context, Bits acceptDocs) throws IOException;
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanFilterResult.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanFilterResult.java
index f07d77a3..3337cb6c 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanFilterResult.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanFilterResult.java
@@ -16,7 +16,7 @@
  */
 
 import java.util.ArrayList;
-
+import java.util.Collections;
 import java.util.List;
 
 
@@ -30,6 +30,9 @@
   private DocIdSet docIdSet;
   private List<PositionInfo> positions;//Spans spans;
   
+  public static final SpanFilterResult EMPTY_SPAN_FILTER_RESULT =
+    new SpanFilterResult(DocIdSet.EMPTY_DOCIDSET, Collections.<PositionInfo>emptyList());
+  
   /**
   *
   * @param docIdSet The DocIdSet for the Filter
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanQueryFilter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanQueryFilter.java
index 16079632..de758ac2 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanQueryFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/SpanQueryFilter.java
@@ -19,6 +19,7 @@
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.spans.SpanQuery;
 import org.apache.lucene.search.spans.Spans;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.FixedBitSet;
 
 import java.io.IOException;
@@ -52,16 +53,16 @@ public SpanQueryFilter(SpanQuery query) {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
-    SpanFilterResult result = bitSpans(context);
+  public final DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    SpanFilterResult result = bitSpans(context, acceptDocs);
     return result.getDocIdSet();
   }
 
   @Override
-  public SpanFilterResult bitSpans(AtomicReaderContext context) throws IOException {
+  public SpanFilterResult bitSpans(AtomicReaderContext context, Bits acceptDocs) throws IOException {
 
     final FixedBitSet bits = new FixedBitSet(context.reader.maxDoc());
-    Spans spans = query.getSpans(context, context.reader.getLiveDocs());
+    Spans spans = query.getSpans(context, acceptDocs);
     List<SpanFilterResult.PositionInfo> tmp = new ArrayList<SpanFilterResult.PositionInfo>(20);
     int currentDoc = -1;
     SpanFilterResult.PositionInfo currentInfo = null;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/TermScorer.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/TermScorer.java
index 20a7fb4b..fd98b220 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/TermScorer.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/TermScorer.java
@@ -25,15 +25,15 @@
 /** Expert: A <code>Scorer</code> for documents matching a <code>Term</code>.
  */
 final class TermScorer extends Scorer {
-  private DocsEnum docsEnum;
+  private final DocsEnum docsEnum;
   private int doc = -1;
   private int freq;
 
   private int pointer;
   private int pointerMax;
 
-  private int[] docs;
-  private int[] freqs;
+  private final int[] docs;
+  private final int[] freqs;
   private final DocsEnum.BulkReadResult bulkResult;
   private final Similarity.ExactDocScorer docScorer;
   
@@ -53,6 +53,8 @@
     this.docScorer = docScorer;
     this.docsEnum = td;
     bulkResult = td.getBulkResult();
+    docs = bulkResult.docs.ints;
+    freqs = bulkResult.freqs.ints;
   }
 
   @Override
@@ -60,12 +62,6 @@ public void score(Collector c) throws IOException {
     score(c, Integer.MAX_VALUE, nextDoc());
   }
 
-  private final void refillBuffer() throws IOException {
-    pointerMax = docsEnum.read();  // refill
-    docs = bulkResult.docs.ints;
-    freqs = bulkResult.freqs.ints;
-  }
-
   // firstDocID is ignored since nextDoc() sets 'doc'
   @Override
   public boolean score(Collector c, int end, int firstDocID) throws IOException {
@@ -74,7 +70,7 @@ public boolean score(Collector c, int end, int firstDocID) throws IOException {
       //System.out.println("TS: collect doc=" + doc);
       c.collect(doc);                      // collect score
       if (++pointer >= pointerMax) {
-        refillBuffer();
+        pointerMax = docsEnum.read();  // refill
         if (pointerMax != 0) {
           pointer = 0;
         } else {
@@ -109,7 +105,7 @@ public float freq() {
   public int nextDoc() throws IOException {
     pointer++;
     if (pointer >= pointerMax) {
-      refillBuffer();
+      pointerMax = docsEnum.read();  // refill
       if (pointerMax != 0) {
         pointer = 0;
       } else {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/cache/ShortValuesCreator.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/cache/ShortValuesCreator.java
index a6c11010..a628c536 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/cache/ShortValuesCreator.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/search/cache/ShortValuesCreator.java
@@ -119,7 +119,7 @@ protected void fillShortValues( ShortValues vals, IndexReader reader, String fie
           if (term == null) {
             break;
           }
-          final Short termval = parser.parseShort(term);
+          final short termval = parser.parseShort(term);
           docs = termsEnum.docs(null, docs);
           while (true) {
             final int docID = docs.nextDoc();
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/FSDirectory.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/FSDirectory.java
index f1bbe80b..5bd5e7e2 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/FSDirectory.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/FSDirectory.java
@@ -122,7 +122,7 @@
   protected final Set<String> staleFiles = synchronizedSet(new HashSet<String>()); // Files written, but not yet sync'ed
   private int chunkSize = DEFAULT_READ_CHUNK_SIZE; // LUCENE-1566
 
-  // null means no limite
+  // null means no limit
   private volatile RateLimiter mergeWriteRateLimiter;
 
   // returns the canonical version of the directory, creating it if it doesn't exist.
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/IndexInput.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/IndexInput.java
index 87dda60d..b2ef20c3 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/IndexInput.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/IndexInput.java
@@ -26,8 +26,6 @@
  */
 public abstract class IndexInput extends DataInput implements Cloneable,Closeable {
 
-  protected byte[] copyBuf = null;
-
   /** Closes the stream to further operations. */
   public abstract void close() throws IOException;
 
@@ -59,9 +57,7 @@
   public void copyBytes(IndexOutput out, long numBytes) throws IOException {
     assert numBytes >= 0: "numBytes=" + numBytes;
 
-    if (copyBuf == null) {
-      copyBuf = new byte[BufferedIndexInput.BUFFER_SIZE];
-    }
+    byte copyBuf[] = new byte[BufferedIndexInput.BUFFER_SIZE];
 
     while (numBytes > 0) {
       final int toCopy = (int) (numBytes > copyBuf.length ? copyBuf.length : numBytes);
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/RateLimiter.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/RateLimiter.java
index 30ed17b2..59ecbc23 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/RateLimiter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/store/RateLimiter.java
@@ -63,7 +63,7 @@ public double getMbPerSec() {
    *  with a biggish count, not one byte at a time. */
   public void pause(long bytes) {
 
-    // TODO: this is purely instantenous rate; maybe we
+    // TODO: this is purely instantaneous rate; maybe we
     // should also offer decayed recent history one?
     final long targetNS = lastNS = lastNS + ((long) (bytes * nsPerByte));
     long curNS = System.nanoTime();
@@ -71,7 +71,7 @@ public void pause(long bytes) {
       lastNS = curNS;
     }
 
-    // While loop because Thread.sleep doesn't alway sleep
+    // While loop because Thread.sleep doesn't always sleep
     // enough:
     while(true) {
       final long pauseNS = targetNS - curNS;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/BytesRef.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/BytesRef.java
index fe40d00d..6473fba9 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/BytesRef.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/BytesRef.java
@@ -79,7 +79,7 @@ public BytesRef(IntsRef intsRef) {
 
   /**
    * @param text Initialize the byte[] from the UTF8 bytes
-   * for the provided Sring.  This must be well-formed
+   * for the provided String.  This must be well-formed
    * unicode text, with no unpaired surrogates or U+FFFF.
    */
   public BytesRef(CharSequence text) {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/CharsRef.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/CharsRef.java
index 811ec7d4..02a03a04 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/CharsRef.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/CharsRef.java
@@ -189,18 +189,22 @@ public void grow(int newLength) {
    * Copies the given array into this CharsRef starting at offset 0
    */
   public void copy(char[] otherChars, int otherOffset, int otherLength) {
+    grow(otherLength);
+    System.arraycopy(otherChars, otherOffset, this.chars, 0,
+        otherLength);
     this.offset = 0;
-    append(otherChars, otherOffset, otherLength);
+    this.length = otherLength;
   }
 
   /**
-   * Appends the given array to this CharsRef starting at the current offset
+   * Appends the given array to this CharsRef
    */
   public void append(char[] otherChars, int otherOffset, int otherLength) {
-    grow(this.offset + otherLength);
-    System.arraycopy(otherChars, otherOffset, this.chars, this.offset,
+    final int newLength = length + otherLength;
+    grow(this.offset + newLength);
+    System.arraycopy(otherChars, otherOffset, this.chars, this.offset+length,
         otherLength);
-    this.length = otherLength;
+    this.length += otherLength;
   }
 
   @Override
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/DocIdBitSet.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/DocIdBitSet.java
index 28fa7a9c..8752ef73 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/DocIdBitSet.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/DocIdBitSet.java
@@ -24,8 +24,8 @@
 
 
 /** Simple DocIdSet and DocIdSetIterator backed by a BitSet */
-public class DocIdBitSet extends DocIdSet {
-  private BitSet bitSet;
+public class DocIdBitSet extends DocIdSet implements Bits {
+  private final BitSet bitSet;
     
   public DocIdBitSet(BitSet bitSet) {
     this.bitSet = bitSet;
@@ -36,6 +36,11 @@ public DocIdSetIterator iterator() {
     return new DocIdBitSetIterator(bitSet);
   }
 
+  @Override
+  public Bits bits() {
+    return this;
+  }
+
   /** This DocIdSet implementation is cacheable. */
   @Override
   public boolean isCacheable() {
@@ -49,6 +54,17 @@ public BitSet getBitSet() {
 	return this.bitSet;
   }
   
+  @Override
+  public boolean get(int index) {
+    return bitSet.get(index);
+  }
+  
+  @Override
+  public int length() {
+    // the size may not be correct...
+    return bitSet.size(); 
+  }
+  
   private static class DocIdBitSetIterator extends DocIdSetIterator {
     private int docId;
     private BitSet bitSet;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/FixedBitSet.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/FixedBitSet.java
index e0fcef08..6a2ea407 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/FixedBitSet.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/FixedBitSet.java
@@ -66,6 +66,11 @@ public DocIdSetIterator iterator() {
     return new OpenBitSetIterator(bits, bits.length);
   }
 
+  @Override
+  public Bits bits() {
+    return this;
+  }
+
   @Override
   public int length() {
     return numBits;
@@ -141,7 +146,7 @@ public boolean getAndClear(int index) {
   public int nextSetBit(int index) {
     assert index >= 0 && index < numBits;
     int i = index >> 6;
-    int subIndex = index & 0x3f;      // index within the word
+    final int subIndex = index & 0x3f;      // index within the word
     long word = bits[i] >> subIndex;  // skip all the bits to the right of index
 
     if (word!=0) {
@@ -158,6 +163,9 @@ public int nextSetBit(int index) {
     return -1;
   }
 
+  /** Returns the index of the last set bit before or on the index specified.
+   *  -1 is returned if there are no more set bits.
+   */
   public int prevSetBit(int index) {
     assert index >= 0 && index < numBits: "index=" + index + " numBits=" + numBits;
     int i = index >> 6;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/OpenBitSet.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/OpenBitSet.java
index fadb1995..ff7acf58 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/OpenBitSet.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/OpenBitSet.java
@@ -119,6 +119,11 @@ public DocIdSetIterator iterator() {
     return new OpenBitSetIterator(bits, wlen);
   }
 
+  @Override
+  public Bits bits() {
+    return this;
+  }
+
   /** This DocIdSet implementation is cacheable. */
   @Override
   public boolean isCacheable() {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/PagedBytes.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/PagedBytes.java
index d53e9b31..2e29464b 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/PagedBytes.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/PagedBytes.java
@@ -17,12 +17,14 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.store.IndexInput;
-
-import java.util.List;
-import java.util.ArrayList;
 import java.io.Closeable;
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.IndexInput;
 
 /** Represents a logical byte[] as a series of pages.  You
  *  can write-once into the logical byte[] (append only),
@@ -37,6 +39,8 @@
   private final int blockSize;
   private final int blockBits;
   private final int blockMask;
+  private boolean didSkipBytes;
+  private boolean frozen;
   private int upto;
   private byte[] currentBlock;
 
@@ -320,6 +324,7 @@ public void copy(BytesRef bytes, BytesRef out) throws IOException {
       if (currentBlock != null) {
         blocks.add(currentBlock);
         blockEnd.add(upto);
+        didSkipBytes = true;
       }
       currentBlock = new byte[blockSize];
       upto = 0;
@@ -338,6 +343,12 @@ public void copy(BytesRef bytes, BytesRef out) throws IOException {
 
   /** Commits final byte[], trimming it if necessary and if trim=true */
   public Reader freeze(boolean trim) {
+    if (frozen) {
+      throw new IllegalStateException("already frozen");
+    }
+    if (didSkipBytes) {
+      throw new IllegalStateException("cannot freeze when copy(BytesRef, BytesRef) was used");
+    }
     if (trim && upto < blockSize) {
       final byte[] newBlock = new byte[upto];
       System.arraycopy(currentBlock, 0, newBlock, 0, upto);
@@ -348,6 +359,7 @@ public Reader freeze(boolean trim) {
     }
     blocks.add(currentBlock);
     blockEnd.add(upto); 
+    frozen = true;
     currentBlock = null;
     return new Reader(this);
   }
@@ -389,4 +401,150 @@ public long copyUsingLengthPrefix(BytesRef bytes) throws IOException {
 
     return pointer;
   }
+
+  public final class PagedBytesDataInput extends DataInput {
+    private int currentBlockIndex;
+    private int currentBlockUpto;
+    private byte[] currentBlock;
+
+    PagedBytesDataInput() {
+      currentBlock = blocks.get(0);
+    }
+
+    @Override
+    public Object clone() {
+      PagedBytesDataInput clone = getDataInput();
+      clone.setPosition(getPosition());
+      return clone;
+    }
+
+    /** Returns the current byte position. */
+    public long getPosition() {
+      return currentBlockIndex * blockSize + currentBlockUpto;
+    }
+  
+    /** Seek to a position previously obtained from
+     *  {@link #getPosition}. */
+    public void setPosition(long pos) {
+      currentBlockIndex = (int) (pos >> blockBits);
+      currentBlock = blocks.get(currentBlockIndex);
+      currentBlockUpto = (int) (pos & blockMask);
+    }
+
+    @Override
+    public byte readByte() {
+      if (currentBlockUpto == blockSize) {
+        nextBlock();
+      }
+      return currentBlock[currentBlockUpto++];
+    }
+
+    @Override
+    public void readBytes(byte[] b, int offset, int len) {
+      final int offsetEnd = offset + len;
+      while (true) {
+        final int blockLeft = blockSize - currentBlockUpto;
+        final int left = offsetEnd - offset;
+        if (blockLeft < left) {
+          System.arraycopy(currentBlock, currentBlockUpto,
+                           b, offset,
+                           blockLeft);
+          nextBlock();
+          offset += blockLeft;
+        } else {
+          // Last block
+          System.arraycopy(currentBlock, currentBlockUpto,
+                           b, offset,
+                           left);
+          currentBlockUpto += left;
+          break;
+        }
+      }
+    }
+
+    private void nextBlock() {
+      currentBlockIndex++;
+      currentBlockUpto = 0;
+      currentBlock = blocks.get(currentBlockIndex);
+    }
+  }
+
+  public final class PagedBytesDataOutput extends DataOutput {
+    @Override
+    public void writeByte(byte b) {
+      if (upto == blockSize) {
+        if (currentBlock != null) {
+          blocks.add(currentBlock);
+          blockEnd.add(upto);
+        }
+        currentBlock = new byte[blockSize];
+        upto = 0;
+      }
+      currentBlock[upto++] = b;
+    }
+
+    @Override
+    public void writeBytes(byte[] b, int offset, int length) throws IOException {
+      if (length == 0) {
+        return;
+      }
+
+      if (upto == blockSize) {
+        if (currentBlock != null) {
+          blocks.add(currentBlock);
+          blockEnd.add(upto);
+        }
+        currentBlock = new byte[blockSize];
+        upto = 0;
+      }
+          
+      final int offsetEnd = offset + length;
+      while(true) {
+        final int left = offsetEnd - offset;
+        final int blockLeft = blockSize - upto;
+        if (blockLeft < left) {
+          System.arraycopy(b, offset, currentBlock, upto, blockLeft);
+          blocks.add(currentBlock);
+          blockEnd.add(blockSize);
+          currentBlock = new byte[blockSize];
+          upto = 0;
+          offset += blockLeft;
+        } else {
+          // Last block
+          System.arraycopy(b, offset, currentBlock, upto, left);
+          upto += left;
+          break;
+        }
+      }
+    }
+
+    /** Return the current byte position. */
+    public long getPosition() {
+      if (currentBlock == null) {
+        return 0;
+      } else {
+        return blocks.size() * blockSize + upto;
+      }
+    }
+  }
+
+  /** Returns a DataInput to read values from this
+   *  PagedBytes instance. */
+  public PagedBytesDataInput getDataInput() {
+    if (!frozen) {
+      throw new IllegalStateException("must call freeze() before getDataInput");
+    }
+    return new PagedBytesDataInput();
+  }
+
+  /** Returns a DataOutput that you may use to write into
+   *  this PagedBytes instance.  If you do this, you should
+   *  not call the other writing methods (eg, copy);
+   *  results are undefined. */
+  public PagedBytesDataOutput getDataOutput() {
+    if (frozen) {
+      throw new IllegalStateException("cannot get DataOutput after freeze()");
+    }
+    return new PagedBytesDataOutput();
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/automaton/BasicOperations.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/automaton/BasicOperations.java
index e7e9b301..06713c6f 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/automaton/BasicOperations.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/automaton/BasicOperations.java
@@ -322,7 +322,7 @@ static public Automaton intersection(Automaton a1, Automaton a2) {
     return c;
   }
 
-  /** Returns true if these two auotomata accept exactly the
+  /** Returns true if these two automata accept exactly the
    *  same language.  This is a costly computation!  Note
    *  also that a1 and a2 will be determinized as a side
    *  effect. */
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/fst/UpToTwoPositiveIntOutputs.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/fst/UpToTwoPositiveIntOutputs.java
index 4908301f..b05bdb32 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/fst/UpToTwoPositiveIntOutputs.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/fst/UpToTwoPositiveIntOutputs.java
@@ -25,7 +25,7 @@
 /**
  * Holds one or two longs for each input term.  If it's a
  * single output, Long is returned; else, TwoLongs.  Order
- * is preseved in the TwoLongs case, ie .first is the first
+ * is preserved in the TwoLongs case, ie .first is the first
  * input/output added to Builder, and .second is the
  * second.  You cannot store 0 output with this (that's
  * reserved to mean "no output")!
diff --git a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/mutable/MutableValue.java b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/mutable/MutableValue.java
index e5889944..fd95d053 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/mutable/MutableValue.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/java/org/apache/lucene/util/mutable/MutableValue.java
@@ -17,7 +17,7 @@
 package org.apache.lucene.util.mutable;
 
 /** @lucene.internal */
-public abstract class MutableValue implements Comparable {
+public abstract class MutableValue implements Comparable<MutableValue> {
   public boolean exists = true;
 
   public abstract void copy(MutableValue source);
@@ -30,9 +30,9 @@ public boolean exists() {
     return exists;
   }
 
-  public int compareTo(Object other) {
-    Class c1 = this.getClass();
-    Class c2 = other.getClass();
+  public int compareTo(MutableValue other) {
+    Class<? extends MutableValue> c1 = this.getClass();
+    Class<? extends MutableValue> c2 = other.getClass();
     if (c1 != c2) {
       int c = c1.hashCode() - c2.hashCode();
       if (c == 0) {
@@ -45,9 +45,7 @@ public int compareTo(Object other) {
 
   @Override
   public boolean equals(Object other) {
-    Class c1 = this.getClass();
-    Class c2 = other.getClass();
-    return (c1 == c2) && this.equalsSameType(other);
+    return (getClass() == other.getClass()) && this.equalsSameType(other);
   }
 
   @Override
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/TestAssertions.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/TestAssertions.java
index 34138ae8..eceae88f 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/TestAssertions.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/TestAssertions.java
@@ -17,55 +17,14 @@
  * limitations under the License.
  */
 
-import java.io.Reader;
-
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 
+/**
+ * validate that assertions are enabled during tests
+ */
 public class TestAssertions extends LuceneTestCase {
 
-  public void testBasics() {
-    try {
-      assert Boolean.FALSE.booleanValue();
-      fail("assertions are not enabled!");
-    } catch (AssertionError e) {
-      assert Boolean.TRUE.booleanValue();
-    }
-  }
-  
-  static class TestAnalyzer1 extends Analyzer {
-
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName, Reader aReader) {
-      return null;
-    }
-  }
-
-  static final class TestAnalyzer2 extends Analyzer {
-
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName, Reader aReader) {
-      return null;
-    }
-  }
-
-  static class TestAnalyzer3 extends Analyzer {
-
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName, Reader aReader) {
-      return null;
-    }
-  }
-
-  static class TestAnalyzer4 extends Analyzer {
-
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName, Reader aReader) {
-      return null;
-    }
-  }
-
   static class TestTokenStream1 extends TokenStream {
     @Override
     public final boolean incrementToken() { return false; }
@@ -82,31 +41,15 @@ protected TokenStreamComponents createComponents(String fieldName, Reader aReade
   }
 
   public void testTokenStreams() {
-    new TestAnalyzer1();
-    
-    new TestAnalyzer2();
-    
-    try {
-      new TestAnalyzer3();
-      fail("TestAnalyzer3 should fail assertion");
-    } catch (AssertionError e) {
-    }
-    
-    try {
-      new TestAnalyzer4();
-      fail("TestAnalyzer4 should fail assertion");
-    } catch (AssertionError e) {
-    }
-    
     new TestTokenStream1();
-    
     new TestTokenStream2();
-    
+    boolean doFail = false;
     try {
       new TestTokenStream3();
-      fail("TestTokenStream3 should fail assertion");
+      doFail = true;
     } catch (AssertionError e) {
+      // expected
     }
+    assertFalse("TestTokenStream3 should fail assertion", doFail);
   }
-
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestDocumentsWriterDeleteQueue.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestDocumentsWriterDeleteQueue.java
index 8c78a9f9..e48a559a 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestDocumentsWriterDeleteQueue.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestDocumentsWriterDeleteQueue.java
@@ -16,10 +16,12 @@
  * License for the specific language governing permissions and limitations under
  * the License.
  */
+import java.lang.reflect.Field;
 import java.util.HashSet;
 import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.locks.ReentrantLock;
 
 import org.apache.lucene.index.DocumentsWriterDeleteQueue.DeleteSlice;
 import org.apache.lucene.search.TermQuery;
@@ -143,6 +145,32 @@ public void testAnyChanges() {
     }
   }
 
+  public void testPartiallyAppliedGlobalSlice() throws SecurityException,
+      NoSuchFieldException, IllegalArgumentException, IllegalAccessException,
+      InterruptedException {
+    final DocumentsWriterDeleteQueue queue = new DocumentsWriterDeleteQueue();
+    Field field = DocumentsWriterDeleteQueue.class
+        .getDeclaredField("globalBufferLock");
+    field.setAccessible(true);
+    ReentrantLock lock = (ReentrantLock) field.get(queue);
+    lock.lock();
+    Thread t = new Thread() {
+      public void run() {
+        queue.addDelete(new Term("foo", "bar"));
+      }
+    };
+    t.start();
+    t.join();
+    lock.unlock();
+    assertTrue("changes in del queue but not in slice yet", queue.anyChanges());
+    queue.tryApplyGlobalSlice();
+    assertTrue("changes in global buffer", queue.anyChanges());
+    FrozenBufferedDeletes freezeGlobalBuffer = queue.freezeGlobalBuffer(null);
+    assertTrue(freezeGlobalBuffer.any());
+    assertEquals(1, freezeGlobalBuffer.termCount);
+    assertFalse("all changes applied", queue.anyChanges());
+  }
+
   public void testStressDeleteQueue() throws InterruptedException {
     DocumentsWriterDeleteQueue queue = new DocumentsWriterDeleteQueue();
     Set<Term> uniqueValues = new HashSet<Term>();
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning.java
index e69de29b..ad997d14 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning.java
@@ -0,0 +1,80 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.*;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+public class TestForTooMuchCloning extends LuceneTestCase {
+
+  // Make sure we don't clone IndexInputs too frequently
+  // during merging:
+  public void test() throws Exception {
+    final MockDirectoryWrapper dir = newDirectory();
+    final TieredMergePolicy tmp = new TieredMergePolicy();
+    tmp.setMaxMergeAtOnce(2);
+    final RandomIndexWriter w = new RandomIndexWriter(random, dir,
+                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));
+    final int numDocs = 20;
+    for(int docs=0;docs<numDocs;docs++) {
+      StringBuilder sb = new StringBuilder();
+      for(int terms=0;terms<100;terms++) {
+        sb.append(_TestUtil.randomRealisticUnicodeString(random));
+        sb.append(' ');
+      }
+      final Document doc = new Document();
+      doc.add(new TextField("field", sb.toString()));
+      w.addDocument(doc);
+    }
+    final IndexReader r = w.getReader();
+    w.close();
+
+    final int cloneCount = dir.getInputCloneCount();
+    //System.out.println("merge clone count=" + cloneCount);
+    assertTrue("too many calls to IndexInput.clone during merging: " + dir.getInputCloneCount(), cloneCount < 500);
+
+    final IndexSearcher s = new IndexSearcher(r);
+
+    // MTQ that matches all terms so the AUTO_REWRITE should
+    // cutover to filter rewrite and reuse a single DocsEnum
+    // across all terms;
+    final TopDocs hits = s.search(new TermRangeQuery("field",
+                                                     new BytesRef(),
+                                                     new BytesRef("\uFFFF"),
+                                                     true,
+                                                     true), 10);
+    assertTrue(hits.totalHits > 0);
+    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;
+    //System.out.println("query clone count=" + queryCloneCount);
+    assertTrue("too many calls to IndexInput.clone during TermRangeQuery: " + queryCloneCount, queryCloneCount < 50);
+    s.close();
+    r.close();
+    dir.close();
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexReader.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
index e694e62b..c389be95 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
@@ -1202,7 +1202,7 @@ public void testFieldCacheReuseAfterReopen() throws Exception {
   // LUCENE-1586: getUniqueTermCount
   public void testUniqueTermCount() throws Exception {
     Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodecProvider(_TestUtil.alwaysCodec("Standard")));
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
     Document doc = new Document();
     doc.add(newField("field", "a b c d e f g h i j k l m n o p q r s t u v w x y z", TextField.TYPE_UNSTORED));
     doc.add(newField("number", "0 1 2 3 4 5 6 7 8 9", TextField.TYPE_UNSTORED));
@@ -1218,12 +1218,8 @@ public void testUniqueTermCount() throws Exception {
     IndexReader r2 = IndexReader.openIfChanged(r);
     assertNotNull(r2);
     r.close();
-    try {
-      r2.getUniqueTermCount();
-      fail("expected exception");
-    } catch (UnsupportedOperationException uoe) {
-      // expected
-    }
+    assertEquals(-1, r2.getUniqueTermCount());
+
     IndexReader[] subs = r2.getSequentialSubReaders();
     for(int i=0;i<subs.length;i++) {
       assertEquals(36, subs[i].getUniqueTermCount());
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
index d7e5e8e1..493a2535 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -870,6 +870,46 @@ public void testEmptyFieldName() throws IOException {
     dir.close();
   }
 
+  public void testEmptyFieldNameTerms() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
+    Document doc = new Document();
+    doc.add(newField("", "a b c", TextField.TYPE_UNSTORED));
+    writer.addDocument(doc);  
+    writer.close();
+    IndexReader reader = IndexReader.open(dir, true);
+    IndexReader subreader = getOnlySegmentReader(reader);
+    TermsEnum te = subreader.fields().terms("").iterator();
+    assertEquals(new BytesRef("a"), te.next());
+    assertEquals(new BytesRef("b"), te.next());
+    assertEquals(new BytesRef("c"), te.next());
+    assertNull(te.next());
+    reader.close();
+    dir.close();
+  }
+  
+  public void testEmptyFieldNameWithEmptyTerm() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
+    Document doc = new Document();
+    doc.add(newField("", "", StringField.TYPE_UNSTORED));
+    doc.add(newField("", "a", StringField.TYPE_UNSTORED));
+    doc.add(newField("", "b", StringField.TYPE_UNSTORED));
+    doc.add(newField("", "c", StringField.TYPE_UNSTORED));
+    writer.addDocument(doc);  
+    writer.close();
+    IndexReader reader = IndexReader.open(dir, true);
+    IndexReader subreader = getOnlySegmentReader(reader);
+    TermsEnum te = subreader.fields().terms("").iterator();
+    assertEquals(new BytesRef(""), te.next());
+    assertEquals(new BytesRef("a"), te.next());
+    assertEquals(new BytesRef("b"), te.next());
+    assertEquals(new BytesRef("c"), te.next());
+    assertNull(te.next());
+    reader.close();
+    dir.close();
+  }
+
 
 
   private static final class MockIndexWriter extends IndexWriter {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
index f3199d9a..7cc0dbf8 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
@@ -24,14 +24,15 @@
 import java.util.Collections;
 import java.util.List;
 import java.util.Random;
-import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.codecs.CodecProvider;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TermQuery;
@@ -897,6 +898,8 @@ public void testDeleteAllSlowly() throws Exception {
   }
   
   public void testIndexingThenDeleting() throws Exception {
+    assumeFalse("This test cannot run with Memory codec", CodecProvider.getDefault().getFieldCodec("field").equals("Memory"));
+    assumeFalse("This test cannot run with SimpleText codec", CodecProvider.getDefault().getFieldCodec("field").equals("SimpleText"));
     final Random r = random;
     Directory dir = newDirectory();
     // note this test explicitly disables payloads
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java
index dffc9662..c012cdac 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java
@@ -857,7 +857,7 @@ public void run() {
     int sum = 0;
     while(System.currentTimeMillis() < endTime) {
       IndexReader r2 = IndexReader.openIfChanged(r);
-      if (r2 != r) {
+      if (r2 != null) {
         r.close();
         r = r2;
       }
@@ -1016,4 +1016,40 @@ public void testNoTermsIndex() throws Exception {
     }
   }
   
+  public void testReopenAfterNoRealChange() throws Exception {
+    Directory d = newDirectory();
+    IndexWriter w = new IndexWriter(
+        d,
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
+    w.setInfoStream(VERBOSE ? System.out : null);
+
+    IndexReader r = w.getReader(); // start pooling readers
+
+    IndexReader r2 = IndexReader.openIfChanged(r);
+    assertNull(r2);
+    
+    w.addDocument(new Document());
+    IndexReader r3 = IndexReader.openIfChanged(r);
+    assertNotNull(r3);
+    assertTrue(r3.getVersion() != r.getVersion());
+    assertTrue(r3.isCurrent());
+
+    // Deletes nothing in reality...:
+    w.deleteDocuments(new Term("foo", "bar"));
+
+    // ... but IW marks this as not current:
+    assertFalse(r3.isCurrent());
+    IndexReader r4 = IndexReader.openIfChanged(r3);
+    assertNull(r4);
+
+    // Deletes nothing in reality...:
+    w.deleteDocuments(new Term("foo", "bar"));
+    IndexReader r5 = IndexReader.openIfChanged(r3, w, true);
+    assertNull(r5);
+
+    r3.close();
+
+    w.close();
+    d.close();
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestNRTThreads.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestNRTThreads.java
index f40d653d..b792eca7 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestNRTThreads.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestNRTThreads.java
@@ -22,11 +22,13 @@
 
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.LuceneTestCase.UseNoMemoryExpensiveCodec;
 
 // TODO
 //   - mix in optimize, addIndexes
 //   - randomoly mix in non-congruent docs
 
+@UseNoMemoryExpensiveCodec
 public class TestNRTThreads extends ThreadedIndexingAndSearchingTestCase {
   
   @Override
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java
index aaaca02d..248baf0e 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java
@@ -37,11 +37,12 @@ public void testRollingUpdates() throws Exception {
 
     CodecProvider provider = CodecProvider.getDefault();
     //provider.register(new MemoryCodec());
-    if (random.nextBoolean()) {
+    if ( (!"PreFlex".equals(provider.getDefaultFieldCodec())) && random.nextBoolean()) {
       provider.setFieldCodec("docid", "Memory");
     }
 
     final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodecProvider(provider));
+    w.setInfoStream(VERBOSE ? System.out : null);
     final int SIZE = atLeast(TEST_NIGHTLY ? 100 : 20);
     int id = 0;
     IndexReader r = null;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
index a84a96d4..0348bd8d 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
@@ -146,12 +146,14 @@ public void testInvalidFilesToCreateCompound() throws Exception {
     
     // Assert that SM fails if .del exists
     SegmentMerger sm = new SegmentMerger(dir, 1, "a", null, null, null, newIOContext(random));
+    boolean doFail = false;
     try {
       sm.createCompoundFile("b1", w.segmentInfos.info(0), newIOContext(random));
-      fail("should not have been able to create a .cfs with .del and .s* files");
+      doFail = true; // should never get here
     } catch (AssertionError e) {
       // expected
     }
+    assertFalse("should not have been able to create a .cfs with .del and .s* files", doFail);
     
     // Create an index w/ .s*
     w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE));
@@ -164,12 +166,15 @@ public void testInvalidFilesToCreateCompound() throws Exception {
     r.close();
     
     // Assert that SM fails if .s* exists
+    SegmentInfos sis = new SegmentInfos();
+    sis.read(dir);
     try {
-      sm.createCompoundFile("b2", w.segmentInfos.info(0), newIOContext(random));
-      fail("should not have been able to create a .cfs with .del and .s* files");
+      sm.createCompoundFile("b2", sis.info(0), newIOContext(random));
+      doFail = true; // should never get here
     } catch (AssertionError e) {
       // expected
     }
+    assertFalse("should not have been able to create a .cfs with .del and .s* files", doFail);
 
     dir.close();
   }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestTermsEnum.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestTermsEnum.java
index 40159eb1..00bcbfbf 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestTermsEnum.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/TestTermsEnum.java
@@ -191,7 +191,7 @@ public void testIntersectRandom() throws IOException {
     final Directory dir = newDirectory();
     final RandomIndexWriter w = new RandomIndexWriter(random, dir);
     
-    final int numTerms = atLeast(1000);
+    final int numTerms = atLeast(300);
 
     final Set<String> terms = new HashSet<String>();
     final Collection<String> pendingTerms = new ArrayList<String>();
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/codecs/preflex/TestTermInfosReaderIndex.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/codecs/preflex/TestTermInfosReaderIndex.java
index 3f24d583..ff7c5ac2 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/codecs/preflex/TestTermInfosReaderIndex.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/codecs/preflex/TestTermInfosReaderIndex.java
@@ -1 +1,194 @@
   + native
+package org.apache.lucene.index.codecs.preflex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LogMergePolicy;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SegmentReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.index.codecs.CoreCodecProvider;
+import org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+public class TestTermInfosReaderIndex extends LuceneTestCase {
+  
+  private static final int NUMBER_OF_DOCUMENTS = 1000;
+  private static final int NUMBER_OF_FIELDS = 100;
+  private TermInfosReaderIndex index;
+  private Directory directory;
+  private SegmentTermEnum termEnum;
+  private int indexDivisor;
+  private int termIndexInterval;
+  private IndexReader reader;
+  private List<Term> sampleTerms;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    indexDivisor = _TestUtil.nextInt(random, 1, 10);
+    directory = newDirectory();
+    termIndexInterval = populate(directory);
+
+    IndexReader r0 = IndexReader.open(directory);
+    SegmentReader r = (SegmentReader) r0.getSequentialSubReaders()[0];
+    String segment = r.getSegmentName();
+    r.close();
+
+    FieldInfos fieldInfos = new FieldInfos(directory, IndexFileNames.segmentFileName(segment, "", IndexFileNames.FIELD_INFOS_EXTENSION));
+    String segmentFileName = IndexFileNames.segmentFileName(segment, "", PreFlexCodec.TERMS_INDEX_EXTENSION);
+    long tiiFileLength = directory.fileLength(segmentFileName);
+    IndexInput input = directory.openInput(segmentFileName, newIOContext(random));
+    termEnum = new SegmentTermEnum(directory.openInput(IndexFileNames.segmentFileName(segment, "", PreFlexCodec.TERMS_EXTENSION), newIOContext(random)), fieldInfos, false);
+    int totalIndexInterval = termEnum.indexInterval * indexDivisor;
+    
+    SegmentTermEnum indexEnum = new SegmentTermEnum(input, fieldInfos, true);
+    index = new TermInfosReaderIndex(indexEnum, indexDivisor, tiiFileLength, totalIndexInterval);
+    indexEnum.close();
+    input.close();
+    
+    reader = IndexReader.open(directory);
+    sampleTerms = sample(reader,1000);
+    
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    termEnum.close();
+    reader.close();
+    directory.close();
+    super.tearDown();
+  }
+  
+  public void testSeekEnum() throws CorruptIndexException, IOException {
+    int indexPosition = 3;
+    SegmentTermEnum clone = (SegmentTermEnum) termEnum.clone();
+    Term term = findTermThatWouldBeAtIndex(clone, indexPosition);
+    SegmentTermEnum enumerator = clone;
+    index.seekEnum(enumerator, indexPosition);
+    assertEquals(term, enumerator.term());
+    clone.close();
+  }
+  
+  public void testCompareTo() throws IOException {
+    Term term = new Term("field" + random.nextInt(NUMBER_OF_FIELDS) ,getText());
+    for (int i = 0; i < index.length(); i++) {
+      Term t = index.getTerm(i);
+      int compareTo = term.compareTo(t);
+      assertEquals(compareTo, index.compareTo(term, i));
+    }
+  }
+  
+  public void testRandomSearchPerformance() throws CorruptIndexException, IOException {
+    IndexSearcher searcher = new IndexSearcher(reader);
+    for (Term t : sampleTerms) {
+      TermQuery query = new TermQuery(t);
+      TopDocs topDocs = searcher.search(query, 10);
+      assertTrue(topDocs.totalHits > 0);
+    }
+    searcher.close();
+  }
+
+  private List<Term> sample(IndexReader reader, int size) throws IOException {
+    List<Term> sample = new ArrayList<Term>();
+    Random random = new Random();
+    FieldsEnum fieldsEnum = MultiFields.getFields(reader).iterator();
+    String field;
+    while((field = fieldsEnum.next()) != null) {
+      TermsEnum terms = fieldsEnum.terms();
+      while (terms.next() != null) {
+        if (sample.size() >= size) {
+          int pos = random.nextInt(size);
+          sample.set(pos, new Term(field, terms.term()));
+        } else {
+          sample.add(new Term(field, terms.term()));
+        }
+      }
+    }
+    Collections.shuffle(sample);
+    return sample;
+  }
+
+  private Term findTermThatWouldBeAtIndex(SegmentTermEnum termEnum, int index) throws IOException {
+    int termPosition = index * termIndexInterval * indexDivisor;
+    for (int i = 0; i < termPosition; i++) {
+      if (!termEnum.next()) {
+        fail("Should not have run out of terms.");
+      }
+    }
+    return termEnum.term();
+  }
+
+  private int populate(Directory directory) throws CorruptIndexException, LockObtainFailedException, IOException {
+    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, 
+        new MockAnalyzer(random, MockTokenizer.KEYWORD, false));
+    CoreCodecProvider cp = new CoreCodecProvider();
+    cp.unregister(cp.lookup("PreFlex"));
+    cp.register(new PreFlexRWCodec());
+    cp.setDefaultFieldCodec("PreFlex");
+    config.setCodecProvider(cp);
+    // turn off compound file, this test will open some index files directly.
+    LogMergePolicy mp = newLogMergePolicy();
+    mp.setUseCompoundFile(false);
+    config.setMergePolicy(mp);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random, directory, config);
+    for (int i = 0; i < NUMBER_OF_DOCUMENTS; i++) {
+      Document document = new Document();
+      for (int f = 0; f < NUMBER_OF_FIELDS; f++) {
+        document.add(newField("field" + f, getText(), StringField.TYPE_UNSTORED));
+      }
+      writer.addDocument(document);
+    }
+    writer.optimize();
+    writer.close();
+    return config.getTermIndexInterval();
+  }
+  
+  private String getText() {
+    return Long.toString(random.nextLong(),Character.MAX_RADIX);
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java
index e69de29b..e4dcc844 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java
@@ -0,0 +1,216 @@
+package org.apache.lucene.index.codecs.pulsing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.IdentityHashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CheckIndex;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.codecs.BlockTreeTermsReader;
+import org.apache.lucene.index.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.index.codecs.DefaultDocValuesConsumer;
+import org.apache.lucene.index.codecs.DefaultDocValuesProducer;
+import org.apache.lucene.index.codecs.FieldsConsumer;
+import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.index.codecs.PerDocConsumer;
+import org.apache.lucene.index.codecs.PerDocValues;
+import org.apache.lucene.index.codecs.PostingsReaderBase;
+import org.apache.lucene.index.codecs.PostingsWriterBase;
+import org.apache.lucene.index.codecs.standard.StandardCodec;
+import org.apache.lucene.index.codecs.standard.StandardPostingsReader;
+import org.apache.lucene.index.codecs.standard.StandardPostingsWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Tests that pulsing codec reuses its enums and wrapped enums
+ */
+public class TestPulsingReuse extends LuceneTestCase {
+  // TODO: this is a basic test. this thing is complicated, add more
+  public void testSophisticatedReuse() throws Exception {
+    // we always run this test with pulsing codec.
+    CodecProvider cp = _TestUtil.alwaysCodec(new PulsingCodec(1));
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodecProvider(cp));
+    Document doc = new Document();
+    doc.add(new Field("foo", "a b b c c c d e f g g h i i j j k", TextField.TYPE_UNSTORED));
+    iw.addDocument(doc);
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexReader segment = ir.getSequentialSubReaders()[0];
+    DocsEnum reuse = null;
+    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
+    TermsEnum te = segment.terms("foo").iterator();
+    while (te.next() != null) {
+      reuse = te.docs(null, reuse);
+      allEnums.put(reuse, true);
+    }
+    
+    assertEquals(2, allEnums.size());
+    
+    allEnums.clear();
+    DocsAndPositionsEnum posReuse = null;
+    te = segment.terms("foo").iterator();
+    while (te.next() != null) {
+      posReuse = te.docsAndPositions(null, posReuse);
+      allEnums.put(posReuse, true);
+    }
+    
+    assertEquals(2, allEnums.size());
+    
+    ir.close();
+    dir.close();
+  }
+  
+  /** tests reuse with Pulsing1(Pulsing2(Standard)) */
+  public void testNestedPulsing() throws Exception {
+    // we always run this test with pulsing codec.
+    CodecProvider cp = _TestUtil.alwaysCodec(new NestedPulsing());
+    MockDirectoryWrapper dir = newDirectory();
+    dir.setCheckIndexOnClose(false); // will do this ourselves, custom codec
+    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodecProvider(cp));
+    Document doc = new Document();
+    doc.add(new Field("foo", "a b b c c c d e f g g g h i i j j k l l m m m", TextField.TYPE_UNSTORED));
+    // note: the reuse is imperfect, here we would have 4 enums (lost reuse when we get an enum for 'm')
+    // this is because we only track the 'last' enum we reused (not all).
+    // but this seems 'good enough' for now.
+    iw.addDocument(doc);
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexReader segment = ir.getSequentialSubReaders()[0];
+    DocsEnum reuse = null;
+    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
+    TermsEnum te = segment.terms("foo").iterator();
+    while (te.next() != null) {
+      reuse = te.docs(null, reuse);
+      allEnums.put(reuse, true);
+    }
+    
+    assertEquals(4, allEnums.size());
+    
+    allEnums.clear();
+    DocsAndPositionsEnum posReuse = null;
+    te = segment.terms("foo").iterator();
+    while (te.next() != null) {
+      posReuse = te.docsAndPositions(null, posReuse);
+      allEnums.put(posReuse, true);
+    }
+    
+    assertEquals(4, allEnums.size());
+    
+    ir.close();
+    CheckIndex ci = new CheckIndex(dir);
+    ci.checkIndex(null, cp);
+    dir.close();
+  }
+  
+  static class NestedPulsing extends Codec {
+    public NestedPulsing() {
+      super("NestedPulsing");
+    }
+    
+    @Override
+    public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+      PostingsWriterBase docsWriter = new StandardPostingsWriter(state);
+
+      PostingsWriterBase pulsingWriterInner = new PulsingPostingsWriter(2, docsWriter);
+      PostingsWriterBase pulsingWriter = new PulsingPostingsWriter(1, pulsingWriterInner);
+      
+      // Terms dict
+      boolean success = false;
+      try {
+        FieldsConsumer ret = new BlockTreeTermsWriter(state, pulsingWriter, 
+            BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+        success = true;
+        return ret;
+      } finally {
+        if (!success) {
+          pulsingWriter.close();
+        }
+      }
+    }
+
+    @Override
+    public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+      PostingsReaderBase docsReader = new StandardPostingsReader(state.dir, state.segmentInfo, state.context, state.codecId);
+      PostingsReaderBase pulsingReaderInner = new PulsingPostingsReader(docsReader);
+      PostingsReaderBase pulsingReader = new PulsingPostingsReader(pulsingReaderInner);
+      boolean success = false;
+      try {
+        FieldsProducer ret = new BlockTreeTermsReader(
+                                                      state.dir, state.fieldInfos, state.segmentInfo.name,
+                                                      pulsingReader,
+                                                      state.context,
+                                                      state.codecId,
+                                                      state.termsIndexDivisor);
+        success = true;
+        return ret;
+      } finally {
+        if (!success) {
+          pulsingReader.close();
+        }
+      }
+    }
+
+    @Override
+    public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
+      return new DefaultDocValuesConsumer(state);
+    }
+
+    @Override
+    public PerDocValues docsProducer(SegmentReadState state) throws IOException {
+      return new DefaultDocValuesProducer(state);
+    }
+
+    @Override
+    public void files(Directory dir, SegmentInfo segmentInfo, int id, Set<String> files) throws IOException {
+      StandardPostingsReader.files(dir, segmentInfo, id, files);
+      BlockTreeTermsReader.files(dir, segmentInfo, id, files);
+      DefaultDocValuesConsumer.files(dir, segmentInfo, id, files);
+    }
+
+    @Override
+    public void getExtensions(Set<String> extensions) {
+      StandardCodec.getStandardExtensions(extensions);
+      DefaultDocValuesConsumer.getExtensions(extensions);
+    }
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java
index 6da85349..2c6ecc97 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java
@@ -24,6 +24,7 @@
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.OpenBitSet;
 
@@ -109,7 +110,7 @@ public void clearTotalNumberOfTerms() {
      * results.
      */
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+    public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
       final FieldCache.DocTermsIndex fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader, query.field);
       // Cannot use FixedBitSet because we require long index (ord):
       final OpenBitSet termSet = new OpenBitSet(fcsi.numOrd());
@@ -139,6 +140,11 @@ public long getSumDocFreq() throws IOException {
         public int getDocCount() throws IOException {
           return -1;
         }
+
+        @Override
+        public long getUniqueTermCount() throws IOException {
+          return -1;
+        }
       });
       
       assert termsEnum != null;
@@ -158,7 +164,8 @@ public int getDocCount() throws IOException {
         return DocIdSet.EMPTY_DOCIDSET;
       }
       
-      return new FieldCacheRangeFilter.FieldCacheDocIdSet(context.reader, true) {
+      final int maxDoc = context.reader.maxDoc();
+      return new FieldCacheRangeFilter.FieldCacheDocIdSet(maxDoc, acceptDocs) {
         @Override
         boolean matchDoc(int doc) throws ArrayIndexOutOfBoundsException {
           return termSet.get(fcsi.getOrd(doc));
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/JustCompileSearch.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/JustCompileSearch.java
index 816e6eb2..9327dd31 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/JustCompileSearch.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/JustCompileSearch.java
@@ -160,7 +160,7 @@ public FieldComparator newComparator(String fieldname, int numHits,
     // still added here in case someone will add abstract methods in the future.
     
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
       return null;
     }
   }
@@ -288,12 +288,12 @@ public Similarity get(String field) {
   static final class JustCompileSpanFilter extends SpanFilter {
 
     @Override
-    public SpanFilterResult bitSpans(AtomicReaderContext context) throws IOException {
+    public SpanFilterResult bitSpans(AtomicReaderContext context, Bits acceptDocs) throws IOException {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
     
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
       return null;
     }    
   }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/MockFilter.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/MockFilter.java
index 1152db0f..449b2220 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/MockFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/MockFilter.java
@@ -19,15 +19,16 @@
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.util.DocIdBitSet;
-import java.util.BitSet;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.Bits;
 
 public class MockFilter extends Filter {
   private boolean wasCalled;
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) {
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
     wasCalled = true;
-    return new DocIdBitSet(new BitSet());
+    return new FixedBitSet(context.reader.maxDoc());
   }
 
   public void clear() {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/SingleDocTestFilter.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/SingleDocTestFilter.java
index a33a6c17..191a4b7d 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/SingleDocTestFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/SingleDocTestFilter.java
@@ -18,9 +18,9 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.util.DocIdBitSet;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.FixedBitSet;
 
-import java.util.BitSet;
 import java.io.IOException;
 
 public class SingleDocTestFilter extends Filter {
@@ -31,9 +31,10 @@ public SingleDocTestFilter(int doc) {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
-    BitSet bits = new BitSet(context.reader.maxDoc());
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    FixedBitSet bits = new FixedBitSet(context.reader.maxDoc());
     bits.set(doc);
-    return new DocIdBitSet(bits);
+    if (acceptDocs != null && !acceptDocs.get(doc)) bits.clear(doc);
+    return bits;
   }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java
index 94d4598a..aa5e15bc 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java
@@ -18,16 +18,19 @@
  */
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.List;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanQuery.BooleanWeight;
 import org.apache.lucene.store.Directory;
-
 import org.apache.lucene.util.LuceneTestCase;
 
 public class TestBooleanScorer extends LuceneTestCase
@@ -94,12 +97,87 @@ public void testEmptyBucketWithMoreDocs() throws Exception {
     
     BooleanScorer bs = new BooleanScorer(weight, false, 1, Arrays.asList(scorers), null, scorers.length);
     
-    assertEquals("should have received 3000", 3000, bs.nextDoc());
-    assertEquals("should have received NO_MORE_DOCS", DocIdSetIterator.NO_MORE_DOCS, bs.nextDoc());
+    final List<Integer> hits = new ArrayList<Integer>();
+    bs.score(new Collector() {
+      int docBase;
+      @Override
+      public void setScorer(Scorer scorer) {
+      }
+      
+      @Override
+      public void collect(int doc) throws IOException {
+        hits.add(docBase+doc);
+      }
+      
+      @Override
+      public void setNextReader(AtomicReaderContext context) {
+        docBase = context.docBase;
+      }
+      
+      @Override
+      public boolean acceptsDocsOutOfOrder() {
+        return true;
+      }
+      });
+
+    assertEquals("should have only 1 hit", 1, hits.size());
+    assertEquals("hit should have been docID=3000", 3000, hits.get(0).intValue());
     searcher.close();
     ir.close();
     directory.close();
+  }
     
+  public void testMoreThan32ProhibitedClauses() throws Exception {
+    final Directory d = newDirectory();
+    final RandomIndexWriter w = new RandomIndexWriter(random, d);
+    Document doc = new Document();
+    doc.add(new TextField("field", "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33"));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new TextField("field", "33"));
+    w.addDocument(doc);
+    final IndexReader r = w.getReader();
+    w.close();
+    final IndexSearcher s = newSearcher(r);
+
+    final BooleanQuery q = new BooleanQuery();
+    for(int term=0;term<33;term++) {
+      q.add(new BooleanClause(new TermQuery(new Term("field", ""+term)),
+                              BooleanClause.Occur.MUST_NOT));
   }
+    q.add(new BooleanClause(new TermQuery(new Term("field", "33")),
+                            BooleanClause.Occur.SHOULD));
 
+    final int[] count = new int[1];
+    s.search(q, new Collector() {
+      private Scorer scorer;
+    
+      @Override
+      public void setScorer(Scorer scorer) {
+        // Make sure we got BooleanScorer:
+        this.scorer = scorer;
+        assertEquals("Scorer is implemented by wrong class", BooleanScorer.class.getName() + "$BucketScorer", scorer.getClass().getName());
+      }
+      
+      @Override
+      public void collect(int doc) throws IOException {
+        count[0]++;
+      }
+      
+      @Override
+      public void setNextReader(AtomicReaderContext context) {
+      }
+      
+      @Override
+      public boolean acceptsDocsOutOfOrder() {
+        return true;
+      }
+    });
+
+    assertEquals(1, count[0]);
+    
+    s.close();
+    r.close();
+    d.close();
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java
index 702c2fa3..07938cf7 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java
@@ -69,33 +69,21 @@ public void testEnforceDeletions() throws Exception {
 
     final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term("id", "1")));
 
-    // ignore deletions
-    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);
+    CachingSpanFilter filter = new CachingSpanFilter(startFilter);
         
     docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
     assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
-    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);
-    docs = searcher.search(constantScore, 1);
-    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
-
-    // now delete the doc, refresh the reader, and see that
-    // it's not there
-    _TestUtil.keepFullyDeletedSegments(writer.w);
-    writer.deleteDocuments(new Term("id", "1"));
-
-    reader = refreshReader(reader);
-    searcher.close();
-    searcher = newSearcher(reader, false);
-
-    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
-    assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
-
+    int missCount = filter.missCount;
+    assertTrue(missCount > 0);
+    Query constantScore = new ConstantScoreQuery(filter);
     docs = searcher.search(constantScore, 1);
     assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+    assertEquals(missCount, filter.missCount);
 
-
-    // force cache to regenerate:
-    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);
+    // NOTE: important to hold ref here so GC doesn't clear
+    // the cache entry!  Else the assert below may sometimes
+    // fail:
+    IndexReader oldReader = reader;
 
     writer.addDocument(doc);
     reader = refreshReader(reader);
@@ -103,27 +91,19 @@ public void testEnforceDeletions() throws Exception {
     searcher = newSearcher(reader, false);
         
     docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
-    assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
+    assertEquals("[query + filter] Should find 2 hits...", 2, docs.totalHits);
+    assertTrue(filter.missCount > missCount);
+    missCount = filter.missCount;
 
     constantScore = new ConstantScoreQuery(filter);
     docs = searcher.search(constantScore, 1);
-    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+    assertEquals("[just filter] Should find a hit...", 2, docs.totalHits);
+    assertEquals(missCount, filter.missCount);
 
     // NOTE: important to hold ref here so GC doesn't clear
     // the cache entry!  Else the assert below may sometimes
     // fail:
-    IndexReader oldReader = reader;
-
-    // make sure we get a cache hit when we reopen readers
-    // that had no new deletions
-    reader = refreshReader(reader);
-    assertTrue(reader != oldReader);
-    searcher.close();
-    searcher = newSearcher(reader, false);
-    int missCount = filter.missCount;
-    docs = searcher.search(constantScore, 1);
-    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
-    assertEquals(missCount, filter.missCount);
+    IndexReader oldReader2 = reader;
 
     // now delete the doc, refresh the reader, and see that it's not there
     writer.deleteDocuments(new Term("id", "1"));
@@ -134,15 +114,18 @@ public void testEnforceDeletions() throws Exception {
 
     docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
     assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
+    assertEquals(missCount, filter.missCount);
 
     docs = searcher.search(constantScore, 1);
     assertEquals("[just filter] Should *not* find a hit...", 0, docs.totalHits);
+    assertEquals(missCount, filter.missCount);
 
     // NOTE: silliness to make sure JRE does not optimize
     // away our holding onto oldReader to prevent
     // CachingWrapperFilter's WeakHashMap from dropping the
     // entry:
     assertTrue(oldReader != null);
+    assertTrue(oldReader2 != null);
 
     searcher.close();
     writer.close();
@@ -160,4 +143,5 @@ private static IndexReader refreshReader(IndexReader reader) throws IOException
       return oldReader;
     }
   }
+  
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
index 48bf9915..3556eea0 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
@@ -29,6 +29,7 @@
 import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util._TestUtil;
@@ -46,15 +47,15 @@ public void testCachingWorks() throws Exception {
     CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
 
     // first time, nested filter is called
-    cacher.getDocIdSet(context);
+    cacher.getDocIdSet(context, context.reader.getLiveDocs());
     assertTrue("first time", filter.wasCalled());
 
     // make sure no exception if cache is holding the wrong docIdSet
-    cacher.getDocIdSet(context);
+    cacher.getDocIdSet(context, context.reader.getLiveDocs());
 
     // second time, nested filter should not be called
     filter.clear();
-    cacher.getDocIdSet(context);
+    cacher.getDocIdSet(context, context.reader.getLiveDocs());
     assertFalse("second time", filter.wasCalled());
 
     reader.close();
@@ -71,14 +72,14 @@ public void testNullDocIdSet() throws Exception {
 
     final Filter filter = new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
         return null;
       }
     };
     CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
 
     // the caching filter should return the empty set constant
-    assertSame(DocIdSet.EMPTY_DOCIDSET, cacher.getDocIdSet(context));
+    assertSame(DocIdSet.EMPTY_DOCIDSET, cacher.getDocIdSet(context, context.reader.getLiveDocs()));
     
     reader.close();
     dir.close();
@@ -94,7 +95,7 @@ public void testNullDocIdSetIterator() throws Exception {
 
     final Filter filter = new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
         return new DocIdSet() {
           @Override
           public DocIdSetIterator iterator() {
@@ -106,7 +107,7 @@ public DocIdSetIterator iterator() {
     CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
 
     // the caching filter should return the empty set constant
-    assertSame(DocIdSet.EMPTY_DOCIDSET, cacher.getDocIdSet(context));
+    assertSame(DocIdSet.EMPTY_DOCIDSET, cacher.getDocIdSet(context, context.reader.getLiveDocs()));
     
     reader.close();
     dir.close();
@@ -116,8 +117,8 @@ private static void assertDocIdSetCacheable(IndexReader reader, Filter filter, b
     assertTrue(reader.getTopReaderContext().isAtomic);
     AtomicReaderContext context = (AtomicReaderContext) reader.getTopReaderContext();
     final CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
-    final DocIdSet originalSet = filter.getDocIdSet(context);
-    final DocIdSet cachedSet = cacher.getDocIdSet(context);
+    final DocIdSet originalSet = filter.getDocIdSet(context, context.reader.getLiveDocs());
+    final DocIdSet cachedSet = cacher.getDocIdSet(context, context.reader.getLiveDocs());
     assertTrue(cachedSet.isCacheable());
     assertEquals(shouldCacheable, originalSet.isCacheable());
     //System.out.println("Original: "+originalSet.getClass().getName()+" -- cached: "+cachedSet.getClass().getName());
@@ -145,7 +146,7 @@ public void testIsCacheAble() throws Exception {
     // a fixedbitset filter is always cacheable
     assertDocIdSetCacheable(reader, new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
         return new FixedBitSet(context.reader.maxDoc());
       }
     }, true);
@@ -187,91 +188,41 @@ public void testEnforceDeletions() throws Exception {
 
     final Filter startFilter = new QueryWrapperFilter(new TermQuery(new Term("id", "1")));
 
-    // ignore deletions
-    CachingWrapperFilter filter = new CachingWrapperFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);
+    CachingWrapperFilter filter = new CachingWrapperFilter(startFilter);
         
     docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
     assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
-    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);
-    docs = searcher.search(constantScore, 1);
-    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
-
-    // now delete the doc, refresh the reader, and see that it's not there
-    _TestUtil.keepFullyDeletedSegments(writer.w);
-    writer.deleteDocuments(new Term("id", "1"));
-
-    reader = refreshReader(reader);
-    searcher.close();
-    searcher = newSearcher(reader, false);
-
-    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
-    assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
-
+    int missCount = filter.missCount;
+    assertTrue(missCount > 0);
+    Query constantScore = new ConstantScoreQuery(filter);
     docs = searcher.search(constantScore, 1);
     assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+    assertEquals(missCount, filter.missCount);
 
-
-    // force cache to regenerate:
-    filter = new CachingWrapperFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);
+    // NOTE: important to hold ref here so GC doesn't clear
+    // the cache entry!  Else the assert below may sometimes
+    // fail:
+    IndexReader oldReader = reader;
 
     writer.addDocument(doc);
-
     reader = refreshReader(reader);
     searcher.close();
     searcher = newSearcher(reader, false);
         
     docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
-
-    assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
+    assertEquals("[query + filter] Should find 2 hits...", 2, docs.totalHits);
+    assertTrue(filter.missCount > missCount);
+    missCount = filter.missCount;
 
     constantScore = new ConstantScoreQuery(filter);
     docs = searcher.search(constantScore, 1);
-    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+    assertEquals("[just filter] Should find a hit...", 2, docs.totalHits);
+    assertEquals(missCount, filter.missCount);
 
     // NOTE: important to hold ref here so GC doesn't clear
     // the cache entry!  Else the assert below may sometimes
     // fail:
-    IndexReader oldReader = reader;
-
-    // make sure we get a cache hit when we reopen reader
-    // that had no change to deletions
-    reader = refreshReader(reader);
-    assertTrue(reader != oldReader);
-    searcher.close();
-    searcher = newSearcher(reader, false);
-    int missCount = filter.missCount;
-    docs = searcher.search(constantScore, 1);
-    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
-    assertEquals(missCount, filter.missCount);
-
-    // now delete the doc, refresh the reader, and see that it's not there
-    writer.deleteDocuments(new Term("id", "1"));
-
-    reader = refreshReader(reader);
-    searcher.close();
-    searcher = newSearcher(reader, false);
-
-    missCount = filter.missCount;
-    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
-    assertEquals(missCount+1, filter.missCount);
-    assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
-    docs = searcher.search(constantScore, 1);
-    assertEquals("[just filter] Should *not* find a hit...", 0, docs.totalHits);
-
-
-    // apply deletions dynamically
-    filter = new CachingWrapperFilter(startFilter, CachingWrapperFilter.DeletesMode.DYNAMIC);
-
-    writer.addDocument(doc);
-    reader = refreshReader(reader);
-    searcher.close();
-    searcher = newSearcher(reader, false);
-        
-    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
-    assertEquals("[query + filter] Should find a hit...", 1, docs.totalHits);
-    constantScore = new ConstantScoreQuery(filter);
-    docs = searcher.search(constantScore, 1);
-    assertEquals("[just filter] Should find a hit...", 1, docs.totalHits);
+    IndexReader oldReader2 = reader;
 
     // now delete the doc, refresh the reader, and see that it's not there
     writer.deleteDocuments(new Term("id", "1"));
@@ -282,12 +233,10 @@ public void testEnforceDeletions() throws Exception {
 
     docs = searcher.search(new MatchAllDocsQuery(), filter, 1);
     assertEquals("[query + filter] Should *not* find a hit...", 0, docs.totalHits);
+    assertEquals(missCount, filter.missCount);
 
-    missCount = filter.missCount;
     docs = searcher.search(constantScore, 1);
     assertEquals("[just filter] Should *not* find a hit...", 0, docs.totalHits);
-
-    // doesn't count as a miss
     assertEquals(missCount, filter.missCount);
 
     // NOTE: silliness to make sure JRE does not optimize
@@ -295,6 +244,7 @@ public void testEnforceDeletions() throws Exception {
     // CachingWrapperFilter's WeakHashMap from dropping the
     // entry:
     assertTrue(oldReader != null);
+    assertTrue(oldReader2 != null);
 
     searcher.close();
     reader.close();
@@ -312,4 +262,5 @@ private static IndexReader refreshReader(IndexReader reader) throws IOException
       return oldReader;
     }
   }
+
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestConstantScoreQuery.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestConstantScoreQuery.java
index 3ee0f8ef..91509649 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestConstantScoreQuery.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestConstantScoreQuery.java
@@ -131,4 +131,31 @@ public float queryNorm(float sumOfSquaredWeights) {
     }
   }
   
+  public void testConstantScoreQueryAndFilter() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random, d);
+    Document doc = new Document();
+    doc.add(newField("field", "a", StringField.TYPE_UNSTORED));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(newField("field", "b", StringField.TYPE_UNSTORED));
+    w.addDocument(doc);
+    IndexReader r = w.getReader();
+    w.close();
+
+    Filter filterB = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("field", "b"))));
+    Query query = new ConstantScoreQuery(filterB);
+
+    IndexSearcher s = new IndexSearcher(r);
+    assertEquals(1, s.search(query, filterB, 1).totalHits); // Query for field:b, Filter field:b
+
+    Filter filterA = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("field", "a"))));
+    query = new ConstantScoreQuery(filterA);
+
+    assertEquals(0, s.search(query, filterB, 1).totalHits); // Query field:b, Filter field:a
+
+    r.close();
+    d.close();
+  }
+  
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestDocIdSet.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestDocIdSet.java
index 86dacf3e..a02dfc7b 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestDocIdSet.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestDocIdSet.java
@@ -30,6 +30,7 @@
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.LuceneTestCase;
 
 public class TestDocIdSet extends LuceneTestCase {
@@ -114,7 +115,7 @@ public void testNullDocIdSet() throws Exception {
     // Now search w/ a Filter which returns a null DocIdSet
     Filter f = new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
         return null;
       }
     };
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFieldCacheRewriteMethod.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFieldCacheRewriteMethod.java
index b261cdea..aa724570 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFieldCacheRewriteMethod.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFieldCacheRewriteMethod.java
@@ -30,10 +30,10 @@
   /** Test fieldcache rewrite against filter rewrite */
   @Override
   protected void assertSame(String regexp) throws IOException {   
-    RegexpQuery fieldCache = new RegexpQuery(new Term("field", regexp), RegExp.NONE);
+    RegexpQuery fieldCache = new RegexpQuery(new Term(fieldName, regexp), RegExp.NONE);
     fieldCache.setRewriteMethod(new FieldCacheRewriteMethod());
     
-    RegexpQuery filter = new RegexpQuery(new Term("field", regexp), RegExp.NONE);
+    RegexpQuery filter = new RegexpQuery(new Term(fieldName, regexp), RegExp.NONE);
     filter.setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE);
     
     TopDocs fieldCacheDocs = searcher1.search(fieldCache, 25);
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java
index a68c11eb..13b12614 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java
@@ -28,6 +28,7 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.DocIdBitSet;
 import org.apache.lucene.util.LuceneTestCase;
 
@@ -82,6 +83,7 @@ public void setUp() throws Exception {
     writer.close ();
 
     searcher = newSearcher(reader);
+
     query = new TermQuery (new Term ("field", "three"));
     filter = newStaticFilterB();
   }
@@ -90,10 +92,11 @@ public void setUp() throws Exception {
   private static Filter newStaticFilterB() {
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet (AtomicReaderContext context) {
+      public DocIdSet getDocIdSet (AtomicReaderContext context, Bits acceptDocs) {
+        if (acceptDocs == null) acceptDocs = new Bits.MatchAllBits(5);
         BitSet bitset = new BitSet(5);
-        bitset.set (1);
-        bitset.set (3);
+        if (acceptDocs.get(1)) bitset.set(1);
+        if (acceptDocs.get(3)) bitset.set(3);
         return new DocIdBitSet(bitset);
       }
     };
@@ -107,9 +110,15 @@ public void tearDown() throws Exception {
     super.tearDown();
   }
 
-  public void testFilteredQuery()
-  throws Exception {
-    Query filteredquery = new FilteredQuery (query, filter);
+  public void testFilteredQuery() throws Exception {
+    // force the filter to be executed as bits
+    tFilteredQuery(true);
+    // force the filter to be executed as iterator
+    tFilteredQuery(false);
+  }
+
+  private void tFilteredQuery(final boolean useRandomAccess) throws Exception {
+    Query filteredquery = new FilteredQueryRA(query, filter, useRandomAccess);
     ScoreDoc[] hits = searcher.search (filteredquery, null, 1000).scoreDocs;
     assertEquals (1, hits.length);
     assertEquals (1, hits[0].doc);
@@ -119,18 +128,18 @@ public void testFilteredQuery()
     assertEquals (1, hits.length);
     assertEquals (1, hits[0].doc);
 
-    filteredquery = new FilteredQuery (new TermQuery (new Term ("field", "one")), filter);
+    filteredquery = new FilteredQueryRA(new TermQuery (new Term ("field", "one")), filter, useRandomAccess);
     hits = searcher.search (filteredquery, null, 1000).scoreDocs;
     assertEquals (2, hits.length);
     QueryUtils.check(random, filteredquery,searcher);
 
-    filteredquery = new FilteredQuery (new TermQuery (new Term ("field", "x")), filter);
+    filteredquery = new FilteredQueryRA(new TermQuery (new Term ("field", "x")), filter, useRandomAccess);
     hits = searcher.search (filteredquery, null, 1000).scoreDocs;
     assertEquals (1, hits.length);
     assertEquals (3, hits[0].doc);
     QueryUtils.check(random, filteredquery,searcher);
 
-    filteredquery = new FilteredQuery (new TermQuery (new Term ("field", "y")), filter);
+    filteredquery = new FilteredQueryRA(new TermQuery (new Term ("field", "y")), filter, useRandomAccess);
     hits = searcher.search (filteredquery, null, 1000).scoreDocs;
     assertEquals (0, hits.length);
     QueryUtils.check(random, filteredquery,searcher);
@@ -147,7 +156,7 @@ public void testFilteredQuery()
     
     BooleanQuery bq2 = new BooleanQuery();
     tq = new TermQuery (new Term ("field", "one"));
-    filteredquery = new FilteredQuery(tq, f);
+    filteredquery = new FilteredQueryRA(tq, f, useRandomAccess);
     filteredquery.setBoost(boost);
     bq2.add(filteredquery, Occur.MUST);
     bq2.add(new TermQuery (new Term ("field", "five")), Occur.MUST);
@@ -161,7 +170,8 @@ public void testFilteredQuery()
   private static Filter newStaticFilterA() {
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet (AtomicReaderContext context) {
+      public DocIdSet getDocIdSet (AtomicReaderContext context, Bits acceptDocs) {
+        assertNull("acceptDocs should be null, as we have an index without deletions", acceptDocs);
         BitSet bitset = new BitSet(5);
         bitset.set(0, 5);
         return new DocIdBitSet(bitset);
@@ -187,40 +197,112 @@ public void assertScoreEquals(Query q1, Query q2) throws Exception {
    * This tests FilteredQuery's rewrite correctness
    */
   public void testRangeQuery() throws Exception {
+    // force the filter to be executed as bits
+    tRangeQuery(true);
+    tRangeQuery(false);
+  }
+
+  private void tRangeQuery(final boolean useRandomAccess) throws Exception {
     TermRangeQuery rq = TermRangeQuery.newStringRange(
         "sorter", "b", "d", true, true);
 
-    Query filteredquery = new FilteredQuery(rq, filter);
+    Query filteredquery = new FilteredQueryRA(rq, filter, useRandomAccess);
     ScoreDoc[] hits = searcher.search(filteredquery, null, 1000).scoreDocs;
     assertEquals(2, hits.length);
     QueryUtils.check(random, filteredquery,searcher);
   }
 
-  public void testBoolean() throws Exception {
+  public void testBooleanMUST() throws Exception {
+    // force the filter to be executed as bits
+    tBooleanMUST(true);
+    // force the filter to be executed as iterator
+    tBooleanMUST(false);
+  }
+
+  private void tBooleanMUST(final boolean useRandomAccess) throws Exception {
     BooleanQuery bq = new BooleanQuery();
-    Query query = new FilteredQuery(new MatchAllDocsQuery(),
-        new SingleDocTestFilter(0));
+    Query query = new FilteredQueryRA(new MatchAllDocsQuery(), new SingleDocTestFilter(0), useRandomAccess);
     bq.add(query, BooleanClause.Occur.MUST);
-    query = new FilteredQuery(new MatchAllDocsQuery(),
-        new SingleDocTestFilter(1));
+    query = new FilteredQueryRA(new MatchAllDocsQuery(), new SingleDocTestFilter(1), useRandomAccess);
     bq.add(query, BooleanClause.Occur.MUST);
     ScoreDoc[] hits = searcher.search(bq, null, 1000).scoreDocs;
     assertEquals(0, hits.length);
     QueryUtils.check(random, query,searcher);    
   }
 
+  public void testBooleanSHOULD() throws Exception {
+    // force the filter to be executed as bits
+    tBooleanSHOULD(true);
+    // force the filter to be executed as iterator
+    tBooleanSHOULD(false);
+  }
+
+  private void tBooleanSHOULD(final boolean useRandomAccess) throws Exception {
+    BooleanQuery bq = new BooleanQuery();
+    Query query = new FilteredQueryRA(new MatchAllDocsQuery(), new SingleDocTestFilter(0), useRandomAccess);
+    bq.add(query, BooleanClause.Occur.SHOULD);
+    query = new FilteredQueryRA(new MatchAllDocsQuery(), new SingleDocTestFilter(1), useRandomAccess);
+    bq.add(query, BooleanClause.Occur.SHOULD);
+    ScoreDoc[] hits = searcher.search(bq, null, 1000).scoreDocs;
+    assertEquals(2, hits.length);
+    QueryUtils.check(random, query,searcher);    
+  }
+
   // Make sure BooleanQuery, which does out-of-order
   // scoring, inside FilteredQuery, works
   public void testBoolean2() throws Exception {
+    // force the filter to be executed as bits
+    tBoolean2(true);
+    // force the filter to be executed as iterator
+    tBoolean2(false);
+  }
+
+  private void tBoolean2(final boolean useRandomAccess) throws Exception {
     BooleanQuery bq = new BooleanQuery();
-    Query query = new FilteredQuery(bq,
-        new SingleDocTestFilter(0));
+    Query query = new FilteredQueryRA(bq, new SingleDocTestFilter(0), useRandomAccess);
     bq.add(new TermQuery(new Term("field", "one")), BooleanClause.Occur.SHOULD);
     bq.add(new TermQuery(new Term("field", "two")), BooleanClause.Occur.SHOULD);
     ScoreDoc[] hits = searcher.search(query, 1000).scoreDocs;
     assertEquals(1, hits.length);
     QueryUtils.check(random, query, searcher);    
   }
+  
+  public void testChainedFilters() throws Exception {
+    // force the filter to be executed as bits
+    tChainedFilters(true);
+    // force the filter to be executed as iterator
+    tChainedFilters(false);
+  }
+  
+  private void tChainedFilters(final boolean useRandomAccess) throws Exception {
+    Query query = new TestFilteredQuery.FilteredQueryRA(new TestFilteredQuery.FilteredQueryRA(
+      new MatchAllDocsQuery(), new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("field", "three")))), useRandomAccess),
+      new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("field", "four")))), useRandomAccess);
+    ScoreDoc[] hits = searcher.search(query, 10).scoreDocs;
+    assertEquals(2, hits.length);
+    QueryUtils.check(random, query, searcher);    
+
+    // one more:
+    query = new TestFilteredQuery.FilteredQueryRA(query,
+      new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("field", "five")))), useRandomAccess);
+    hits = searcher.search(query, 10).scoreDocs;
+    assertEquals(1, hits.length);
+    QueryUtils.check(random, query, searcher);    
+  }
+
+  public static final class FilteredQueryRA extends FilteredQuery {
+    private final boolean useRandomAccess;
+  
+    public FilteredQueryRA(Query q, Filter f, boolean useRandomAccess) {
+      super(q,f);
+      this.useRandomAccess = useRandomAccess;
+    }
+    
+    @Override
+    protected boolean useRandomAccess(Bits bits, int firstFilterDoc) {
+      return useRandomAccess;
+    }
+  }
 }
 
 
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java
index 5552408d..d4e1f14d 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java
@@ -30,6 +30,7 @@
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.FixedBitSet;
 
 
@@ -95,7 +96,8 @@ public SimpleDocIdSetFilter(int[] docs) {
     }
 
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context) {
+    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+      assertNull("acceptDocs should be null, as we have an index without deletions", acceptDocs);
       assert context.isAtomic;
       final FixedBitSet set = new FixedBitSet(context.reader.maxDoc());
       int docBase = context.docBase;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
index d503dad0..e04a4f5e 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
@@ -182,13 +182,13 @@ public void testRange_2bit() throws Exception {
   public void testInverseRange() throws Exception {
     AtomicReaderContext context = (AtomicReaderContext) new SlowMultiReaderWrapper(searcher.getIndexReader()).getTopReaderContext();
     NumericRangeFilter<Integer> f = NumericRangeFilter.newIntRange("field8", 8, 1000, -1000, true, true);
-    assertSame("A inverse range should return the EMPTY_DOCIDSET instance", DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context));
+    assertSame("A inverse range should return the EMPTY_DOCIDSET instance", DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
     f = NumericRangeFilter.newIntRange("field8", 8, Integer.MAX_VALUE, null, false, false);
     assertSame("A exclusive range starting with Integer.MAX_VALUE should return the EMPTY_DOCIDSET instance",
-               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context));
+               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
     f = NumericRangeFilter.newIntRange("field8", 8, null, Integer.MIN_VALUE, false, false);
     assertSame("A exclusive range ending with Integer.MIN_VALUE should return the EMPTY_DOCIDSET instance",
-               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context));
+               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
   }
   
   @Test
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
index 65ff32e7..8d8abc55 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
@@ -188,13 +188,13 @@ public void testInverseRange() throws Exception {
     AtomicReaderContext context = (AtomicReaderContext) new SlowMultiReaderWrapper(searcher.getIndexReader()).getTopReaderContext();
     NumericRangeFilter<Long> f = NumericRangeFilter.newLongRange("field8", 8, 1000L, -1000L, true, true);
     assertSame("A inverse range should return the EMPTY_DOCIDSET instance", DocIdSet.EMPTY_DOCIDSET,
-        f.getDocIdSet(context));
+        f.getDocIdSet(context, context.reader.getLiveDocs()));
     f = NumericRangeFilter.newLongRange("field8", 8, Long.MAX_VALUE, null, false, false);
     assertSame("A exclusive range starting with Long.MAX_VALUE should return the EMPTY_DOCIDSET instance",
-               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context));
+               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
     f = NumericRangeFilter.newLongRange("field8", 8, null, Long.MIN_VALUE, false, false);
     assertSame("A exclusive range ending with Long.MIN_VALUE should return the EMPTY_DOCIDSET instance",
-               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context));
+               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
   }
   
   @Test
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
index 2f74d8da..a71559ac 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
@@ -55,16 +55,18 @@
   protected IndexSearcher searcher2;
   private IndexReader reader;
   private Directory dir;
+  protected String fieldName;
   
   @Override
   public void setUp() throws Exception {
     super.setUp();
     dir = newDirectory();
+    fieldName = random.nextBoolean() ? "field" : ""; // sometimes use an empty string as field name
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, 
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))
         .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));
     Document doc = new Document();
-    Field field = newField("field", "", StringField.TYPE_UNSTORED);
+    Field field = newField(fieldName, "", StringField.TYPE_UNSTORED);
     doc.add(field);
     List<String> terms = new ArrayList<String>();
     int num = atLeast(200);
@@ -141,7 +143,7 @@ public String toString(String field) {
   public void testRegexps() throws Exception {
     // we generate aweful regexps: good for testing.
     // but for preflex codec, the test can be very slow, so use less iterations.
-    int num = CodecProvider.getDefault().getFieldCodec("field").equals("PreFlex") ? 100 * RANDOM_MULTIPLIER : atLeast(1000);
+    int num = CodecProvider.getDefault().getFieldCodec(fieldName).equals("PreFlex") ? 100 * RANDOM_MULTIPLIER : atLeast(1000);
     for (int i = 0; i < num; i++) {
       String reg = AutomatonTestUtil.randomRegexp(random);
       if (VERBOSE) {
@@ -155,8 +157,8 @@ public void testRegexps() throws Exception {
    * simple regexpquery implementation.
    */
   protected void assertSame(String regexp) throws IOException {   
-    RegexpQuery smart = new RegexpQuery(new Term("field", regexp), RegExp.NONE);
-    DumbRegexpQuery dumb = new DumbRegexpQuery(new Term("field", regexp), RegExp.NONE);
+    RegexpQuery smart = new RegexpQuery(new Term(fieldName, regexp), RegExp.NONE);
+    DumbRegexpQuery dumb = new DumbRegexpQuery(new Term(fieldName, regexp), RegExp.NONE);
    
     TopDocs smartDocs = searcher1.search(smart, 25);
     TopDocs dumbDocs = searcher2.search(dumb, 25);
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestScorerPerf.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestScorerPerf.java
index 61fe43fb..b46711ba 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestScorerPerf.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestScorerPerf.java
@@ -1,5 +1,6 @@
 package org.apache.lucene.search;
 
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.DocIdBitSet;
 import org.apache.lucene.util.LuceneTestCase;
 
@@ -141,7 +142,8 @@ BitSet addClause(BooleanQuery bq, BitSet result) {
     final BitSet rnd = sets[random.nextInt(sets.length)];
     Query q = new ConstantScoreQuery(new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) {
+      public DocIdSet getDocIdSet (AtomicReaderContext context, Bits acceptDocs) {
+        assertNull("acceptDocs should be null, as we have an index without deletions", acceptDocs);
         return new DocIdBitSet(rnd);
       }
     });
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestSort.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestSort.java
index 6b93e8d7..4b82e1fd 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestSort.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestSort.java
@@ -53,6 +53,7 @@
 import org.apache.lucene.search.cache.ShortValuesCreator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.DocIdBitSet;
 import org.apache.lucene.util.LuceneTestCase;
@@ -730,7 +731,8 @@ public void testTopDocsScores() throws Exception {
     // a filter that only allows through the first hit
     Filter filt = new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet (AtomicReaderContext context, Bits acceptDocs) {
+        assertNull("acceptDocs should be null, as we have no deletions", acceptDocs);
         BitSet bs = new BitSet(context.reader.maxDoc());
         bs.set(0, context.reader.maxDoc());
         bs.set(docs1.scoreDocs[0].doc);
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestSpanQueryFilter.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestSpanQueryFilter.java
index f4e53d9d..130cb711 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestSpanQueryFilter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestSpanQueryFilter.java
@@ -49,7 +49,7 @@ public void testFilterWorks() throws Exception {
     int subIndex = ReaderUtil.subIndex(number, leaves); // find the reader with this document in it
     SpanTermQuery query = new SpanTermQuery(new Term("field", English.intToEnglish(number).trim()));
     SpanQueryFilter filter = new SpanQueryFilter(query);
-    SpanFilterResult result = filter.bitSpans(leaves[subIndex]);
+    SpanFilterResult result = filter.bitSpans(leaves[subIndex], leaves[subIndex].reader.getLiveDocs());
     DocIdSet docIdSet = result.getDocIdSet();
     assertTrue("docIdSet is null and it shouldn't be", docIdSet != null);
     assertContainsDocId("docIdSet doesn't contain docId 10", docIdSet, number - leaves[subIndex].docBase);
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestTopDocsMerge.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestTopDocsMerge.java
index eeb5fdc1..ad3c420c 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestTopDocsMerge.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/TestTopDocsMerge.java
@@ -44,11 +44,11 @@ public ShardSearcher(IndexReader.AtomicReaderContext ctx, IndexReader.ReaderCont
     }
 
     public void search(Weight weight, Collector collector) throws IOException {
-      search(ctx, weight, null, collector);
+      search(ctx, weight, collector);
     }
 
     public TopDocs search(Weight weight, int topN) throws IOException {
-      return search(ctx, weight, null, null, topN);
+      return search(ctx, weight, null, topN);
     }
 
     @Override
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/similarities/SpoofIndexSearcher.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/similarities/SpoofIndexSearcher.java
index ab0ea184..cafe2ee1 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/similarities/SpoofIndexSearcher.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/search/similarities/SpoofIndexSearcher.java
@@ -203,11 +203,17 @@ public int getDocCount() throws IOException {
     
     // ------------------------ Not implemented methods ------------------------
 
+    
     @Override
     public TermsEnum iterator() throws IOException {
       return null;
     }
 
+    @Override
+    public long getUniqueTermCount() throws IOException {
+      return -1;
+    }
+
     @Override
     public Comparator<BytesRef> getComparator() throws IOException {
       return null;
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/store/TestCopyBytes.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/store/TestCopyBytes.java
index 156a58f1..30ff5e43 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/store/TestCopyBytes.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/store/TestCopyBytes.java
@@ -17,6 +17,8 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
@@ -104,4 +106,68 @@ public void testCopyBytes() throws Exception {
       dir.close();
     }
   }
+  
+  // LUCENE-3541
+  public void testCopyBytesWithThreads() throws Exception {
+    int datalen = _TestUtil.nextInt(random, 101, 10000);
+    byte data[] = new byte[datalen];
+    random.nextBytes(data);
+    
+    Directory d = newDirectory();
+    IndexOutput output = d.createOutput("data", IOContext.DEFAULT);
+    output.writeBytes(data, 0, datalen);
+    output.close();
+    
+    IndexInput input = d.openInput("data", IOContext.DEFAULT);
+    IndexOutput outputHeader = d.createOutput("header", IOContext.DEFAULT);
+    // copy our 100-byte header
+    input.copyBytes(outputHeader, 100);
+    outputHeader.close();
+    
+    // now make N copies of the remaining bytes
+    CopyThread copies[] = new CopyThread[10];
+    for (int i = 0; i < copies.length; i++) {
+      copies[i] = new CopyThread((IndexInput) input.clone(), d.createOutput("copy" + i, IOContext.DEFAULT));
+    }
+    
+    for (int i = 0; i < copies.length; i++) {
+      copies[i].start();
+    }
+    
+    for (int i = 0; i < copies.length; i++) {
+      copies[i].join();
+    }
+    
+    for (int i = 0; i < copies.length; i++) {
+      IndexInput copiedData = d.openInput("copy" + i, IOContext.DEFAULT);
+      byte[] dataCopy = new byte[datalen];
+      System.arraycopy(data, 0, dataCopy, 0, 100); // copy the header for easy testing
+      copiedData.readBytes(dataCopy, 100, datalen-100);
+      assertArrayEquals(data, dataCopy);
+      copiedData.close();
+    }
+    input.close();
+    d.close();
+    
+  }
+  
+  static class CopyThread extends Thread {
+    final IndexInput src;
+    final IndexOutput dst;
+    
+    CopyThread(IndexInput src, IndexOutput dst) {
+      this.src = src;
+      this.dst = dst;
+    }
+
+    @Override
+    public void run() {
+      try {
+        src.copyBytes(dst, src.length()-100);
+        dst.close();
+      } catch (IOException ex) {
+        throw new RuntimeException(ex);
+      }
+    }
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/util/TestCharsRef.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/util/TestCharsRef.java
index 155890ca..f9d4faca 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/util/TestCharsRef.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/util/TestCharsRef.java
@@ -39,4 +39,33 @@ public void testUTF16InUTF8Order() {
       assertEquals(utf8[i].utf8ToString(), utf16[i].toString());
     }
   }
+  
+  public void testAppend() {
+    CharsRef ref = new CharsRef();
+    StringBuilder builder = new StringBuilder();
+    int numStrings = atLeast(10);
+    for (int i = 0; i < numStrings; i++) {
+      char[] charArray = _TestUtil.randomRealisticUnicodeString(random, 1, 100).toCharArray();
+      int offset = random.nextInt(charArray.length);
+      int length = charArray.length - offset;
+      builder.append(charArray, offset, length);
+      ref.append(charArray, offset, length);  
+    }
+    
+    assertEquals(builder.toString(), ref.toString());
+  }
+  
+  public void testCopy() {
+    int numIters = atLeast(10);
+    for (int i = 0; i < numIters; i++) {
+      CharsRef ref = new CharsRef();
+      char[] charArray = _TestUtil.randomRealisticUnicodeString(random, 1, 100).toCharArray();
+      int offset = random.nextInt(charArray.length);
+      int length = charArray.length - offset;
+      String str = new String(charArray, offset, length);
+      ref.copy(charArray, offset, length);
+      assertEquals(str, ref.toString());  
+    }
+    
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/util/TestPagedBytes.java b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/util/TestPagedBytes.java
index e69de29b..5205300b 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/util/TestPagedBytes.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test/org/apache/lucene/util/TestPagedBytes.java
@@ -0,0 +1,64 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.util;
+
+import java.util.Arrays;
+
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.DataOutput;
+
+public class TestPagedBytes extends LuceneTestCase {
+
+  public void testDataInputOutput() throws Exception {
+    for(int iter=0;iter<5*RANDOM_MULTIPLIER;iter++) {
+      final PagedBytes p = new PagedBytes(_TestUtil.nextInt(random, 1, 20));
+      final DataOutput out = p.getDataOutput();
+      final int numBytes = random.nextInt(10000000);
+
+      final byte[] answer = new byte[numBytes];
+      random.nextBytes(answer);
+      int written = 0;
+      while(written < numBytes) {
+        if (random.nextInt(10) == 7) {
+          out.writeByte(answer[written++]);
+        } else {
+          int chunk = Math.max(random.nextInt(1000), numBytes - written);
+          out.writeBytes(answer, written, chunk);
+          written += chunk;
+        }
+      }
+
+      p.freeze(random.nextBoolean());
+
+      final DataInput in = p.getDataInput();
+
+      final byte[] verify = new byte[numBytes];
+      int read = 0;
+      while(read < numBytes) {
+        if (random.nextInt(10) == 7) {
+          verify[read++] = in.readByte();
+        } else {
+          int chunk = Math.max(random.nextInt(1000), numBytes - read);
+          in.readBytes(verify, read, chunk);
+          read += chunk;
+        }
+      }
+      assertTrue(Arrays.equals(answer, verify));
+    }
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
index 4c5d3a87..425fe7b6 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
@@ -55,7 +55,7 @@
 
 // TODO
 //   - mix in optimize, addIndexes
-//   - randomoly mix in non-congruent docs
+//   - randomly mix in non-congruent docs
 
 /** Utility class that spawns multiple indexing and
  *  searching threads. */
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java
index 34a79ef9..91e07a19 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java
@@ -199,6 +199,11 @@ private int compareToLastTerm(int fieldNumber, BytesRef term) {
       if (ch1 != ch2)
         return ch1-ch2;
     }
+    if (utf16Result1.length == 0 && lastFieldNumber == -1) {
+      // If there is a field named "" (empty string) with a term text of "" (empty string) then we
+      // will get 0 on this comparison, yet, it's "OK". 
+      return -1;
+    }
     return utf16Result1.length - utf16Result2.length;
   }
 
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/search/AssertingIndexSearcher.java b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/search/AssertingIndexSearcher.java
index 60210c60..b945a9b3 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/search/AssertingIndexSearcher.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/search/AssertingIndexSearcher.java
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import java.util.Random;
 import java.util.concurrent.ExecutorService;
 import java.io.IOException;
 
@@ -31,20 +32,25 @@
  * TODO: Extend this by more checks, that's just a start.
  */
 public class AssertingIndexSearcher extends IndexSearcher {
-  public  AssertingIndexSearcher(IndexReader r) {
+  final Random random;
+  public  AssertingIndexSearcher(Random random, IndexReader r) {
     super(r);
+    this.random = new Random(random.nextLong());
   }
   
-  public  AssertingIndexSearcher(ReaderContext context) {
+  public  AssertingIndexSearcher(Random random, ReaderContext context) {
     super(context);
+    this.random = new Random(random.nextLong());
   }
   
-  public  AssertingIndexSearcher(IndexReader r, ExecutorService ex) {
+  public  AssertingIndexSearcher(Random random, IndexReader r, ExecutorService ex) {
     super(r, ex);
+    this.random = new Random(random.nextLong());
   }
   
-  public  AssertingIndexSearcher(ReaderContext context, ExecutorService ex) {
+  public  AssertingIndexSearcher(Random random, ReaderContext context, ExecutorService ex) {
     super(context, ex);
+    this.random = new Random(random.nextLong());
   }
   
   /** Ensures, that the returned {@code Weight} is not normalized again, which may produce wrong scores. */
@@ -84,4 +90,16 @@ public boolean scoresDocsOutOfOrder() {
       }
     };
   }
+
+  @Override
+  protected Query wrapFilter(Query query, Filter filter) {
+    if (random.nextBoolean())
+      return super.wrapFilter(query, filter);
+    return (filter == null) ? query : new FilteredQuery(query, filter) {
+      @Override
+      protected boolean useRandomAccess(Bits bits, int firstFilterDoc) {
+        return random.nextBoolean();
+      }
+    };
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/search/CachingWrapperFilterHelper.java b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/search/CachingWrapperFilterHelper.java
index 41872acd..105e72bf 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/search/CachingWrapperFilterHelper.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/search/CachingWrapperFilterHelper.java
@@ -22,6 +22,7 @@
 import junit.framework.Assert;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.util.Bits;
 
 /**
  * A unit test helper class to test when the filter is getting cached and when it is not.
@@ -42,10 +43,10 @@ public void setShouldHaveCache(boolean shouldHaveCache) {
   }
   
   @Override
-  public synchronized DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+  public synchronized DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
 
     final int saveMissCount = missCount;
-    DocIdSet docIdSet = super.getDocIdSet(context);
+    DocIdSet docIdSet = super.getDocIdSet(context, acceptDocs);
 
     if (shouldHaveCache) {
       Assert.assertEquals("Cache should have data ", saveMissCount, missCount);
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java
index 8b23109b..c0d64230 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/store/MockDirectoryWrapper.java
@@ -31,6 +31,7 @@
 import java.util.Map;
 import java.util.Random;
 import java.util.Set;
+import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.codecs.CodecProvider;
@@ -75,6 +76,8 @@
   private ThrottledIndexOutput throttledOutput;
   private Throttling throttling = Throttling.SOMETIMES;
 
+  final AtomicInteger inputCloneCount = new AtomicInteger();
+
   // use this for tracking files for crash.
   // additionally: provides debugging information in case you leave one open
   private Map<Closeable,Exception> openFileHandles = Collections.synchronizedMap(new IdentityHashMap<Closeable,Exception>());
@@ -118,6 +121,10 @@ public MockDirectoryWrapper(Random random, Directory delegate) {
     init();
   }
 
+  public int getInputCloneCount() {
+    return inputCloneCount.get();
+  }
+
   public void setTrackDiskUsage(boolean v) {
     trackDiskUsage = v;
   }
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/store/MockIndexInputWrapper.java b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/store/MockIndexInputWrapper.java
index f04e4e37..09c0dcc9 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/store/MockIndexInputWrapper.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/store/MockIndexInputWrapper.java
@@ -30,6 +30,7 @@
   final String name;
   private IndexInput delegate;
   private boolean isClone;
+  private boolean closed;
 
   /** Construct an empty output buffer. */
   public MockIndexInputWrapper(MockDirectoryWrapper dir, String name, IndexInput delegate) {
@@ -45,6 +46,7 @@ public void close() throws IOException {
       // after fixing TestTransactions
       // dir.maybeThrowDeterministicException();
     } finally {
+      closed = true;
       delegate.close();
       // Pending resolution on LUCENE-686 we may want to
       // remove the conditional check so we also track that
@@ -55,8 +57,16 @@ public void close() throws IOException {
     }
   }
 
+  private void ensureOpen() {
+    if (closed) {
+      throw new RuntimeException("Abusing closed IndexInput!");
+    }
+  }
+
   @Override
   public Object clone() {
+    ensureOpen();
+    dir.inputCloneCount.incrementAndGet();
     IndexInput iiclone = (IndexInput) delegate.clone();
     MockIndexInputWrapper clone = new MockIndexInputWrapper(dir, name, iiclone);
     clone.isClone = true;
@@ -79,72 +89,86 @@ public Object clone() {
 
   @Override
   public long getFilePointer() {
+    ensureOpen();
     return delegate.getFilePointer();
   }
 
   @Override
   public void seek(long pos) throws IOException {
+    ensureOpen();
     delegate.seek(pos);
   }
 
   @Override
   public long length() {
+    ensureOpen();
     return delegate.length();
   }
 
   @Override
   public byte readByte() throws IOException {
+    ensureOpen();
     return delegate.readByte();
   }
 
   @Override
   public void readBytes(byte[] b, int offset, int len) throws IOException {
+    ensureOpen();
     delegate.readBytes(b, offset, len);
   }
 
   @Override
   public void copyBytes(IndexOutput out, long numBytes) throws IOException {
+    ensureOpen();
     delegate.copyBytes(out, numBytes);
   }
 
   @Override
   public void readBytes(byte[] b, int offset, int len, boolean useBuffer)
       throws IOException {
+    ensureOpen();
     delegate.readBytes(b, offset, len, useBuffer);
   }
 
   @Override
   public short readShort() throws IOException {
+    ensureOpen();
     return delegate.readShort();
   }
 
   @Override
   public int readInt() throws IOException {
+    ensureOpen();
     return delegate.readInt();
   }
 
   @Override
   public long readLong() throws IOException {
+    ensureOpen();
     return delegate.readLong();
   }
 
   @Override
   public String readString() throws IOException {
+    ensureOpen();
     return delegate.readString();
   }
 
   @Override
   public Map<String,String> readStringStringMap() throws IOException {
+    ensureOpen();
     return delegate.readStringStringMap();
   }
 
   @Override
   public int readVInt() throws IOException {
+    ensureOpen();
     return delegate.readVInt();
   }
 
   @Override
   public long readVLong() throws IOException {
+    ensureOpen();
     return delegate.readVLong();
   }
 
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitResultFormatter.java b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitResultFormatter.java
index a03f780d..c67b9bc4 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitResultFormatter.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/util/LuceneJUnitResultFormatter.java
@@ -66,7 +66,8 @@
   private static final org.apache.lucene.store.Lock lock;
 
   static {
-    File lockDir = new File(System.getProperty("java.io.tmpdir"),
+    File lockDir = new File(
+        System.getProperty("tests.lockdir", System.getProperty("java.io.tmpdir")),
         "lucene_junit_lock");
     lockDir.mkdirs();
     if (!lockDir.exists()) {
diff --git a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java
index f9df8e5f..826e1659 100644
--- a/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/dev/branches/solrcloud/lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java
@@ -400,6 +400,11 @@ public static void beforeClassLuceneTestCaseJ4() {
     TimeZone.setDefault(timeZone);
     similarityProvider = new RandomSimilarityProvider(random);
     testsFailed = false;
+    
+    // verify assertions are enabled (do last, for smooth cleanup)
+    if (!Boolean.parseBoolean(System.getProperty("tests.asserts.gracious", "false"))) {
+      assertTrue("assertions are not enabled!", assertionsEnabled());
+    }
   }
 
   @AfterClass
@@ -1300,7 +1305,7 @@ public static IndexSearcher newSearcher(IndexReader r, boolean maybeWrap) throws
       if (maybeWrap && rarely()) {
         r = new SlowMultiReaderWrapper(r);
       }
-      IndexSearcher ret = random.nextBoolean() ? new AssertingIndexSearcher(r) : new AssertingIndexSearcher(r.getTopReaderContext());
+      IndexSearcher ret = random.nextBoolean() ? new AssertingIndexSearcher(random, r) : new AssertingIndexSearcher(random, r.getTopReaderContext());
       ret.setSimilarityProvider(similarityProvider);
       return ret;
     } else {
@@ -1312,13 +1317,13 @@ public static IndexSearcher newSearcher(IndexReader r, boolean maybeWrap) throws
         System.out.println("NOTE: newSearcher using ExecutorService with " + threads + " threads");
       }
       IndexSearcher ret = random.nextBoolean() ? 
-        new AssertingIndexSearcher(r, ex) {
+        new AssertingIndexSearcher(random, r, ex) {
           @Override
           public void close() throws IOException {
             super.close();
             shutdownExecutorService(ex);
           }
-        } : new AssertingIndexSearcher(r.getTopReaderContext(), ex) {
+        } : new AssertingIndexSearcher(random, r.getTopReaderContext(), ex) {
           @Override
           public void close() throws IOException {
             super.close();
@@ -1442,4 +1447,15 @@ public static IOContext newIOContext(Random random) {
 
   @Ignore("just a hack")
   public final void alwaysIgnoredTestMethod() {}
+  
+  /** check if assertions are enabled */
+  private static boolean assertionsEnabled() {
+    try {
+      assert Boolean.FALSE.booleanValue();
+      return false; // should never get here
+    } catch (AssertionError e) {
+      return true;
+    }
+  }
+  
 }
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
index 7eafcd2d..aa7f25a8 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
@@ -37,7 +37,7 @@
   /**
    * File containing default CJK stopwords.
    * <p/>
-   * Currently it concains some common English words that are not usually
+   * Currently it contains some common English words that are not usually
    * useful for searching and some double-byte interpunctions.
    */
   public final static String DEFAULT_STOPWORD_FILE = "stopwords.txt";
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java
index 4e595f08..df3b9cff 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java
@@ -24,20 +24,17 @@
 import java.util.Locale;
 import java.util.Set;
 
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Version;
 
 /**
- * Base class for decomposition token filters. <a name="version"/>
+ * Base class for decomposition token filters.
  * <p>
  * You must specify the required {@link Version} compatibility when creating
  * CompoundWordTokenFilterBase:
@@ -46,6 +43,13 @@
  * supplementary characters in strings and char arrays provided as compound word
  * dictionaries.
  * </ul>
+ * <p>If you pass in a {@link org.apache.lucene.analysis.util.CharArraySet} as dictionary,
+ * it should be case-insensitive unless it contains only lowercased entries and you
+ * have {@link org.apache.lucene.analysis.core.LowerCaseFilter} before this filter in your analysis chain.
+ * For optional performance (as this filter does lots of lookups to the dictionary,
+ * you should use the latter analysis chain/CharArraySet). Be aware: If you supply arbitrary
+ * {@link Set Sets} to the ctors, they will be automatically
+ * transformed to case-insensitive!
  */
 public abstract class CompoundWordTokenFilterBase extends TokenFilter {
   /**
@@ -64,37 +68,22 @@
   public static final int DEFAULT_MAX_SUBWORD_SIZE = 15;
   
   protected final CharArraySet dictionary;
-  protected final LinkedList<Token> tokens;
+  protected final LinkedList<CompoundToken> tokens;
   protected final int minWordSize;
   protected final int minSubwordSize;
   protected final int maxSubwordSize;
   protected final boolean onlyLongestMatch;
   
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-  private final FlagsAttribute flagsAtt = addAttribute(FlagsAttribute.class);
+  protected final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  protected final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
   private final PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
-  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-  private final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);
-  
-  private final Token wrapper = new Token();
 
-  protected CompoundWordTokenFilterBase(Version matchVersion, TokenStream input, String[] dictionary, int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {
-    this(matchVersion, input,makeDictionary(dictionary),minWordSize,minSubwordSize,maxSubwordSize, onlyLongestMatch);
-  }
-  
-  protected CompoundWordTokenFilterBase(Version matchVersion, TokenStream input, String[] dictionary, boolean onlyLongestMatch) {
-    this(matchVersion, input,makeDictionary(dictionary),DEFAULT_MIN_WORD_SIZE,DEFAULT_MIN_SUBWORD_SIZE,DEFAULT_MAX_SUBWORD_SIZE, onlyLongestMatch);
-  }
+  private AttributeSource.State current;
 
   protected CompoundWordTokenFilterBase(Version matchVersion, TokenStream input, Set<?> dictionary, boolean onlyLongestMatch) {
     this(matchVersion, input,dictionary,DEFAULT_MIN_WORD_SIZE,DEFAULT_MIN_SUBWORD_SIZE,DEFAULT_MAX_SUBWORD_SIZE, onlyLongestMatch);
   }
 
-  protected CompoundWordTokenFilterBase(Version matchVersion, TokenStream input, String[] dictionary) {
-    this(matchVersion, input,makeDictionary(dictionary),DEFAULT_MIN_WORD_SIZE,DEFAULT_MIN_SUBWORD_SIZE,DEFAULT_MAX_SUBWORD_SIZE, false);
-  }
-
   protected CompoundWordTokenFilterBase(Version matchVersion, TokenStream input, Set<?> dictionary) {
     this(matchVersion, input,dictionary,DEFAULT_MIN_WORD_SIZE,DEFAULT_MIN_SUBWORD_SIZE,DEFAULT_MAX_SUBWORD_SIZE, false);
   }
@@ -102,7 +91,7 @@ protected CompoundWordTokenFilterBase(Version matchVersion, TokenStream input, S
   protected CompoundWordTokenFilterBase(Version matchVersion, TokenStream input, Set<?> dictionary, int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {
     super(input);
     
-    this.tokens=new LinkedList<Token>();
+    this.tokens=new LinkedList<CompoundToken>();
     this.minWordSize=minWordSize;
     this.minSubwordSize=minSubwordSize;
     this.maxSubwordSize=maxSubwordSize;
@@ -111,113 +100,68 @@ protected CompoundWordTokenFilterBase(Version matchVersion, TokenStream input, S
     if (dictionary==null || dictionary instanceof CharArraySet) {
       this.dictionary = (CharArraySet) dictionary;
     } else {
-      this.dictionary = new CharArraySet(matchVersion, dictionary.size(), false);
-      addAllLowerCase(this.dictionary, dictionary);
-    }
-  }
-
-  /**
-   * Create a set of words from an array
-   * The resulting Set does case insensitive matching
-   * TODO We should look for a faster dictionary lookup approach.
-   * @param dictionary 
-   * @return {@link Set} of lowercased terms 
-   */
-  public static Set<?> makeDictionary(final String[] dictionary) {
-    return makeDictionary(Version.LUCENE_30, dictionary);
-  }
-  
-  public static Set<?> makeDictionary(final Version matchVersion, final String[] dictionary) {
-    if (dictionary == null) {
-      return null;
-    }
-    // is the below really case insensitive? 
-    CharArraySet dict = new CharArraySet(matchVersion, dictionary.length, false);
-    addAllLowerCase(dict, Arrays.asList(dictionary));
-    return dict;
+      this.dictionary = new CharArraySet(matchVersion, dictionary, true);
   }
-  
-  private void setToken(final Token token) throws IOException {
-    clearAttributes();
-    termAtt.copyBuffer(token.buffer(), 0, token.length());
-    flagsAtt.setFlags(token.getFlags());
-    typeAtt.setType(token.type());
-    offsetAtt.setOffset(token.startOffset(), token.endOffset());
-    posIncAtt.setPositionIncrement(token.getPositionIncrement());
-    payloadAtt.setPayload(token.getPayload());
   }
   
   @Override
   public final boolean incrementToken() throws IOException {
-    if (tokens.size() > 0) {
-      setToken(tokens.removeFirst());
+    if (!tokens.isEmpty()) {
+      assert current != null;
+      CompoundToken token = tokens.removeFirst();
+      restoreState(current); // keep all other attributes untouched
+      termAtt.setEmpty().append(token.txt);
+      offsetAtt.setOffset(token.startOffset, token.endOffset);
+      posIncAtt.setPositionIncrement(0);
       return true;
     }
 
-    if (!input.incrementToken())
-      return false;
-    
-    wrapper.copyBuffer(termAtt.buffer(), 0, termAtt.length());
-    wrapper.setStartOffset(offsetAtt.startOffset());
-    wrapper.setEndOffset(offsetAtt.endOffset());
-    wrapper.setFlags(flagsAtt.getFlags());
-    wrapper.setType(typeAtt.type());
-    wrapper.setPositionIncrement(posIncAtt.getPositionIncrement());
-    wrapper.setPayload(payloadAtt.getPayload());
-    
-    decompose(wrapper);
-
-    if (tokens.size() > 0) {
-      setToken(tokens.removeFirst());
+    current = null; // not really needed, but for safety
+    if (input.incrementToken()) {
+      // Only words longer than minWordSize get processed
+      if (termAtt.length() >= this.minWordSize) {
+        decompose();
+        // only capture the state if we really need it for producing new tokens
+        if (!tokens.isEmpty()) {
+          current = captureState();
+        }
+      }
+      // return original token:
       return true;
     } else {
       return false;
     }
   }
   
-  protected static void addAllLowerCase(CharArraySet target, Collection<?> col) {
-    for (Object obj : col) {
-      String string = (String) obj;
-      target.add(string.toLowerCase(Locale.ENGLISH));
-    }
-  }
-  
-  protected static char[] makeLowerCaseCopy(final char[] buffer) {
-    char[] result=new char[buffer.length];
-    System.arraycopy(buffer, 0, result, 0, buffer.length);
-    
-    for (int i=0;i<buffer.length;++i) {
-       result[i]=Character.toLowerCase(buffer[i]);
-    }
-    
-    return result;
-  }
-  
-  protected final Token createToken(final int offset, final int length,
-      final Token prototype) {
-    int newStart = prototype.startOffset() + offset;
-    Token t = prototype.clone(prototype.buffer(), offset, length, newStart, newStart+length);
-    t.setPositionIncrement(0);
-    return t;
-  }
-
-  protected void decompose(final Token token) {
-    // In any case we give the original token back
-    tokens.add((Token) token.clone());
-
-    // Only words longer than minWordSize get processed
-    if (token.length() < this.minWordSize) {
-      return;
-    }
-    
-    decomposeInternal(token);
-  }
-  
-  protected abstract void decomposeInternal(final Token token);
+  /** Decomposes the current {@link #termAtt} and places {@link CompoundToken} instances in the {@link #tokens} list.
+   * The original token may not be placed in the list, as it is automatically passed through this filter.
+   */
+  protected abstract void decompose();
 
   @Override
   public void reset() throws IOException {
     super.reset();
     tokens.clear();
+    current = null;
+  }
+  
+  /**
+   * Helper class to hold decompounded token information
+   */
+  protected class CompoundToken {
+    public final CharSequence txt;
+    public final int startOffset, endOffset;
+
+    /** Construct the compound token based on a slice of the current {@link CompoundWordTokenFilterBase#termAtt}. */
+    public CompoundToken(int offset, int length) {
+      final int newStart = CompoundWordTokenFilterBase.this.offsetAtt.startOffset() + offset;
+      this.txt = CompoundWordTokenFilterBase.this.termAtt.subSequence(offset, offset + length);
+      // TODO: This ignores the original endOffset, if a CharFilter/Tokenizer/Filter removed
+      // chars from the term, offsets may not match correctly (other filters producing tokens
+      // may also have this problem):
+      this.startOffset = newStart;
+      this.endOffset = newStart + length;
+    }
+
   }
 }
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java
index 22064e34..49834e28 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java
@@ -21,7 +21,6 @@
 
 import java.util.Set;
 
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.util.Version;
@@ -32,53 +31,24 @@
  * "Donaudampfschiff" becomes Donau, dampf, schiff so that you can find
  * "Donaudampfschiff" even when you only enter "schiff". 
  *  It uses a brute-force algorithm to achieve this.
- * </p>
+ * <p>
+ * You must specify the required {@link Version} compatibility when creating
+ * CompoundWordTokenFilterBase:
+ * <ul>
+ * <li>As of 3.1, CompoundWordTokenFilterBase correctly handles Unicode 4.0
+ * supplementary characters in strings and char arrays provided as compound word
+ * dictionaries.
+ * </ul>
+ * <p>If you pass in a {@link org.apache.lucene.analysis.util.CharArraySet} as dictionary,
+ * it should be case-insensitive unless it contains only lowercased entries and you
+ * have {@link org.apache.lucene.analysis.core.LowerCaseFilter} before this filter in your analysis chain.
+ * For optional performance (as this filter does lots of lookups to the dictionary,
+ * you should use the latter analysis chain/CharArraySet). Be aware: If you supply arbitrary
+ * {@link Set Sets} to the ctors, they will be automatically
+ * transformed to case-insensitive!
  */
 public class DictionaryCompoundWordTokenFilter extends CompoundWordTokenFilterBase {
-  /**
-   * Creates a new {@link DictionaryCompoundWordTokenFilter}
-   * 
-   * @param matchVersion
-   *          Lucene version to enable correct Unicode 4.0 behavior in the
-   *          dictionaries if Version > 3.0. See <a
-   *          href="CompoundWordTokenFilterBase#version"
-   *          >CompoundWordTokenFilterBase</a> for details.
-   * @param input
-   *          the {@link TokenStream} to process
-   * @param dictionary
-   *          the word dictionary to match against
-   * @param minWordSize
-   *          only words longer than this get processed
-   * @param minSubwordSize
-   *          only subwords longer than this get to the output stream
-   * @param maxSubwordSize
-   *          only subwords shorter than this get to the output stream
-   * @param onlyLongestMatch
-   *          Add only the longest matching subword to the stream
-   */
-  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, String[] dictionary,
-      int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {
-    super(matchVersion, input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);
-  }
 
-  /**
-   * Creates a new {@link DictionaryCompoundWordTokenFilter}
-   * 
-   * @param matchVersion
-   *          Lucene version to enable correct Unicode 4.0 behavior in the
-   *          dictionaries if Version > 3.0. See <a
-   *          href="CompoundWordTokenFilterBase#version"
-   *          >CompoundWordTokenFilterBase</a> for details.
-   * 
-   * @param input
-   *          the {@link TokenStream} to process
-   * @param dictionary
-   *          the word dictionary to match against
-   */
-  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, String[] dictionary) {
-    super(matchVersion, input, dictionary);
-  }
-  
   /**
    * Creates a new {@link DictionaryCompoundWordTokenFilter}
    * 
@@ -90,12 +60,9 @@ public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input
    * @param input
    *          the {@link TokenStream} to process
    * @param dictionary
-   *          the word dictionary to match against. If this is a
-   *          {@link org.apache.lucene.analysis.util.CharArraySet CharArraySet} it
-   *          must have set ignoreCase=false and only contain lower case
-   *          strings.
+   *          the word dictionary to match against.
    */
-  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, Set dictionary) {
+  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, Set<?> dictionary) {
     super(matchVersion, input, dictionary);
   }
   
@@ -110,10 +77,7 @@ public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input
    * @param input
    *          the {@link TokenStream} to process
    * @param dictionary
-   *          the word dictionary to match against. If this is a
-   *          {@link org.apache.lucene.analysis.util.CharArraySet CharArraySet} it
-   *          must have set ignoreCase=false and only contain lower case
-   *          strings.
+   *          the word dictionary to match against.
    * @param minWordSize
    *          only words longer than this get processed
    * @param minSubwordSize
@@ -123,37 +87,31 @@ public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input
    * @param onlyLongestMatch
    *          Add only the longest matching subword to the stream
    */
-  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, Set dictionary,
+  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, Set<?> dictionary,
       int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {
     super(matchVersion, input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);
   }
 
   @Override
-  protected void decomposeInternal(final Token token) {
-    // Only words longer than minWordSize get processed
-    if (token.length() < this.minWordSize) {
-      return;
-    }
-    
-    char[] lowerCaseTermBuffer=makeLowerCaseCopy(token.buffer());
-    
-    for (int i=0;i<=token.length()-this.minSubwordSize;++i) {
-        Token longestMatchToken=null;
+  protected void decompose() {
+    final int len = termAtt.length();
+    for (int i=0;i<=len-this.minSubwordSize;++i) {
+        CompoundToken longestMatchToken=null;
         for (int j=this.minSubwordSize;j<=this.maxSubwordSize;++j) {
-            if(i+j>token.length()) {
+            if(i+j>len) {
                 break;
             }
-            if(dictionary.contains(lowerCaseTermBuffer, i, j)) {
+            if(dictionary.contains(termAtt.buffer(), i, j)) {
                 if (this.onlyLongestMatch) {
                    if (longestMatchToken!=null) {
-                     if (longestMatchToken.length()<j) {
-                       longestMatchToken=createToken(i,j,token);
+                     if (longestMatchToken.txt.length()<j) {
+                       longestMatchToken=new CompoundToken(i,j);
                      }
                    } else {
-                     longestMatchToken=createToken(i,j,token);
+                     longestMatchToken=new CompoundToken(i,j);
                    }
                 } else {
-                   tokens.add(createToken(i,j,token));
+                   tokens.add(new CompoundToken(i,j));
                 }
             } 
         }
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilter.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilter.java
index b821fd13..d18e7967 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilter.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilter.java
@@ -20,7 +20,6 @@
 import java.io.File;
 import java.util.Set;
 
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.compound.hyphenation.Hyphenation;
@@ -34,7 +33,21 @@
  * "Donaudampfschiff" becomes Donau, dampf, schiff so that you can find
  * "Donaudampfschiff" even when you only enter "schiff". It uses a hyphenation
  * grammar and a word dictionary to achieve this.
- * </p>
+ * <p>
+ * You must specify the required {@link Version} compatibility when creating
+ * CompoundWordTokenFilterBase:
+ * <ul>
+ * <li>As of 3.1, CompoundWordTokenFilterBase correctly handles Unicode 4.0
+ * supplementary characters in strings and char arrays provided as compound word
+ * dictionaries.
+ * </ul>
+ * <p>If you pass in a {@link org.apache.lucene.analysis.util.CharArraySet} as dictionary,
+ * it should be case-insensitive unless it contains only lowercased entries and you
+ * have {@link org.apache.lucene.analysis.core.LowerCaseFilter} before this filter in your analysis chain.
+ * For optional performance (as this filter does lots of lookups to the dictionary,
+ * you should use the latter analysis chain/CharArraySet). Be aware: If you supply arbitrary
+ * {@link Set Sets} to the ctors, they will be automatically
+ * transformed to case-insensitive!
  */
 public class HyphenationCompoundWordTokenFilter extends
     CompoundWordTokenFilterBase {
@@ -53,63 +66,7 @@
    * @param hyphenator
    *          the hyphenation pattern tree to use for hyphenation
    * @param dictionary
-   *          the word dictionary to match against
-   * @param minWordSize
-   *          only words longer than this get processed
-   * @param minSubwordSize
-   *          only subwords longer than this get to the output stream
-   * @param maxSubwordSize
-   *          only subwords shorter than this get to the output stream
-   * @param onlyLongestMatch
-   *          Add only the longest matching subword to the stream
-   */
-  public HyphenationCompoundWordTokenFilter(Version matchVersion, TokenStream input,
-      HyphenationTree hyphenator, String[] dictionary, int minWordSize,
-      int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {
-    super(matchVersion, input, dictionary, minWordSize, minSubwordSize, maxSubwordSize,
-        onlyLongestMatch);
-
-    this.hyphenator = hyphenator;
-  }
-
-  /**
-   * Creates a new {@link HyphenationCompoundWordTokenFilter} instance.
-   *  
-   * @param matchVersion
-   *          Lucene version to enable correct Unicode 4.0 behavior in the
-   *          dictionaries if Version > 3.0. See <a
-   *          href="CompoundWordTokenFilterBase#version"
-   *          >CompoundWordTokenFilterBase</a> for details.
-   * @param input
-   *          the {@link TokenStream} to process
-   * @param hyphenator
-   *          the hyphenation pattern tree to use for hyphenation
-   * @param dictionary
-   *          the word dictionary to match against
-   */
-  public HyphenationCompoundWordTokenFilter(Version matchVersion, TokenStream input,
-      HyphenationTree hyphenator, String[] dictionary) {
-    this(matchVersion, input, hyphenator, makeDictionary(dictionary), DEFAULT_MIN_WORD_SIZE,
-        DEFAULT_MIN_SUBWORD_SIZE, DEFAULT_MAX_SUBWORD_SIZE, false);
-  }
-
-  /**
-   * Creates a new {@link HyphenationCompoundWordTokenFilter} instance. 
-   * 
-   * @param matchVersion
-   *          Lucene version to enable correct Unicode 4.0 behavior in the
-   *          dictionaries if Version > 3.0. See <a
-   *          href="CompoundWordTokenFilterBase#version"
-   *          >CompoundWordTokenFilterBase</a> for details.
-   * @param input
-   *          the {@link TokenStream} to process
-   * @param hyphenator
-   *          the hyphenation pattern tree to use for hyphenation
-   * @param dictionary
-   *          the word dictionary to match against. If this is a
-   *          {@link org.apache.lucene.analysis.util.CharArraySet CharArraySet} it
-   *          must have set ignoreCase=false and only contain lower case
-   *          strings.
+   *          the word dictionary to match against.
    */
   public HyphenationCompoundWordTokenFilter(Version matchVersion, TokenStream input,
       HyphenationTree hyphenator, Set<?> dictionary) {
@@ -130,10 +87,7 @@ public HyphenationCompoundWordTokenFilter(Version matchVersion, TokenStream inpu
    * @param hyphenator
    *          the hyphenation pattern tree to use for hyphenation
    * @param dictionary
-   *          the word dictionary to match against. If this is a
-   *          {@link org.apache.lucene.analysis.util.CharArraySet CharArraySet} it
-   *          must have set ignoreCase=false and only contain lower case
-   *          strings.
+   *          the word dictionary to match against.
    * @param minWordSize
    *          only words longer than this get processed
    * @param minSubwordSize
@@ -218,22 +172,20 @@ public static HyphenationTree getHyphenationTree(InputSource hyphenationSource)
   }
 
   @Override
-  protected void decomposeInternal(final Token token) {
+  protected void decompose() {
     // get the hyphenation points
-    Hyphenation hyphens = hyphenator.hyphenate(token.buffer(), 0, token
-        .length(), 1, 1);
+    Hyphenation hyphens = hyphenator.hyphenate(termAtt.buffer(), 0, termAtt.length(), 1, 1);
     // No hyphen points found -> exit
     if (hyphens == null) {
       return;
     }
 
     final int[] hyp = hyphens.getHyphenationPoints();
-    char[] lowerCaseTermBuffer=makeLowerCaseCopy(token.buffer());
 
     for (int i = 0; i < hyp.length; ++i) {
       int remaining = hyp.length - i;
       int start = hyp[i];
-      Token longestMatchToken = null;
+      CompoundToken longestMatchToken = null;
       for (int j = 1; j < remaining; j++) {
         int partLength = hyp[i + j] - start;
 
@@ -250,34 +202,33 @@ protected void decomposeInternal(final Token token) {
         }
 
         // check the dictionary
-        if (dictionary == null || dictionary.contains(lowerCaseTermBuffer, start, partLength)) {
+        if (dictionary == null || dictionary.contains(termAtt.buffer(), start, partLength)) {
           if (this.onlyLongestMatch) {
             if (longestMatchToken != null) {
-              if (longestMatchToken.length() < partLength) {
-                longestMatchToken = createToken(start, partLength, token);
+              if (longestMatchToken.txt.length() < partLength) {
+                longestMatchToken = new CompoundToken(start, partLength);
               }
             } else {
-              longestMatchToken = createToken(start, partLength, token);
+              longestMatchToken = new CompoundToken(start, partLength);
             }
           } else {
-            tokens.add(createToken(start, partLength, token));
+            tokens.add(new CompoundToken(start, partLength));
           }
-        } else if (dictionary.contains(lowerCaseTermBuffer, start,
-            partLength - 1)) {
+        } else if (dictionary.contains(termAtt.buffer(), start, partLength - 1)) {
           // check the dictionary again with a word that is one character
           // shorter
           // to avoid problems with genitive 's characters and other binding
           // characters
           if (this.onlyLongestMatch) {
             if (longestMatchToken != null) {
-              if (longestMatchToken.length() < partLength - 1) {
-                longestMatchToken = createToken(start, partLength - 1, token);
+              if (longestMatchToken.txt.length() < partLength - 1) {
+                longestMatchToken = new CompoundToken(start, partLength - 1);
               }
             } else {
-              longestMatchToken = createToken(start, partLength - 1, token);
+              longestMatchToken = new CompoundToken(start, partLength - 1);
             }
           } else {
-            tokens.add(createToken(start, partLength - 1, token));
+            tokens.add(new CompoundToken(start, partLength - 1));
           }
         }
       }
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellDictionary.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellDictionary.java
index 00e7afaf..6b581855 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellDictionary.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellDictionary.java
@@ -225,7 +225,7 @@ private void parseAffix(CharArrayMap<List<HunspellAffix>> affixes,
   }
 
   /**
-   * Parses the encoding specificed in the affix file readable through the provided InputStream
+   * Parses the encoding specified in the affix file readable through the provided InputStream
    *
    * @param affix InputStream for reading the affix file
    * @return Encoding specified in the affix file
@@ -277,10 +277,10 @@ private CharsetDecoder getJavaEncoding(String encoding) {
   }
 
   /**
-   * Determines the appropriate {@link FlagParsingStrategy} based on the FLAG definiton line taken from the affix file
+   * Determines the appropriate {@link FlagParsingStrategy} based on the FLAG definition line taken from the affix file
    *
    * @param flagLine Line containing the flag information
-   * @return FlagParsingStrategy that handles parsing flags in the way specified in the FLAG definiton
+   * @return FlagParsingStrategy that handles parsing flags in the way specified in the FLAG definition
    */
   private FlagParsingStrategy getFlagParsingStrategy(String flagLine) {
     String flagType = flagLine.substring(5);
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellWord.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellWord.java
index 333bf1c3..fbb4ae93 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellWord.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellWord.java
@@ -52,7 +52,7 @@ public boolean hasFlag(char flag) {
   /**
    * Returns the flags associated with the word
    *
-   * @return Flags asssociated with the word
+   * @return Flags associated with the word
    */
   public char[] getFlags() {
     return flags;
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
index 98429845..8821715d 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
@@ -65,13 +65,13 @@
  * <p><b>NOTE</b>: when a match occurs, the output tokens
  * associated with the matching rule are "stacked" on top of
  * the input stream (if the rule had
- * <code>keepOrig=true</code>) and also on top of aother
+ * <code>keepOrig=true</code>) and also on top of another
  * matched rule's output tokens.  This is not a correct
- * solution, as really the output should be an abitrary
+ * solution, as really the output should be an arbitrary
  * graph/lattice.  For example, with the above match, you
  * would expect an exact <code>PhraseQuery</code> <code>"y b
  * c"</code> to match the parsed tokens, but it will fail to
- * do so.  This limitations is necessary because Lucene's
+ * do so.  This limitation is necessary because Lucene's
  * TokenStream (and index) cannot yet represent an arbitrary
  * graph.</p>
  *
@@ -90,7 +90,7 @@
 // http://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm
 // It improves over the current approach here
 // because it does not fully re-start matching at every
-// token.  For exampl,e if one pattern is "a b c x"
+// token.  For example if one pattern is "a b c x"
 // and another is "b c d" and the input is "a b c d", on
 // trying to parse "a b c x" but failing when you got to x,
 // rather than starting over again your really should
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java
index 12f7dd1e..8b3b282e 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java
@@ -44,9 +44,9 @@
 public class SynonymMap {
   /** for multiword support, you must separate words with this separator */
   public static final char WORD_SEPARATOR = 0;
-  /** map<input word, list<ord>> */
+  /** map&lt;input word, list&lt;ord&gt;&gt; */
   public final FST<BytesRef> fst;
-  /** map<ord, outputword> */
+  /** map&lt;ord, outputword&gt; */
   public final BytesRefHash words;
   /** maxHorizontalContext: maximum context we need on the tokenstream */
   public final int maxHorizontalContext;
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
index 5f3b7c79..67397512 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
@@ -20,7 +20,6 @@
 import java.lang.Character.UnicodeBlock;
 import java.text.BreakIterator;
 import java.util.Locale;
-import javax.swing.text.Segment;
 
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
@@ -28,6 +27,7 @@
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.util.CharArrayIterator;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Version;
 
@@ -56,7 +56,7 @@
     DBBI_AVAILABLE = proto.isBoundary(4);
   }
   private final BreakIterator breaker = (BreakIterator) proto.clone();
-  private final Segment charIterator = new Segment();
+  private final CharArrayIterator charIterator = CharArrayIterator.newWordInstance();
   
   private final boolean handlePosIncr;
   
@@ -113,9 +113,7 @@ public boolean incrementToken() throws IOException {
     }
     
     // reinit CharacterIterator
-    charIterator.array = clonedTermAtt.buffer();
-    charIterator.offset = 0;
-    charIterator.count = clonedTermAtt.length();
+    charIterator.setText(clonedTermAtt.buffer(), 0, clonedTermAtt.length());
     breaker.setText(charIterator);
     int end = breaker.next();
     if (end != BreakIterator.DONE) {
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/util/CharArrayIterator.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/util/CharArrayIterator.java
index e69de29b..4d144794 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/util/CharArrayIterator.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/analysis/util/CharArrayIterator.java
@@ -0,0 +1,175 @@
+package org.apache.lucene.analysis.util;
+
+import java.text.BreakIterator; // javadoc
+import java.text.CharacterIterator;
+import java.util.Locale;
+
+/** 
+ * A CharacterIterator used internally for use with {@link BreakIterator}
+ * @lucene.internal
+ */
+public abstract class CharArrayIterator implements CharacterIterator {
+  private char array[];
+  private int start;
+  private int index;
+  private int length;
+  private int limit;
+
+  public char [] getText() {
+    return array;
+  }
+  
+  public int getStart() {
+    return start;
+  }
+  
+  public int getLength() {
+    return length;
+  }
+  
+  /**
+   * Set a new region of text to be examined by this iterator
+   * 
+   * @param array text buffer to examine
+   * @param start offset into buffer
+   * @param length maximum length to examine
+   */
+  public void setText(final char array[], int start, int length) {
+    this.array = array;
+    this.start = start;
+    this.index = start;
+    this.length = length;
+    this.limit = start + length;
+  }
+
+  public char current() {
+    return (index == limit) ? DONE : jreBugWorkaround(array[index]);
+  }
+  
+  protected abstract char jreBugWorkaround(char ch);
+
+  public char first() {
+    index = start;
+    return current();
+  }
+
+  public int getBeginIndex() {
+    return 0;
+  }
+
+  public int getEndIndex() {
+    return length;
+  }
+
+  public int getIndex() {
+    return index - start;
+  }
+
+  public char last() {
+    index = (limit == start) ? limit : limit - 1;
+    return current();
+  }
+
+  public char next() {
+    if (++index >= limit) {
+      index = limit;
+      return DONE;
+    } else {
+      return current();
+    }
+  }
+
+  public char previous() {
+    if (--index < start) {
+      index = start;
+      return DONE;
+    } else {
+      return current();
+    }
+  }
+
+  public char setIndex(int position) {
+    if (position < getBeginIndex() || position > getEndIndex())
+      throw new IllegalArgumentException("Illegal Position: " + position);
+    index = start + position;
+    return current();
+  }
+  
+  @Override
+  public Object clone() {
+    try {
+      return super.clone();
+    } catch (CloneNotSupportedException e) {
+      // CharacterIterator does not allow you to throw CloneNotSupported
+      throw new RuntimeException(e);
+    }
+  }
+  
+  /**
+   * Create a new CharArrayIterator that works around JRE bugs
+   * in a manner suitable for {@link BreakIterator#getSentenceInstance()}
+   */
+  public static CharArrayIterator newSentenceInstance() {
+    if (HAS_BUGGY_BREAKITERATORS) {
+      return new CharArrayIterator() {
+        // work around this for now by lying about all surrogates to 
+        // the sentence tokenizer, instead we treat them all as 
+        // SContinue so we won't break around them.
+        @Override
+        protected char jreBugWorkaround(char ch) {
+          return ch >= 0xD800 && ch <= 0xDFFF ? 0x002C : ch;
+        }
+      };
+    } else {
+      return new CharArrayIterator() {
+        // no bugs
+        @Override
+        protected char jreBugWorkaround(char ch) {
+          return ch;
+        }
+      };
+    }
+  }
+  
+  /**
+   * Create a new CharArrayIterator that works around JRE bugs
+   * in a manner suitable for {@link BreakIterator#getWordInstance()}
+   */
+  public static CharArrayIterator newWordInstance() {
+    if (HAS_BUGGY_BREAKITERATORS) {
+      return new CharArrayIterator() {
+        // work around this for now by lying about all surrogates to the word, 
+        // instead we treat them all as ALetter so we won't break around them.
+        @Override
+        protected char jreBugWorkaround(char ch) {
+          return ch >= 0xD800 && ch <= 0xDFFF ? 0x0041 : ch;
+        }
+      };
+    } else {
+      return new CharArrayIterator() {
+        // no bugs
+        @Override
+        protected char jreBugWorkaround(char ch) {
+          return ch;
+        }
+      };
+    }
+  }
+  
+  /**
+   * True if this JRE has a buggy BreakIterator implementation
+   */
+  public static final boolean HAS_BUGGY_BREAKITERATORS;
+  static {
+    boolean v;
+    try {
+      BreakIterator bi = BreakIterator.getSentenceInstance(Locale.US);
+      bi.setText("\udb40\udc53");
+      bi.next();
+      v = false;
+    } catch (Exception e) {
+      v = true;
+    }
+    HAS_BUGGY_BREAKITERATORS = v;
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/collation/tokenattributes/CollatedTermAttributeImpl.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/collation/tokenattributes/CollatedTermAttributeImpl.java
index 010c8325..c6316383 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/collation/tokenattributes/CollatedTermAttributeImpl.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/java/org/apache/lucene/collation/tokenattributes/CollatedTermAttributeImpl.java
@@ -35,7 +35,7 @@
    * @param collator Collation key generator
    */
   public CollatedTermAttributeImpl(Collator collator) {
-    // clone in case JRE doesnt properly sync,
+    // clone in case JRE doesn't properly sync,
     // or to reduce contention in case they do
     this.collator = (Collator) collator.clone();
   }
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
index 74706391..3c2a2177 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
@@ -18,19 +18,31 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
 import java.io.StringReader;
-import org.xml.sax.InputSource;
+import java.util.Arrays;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.compound.hyphenation.HyphenationTree;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
+import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeImpl;
+import org.xml.sax.InputSource;
 
 public class TestCompoundWordTokenFilter extends BaseTokenStreamTestCase {
+
+  private static CharArraySet makeDictionary(String... dictionary) {
+    return new CharArraySet(TEST_VERSION_CURRENT, Arrays.asList(dictionary), true);
+  }
+
   public void testHyphenationCompoundWordsDA() throws Exception {
-    String[] dict = { "læse", "hest" };
+    CharArraySet dict = makeDictionary("læse", "hest");
 
     InputSource is = new InputSource(getClass().getResource("da_UTF8.xml").toExternalForm());
     HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter
@@ -49,7 +61,7 @@ public void testHyphenationCompoundWordsDA() throws Exception {
   }
 
   public void testHyphenationCompoundWordsDELongestMatch() throws Exception {
-    String[] dict = { "basketball", "basket", "ball", "kurv" };
+    CharArraySet dict = makeDictionary("basketball", "basket", "ball", "kurv");
 
     InputSource is = new InputSource(getClass().getResource("da_UTF8.xml").toExternalForm());
     HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter
@@ -117,9 +129,9 @@ public void testHyphenationOnly() throws Exception {
   }
 
   public void testDumbCompoundWordsSE() throws Exception {
-    String[] dict = { "Bil", "Dörr", "Motor", "Tak", "Borr", "Slag", "Hammar",
+    CharArraySet dict = makeDictionary("Bil", "Dörr", "Motor", "Tak", "Borr", "Slag", "Hammar",
         "Pelar", "Glas", "Ögon", "Fodral", "Bas", "Fiol", "Makare", "Gesäll",
-        "Sko", "Vind", "Rute", "Torkare", "Blad" };
+        "Sko", "Vind", "Rute", "Torkare", "Blad");
 
     DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, 
         new MockTokenizer( 
@@ -147,9 +159,9 @@ public void testDumbCompoundWordsSE() throws Exception {
   }
 
   public void testDumbCompoundWordsSELongestMatch() throws Exception {
-    String[] dict = { "Bil", "Dörr", "Motor", "Tak", "Borr", "Slag", "Hammar",
+    CharArraySet dict = makeDictionary("Bil", "Dörr", "Motor", "Tak", "Borr", "Slag", "Hammar",
         "Pelar", "Glas", "Ögon", "Fodral", "Bas", "Fiols", "Makare", "Gesäll",
-        "Sko", "Vind", "Rute", "Torkare", "Blad", "Fiolsfodral" };
+        "Sko", "Vind", "Rute", "Torkare", "Blad", "Fiolsfodral");
 
     DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, 
         new MockTokenizer(new StringReader("Basfiolsfodralmakaregesäll"), MockTokenizer.WHITESPACE, false),
@@ -164,7 +176,7 @@ public void testDumbCompoundWordsSELongestMatch() throws Exception {
   }
 
   public void testTokenEndingWithWordComponentOfMinimumLength() throws Exception {
-    String[] dict = {"ab", "cd", "ef"};
+    CharArraySet dict = makeDictionary("ab", "cd", "ef");
 
     DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT,
 			new WhitespaceTokenizer(TEST_VERSION_CURRENT,
@@ -185,7 +197,7 @@ public void testTokenEndingWithWordComponentOfMinimumLength() throws Exception {
   }
 
   public void testWordComponentWithLessThanMinimumLength() throws Exception {
-    String[] dict = {"abc", "d", "efg"};
+    CharArraySet dict = makeDictionary("abc", "d", "efg");
 
     DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT,
 			new WhitespaceTokenizer(TEST_VERSION_CURRENT,
@@ -207,8 +219,8 @@ public void testWordComponentWithLessThanMinimumLength() throws Exception {
   }
   
   public void testReset() throws Exception {
-    String[] dict = { "Rind", "Fleisch", "Draht", "Schere", "Gesetz",
-        "Aufgabe", "Überwachung" };
+    CharArraySet dict = makeDictionary("Rind", "Fleisch", "Draht", "Schere", "Gesetz",
+        "Aufgabe", "Überwachung");
 
     Tokenizer wsTokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(
         "Rindfleischüberwachungsgesetz"));
@@ -229,4 +241,64 @@ public void testReset() throws Exception {
     assertEquals("Rindfleischüberwachungsgesetz", termAtt.toString());
   }
 
+  public void testRetainMockAttribute() throws Exception {
+    CharArraySet dict = makeDictionary("abc", "d", "efg");
+    Tokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT,
+        new StringReader("abcdefg"));
+    TokenStream stream = new MockRetainAttributeFilter(tokenizer);
+    stream = new DictionaryCompoundWordTokenFilter(
+        TEST_VERSION_CURRENT, stream, dict,
+        CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
+        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
+        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);
+    MockRetainAttribute retAtt = stream.addAttribute(MockRetainAttribute.class);
+    while (stream.incrementToken()) {
+      assertTrue("Custom attribute value was lost", retAtt.getRetain());
+    }
+
+  }
+
+  public static interface MockRetainAttribute extends Attribute {
+    void setRetain(boolean attr);
+    boolean getRetain();
+  }
+
+  public static final class MockRetainAttributeImpl extends AttributeImpl implements MockRetainAttribute {
+    private boolean retain = false;
+    @Override
+    public void clear() {
+      retain = false;
+    }
+    public boolean getRetain() {
+      return retain;
+    }
+    public void setRetain(boolean retain) {
+      this.retain = retain;
+    }
+    @Override
+    public void copyTo(AttributeImpl target) {
+      MockRetainAttribute t = (MockRetainAttribute) target;
+      t.setRetain(retain);
+    }
+  }
+
+  private static class MockRetainAttributeFilter extends TokenFilter {
+    
+    MockRetainAttribute retainAtt = addAttribute(MockRetainAttribute.class);
+    
+    MockRetainAttributeFilter(TokenStream input) {
+      super(input);
+    }
+    
+    @Override
+    public boolean incrementToken() throws IOException {
+      if (input.incrementToken()){
+        retainAtt.setRetain(true); 
+        return true;
+      } else {
+      return false;
+      }
+    }
+  }
+
 }
diff --git a/lucene/dev/branches/solrcloud/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java b/lucene/dev/branches/solrcloud/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java
index e69de29b..bfcab81e 100644
--- a/lucene/dev/branches/solrcloud/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java
+++ b/lucene/dev/branches/solrcloud/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java
@@ -0,0 +1,162 @@
+package org.apache.lucene.analysis.util;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.text.BreakIterator;
+import java.text.CharacterIterator;
+import java.util.Locale;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.UnicodeUtil;
+import org.apache.lucene.util._TestUtil;
+
+public class TestCharArrayIterator extends LuceneTestCase {
+  
+  public void testWordInstance() {
+    doTests(CharArrayIterator.newWordInstance());
+  }
+  
+  public void testConsumeWordInstance() {
+    BreakIterator bi = BreakIterator.getWordInstance();
+    CharArrayIterator ci = CharArrayIterator.newWordInstance();
+    for (int i = 0; i < 10000; i++) {
+      char text[] = _TestUtil.randomUnicodeString(random).toCharArray();
+      ci.setText(text, 0, text.length);
+      consume(bi, ci);
+    }
+  }
+  
+  /* run this to test if your JRE is buggy
+  public void testWordInstanceJREBUG() {
+    BreakIterator bi = BreakIterator.getWordInstance();
+    Segment ci = new Segment();
+    for (int i = 0; i < 10000; i++) {
+      char text[] = _TestUtil.randomUnicodeString(random).toCharArray();
+      ci.array = text;
+      ci.offset = 0;
+      ci.count = text.length;
+      consume(bi, ci);
+    }
+  }
+  */
+  
+  public void testSentenceInstance() {
+    doTests(CharArrayIterator.newSentenceInstance());
+  }
+  
+  public void testConsumeSentenceInstance() {
+    BreakIterator bi = BreakIterator.getSentenceInstance();
+    CharArrayIterator ci = CharArrayIterator.newSentenceInstance();
+    for (int i = 0; i < 10000; i++) {
+      char text[] = _TestUtil.randomUnicodeString(random).toCharArray();
+      ci.setText(text, 0, text.length);
+      consume(bi, ci);
+    }
+  }
+  
+  /* run this to test if your JRE is buggy
+  public void testSentenceInstanceJREBUG() {
+    BreakIterator bi = BreakIterator.getSentenceInstance();
+    Segment ci = new Segment();
+    for (int i = 0; i < 10000; i++) {
+      char text[] = _TestUtil.randomUnicodeString(random).toCharArray();
+      ci.array = text;
+      ci.offset = 0;
+      ci.count = text.length;
+      consume(bi, ci);
+    }
+  }
+  */
+  
+  private void doTests(CharArrayIterator ci) {
+    // basics
+    ci.setText("testing".toCharArray(), 0, "testing".length());
+    assertEquals(0, ci.getBeginIndex());
+    assertEquals(7, ci.getEndIndex());
+    assertEquals(0, ci.getIndex());
+    assertEquals('t', ci.current());
+    assertEquals('e', ci.next());
+    assertEquals('g', ci.last());
+    assertEquals('n', ci.previous());
+    assertEquals('t', ci.first());
+    assertEquals(CharacterIterator.DONE, ci.previous());
+    
+    // first()
+    ci.setText("testing".toCharArray(), 0, "testing".length());
+    ci.next();
+    // Sets the position to getBeginIndex() and returns the character at that position. 
+    assertEquals('t', ci.first());
+    assertEquals(ci.getBeginIndex(), ci.getIndex());
+    // or DONE if the text is empty
+    ci.setText(new char[] {}, 0, 0);
+    assertEquals(CharacterIterator.DONE, ci.first());
+    
+    // last()
+    ci.setText("testing".toCharArray(), 0, "testing".length());
+    // Sets the position to getEndIndex()-1 (getEndIndex() if the text is empty) 
+    // and returns the character at that position. 
+    assertEquals('g', ci.last());
+    assertEquals(ci.getIndex(), ci.getEndIndex() - 1);
+    // or DONE if the text is empty
+    ci.setText(new char[] {}, 0, 0);
+    assertEquals(CharacterIterator.DONE, ci.last());
+    assertEquals(ci.getEndIndex(), ci.getIndex());
+    
+    // current()
+    // Gets the character at the current position (as returned by getIndex()). 
+    ci.setText("testing".toCharArray(), 0, "testing".length());
+    assertEquals('t', ci.current());
+    ci.last();
+    ci.next();
+    // or DONE if the current position is off the end of the text.
+    assertEquals(CharacterIterator.DONE, ci.current());
+    
+    // next()
+    ci.setText("te".toCharArray(), 0, 2);
+    // Increments the iterator's index by one and returns the character at the new index.
+    assertEquals('e', ci.next());
+    assertEquals(1, ci.getIndex());
+    // or DONE if the new position is off the end of the text range.
+    assertEquals(CharacterIterator.DONE, ci.next());
+    assertEquals(ci.getEndIndex(), ci.getIndex());
+    
+    // setIndex()
+    ci.setText("test".toCharArray(), 0, "test".length());
+    try {
+      ci.setIndex(5);
+      fail();
+    } catch (Exception e) {
+      assertTrue(e instanceof IllegalArgumentException);
+    }
+    
+    // clone()
+    char text[] = "testing".toCharArray();
+    ci.setText(text, 0, text.length);
+    ci.next();
+    CharArrayIterator ci2 = (CharArrayIterator) ci.clone();
+    assertEquals(ci.getIndex(), ci2.getIndex());
+    assertEquals(ci.next(), ci2.next());
+    assertEquals(ci.last(), ci2.last());
+  }
+  
+  private void consume(BreakIterator bi, CharacterIterator ci) {
+    bi.setText(ci);
+    while (bi.next() != BreakIterator.DONE)
+      ;
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateTaxonomyIndexTask.java b/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateTaxonomyIndexTask.java
index 3752f498..0c469f68 100644
--- a/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateTaxonomyIndexTask.java
+++ b/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateTaxonomyIndexTask.java
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 
 import java.io.IOException;
@@ -37,7 +37,7 @@ public CreateTaxonomyIndexTask(PerfRunData runData) {
   @Override
   public int doLogic() throws IOException {
     PerfRunData runData = getRunData();
-    runData.setTaxonomyWriter(new LuceneTaxonomyWriter(runData.getTaxonomyDir(), OpenMode.CREATE));
+    runData.setTaxonomyWriter(new DirectoryTaxonomyWriter(runData.getTaxonomyDir(), OpenMode.CREATE));
     return 1;
   }
 
diff --git a/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyIndexTask.java b/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyIndexTask.java
index 613578dd..b6aacd70 100644
--- a/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyIndexTask.java
+++ b/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyIndexTask.java
@@ -18,7 +18,8 @@
  */
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+
 import java.io.IOException;
 
 
@@ -35,7 +36,7 @@ public OpenTaxonomyIndexTask(PerfRunData runData) {
   @Override
   public int doLogic() throws IOException {
     PerfRunData runData = getRunData();
-    runData.setTaxonomyWriter(new LuceneTaxonomyWriter(runData.getTaxonomyDir()));
+    runData.setTaxonomyWriter(new DirectoryTaxonomyWriter(runData.getTaxonomyDir()));
     return 1;
   }
 
diff --git a/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyReaderTask.java b/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyReaderTask.java
index cab7f117..44095f99 100644
--- a/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyReaderTask.java
+++ b/lucene/dev/branches/solrcloud/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyReaderTask.java
@@ -21,7 +21,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Open a taxonomy index reader.
@@ -36,7 +36,7 @@ public OpenTaxonomyReaderTask(PerfRunData runData) {
   @Override
   public int doLogic() throws IOException {
     PerfRunData runData = getRunData();
-    LuceneTaxonomyReader taxoReader = new LuceneTaxonomyReader(runData.getTaxonomyDir());
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(runData.getTaxonomyDir());
     runData.setTaxonomyReader(taxoReader);
     // We transfer reference to the run data
     taxoReader.decRef();
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/ExampleUtils.java b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/ExampleUtils.java
index 2e12c987..f5e211bd 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/ExampleUtils.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/ExampleUtils.java
@@ -27,7 +27,7 @@
   public static final boolean VERBOSE = Boolean.getBoolean("tests.verbose");
 
   /** The Lucene {@link Version} used by the example code. */
-  public static final Version EXAMPLE_VER = Version.LUCENE_31;
+  public static final Version EXAMPLE_VER = Version.LUCENE_40;
   
   public static void log(Object msg) {
     if (VERBOSE) {
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java
index 93326e52..6cf8083e 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java
@@ -20,7 +20,7 @@
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -55,7 +55,7 @@
    */
   public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {
     // prepare index reader and taxonomy.
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     IndexReader indexReader = IndexReader.open(indexDir);
     
     // prepare searcher to search against
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationIndexer.java b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationIndexer.java
index 13828370..6c910400 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationIndexer.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationIndexer.java
@@ -16,7 +16,7 @@
 import org.apache.lucene.facet.index.CategoryDocumentBuilder;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -60,7 +60,7 @@ public static void index(Directory indexDir, Directory taxoDir) throws Exception
     IndexWriter iw = new IndexWriter(indexDir, new IndexWriterConfig(ExampleUtils.EXAMPLE_VER, SimpleUtils.analyzer));
 
     // create and open a taxonomy writer
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
 
     // loop over sample documents
     int nDocsAdded = 0;
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java
index 08a8e715..f429c436 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java
@@ -11,7 +11,7 @@
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -43,7 +43,7 @@
       Directory taxoDir) throws Exception {
     // prepare index reader 
     IndexReader indexReader = IndexReader.open(indexDir);
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     AssociationIntSumFacetRequest facetRequest = new AssociationIntSumFacetRequest(
         new CategoryPath("tags"), 10);
@@ -63,7 +63,7 @@
       Directory taxoDir) throws Exception {
     // prepare index reader 
     IndexReader indexReader = IndexReader.open(indexDir);
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     AssociationFloatSumFacetRequest facetRequest = new AssociationFloatSumFacetRequest(
         new CategoryPath("genre"), 10);
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java
index f310f9ff..40dfac53 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java
@@ -11,10 +11,10 @@
 import org.apache.lucene.facet.example.ExampleUtils;
 import org.apache.lucene.facet.index.FacetsPayloadProcessorProvider;
 import org.apache.lucene.facet.index.params.DefaultFacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.DiskOrdinalMap;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.MemoryOrdinalMap;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.DiskOrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -40,19 +40,19 @@
 
   /**
    * Merges the given taxonomy and index directories. Note that this method
-   * opens {@link LuceneTaxonomyWriter} and {@link IndexWriter} on the
+   * opens {@link DirectoryTaxonomyWriter} and {@link IndexWriter} on the
    * respective destination indexes. Therefore if you have a writer open on any
    * of them, it should be closed, or you should use
-   * {@link #merge(Directory, Directory, IndexWriter, LuceneTaxonomyWriter)}
+   * {@link #merge(Directory, Directory, IndexWriter, DirectoryTaxonomyWriter)}
    * instead.
    * 
-   * @see #merge(Directory, Directory, IndexWriter, LuceneTaxonomyWriter)
+   * @see #merge(Directory, Directory, IndexWriter, DirectoryTaxonomyWriter)
    */
   public static void merge(Directory srcIndexDir, Directory srcTaxDir,
                             Directory destIndexDir, Directory destTaxDir) throws IOException {
     IndexWriter destIndexWriter = new IndexWriter(destIndexDir,
         new IndexWriterConfig(ExampleUtils.EXAMPLE_VER, null));
-    LuceneTaxonomyWriter destTaxWriter = new LuceneTaxonomyWriter(destTaxDir);
+    DirectoryTaxonomyWriter destTaxWriter = new DirectoryTaxonomyWriter(destTaxDir);
     merge(srcIndexDir, srcTaxDir, new MemoryOrdinalMap(), destIndexWriter, destTaxWriter);
     destTaxWriter.close();
     destIndexWriter.close();
@@ -62,14 +62,14 @@ public static void merge(Directory srcIndexDir, Directory srcTaxDir,
    * Merges the given taxonomy and index directories and commits the changes to
    * the given writers. This method uses {@link MemoryOrdinalMap} to store the
    * mapped ordinals. If you cannot afford the memory, you can use
-   * {@link #merge(Directory, Directory, LuceneTaxonomyWriter.OrdinalMap, IndexWriter, LuceneTaxonomyWriter)}
+   * {@link #merge(Directory, Directory, DirectoryTaxonomyWriter.OrdinalMap, IndexWriter, DirectoryTaxonomyWriter)}
    * by passing {@link DiskOrdinalMap}.
    * 
-   * @see #merge(Directory, Directory, LuceneTaxonomyWriter.OrdinalMap, IndexWriter, LuceneTaxonomyWriter)
+   * @see #merge(Directory, Directory, DirectoryTaxonomyWriter.OrdinalMap, IndexWriter, DirectoryTaxonomyWriter)
    */
   public static void merge(Directory srcIndexDir, Directory srcTaxDir,
                             IndexWriter destIndexWriter, 
-                            LuceneTaxonomyWriter destTaxWriter) throws IOException {
+                            DirectoryTaxonomyWriter destTaxWriter) throws IOException {
     merge(srcIndexDir, srcTaxDir, new MemoryOrdinalMap(), destIndexWriter, destTaxWriter);
   }
   
@@ -79,7 +79,7 @@ public static void merge(Directory srcIndexDir, Directory srcTaxDir,
    */
   public static void merge(Directory srcIndexDir, Directory srcTaxDir,
                             OrdinalMap map, IndexWriter destIndexWriter,
-                            LuceneTaxonomyWriter destTaxWriter) throws IOException {
+                            DirectoryTaxonomyWriter destTaxWriter) throws IOException {
     // merge the taxonomies
     destTaxWriter.addTaxonomies(new Directory[] { srcTaxDir }, new OrdinalMap[] { map });
 
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLIndexer.java b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLIndexer.java
index 21e84750..9778386c 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLIndexer.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLIndexer.java
@@ -14,7 +14,6 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 
-import org.apache.lucene.DocumentBuilder;
 import org.apache.lucene.facet.example.ExampleUtils;
 import org.apache.lucene.facet.example.simple.SimpleUtils;
 import org.apache.lucene.facet.index.CategoryDocumentBuilder;
@@ -22,7 +21,7 @@
 import org.apache.lucene.facet.index.params.FacetIndexingParams;
 import org.apache.lucene.facet.index.params.PerDimensionIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -141,7 +140,7 @@ public static void index(Directory indexDir, Directory taxoDir,
     IndexWriter iw = new IndexWriter(indexDir, new IndexWriterConfig(
         ExampleUtils.EXAMPLE_VER, SimpleUtils.analyzer).setOpenMode(OpenMode.CREATE));
     // create and open a taxonomy writer
-    LuceneTaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    DirectoryTaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
     index(iw, taxo, iParams, docTitles, docTexts, cPaths);
   }
   
@@ -154,7 +153,7 @@ public static void index(Directory indexDir, Directory taxoDir,
    *             on error (no detailed exception handling here for sample
    *             simplicity
    */
-  public static void index(IndexWriter iw, LuceneTaxonomyWriter taxo,
+  public static void index(IndexWriter iw, DirectoryTaxonomyWriter taxo,
       FacetIndexingParams iParams, String[] docTitles,
       String[] docTexts, CategoryPath[][] cPaths) throws Exception {
 
@@ -167,7 +166,7 @@ public static void index(IndexWriter iw, LuceneTaxonomyWriter taxo,
       // we do not alter indexing parameters!
       // a category document builder will add the categories to a document
       // once build() is called
-      DocumentBuilder categoryDocBuilder = new CategoryDocumentBuilder(
+      CategoryDocumentBuilder categoryDocBuilder = new CategoryDocumentBuilder(
           taxo, iParams).setCategoryPaths(facetList);
 
       // create a plain Lucene document and add some regular Lucene fields
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java
index 8be59ca3..650b420e 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java
@@ -20,7 +20,7 @@
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -64,7 +64,7 @@
     
     // prepare index reader and taxonomy.
     IndexReader indexReader = IndexReader.open(indexDir);
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     // Get results
     List<FacetResult> results = searchWithFacets(indexReader, taxo, iParams);
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleIndexer.java b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleIndexer.java
index 117fa138..cef9ff7f 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleIndexer.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleIndexer.java
@@ -11,12 +11,11 @@
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.store.Directory;
 
-import org.apache.lucene.DocumentBuilder;
 import org.apache.lucene.facet.example.ExampleUtils;
 import org.apache.lucene.facet.index.CategoryDocumentBuilder;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -54,7 +53,7 @@ public static void index (Directory indexDir, Directory taxoDir) throws Exceptio
     IndexWriter iw = new IndexWriter(indexDir, new IndexWriterConfig(ExampleUtils.EXAMPLE_VER, SimpleUtils.analyzer));
 
     // create and open a taxonomy writer
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
 
     // loop over  sample documents 
     int nDocsAdded = 0;
@@ -66,7 +65,7 @@ public static void index (Directory indexDir, Directory taxoDir) throws Exceptio
 
       // we do not alter indexing parameters!  
       // a category document builder will add the categories to a document once build() is called
-      DocumentBuilder categoryDocBuilder = new CategoryDocumentBuilder(taxo).setCategoryPaths(facetList);
+      CategoryDocumentBuilder categoryDocBuilder = new CategoryDocumentBuilder(taxo).setCategoryPaths(facetList);
 
       // create a plain Lucene document and add some regular Lucene fields to it 
       Document doc = new Document();
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java
index da3a5d33..f3449ce2 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java
@@ -10,7 +10,7 @@
 import org.apache.lucene.facet.example.ExampleUtils;
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -56,7 +56,7 @@ public ExampleResult runSimple() throws Exception {
     SimpleIndexer.index(indexDir, taxoDir);
 
     // open readers
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     IndexReader indexReader = IndexReader.open(indexDir, true);
 
     ExampleUtils.log("search the sample documents...");
@@ -81,7 +81,7 @@ public ExampleResult runDrillDown() throws Exception {
     SimpleIndexer.index(indexDir, taxoDir);
 
     // open readers
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     IndexReader indexReader = IndexReader.open(indexDir, true);
 
     ExampleUtils.log("search the sample documents...");
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/DocumentBuilder.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/DocumentBuilder.java
index 26cee4b2..e69de29b 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/DocumentBuilder.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/DocumentBuilder.java
@@ -1,77 +0,0 @@
-package org.apache.lucene;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An interface which standardizes the process of building an indexable
- * {@link Document}.
- * <p>
- * The idea is that implementations implement {@link #build(Document doc)},
- * which adds to the given Document whatever {@link Field}s it wants to add. A
- * DocumentBuilder is also allowed to inspect or change existing Fields in the
- * Document, if it wishes to.
- * <p>
- * Implementations should normally have a constructor with parameters which
- * determine what {@link #build(Document)} will add to doc.<br>
- * To allow reuse of the DocumentBuilder object, implementations are also
- * encouraged to have a setter method, which remembers its parameters just like
- * the constructor. This setter method cannot be described in this interface,
- * because it will take different parameters in each implementation.
- * <p>
- * The interface defines a builder pattern, which allows applications to invoke
- * several document builders in the following way:
- * 
- * <pre>
- * builder1.build(builder2.build(builder3.build(new Document())));
- * </pre>
- * 
- * @lucene.experimental
- */
-public interface DocumentBuilder {
- 
-  /** An exception thrown from {@link DocumentBuilder}'s build(). */
-  public static class DocumentBuilderException extends Exception {
-
-    public DocumentBuilderException() {
-      super();
-    }
-
-    public DocumentBuilderException(String message) {
-      super(message);
-    }
-
-    public DocumentBuilderException(String message, Throwable cause) {
-      super(message, cause);
-    }
-
-    public DocumentBuilderException(Throwable cause) {
-      super(cause);
-    }
-
-  }
-
-  /**
-   * Adds to the given document whatever {@link Field}s the implementation needs
-   * to add. Return the docunment instance to allow for chaining calls.
-   */
-  public Document build(Document doc) throws DocumentBuilderException;
-  
-}
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/index/CategoryDocumentBuilder.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/index/CategoryDocumentBuilder.java
index fb5c1831..e6622ac2 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/index/CategoryDocumentBuilder.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/index/CategoryDocumentBuilder.java
@@ -13,7 +13,6 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
 
-import org.apache.lucene.DocumentBuilder;
 import org.apache.lucene.facet.index.attributes.CategoryAttribute;
 import org.apache.lucene.facet.index.attributes.CategoryAttributesIterable;
 import org.apache.lucene.facet.index.categorypolicy.OrdinalPolicy;
@@ -64,7 +63,7 @@
  * 
  * @lucene.experimental
  */
-public class CategoryDocumentBuilder implements DocumentBuilder {
+public class CategoryDocumentBuilder {
 
   /**
    * A {@link TaxonomyWriter} for adding categories and retrieving their
@@ -288,9 +287,7 @@ protected CategoryTokenizer getCategoryTokenizer(TokenStream categoryStream)
     return new CategoryTokenizer(categoryStream, indexingParams);
   }
 
-  /**
-   * Adds the fields created in one of the "set" methods to the document
-   */
+  /** Adds the fields created in one of the "set" methods to the document */
   public Document build(Document doc) {
     for (Field f : fieldList) {
       doc.add(f);
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadProcessorProvider.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadProcessorProvider.java
index 881ebf00..4fab2a8e 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadProcessorProvider.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadProcessorProvider.java
@@ -13,7 +13,7 @@
 
 import org.apache.lucene.facet.index.params.CategoryListParams;
 import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.encoding.IntDecoder;
 import org.apache.lucene.util.encoding.IntEncoder;
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java
index 6cc9ca86..fb3df371 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java
@@ -2,9 +2,8 @@
 
 import java.io.Closeable;
 import java.io.IOException;
-import java.util.Map;
 
-import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.util.TwoPhaseCommit;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -52,7 +51,7 @@
  * 
  * @lucene.experimental
  */
-public interface TaxonomyWriter extends Closeable {
+public interface TaxonomyWriter extends Closeable, TwoPhaseCommit {
   
   /**
    * addCategory() adds a category with a given path name to the taxonomy,
@@ -66,32 +65,6 @@
    */ 
   public int addCategory(CategoryPath categoryPath) throws IOException;
   
-  /**
-   * Calling commit() ensures that all the categories written so far are
-   * visible to a reader that is opened (or reopened) after that call.
-   * When the index is closed(), commit() is also implicitly done. 
-   */
-  public void commit() throws IOException;
-
-  /**
-   * Like commit(), but also store properties with the index. These properties
-   * are retrievable by {@link TaxonomyReader#getCommitUserData}.
-   * See {@link IndexWriter#commit(Map)}. 
-   */
-  public void commit(Map<String,String> commitUserData) throws IOException;
-  
-  /**
-   * prepare most of the work needed for a two-phase commit.
-   * See {@link IndexWriter#prepareCommit}.
-   */
-  public void prepareCommit() throws IOException;
-  
-  /**
-   * Like above, and also prepares to store user data with the index.
-   * See {@link IndexWriter#prepareCommit(Map)}
-   */
-  public void prepareCommit(Map<String,String> commitUserData) throws IOException;
-  
   /**
    * getParent() returns the ordinal of the parent category of the category
    * with the given ordinal.
@@ -108,8 +81,8 @@
    * ordinal), an ArrayIndexOutOfBoundsException is thrown. However, it is
    * expected that getParent will only be called for ordinals which are
    * already known to be in the taxonomy.
-   * <P>
    * TODO (Facet): instead of a getParent(ordinal) method, consider having a
+   * <P>
    * getCategory(categorypath, prefixlen) which is similar to addCategory
    * except it doesn't add new categories; This method can be used to get
    * the ordinals of all prefixes of the given category, and it can use
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java
index 3f24d583..f009291b 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java
@@ -1 +1,71 @@
   + native
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+abstract class Consts {
+
+  static final String FULL = "$full_path$";
+  static final String FIELD_PAYLOADS = "$payloads$";
+  static final String PAYLOAD_PARENT = "p";
+  static final char[] PAYLOAD_PARENT_CHARS = PAYLOAD_PARENT.toCharArray();
+
+  /**
+   * The following is a "stored field visitor", an object
+   * which tells Lucene to extract only a single field
+   * rather than a whole document.
+   */
+  public static final class LoadFullPathOnly extends StoredFieldVisitor {
+    private String fullPath;
+
+    public boolean stringField(FieldInfo fieldInfo, IndexInput in, int numUTF8Bytes) throws IOException {
+      final byte[] bytes = new byte[numUTF8Bytes];
+      in.readBytes(bytes, 0, bytes.length);
+      fullPath = new String(bytes, "UTF-8");
+
+      // Stop loading:
+      return true;
+    }
+
+    public String getFullPath() {
+      return fullPath;
+    }
+  }
+
+  /**
+   * Delimiter used for creating the full path of a category from the list of
+   * its labels from root. It is forbidden for labels to contain this
+   * character.
+   * <P>
+   * Originally, we used \uFFFE, officially a "unicode noncharacter" (invalid
+   * unicode character) for this purpose. Recently, we switched to the
+   * "private-use" character \uF749.
+   */
+  //static final char DEFAULT_DELIMITER = '\uFFFE';
+  static final char DEFAULT_DELIMITER = '\uF749';
+  
+}
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
index 3f24d583..36f38b82 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
@@ -1 +1,575 @@
   + native
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.logging.Level;
+import java.util.logging.Logger;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.Consts.LoadFullPathOnly;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.collections.LRUHashMap;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link TaxonomyReader} which retrieves stored taxonomy information from a
+ * {@link Directory}.
+ * <P>
+ * Reading from the on-disk index on every method call is too slow, so this
+ * implementation employs caching: Some methods cache recent requests and their
+ * results, while other methods prefetch all the data into memory and then
+ * provide answers directly from in-memory tables. See the documentation of
+ * individual methods for comments on their performance.
+ * 
+ * @lucene.experimental
+ */
+public class DirectoryTaxonomyReader implements TaxonomyReader {
+
+  private static final Logger logger = Logger.getLogger(DirectoryTaxonomyReader.class.getName());
+  
+  private IndexReader indexReader;
+
+  // The following lock is used to allow multiple threads to read from the
+  // index concurrently, while having them block during the very short
+  // critical moment of refresh() (see comments below). Note, however, that
+  // we only read from the index when we don't have the entry in our cache,
+  // and the caches are locked separately.
+  private ReadWriteLock indexReaderLock = new ReentrantReadWriteLock();
+
+  // The following are the limited-size LRU caches used to cache the latest
+  // results from getOrdinal() and getLabel().
+  // Because LRUHashMap is not thread-safe, we need to synchronize on this
+  // object when using it. Unfortunately, this is not optimal under heavy
+  // contention because it means that while one thread is using the cache
+  // (reading or modifying) others are blocked from using it - or even
+  // starting to do benign things like calculating the hash function. A more
+  // efficient approach would be to use a non-locking (as much as possible)
+  // concurrent solution, along the lines of java.util.concurrent.ConcurrentHashMap
+  // but with LRU semantics.
+  // However, even in the current sub-optimal implementation we do not make
+  // the mistake of locking out readers while waiting for disk in a cache
+  // miss - below, we do not hold cache lock while reading missing data from
+  // disk.
+  private final LRUHashMap<String, Integer> ordinalCache;
+  private final LRUHashMap<Integer, String> categoryCache;
+
+  // getParent() needs to be extremely efficient, to the point that we need
+  // to fetch all the data in advance into memory, and answer these calls
+  // from memory. Currently we use a large integer array, which is
+  // initialized when the taxonomy is opened, and potentially enlarged
+  // when it is refresh()ed.
+  // These arrays are not syncrhonized. Rather, the reference to the array
+  // is volatile, and the only writing operation (refreshPrefetchArrays)
+  // simply creates a new array and replaces the reference. The volatility
+  // of the reference ensures the correct atomic replacement and its
+  // visibility properties (the content of the array is visible when the
+  // new reference is visible).
+  private ParentArray parentArray;
+
+  private char delimiter = Consts.DEFAULT_DELIMITER;
+
+  private volatile boolean closed = false;
+  
+  /**
+   * Open for reading a taxonomy stored in a given {@link Directory}.
+   * @param directory
+   *    The {@link Directory} in which to the taxonomy lives. Note that
+   *    the taxonomy is read directly to that directory (not from a
+   *    subdirectory of it).
+   * @throws CorruptIndexException if the Taxonomy is corrupted.
+   * @throws IOException if another error occurred.
+   */
+  public DirectoryTaxonomyReader(Directory directory) throws IOException {
+    this.indexReader = openIndexReader(directory);
+
+    // These are the default cache sizes; they can be configured after
+    // construction with the cache's setMaxSize() method
+    ordinalCache = new LRUHashMap<String, Integer>(4000);
+    categoryCache = new LRUHashMap<Integer, String>(4000);
+
+    // TODO (Facet): consider lazily create parent array when asked, not in the constructor
+    parentArray = new ParentArray();
+    parentArray.refresh(indexReader);
+  }
+
+  protected IndexReader openIndexReader(Directory directory) throws CorruptIndexException, IOException {
+    return IndexReader.open(directory);
+  }
+
+  /**
+   * @throws AlreadyClosedException if this IndexReader is closed
+   */
+  protected final void ensureOpen() throws AlreadyClosedException {
+    if (indexReader.getRefCount() <= 0) {
+      throw new AlreadyClosedException("this TaxonomyReader is closed");
+    }
+  }
+  
+  /**
+   * setCacheSize controls the maximum allowed size of each of the caches
+   * used by {@link #getPath(int)} and {@link #getOrdinal(CategoryPath)}.
+   * <P>
+   * Currently, if the given size is smaller than the current size of
+   * a cache, it will not shrink, and rather we be limited to its current
+   * size.
+   * @param size the new maximum cache size, in number of entries.
+   */
+  public void setCacheSize(int size) {
+    ensureOpen();
+    synchronized(categoryCache) {
+      categoryCache.setMaxSize(size);
+    }
+    synchronized(ordinalCache) {
+      ordinalCache.setMaxSize(size);
+    }
+  }
+
+  /**
+   * setDelimiter changes the character that the taxonomy uses in its
+   * internal storage as a delimiter between category components. Do not
+   * use this method unless you really know what you are doing.
+   * <P>
+   * If you do use this method, make sure you call it before any other
+   * methods that actually queries the taxonomy. Moreover, make sure you
+   * always pass the same delimiter for all LuceneTaxonomyWriter and
+   * LuceneTaxonomyReader objects you create.
+   */
+  public void setDelimiter(char delimiter) {
+    ensureOpen();
+    this.delimiter = delimiter;
+  }
+
+  public int getOrdinal(CategoryPath categoryPath) throws IOException {
+    ensureOpen();
+    if (categoryPath.length()==0) {
+      return ROOT_ORDINAL;
+    }
+    String path = categoryPath.toString(delimiter);
+
+    // First try to find the answer in the LRU cache:
+    synchronized(ordinalCache) {
+      Integer res = ordinalCache.get(path);
+      if (res!=null) {
+        return res.intValue();
+      }
+    }
+
+    // If we're still here, we have a cache miss. We need to fetch the
+    // value from disk, and then also put it in the cache:
+    int ret = TaxonomyReader.INVALID_ORDINAL;
+    try {
+      indexReaderLock.readLock().lock();
+      // TODO (Facet): avoid Multi*?
+      Bits liveDocs = MultiFields.getLiveDocs(indexReader);
+      DocsEnum docs = MultiFields.getTermDocsEnum(indexReader, liveDocs, Consts.FULL, new BytesRef(path));
+      if (docs != null && docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+        ret = docs.docID();
+      }
+    } finally {
+      indexReaderLock.readLock().unlock();
+    }
+
+    // Put the new value in the cache. Note that it is possible that while
+    // we were doing the above fetching (without the cache locked), some
+    // other thread already added the same category to the cache. We do
+    // not care about this possibilty, as LRUCache replaces previous values
+    // of the same keys (it doesn't store duplicates).
+    synchronized(ordinalCache) {
+      // GB: new Integer(int); creates a new object each and every time.
+      // Integer.valueOf(int) might not (See JavaDoc). 
+      ordinalCache.put(path, Integer.valueOf(ret));
+    }
+
+    return ret;
+  }
+
+  public CategoryPath getPath(int ordinal) throws CorruptIndexException, IOException {
+    ensureOpen();
+    // TODO (Facet): Currently, the LRU cache we use (getCategoryCache) holds
+    // strings with delimiters, not CategoryPath objects, so even if
+    // we have a cache hit, we need to process the string and build a new
+    // CategoryPath object every time. What is preventing us from putting
+    // the actual CategoryPath object in the cache is the fact that these
+    // objects are mutable. So we should create an immutable (read-only)
+    // interface that CategoryPath implements, and this method should
+    // return this interface, not the writable CategoryPath.
+    String label = getLabel(ordinal);
+    if (label==null) {
+      return null;  
+    }
+    return new CategoryPath(label, delimiter);
+  }
+
+  public boolean getPath(int ordinal, CategoryPath result) throws CorruptIndexException, IOException {
+    ensureOpen();
+    String label = getLabel(ordinal);
+    if (label==null) {
+      return false;
+    }
+    result.clear();
+    result.add(label, delimiter);
+    return true;
+  }
+
+  private String getLabel(int catID) throws CorruptIndexException, IOException {
+    ensureOpen();
+    // First try to find the answer in the LRU cache. It is very
+    // unfortunate that we need to allocate an Integer object here -
+    // it would have been better if we used a hash table specifically
+    // designed for int keys...
+    // GB: new Integer(int); creates a new object each and every time.
+    // Integer.valueOf(int) might not (See JavaDoc). 
+    Integer catIDInteger = Integer.valueOf(catID);
+
+    synchronized(categoryCache) {
+      String res = categoryCache.get(catIDInteger);
+      if (res!=null) {
+        return res;
+      }
+    }
+
+    // If we're still here, we have a cache miss. We need to fetch the
+    // value from disk, and then also put it in the cache:
+    String ret;
+    try {
+      indexReaderLock.readLock().lock();
+      // The taxonomy API dictates that if we get an invalid category
+      // ID, we should return null, If we don't check this here, we
+      // can some sort of an exception from the document() call below.
+      // NOTE: Currently, we *do not* cache this return value; There
+      // isn't much point to do so, because checking the validity of
+      // the docid doesn't require disk access - just comparing with
+      // the number indexReader.maxDoc().
+      if (catID<0 || catID>=indexReader.maxDoc()) {
+        return null;
+      }
+      final LoadFullPathOnly loader = new LoadFullPathOnly();
+      indexReader.document(catID, loader);
+      ret = loader.getFullPath();
+    } finally {
+      indexReaderLock.readLock().unlock();
+    }
+    // Put the new value in the cache. Note that it is possible that while
+    // we were doing the above fetching (without the cache locked), some
+    // other thread already added the same category to the cache. We do
+    // not care about this possibility, as LRUCache replaces previous
+    // values of the same keys (it doesn't store duplicates).
+    synchronized (categoryCache) {
+      categoryCache.put(catIDInteger, ret);
+    }
+
+    return ret;
+  }
+
+  public int getParent(int ordinal) {
+    ensureOpen();
+    // Note how we don't need to hold the read lock to do the following,
+    // because the array reference is volatile, ensuring the correct
+    // visibility and ordering: if we get the new reference, the new
+    // data is also visible to this thread.
+    return getParentArray()[ordinal];
+  }
+
+  /**
+   * getParentArray() returns an int array of size getSize() listing the
+   * ordinal of the parent category of each category in the taxonomy.
+   * <P>
+   * The caller can hold on to the array it got indefinitely - it is
+   * guaranteed that no-one else will modify it. The other side of the
+   * same coin is that the caller must treat the array it got as read-only
+   * and <B>not modify it</B>, because other callers might have gotten the
+   * same array too, and getParent() calls are also answered from the
+   * same array.
+   * <P>
+   * The getParentArray() call is extremely efficient, merely returning
+   * a reference to an array that already exists. For a caller that plans
+   * to call getParent() for many categories, using getParentArray() and
+   * the array it returns is a somewhat faster approach because it avoids
+   * the overhead of method calls and volatile dereferencing.
+   * <P>
+   * If you use getParentArray() instead of getParent(), remember that
+   * the array you got is (naturally) not modified after a refresh(),
+   * so you should always call getParentArray() again after a refresh().
+   */
+
+  public int[] getParentArray() {
+    ensureOpen();
+    // Note how we don't need to hold the read lock to do the following,
+    // because the array reference is volatile, ensuring the correct
+    // visibility and ordering: if we get the new reference, the new
+    // data is also visible to this thread.
+    return parentArray.getArray();
+  }
+
+  // Note that refresh() is synchronized (it is the only synchronized
+  // method in this class) to ensure that it never gets called concurrently
+  // with itself.
+  public synchronized void refresh() throws IOException {
+    ensureOpen();
+    /*
+     * Since refresh() can be a lengthy operation, it is very important that we
+     * avoid locking out all readers for its duration. This is why we don't hold
+     * the indexReaderLock write lock for the entire duration of this method. In
+     * fact, it is enough to hold it only during a single assignment! Other
+     * comments in this method will explain this.
+     */
+
+    // note that the lengthy operation indexReader.reopen() does not
+    // modify the reader, so we can do it without holding a lock. We can
+    // safely read indexReader without holding the write lock, because
+    // no other thread can be writing at this time (this method is the
+    // only possible writer, and it is "synchronized" to avoid this case).
+    IndexReader r2 = IndexReader.openIfChanged(indexReader);
+    if (r2 != null) {
+      IndexReader oldreader = indexReader;
+      // we can close the old searcher, but need to synchronize this
+      // so that we don't close it in the middle that another routine
+      // is reading from it.
+      indexReaderLock.writeLock().lock();
+      indexReader = r2;
+      indexReaderLock.writeLock().unlock();
+      // We can close the old reader, but need to be certain that we
+      // don't close it while another method is reading from it.
+      // Luckily, we can be certain of that even without putting the
+      // oldreader.close() in the locked section. The reason is that
+      // after lock() succeeded above, we know that all existing readers
+      // had finished (this is what a read-write lock ensures). New
+      // readers, starting after the unlock() we just did, already got
+      // the new indexReader we set above. So nobody can be possibly
+      // using the old indexReader, and we can close it:
+      oldreader.close();
+
+      // We prefetch some of the arrays to make requests much faster.
+      // Let's refresh these prefetched arrays; This refresh is much
+      // is made more efficient by assuming that it is enough to read
+      // the values for new categories (old categories could not have been
+      // changed or deleted)
+      // Note that this this done without the write lock being held,
+      // which means that it is possible that during a refresh(), a
+      // reader will have some methods (like getOrdinal and getCategory)
+      // return fresh information, while getParent()
+      // (only to be prefetched now) still return older information.
+      // We consider this to be acceptable. The important thing,
+      // however, is that refreshPrefetchArrays() itself writes to
+      // the arrays in a correct manner (see discussion there)
+      parentArray.refresh(indexReader);
+
+      // Remove any INVALID_ORDINAL values from the ordinal cache,
+      // because it is possible those are now answered by the new data!
+      Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();
+      while (i.hasNext()) {
+        Entry<String, Integer> e = i.next();
+        if (e.getValue().intValue() == INVALID_ORDINAL) {
+          i.remove();
+        }
+      }
+    }
+  }
+
+  public void close() throws IOException {
+    if (!closed) {
+      decRef();
+      closed = true;
+    }
+  }
+  
+  /** Do the actual closing, free up resources */
+  private void doClose() throws IOException {
+    indexReader.close();
+    closed = true;
+
+    parentArray = null;
+    childrenArrays = null;
+    categoryCache.clear();
+    ordinalCache.clear();
+  }
+
+  public int getSize() {
+    ensureOpen();
+    indexReaderLock.readLock().lock();
+    try {
+      return indexReader.numDocs();
+    } finally {
+      indexReaderLock.readLock().unlock();
+    }
+  }
+
+  public Map<String, String> getCommitUserData() {
+    ensureOpen();
+    return indexReader.getCommitUserData();
+  }
+  
+  private ChildrenArrays childrenArrays;
+  Object childrenArraysRebuild = new Object();
+
+  public ChildrenArrays getChildrenArrays() {
+    ensureOpen();
+    // Check if the taxonomy grew since we built the array, and if it
+    // did, create new (and larger) arrays and fill them as required.
+    // We do all this under a lock, two prevent to concurrent calls to
+    // needlessly do the same array building at the same time.
+    synchronized(childrenArraysRebuild) {
+      int num = getSize();
+      int first;
+      if (childrenArrays==null) {
+        first = 0;
+      } else {
+        first = childrenArrays.getYoungestChildArray().length;
+      }
+      // If the taxonomy hasn't grown, we can return the existing object
+      // immediately
+      if (first == num) {
+        return childrenArrays;
+      }
+      // Otherwise, build new arrays for a new ChildrenArray object.
+      // These arrays start with an enlarged copy of the previous arrays,
+      // and then are modified to take into account the new categories:
+      int[] newYoungestChildArray = new int[num];
+      int[] newOlderSiblingArray = new int[num];
+      // In Java 6, we could just do Arrays.copyOf()...
+      if (childrenArrays!=null) {
+        System.arraycopy(childrenArrays.getYoungestChildArray(), 0,
+            newYoungestChildArray, 0, childrenArrays.getYoungestChildArray().length);
+        System.arraycopy(childrenArrays.getOlderSiblingArray(), 0,
+            newOlderSiblingArray, 0, childrenArrays.getOlderSiblingArray().length);
+      }
+      int[] parents = getParentArray();
+      for (int i=first; i<num; i++) {
+        newYoungestChildArray[i] = INVALID_ORDINAL;
+      }
+      // In the loop below we can ignore the root category (0) because
+      // it has no parent
+      if (first==0) {
+        first = 1;
+        newOlderSiblingArray[0] = INVALID_ORDINAL;
+      }
+      for (int i=first; i<num; i++) {
+        // Note that parents[i] is always < i, so the right-hand-side of
+        // the following line is already set when we get here.
+        newOlderSiblingArray[i] = newYoungestChildArray[parents[i]];
+        newYoungestChildArray[parents[i]] = i;
+      }
+      // Finally switch to the new arrays
+      childrenArrays = new ChildrenArraysImpl(newYoungestChildArray,
+          newOlderSiblingArray);
+      return childrenArrays;
+    }
+  }
+
+  public String toString(int max) {
+    ensureOpen();
+    StringBuilder sb = new StringBuilder();
+    int upperl = Math.min(max, this.indexReader.maxDoc());
+    for (int i = 0; i < upperl; i++) {
+      try {
+        CategoryPath category = this.getPath(i);
+        if (category == null) {
+          sb.append(i + ": NULL!! \n");
+          continue;
+        } 
+        if (category.length() == 0) {
+          sb.append(i + ": EMPTY STRING!! \n");
+          continue;
+        }
+        sb.append(i +": "+category.toString()+"\n");
+      } catch (IOException e) {
+        if (logger.isLoggable(Level.FINEST)) {
+          logger.log(Level.FINEST, e.getMessage(), e);
+        }
+      }
+    }
+    return sb.toString();
+  }
+
+  private static final class ChildrenArraysImpl implements ChildrenArrays {
+    private int[] youngestChildArray, olderSiblingArray;
+    public ChildrenArraysImpl(int[] youngestChildArray, int[] olderSiblingArray) {
+      this.youngestChildArray = youngestChildArray;
+      this.olderSiblingArray = olderSiblingArray;
+    }
+    public int[] getOlderSiblingArray() {
+      return olderSiblingArray;
+    }
+    public int[] getYoungestChildArray() {
+      return youngestChildArray;
+    }    
+  }
+
+  /**
+   * Expert:  This method is only for expert use.
+   * Note also that any call to refresh() will invalidate the returned reader,
+   * so the caller needs to take care of appropriate locking.
+   * 
+   * @return lucene indexReader
+   */
+  IndexReader getInternalIndexReader() {
+    ensureOpen();
+    return this.indexReader;
+  }
+
+  /**
+   * Expert: decreases the refCount of this TaxonomyReader instance. 
+   * If the refCount drops to 0, then pending changes (if any) are 
+   * committed to the taxonomy index and this reader is closed. 
+   * @throws IOException 
+   */
+  public void decRef() throws IOException {
+    ensureOpen();
+    if (indexReader.getRefCount() == 1) {
+      // Do not decRef the indexReader - doClose does it by calling reader.close()
+      doClose();
+    } else {
+      indexReader.decRef();
+    }
+  }
+  
+  /**
+   * Expert: returns the current refCount for this taxonomy reader
+   */
+  public int getRefCount() {
+    ensureOpen();
+    return this.indexReader.getRefCount();
+  }
+  
+  /**
+   * Expert: increments the refCount of this TaxonomyReader instance. 
+   * RefCounts are used to determine when a taxonomy reader can be closed 
+   * safely, i.e. as soon as there are no more references. 
+   * Be sure to always call a corresponding decRef(), in a finally clause; 
+   * otherwise the reader may never be closed. 
+   */
+  public void incRef() {
+    ensureOpen();
+    this.indexReader.incRef();
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
index 3f24d583..e81fec30 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
@@ -1 +1,1019 @@
   + native
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.analysis.core.KeywordAnalyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.LogByteSizeMergePolicy;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.store.NativeFSLockFactory;
+import org.apache.lucene.store.SimpleFSLockFactory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Version;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.lru.LruTaxonomyWriterCache;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * {@link TaxonomyWriter} which uses a {@link Directory} to store the taxonomy
+ * information on disk, and keeps an additional in-memory cache of some or all
+ * categories.
+ * <p>
+ * In addition to the permanently-stored information in the {@link Directory},
+ * efficiency dictates that we also keep an in-memory cache of <B>recently
+ * seen</B> or <B>all</B> categories, so that we do not need to go back to disk
+ * for every category addition to see which ordinal this category already has,
+ * if any. A {@link TaxonomyWriterCache} object determines the specific caching
+ * algorithm used.
+ * <p>
+ * This class offers some hooks for extending classes to control the
+ * {@link IndexWriter} instance that is used. See {@link #openIndexWriter}.
+ * 
+ * @lucene.experimental
+ */
+public class DirectoryTaxonomyWriter implements TaxonomyWriter {
+
+  private IndexWriter indexWriter;
+  private int nextID;
+  private char delimiter = Consts.DEFAULT_DELIMITER;
+  private SinglePositionTokenStream parentStream = new SinglePositionTokenStream(Consts.PAYLOAD_PARENT);
+  private Field parentStreamField;
+  private Field fullPathField;
+
+  private TaxonomyWriterCache cache;
+  /**
+   * We call the cache "complete" if we know that every category in our
+   * taxonomy is in the cache. When the cache is <B>not</B> complete, and
+   * we can't find a category in the cache, we still need to look for it
+   * in the on-disk index; Therefore when the cache is not complete, we
+   * need to open a "reader" to the taxonomy index.
+   * The cache becomes incomplete if it was never filled with the existing
+   * categories, or if a put() to the cache ever returned true (meaning
+   * that some of the cached data was cleared).
+   */
+  private boolean cacheIsComplete;
+  private IndexReader reader;
+  private int cacheMisses;
+
+  /**
+   * setDelimiter changes the character that the taxonomy uses in its internal
+   * storage as a delimiter between category components. Do not use this
+   * method unless you really know what you are doing. It has nothing to do
+   * with whatever character the application may be using to represent
+   * categories for its own use.
+   * <P>
+   * If you do use this method, make sure you call it before any other methods
+   * that actually queries the taxonomy. Moreover, make sure you always pass
+   * the same delimiter for all LuceneTaxonomyWriter and LuceneTaxonomyReader
+   * objects you create for the same directory.
+   */
+  public void setDelimiter(char delimiter) {
+    this.delimiter = delimiter;
+  }
+
+  /**
+   * Forcibly unlocks the taxonomy in the named directory.
+   * <P>
+   * Caution: this should only be used by failure recovery code, when it is
+   * known that no other process nor thread is in fact currently accessing
+   * this taxonomy.
+   * <P>
+   * This method is unnecessary if your {@link Directory} uses a
+   * {@link NativeFSLockFactory} instead of the default
+   * {@link SimpleFSLockFactory}. When the "native" lock is used, a lock
+   * does not stay behind forever when the process using it dies. 
+   */
+  public static void unlock(Directory directory) throws IOException {
+    IndexWriter.unlock(directory);
+  }
+
+  /**
+   * Construct a Taxonomy writer.
+   * 
+   * @param directory
+   *    The {@link Directory} in which to store the taxonomy. Note that
+   *    the taxonomy is written directly to that directory (not to a
+   *    subdirectory of it).
+   * @param openMode
+   *    Specifies how to open a taxonomy for writing: <code>APPEND</code>
+   *    means open an existing index for append (failing if the index does
+   *    not yet exist). <code>CREATE</code> means create a new index (first
+   *    deleting the old one if it already existed).
+   *    <code>APPEND_OR_CREATE</code> appends to an existing index if there
+   *    is one, otherwise it creates a new index.
+   * @param cache
+   *    A {@link TaxonomyWriterCache} implementation which determines
+   *    the in-memory caching policy. See for example
+   *    {@link LruTaxonomyWriterCache} and {@link Cl2oTaxonomyWriterCache}.
+   *    If null or missing, {@link #defaultTaxonomyWriterCache()} is used.
+   * @throws CorruptIndexException
+   *     if the taxonomy is corrupted.
+   * @throws LockObtainFailedException
+   *     if the taxonomy is locked by another writer. If it is known
+   *     that no other concurrent writer is active, the lock might
+   *     have been left around by an old dead process, and should be
+   *     removed using {@link #unlock(Directory)}.
+   * @throws IOException
+   *     if another error occurred.
+   */
+  public DirectoryTaxonomyWriter(Directory directory, OpenMode openMode,
+                              TaxonomyWriterCache cache)
+  throws CorruptIndexException, LockObtainFailedException,
+  IOException {
+
+    indexWriter = openIndexWriter(directory, openMode);
+    reader = null;
+
+    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);
+    ft.setOmitNorms(true);
+    parentStreamField = new Field(Consts.FIELD_PAYLOADS, parentStream, ft);
+    fullPathField = new Field(Consts.FULL, "", StringField.TYPE_STORED);
+
+    this.nextID = indexWriter.maxDoc();
+
+    if (cache==null) {
+      cache = defaultTaxonomyWriterCache();
+    }
+    this.cache = cache;
+
+    if (nextID == 0) {
+      cacheIsComplete = true;
+      // Make sure that the taxonomy always contain the root category
+      // with category id 0.
+      addCategory(new CategoryPath());
+      refreshReader();
+    } else {
+      // There are some categories on the disk, which we have not yet
+      // read into the cache, and therefore the cache is incomplete.
+      // We chose not to read all the categories into the cache now,
+      // to avoid terrible performance when a taxonomy index is opened
+      // to add just a single category. We will do it later, after we
+      // notice a few cache misses.
+      cacheIsComplete = false;
+    }
+    cacheMisses = 0;
+  }
+
+  /**
+   * A hook for extensions of this class to provide their own
+   * {@link IndexWriter} implementation or instance. Extending classes can
+   * instantiate and configure the {@link IndexWriter} as they see fit,
+   * including setting a {@link org.apache.lucene.index.MergeScheduler}, or
+   * {@link org.apache.lucene.index.IndexDeletionPolicy}, different RAM size
+   * etc.<br>
+   * <b>NOTE:</b> the instance this method returns will be closed upon calling
+   * to {@link #close()}.
+   * 
+   * @param directory
+   *          the {@link Directory} on top of which an {@link IndexWriter}
+   *          should be opened.
+   * @param openMode
+   *          see {@link OpenMode}
+   */
+  protected IndexWriter openIndexWriter(Directory directory, OpenMode openMode)
+      throws IOException {
+    // Make sure we use a MergePolicy which merges segments in-order and thus
+    // keeps the doc IDs ordered as well (this is crucial for the taxonomy
+    // index).
+    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_40,
+        new KeywordAnalyzer()).setOpenMode(openMode).setMergePolicy(
+        new LogByteSizeMergePolicy());
+    return new IndexWriter(directory, config);
+  }
+
+  // Currently overridden by a unit test that verifies that every index we open
+  // is close()ed.
+  /**
+   * Open an {@link IndexReader} from the {@link #indexWriter} member, by
+   * calling {@link IndexWriter#getReader()}. Extending classes can override
+   * this method to return their own {@link IndexReader}.
+   */
+  protected IndexReader openReader() throws IOException {
+    return IndexReader.open(indexWriter, true); 
+  }
+
+  /**
+   * Creates a new instance with a default cached as defined by
+   * {@link #defaultTaxonomyWriterCache()}.
+   */
+  public DirectoryTaxonomyWriter(Directory directory, OpenMode openMode)
+  throws CorruptIndexException, LockObtainFailedException, IOException {
+    this(directory, openMode, defaultTaxonomyWriterCache());
+  }
+
+  /**
+   * Defines the default {@link TaxonomyWriterCache} to use in constructors
+   * which do not specify one.
+   * <P>  
+   * The current default is {@link Cl2oTaxonomyWriterCache} constructed
+   * with the parameters (1024, 0.15f, 3), i.e., the entire taxonomy is
+   * cached in memory while building it.
+   */
+  public static TaxonomyWriterCache defaultTaxonomyWriterCache() {
+    return new Cl2oTaxonomyWriterCache(1024, 0.15f, 3);
+  }
+
+  // convenience constructors:
+
+  public DirectoryTaxonomyWriter(Directory d)
+  throws CorruptIndexException, LockObtainFailedException,
+  IOException {
+    this(d, OpenMode.CREATE_OR_APPEND);
+  }
+
+  /**
+   * Frees used resources as well as closes the underlying {@link IndexWriter},
+   * which commits whatever changes made to it to the underlying
+   * {@link Directory}.
+   */
+  @Override
+  public synchronized void close() throws CorruptIndexException, IOException {
+    if (indexWriter != null) {
+      indexWriter.close();
+      indexWriter = null;
+    }
+
+    closeResources();
+  }
+
+  /**
+   * Returns the number of memory bytes used by the cache.
+   * @return Number of cache bytes in memory, for CL2O only; zero otherwise.
+   */
+  public int getCacheMemoryUsage() {
+    if (this.cache == null || !(this.cache instanceof Cl2oTaxonomyWriterCache)) {
+      return 0;
+    }
+    return ((Cl2oTaxonomyWriterCache)this.cache).getMemoryUsage();
+  }
+
+  /**
+   * A hook for extending classes to close additional resources that were used.
+   * The default implementation closes the {@link IndexReader} as well as the
+   * {@link TaxonomyWriterCache} instances that were used. <br>
+   * <b>NOTE:</b> if you override this method, you should include a
+   * <code>super.closeResources()</code> call in your implementation.
+   */
+  protected synchronized void closeResources() throws IOException {
+    if (reader != null) {
+      reader.close();
+      reader = null;
+    }
+    if (cache != null) {
+      cache.close();
+      cache = null;
+    }
+  }
+
+  /**
+   * Look up the given category in the cache and/or the on-disk storage,
+   * returning the category's ordinal, or a negative number in case the
+   * category does not yet exist in the taxonomy.
+   */
+  protected int findCategory(CategoryPath categoryPath) throws IOException {
+    // If we can find the category in our cache, we can return the
+    // response directly from it:
+    int res = cache.get(categoryPath);
+    if (res >= 0) {
+      return res;
+    }
+    // If we know that the cache is complete, i.e., contains every category
+    // which exists, we can return -1 immediately. However, if the cache is
+    // not complete, we need to check the disk.
+    if (cacheIsComplete) {
+      return -1;
+    }
+    cacheMisses++;
+    // After a few cache misses, it makes sense to read all the categories
+    // from disk and into the cache. The reason not to do this on the first
+    // cache miss (or even when opening the writer) is that it will
+    // significantly slow down the case when a taxonomy is opened just to
+    // add one category. The idea only spending a long time on reading
+    // after enough time was spent on cache misses is known as a "online
+    // algorithm".
+    if (perhapsFillCache()) {
+      return cache.get(categoryPath);
+    }
+
+    // We need to get an answer from the on-disk index. If a reader
+    // is not yet open, do it now:
+    if (reader == null) {
+      reader = openReader();
+    }
+
+    // TODO (Facet): avoid Multi*?
+    Bits liveDocs = MultiFields.getLiveDocs(reader);
+    DocsEnum docs = MultiFields.getTermDocsEnum(reader, liveDocs, Consts.FULL, 
+        new BytesRef(categoryPath.toString(delimiter)));
+    if (docs == null || docs.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
+      return -1; // category does not exist in taxonomy
+    }
+    // Note: we do NOT add to the cache the fact that the category
+    // does not exist. The reason is that our only use for this
+    // method is just before we actually add this category. If
+    // in the future this usage changes, we should consider caching
+    // the fact that the category is not in the taxonomy.
+    addToCache(categoryPath, docs.docID());
+    return docs.docID();
+  }
+
+  /**
+   * Look up the given prefix of the given category in the cache and/or the
+   * on-disk storage, returning that prefix's ordinal, or a negative number in
+   * case the category does not yet exist in the taxonomy.
+   */
+  private int findCategory(CategoryPath categoryPath, int prefixLen)
+  throws IOException {
+    int res = cache.get(categoryPath, prefixLen);
+    if (res >= 0) {
+      return res;
+    }
+    if (cacheIsComplete) {
+      return -1;
+    }
+    cacheMisses++;
+    if (perhapsFillCache()) {
+      return cache.get(categoryPath, prefixLen);
+    }
+    if (reader == null) {
+      reader = openReader();
+    }
+    Bits liveDocs = MultiFields.getLiveDocs(reader);
+    DocsEnum docs = MultiFields.getTermDocsEnum(reader, liveDocs, Consts.FULL, 
+        new BytesRef(categoryPath.toString(delimiter, prefixLen)));
+    if (docs == null || docs.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
+      return -1; // category does not exist in taxonomy
+    }
+    addToCache(categoryPath, prefixLen, docs.docID());
+    return docs.docID();
+  }
+
+  // TODO (Facet): addCategory() is synchronized. This means that if indexing is
+  // multi-threaded, a new category that needs to be written to disk (and
+  // potentially even trigger a lengthy merge) locks out other addCategory()
+  // calls - even those which could immediately return a cached value.
+  // We definitely need to fix this situation!
+  @Override
+  public synchronized int addCategory(CategoryPath categoryPath)
+  throws IOException {
+    // If the category is already in the cache and/or the taxonomy, we
+    // should return its existing ordinal:
+    int res = findCategory(categoryPath);
+    if (res < 0) {
+      // This is a new category, and we need to insert it into the index
+      // (and the cache). Actually, we might also need to add some of
+      // the category's ancestors before we can add the category itself
+      // (while keeping the invariant that a parent is always added to
+      // the taxonomy before its child). internalAddCategory() does all
+      // this recursively:
+      res = internalAddCategory(categoryPath, categoryPath.length());
+    }
+    return res;
+
+  }
+
+  /**
+   * Add a new category into the index (and the cache), and return its new
+   * ordinal.
+   * <P>
+   * Actually, we might also need to add some of the category's ancestors
+   * before we can add the category itself (while keeping the invariant that a
+   * parent is always added to the taxonomy before its child). We do this by
+   * recursion.
+   */
+  private int internalAddCategory(CategoryPath categoryPath, int length)
+  throws CorruptIndexException, IOException {
+
+    // Find our parent's ordinal (recursively adding the parent category
+    // to the taxonomy if it's not already there). Then add the parent
+    // ordinal as payloads (rather than a stored field; payloads can be
+    // more efficiently read into memory in bulk by LuceneTaxonomyReader)
+    int parent;
+    if (length > 1) {
+      parent = findCategory(categoryPath, length - 1);
+      if (parent < 0) {
+        parent = internalAddCategory(categoryPath, length - 1);
+      }
+    } else if (length == 1) {
+      parent = TaxonomyReader.ROOT_ORDINAL;
+    } else {
+      parent = TaxonomyReader.INVALID_ORDINAL;
+    }
+    int id = addCategoryDocument(categoryPath, length, parent);
+
+    return id;
+  }
+
+  // Note that the methods calling addCategoryDocument() are synchornized,
+  // so this method is effectively synchronized as well, but we'll add
+  // synchronized to be on the safe side, and we can reuse class-local objects
+  // instead of allocating them every time
+  protected synchronized int addCategoryDocument(CategoryPath categoryPath,
+                                                  int length, int parent)
+      throws CorruptIndexException, IOException {
+    // Before Lucene 2.9, position increments >=0 were supported, so we
+    // added 1 to parent to allow the parent -1 (the parent of the root).
+    // Unfortunately, starting with Lucene 2.9, after LUCENE-1542, this is
+    // no longer enough, since 0 is not encoded consistently either (see
+    // comment in SinglePositionTokenStream). But because we must be
+    // backward-compatible with existing indexes, we can't just fix what
+    // we write here (e.g., to write parent+2), and need to do a workaround
+    // in the reader (which knows that anyway only category 0 has a parent
+    // -1).    
+    parentStream.set(parent+1);
+    Document d = new Document();
+    d.add(parentStreamField);
+
+    fullPathField.setValue(categoryPath.toString(delimiter, length));
+    d.add(fullPathField);
+
+    // Note that we do no pass an Analyzer here because the fields that are
+    // added to the Document are untokenized or contains their own TokenStream.
+    // Therefore the IndexWriter's Analyzer has no effect.
+    indexWriter.addDocument(d);
+    int id = nextID++;
+
+    addToCache(categoryPath, length, id);
+
+    // also add to the parent array
+    getParentArray().add(id, parent);
+
+    return id;
+  }
+
+  private static class SinglePositionTokenStream extends TokenStream {
+    private CharTermAttribute termAtt;
+    private PositionIncrementAttribute posIncrAtt;
+    private boolean returned;
+    public SinglePositionTokenStream(String word) {
+      termAtt = addAttribute(CharTermAttribute.class);
+      posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+      termAtt.setEmpty().append(word);
+      returned = true;
+    }
+    /**
+     * Set the value we want to keep, as the position increment.
+     * Note that when TermPositions.nextPosition() is later used to
+     * retrieve this value, val-1 will be returned, not val.
+     * <P>
+     * IMPORTANT NOTE: Before Lucene 2.9, val>=0 were safe (for val==0,
+     * the retrieved position would be -1). But starting with Lucene 2.9,
+     * this unfortunately changed, and only val>0 are safe. val=0 can
+     * still be used, but don't count on the value you retrieve later
+     * (it could be 0 or -1, depending on circumstances or versions).
+     * This change is described in Lucene's JIRA: LUCENE-1542. 
+     */
+    public void set(int val) {
+      posIncrAtt.setPositionIncrement(val);
+      returned = false;
+    }
+    @Override
+    public boolean incrementToken() throws IOException {
+      if (returned) {
+        return false;
+      }
+      returned = true;
+      return true;
+    }
+  }
+
+  private void addToCache(CategoryPath categoryPath, int id)
+  throws CorruptIndexException, IOException {
+    if (cache.put(categoryPath, id)) {
+      // If cache.put() returned true, it means the cache was limited in
+      // size, became full, so parts of it had to be cleared.
+      // Unfortunately we don't know which part was cleared - it is
+      // possible that a relatively-new category that hasn't yet been
+      // committed to disk (and therefore isn't yet visible in our
+      // "reader") was deleted from the cache, and therefore we must
+      // now refresh the reader.
+      // Because this is a slow operation, cache implementations are
+      // expected not to delete entries one-by-one but rather in bulk
+      // (LruTaxonomyWriterCache removes the 2/3rd oldest entries).
+      refreshReader();
+      cacheIsComplete = false;
+    }
+  }
+
+  private void addToCache(CategoryPath categoryPath, int prefixLen, int id)
+  throws CorruptIndexException, IOException {
+    if (cache.put(categoryPath, prefixLen, id)) {
+      refreshReader();
+      cacheIsComplete = false;
+    }
+  }
+
+  private synchronized void refreshReader() throws IOException {
+    if (reader != null) {
+      IndexReader r2 = IndexReader.openIfChanged(reader);
+      if (r2 != null) {
+        reader.close();
+        reader = r2;
+      }
+    }
+  }
+  
+  /**
+   * Calling commit() ensures that all the categories written so far are
+   * visible to a reader that is opened (or reopened) after that call.
+   * When the index is closed(), commit() is also implicitly done.
+   * See {@link TaxonomyWriter#commit()}
+   */ 
+  @Override
+  public synchronized void commit() throws CorruptIndexException, IOException {
+    indexWriter.commit();
+    refreshReader();
+  }
+
+  /**
+   * Like commit(), but also store properties with the index. These properties
+   * are retrievable by {@link DirectoryTaxonomyReader#getCommitUserData}.
+   * See {@link TaxonomyWriter#commit(Map)}. 
+   */
+  @Override
+  public synchronized void commit(Map<String,String> commitUserData) throws CorruptIndexException, IOException {
+    indexWriter.commit(commitUserData);
+    refreshReader();
+  }
+  
+  /**
+   * prepare most of the work needed for a two-phase commit.
+   * See {@link IndexWriter#prepareCommit}.
+   */
+  @Override
+  public synchronized void prepareCommit() throws CorruptIndexException, IOException {
+    indexWriter.prepareCommit();
+  }
+
+  /**
+   * Like above, and also prepares to store user data with the index.
+   * See {@link IndexWriter#prepareCommit(Map)}
+   */
+  @Override
+  public synchronized void prepareCommit(Map<String,String> commitUserData) throws CorruptIndexException, IOException {
+    indexWriter.prepareCommit(commitUserData);
+  }
+  
+  /**
+   * getSize() returns the number of categories in the taxonomy.
+   * <P>
+   * Because categories are numbered consecutively starting with 0, it means
+   * the taxonomy contains ordinals 0 through getSize()-1.
+   * <P>
+   * Note that the number returned by getSize() is often slightly higher than
+   * the number of categories inserted into the taxonomy; This is because when
+   * a category is added to the taxonomy, its ancestors are also added
+   * automatically (including the root, which always get ordinal 0).
+   */
+  @Override
+  synchronized public int getSize() {
+    return indexWriter.maxDoc();
+  }
+
+  private boolean alreadyCalledFillCache = false;
+
+  /**
+   * Set the number of cache misses before an attempt is made to read the
+   * entire taxonomy into the in-memory cache.
+   * <P> 
+   * LuceneTaxonomyWriter holds an in-memory cache of recently seen
+   * categories to speed up operation. On each cache-miss, the on-disk index
+   * needs to be consulted. When an existing taxonomy is opened, a lot of
+   * slow disk reads like that are needed until the cache is filled, so it
+   * is more efficient to read the entire taxonomy into memory at once.
+   * We do this complete read after a certain number (defined by this method)
+   * of cache misses.
+   * <P>
+   * If the number is set to <CODE>0</CODE>, the entire taxonomy is read
+   * into the cache on first use, without fetching individual categories
+   * first.
+   * <P>
+   * Note that if the memory cache of choice is limited in size, and cannot
+   * hold the entire content of the on-disk taxonomy, then it is never
+   * read in its entirety into the cache, regardless of the setting of this
+   * method. 
+   */
+  public void setCacheMissesUntilFill(int i) {
+    cacheMissesUntilFill = i;
+  }
+  private int cacheMissesUntilFill = 11;
+
+  private boolean perhapsFillCache() throws IOException {
+    // Note: we assume that we're only called when cacheIsComplete==false.
+    // TODO (Facet): parametrize this criterion:
+    if (cacheMisses < cacheMissesUntilFill) {
+      return false;
+    }
+    // If the cache was already filled (or we decided not to fill it because
+    // there was no room), there is no sense in trying it again.
+    if (alreadyCalledFillCache) {
+      return false;
+    }
+    alreadyCalledFillCache = true;
+    // TODO (Facet): we should probably completely clear the cache before starting
+    // to read it?
+    if (reader == null) {
+      reader = openReader();
+    }
+
+    if (!cache.hasRoom(reader.numDocs())) {
+      return false;
+    }
+
+    CategoryPath cp = new CategoryPath();
+    Terms terms = MultiFields.getTerms(reader, Consts.FULL);
+    // The check is done here to avoid checking it on every iteration of the
+    // below loop. A null term wlil be returned if there are no terms in the
+    // lexicon, or after the Consts.FULL term. However while the loop is
+    // executed we're safe, because we only iterate as long as there are next()
+    // terms.
+    if (terms != null) {
+      TermsEnum termsEnum = terms.iterator();
+      Bits liveDocs = MultiFields.getLiveDocs(reader);
+      DocsEnum docsEnum = null;
+      while (termsEnum.next() != null) {
+        BytesRef t = termsEnum.term();
+        // Since we guarantee uniqueness of categories, each term has exactly
+        // one document. Also, since we do not allow removing categories (and
+        // hence documents), there are no deletions in the index. Therefore, it
+        // is sufficient to call next(), and then doc(), exactly once with no
+        // 'validation' checks.
+        docsEnum = termsEnum.docs(liveDocs, docsEnum);
+        docsEnum.nextDoc();
+        cp.clear();
+        // TODO (Facet): avoid String creation/use bytes?
+        cp.add(t.utf8ToString(), delimiter);
+        cache.put(cp, docsEnum.docID());
+      }
+    }
+
+    cacheIsComplete = true;
+    // No sense to keep the reader open - we will not need to read from it
+    // if everything is in the cache.
+    reader.close();
+    reader = null;
+    return true;
+  }
+
+  private ParentArray parentArray;
+  private synchronized ParentArray getParentArray() throws IOException {
+    if (parentArray==null) {
+      if (reader == null) {
+        reader = openReader();
+      }
+      parentArray = new ParentArray();
+      parentArray.refresh(reader);
+    }
+    return parentArray;
+  }
+  @Override
+  public int getParent(int ordinal) throws IOException {
+    // Note: the following if() just enforces that a user can never ask
+    // for the parent of a nonexistant category - even if the parent array
+    // was allocated bigger than it really needs to be.
+    if (ordinal >= getSize()) {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+    return getParentArray().getArray()[ordinal];
+  }
+
+  /**
+   * Take all the categories of one or more given taxonomies, and add them to
+   * the main taxonomy (this), if they are not already there.
+   * <P>
+   * Additionally, fill a <I>mapping</I> for each of the added taxonomies,
+   * mapping its ordinals to the ordinals in the enlarged main taxonomy.
+   * These mapping are saved into an array of OrdinalMap objects given by the
+   * user, one for each of the given taxonomies (not including "this", the main
+   * taxonomy). Often the first of these will be a MemoryOrdinalMap and the
+   * others will be a DiskOrdinalMap - see discussion in {OrdinalMap}. 
+   * <P> 
+   * Note that the taxonomies to be added are given as Directory objects,
+   * not opened TaxonomyReader/TaxonomyWriter objects, so if any of them are
+   * currently managed by an open TaxonomyWriter, make sure to commit() (or
+   * close()) it first. The main taxonomy (this) is an open TaxonomyWriter,
+   * and does not need to be commit()ed before this call. 
+   */
+  public void addTaxonomies(Directory[] taxonomies, OrdinalMap[] ordinalMaps) throws IOException {
+    // To prevent us stepping on the rest of this class's decisions on when
+    // to open a reader, and when not, we'll be opening a new reader instead
+    // of using the existing "reader" object:
+    IndexReader mainreader = openReader();
+    // TODO (Facet): can this then go segment-by-segment and avoid MultiDocsEnum etc?
+    Terms terms = MultiFields.getTerms(mainreader, Consts.FULL);
+    assert terms != null; // TODO (Facet): explicit check / throw exception?
+    TermsEnum mainte = terms.iterator();
+    DocsEnum mainde = null;
+
+    IndexReader[] otherreaders = new IndexReader[taxonomies.length];
+    TermsEnum[] othertes = new TermsEnum[taxonomies.length];
+    DocsEnum[] otherdocsEnum = new DocsEnum[taxonomies.length]; // just for reuse
+    for (int i=0; i<taxonomies.length; i++) {
+      otherreaders[i] = IndexReader.open(taxonomies[i]);
+      terms = MultiFields.getTerms(otherreaders[i], Consts.FULL);
+      assert terms != null; // TODO (Facet): explicit check / throw exception?
+      othertes[i] = terms.iterator();
+      // Also tell the ordinal maps their expected sizes:
+      ordinalMaps[i].setSize(otherreaders[i].numDocs());
+    }
+
+    CategoryPath cp = new CategoryPath();
+
+    // We keep a "current" cursor over the alphabetically-ordered list of
+    // categories in each taxonomy. We start the cursor on the first
+    // (alphabetically) category of each taxonomy:
+
+    String currentMain;
+    String[] currentOthers = new String[taxonomies.length];
+    currentMain = nextTE(mainte);
+    int otherTaxonomiesLeft = 0;
+    for (int i=0; i<taxonomies.length; i++) {
+      currentOthers[i] = nextTE(othertes[i]);
+      if (currentOthers[i]!=null) {
+        otherTaxonomiesLeft++;
+      }
+    }
+
+    // And then, at each step look at the first (alphabetically) of the
+    // current taxonomies.
+    // NOTE: The most efficient way we could have done this is using a
+    // PriorityQueue. But for simplicity, and assuming that usually we'll
+    // have a very small number of other taxonomies (often just 1), we use
+    // a more naive algorithm (o(ntaxonomies) instead of o(ln ntaxonomies)
+    // per step)
+
+    while (otherTaxonomiesLeft>0) {
+      // TODO: use a pq here
+      String first=null;
+      for (int i=0; i<taxonomies.length; i++) {
+        if (currentOthers[i]==null) continue;
+        if (first==null || first.compareTo(currentOthers[i])>0) {
+          first = currentOthers[i];
+        }
+      }
+      int comp = 0;
+      if (currentMain==null || (comp = currentMain.compareTo(first))>0) {
+        // If 'first' is before currentMain, or currentMain is null,
+        // then 'first' is a new category and we need to add it to the
+        // main taxonomy. Then for all taxonomies with this 'first'
+        // category, we need to add the new category number to their
+        // map, and move to the next category in all of them.
+        cp.clear();
+        cp.add(first, delimiter);
+        // We can call internalAddCategory() instead of addCategory()
+        // because we know the category hasn't been seen yet.
+        int newordinal = internalAddCategory(cp, cp.length());
+        // TODO (Facet): we already had this term in our hands before, in nextTE...
+        // // TODO (Facet): no need to make this term?
+        for (int i=0; i<taxonomies.length; i++) {
+          if (first.equals(currentOthers[i])) {
+            // remember the remapping of this ordinal. Note how
+            // this requires reading a posting list from the index -
+            // but since we do this in lexical order of terms, just
+            // like Lucene's merge works, we hope there are few seeks.
+            // TODO (Facet): is there a quicker way? E.g., not specifying the
+            // next term by name every time?
+            otherdocsEnum[i] = othertes[i].docs(MultiFields.getLiveDocs(otherreaders[i]), otherdocsEnum[i]);
+            otherdocsEnum[i].nextDoc(); // TODO (Facet): check?
+            int origordinal = otherdocsEnum[i].docID();
+            ordinalMaps[i].addMapping(origordinal, newordinal);
+            // and move to the next category in the i'th taxonomy 
+            currentOthers[i] = nextTE(othertes[i]);
+            if (currentOthers[i]==null) {
+              otherTaxonomiesLeft--;
+            }
+          }
+        }
+      } else if (comp==0) {
+        // 'first' and currentMain are the same, so both the main and some
+        // other taxonomies need to be moved, but a category doesn't need
+        // to be added because it already existed in the main taxonomy.
+
+        // TODO (Facet): Again, is there a quicker way?
+        mainde = mainte.docs(MultiFields.getLiveDocs(mainreader), mainde);
+        mainde.nextDoc(); // TODO (Facet): check?
+        int newordinal = mainde.docID();
+
+        currentMain = nextTE(mainte);
+        for (int i=0; i<taxonomies.length; i++) {
+          if (first.equals(currentOthers[i])) {
+            // TODO (Facet): again, is there a quicker way?
+            otherdocsEnum[i] = othertes[i].docs(MultiFields.getLiveDocs(otherreaders[i]), otherdocsEnum[i]);
+            otherdocsEnum[i].nextDoc(); // TODO (Facet): check?
+            int origordinal = otherdocsEnum[i].docID();
+            ordinalMaps[i].addMapping(origordinal, newordinal);
+
+            // and move to the next category 
+            currentOthers[i] = nextTE(othertes[i]);
+            if (currentOthers[i]==null) {
+              otherTaxonomiesLeft--;
+            }
+          }
+        }
+      } else /* comp > 0 */ {
+        // The currentMain doesn't appear in any of the other taxonomies -
+        // we don't need to do anything, just continue to the next one
+        currentMain = nextTE(mainte);
+      }
+    }
+
+    // Close all the readers we've opened, and also tell the ordinal maps
+    // we're done adding to them
+    mainreader.close();
+    for (int i=0; i<taxonomies.length; i++) {
+      otherreaders[i].close();
+      // We never actually added a mapping for the root ordinal - let's do
+      // it now, just so that the map is complete (every ordinal between 0
+      // and size-1 is remapped)
+      ordinalMaps[i].addMapping(0, 0);
+      ordinalMaps[i].addDone();
+    }
+  }
+
+  /**
+   * Mapping from old ordinal to new ordinals, used when merging indexes 
+   * wit separate taxonomies.
+   * <p> 
+   * addToTaxonomies() merges one or more taxonomies into the given taxonomy
+   * (this). An OrdinalMap is filled for each of the added taxonomies,
+   * containing the new ordinal (in the merged taxonomy) of each of the
+   * categories in the old taxonomy.
+   * <P>  
+   * There exist two implementations of OrdinalMap: MemoryOrdinalMap and
+   * DiskOrdinalMap. As their names suggest, the former keeps the map in
+   * memory and the latter in a temporary disk file. Because these maps will
+   * later be needed one by one (to remap the counting lists), not all at the
+   * same time, it is recommended to put the first taxonomy's map in memory,
+   * and all the rest on disk (later to be automatically read into memory one
+   * by one, when needed).
+   */
+  public static interface OrdinalMap {
+    /**
+     * Set the size of the map. This MUST be called before addMapping().
+     * It is assumed (but not verified) that addMapping() will then be
+     * called exactly 'size' times, with different origOrdinals between 0
+     * and size-1.  
+     */
+    public void setSize(int size) throws IOException;
+    public void addMapping(int origOrdinal, int newOrdinal) throws IOException;
+    /**
+     * Call addDone() to say that all addMapping() have been done.
+     * In some implementations this might free some resources. 
+     */
+    public void addDone() throws IOException;
+    /**
+     * Return the map from the taxonomy's original (consecutive) ordinals
+     * to the new taxonomy's ordinals. If the map has to be read from disk
+     * and ordered appropriately, it is done when getMap() is called.
+     * getMap() should only be called once, and only when the map is actually
+     * needed. Calling it will also free all resources that the map might
+     * be holding (such as temporary disk space), other than the returned int[].
+     */
+    public int[] getMap() throws IOException;
+  }
+
+  /**
+   * {@link OrdinalMap} maintained in memory
+   */
+  public static final class MemoryOrdinalMap implements OrdinalMap {
+    int[] map;
+    @Override
+    public void setSize(int taxonomySize) {
+      map = new int[taxonomySize];
+    }
+    @Override
+    public void addMapping(int origOrdinal, int newOrdinal) {
+      map[origOrdinal] = newOrdinal;
+    }
+    @Override
+    public void addDone() { /* nothing to do */ }
+    @Override
+    public int[] getMap() {
+      return map;
+    }
+  }
+
+  /**
+   * {@link OrdinalMap} maintained on file system
+   */
+  public static final class DiskOrdinalMap implements OrdinalMap {
+    File tmpfile;
+    DataOutputStream out;
+
+    public DiskOrdinalMap(File tmpfile) throws FileNotFoundException {
+      this.tmpfile = tmpfile;
+      out = new DataOutputStream(new BufferedOutputStream(
+          new FileOutputStream(tmpfile)));
+    }
+
+    @Override
+    public void addMapping(int origOrdinal, int newOrdinal) throws IOException {
+      out.writeInt(origOrdinal);
+      out.writeInt(newOrdinal);
+    }
+
+    @Override
+    public void setSize(int taxonomySize) throws IOException {
+      out.writeInt(taxonomySize);
+    }
+
+    @Override
+    public void addDone() throws IOException {
+      if (out!=null) {
+        out.close();
+        out = null;
+      }
+    }
+
+    int[] map = null;
+
+    @Override
+    public int[] getMap() throws IOException {
+      if (map!=null) {
+        return map;
+      }
+      addDone(); // in case this wasn't previously called
+      DataInputStream in = new DataInputStream(new BufferedInputStream(
+          new FileInputStream(tmpfile)));
+      map = new int[in.readInt()];
+      // NOTE: The current code assumes here that the map is complete,
+      // i.e., every ordinal gets one and exactly one value. Otherwise,
+      // we may run into an EOF here, or vice versa, not read everything.
+      for (int i=0; i<map.length; i++) {
+        int origordinal = in.readInt();
+        int newordinal = in.readInt();
+        map[origordinal] = newordinal;
+      }
+      in.close();
+      // Delete the temporary file, which is no longer needed.
+      if (!tmpfile.delete()) {
+        tmpfile.deleteOnExit();
+      }
+      return map;
+    }
+  }
+
+  private static final String nextTE(TermsEnum te) throws IOException {
+    if (te.next() != null) {
+      return te.term().utf8ToString(); // TODO (Facet): avoid String creation/use Bytes?
+    } 
+    return null;
+  }
+
+  @Override
+  public void rollback() throws IOException {
+    indexWriter.rollback();
+    refreshReader();
+  }
+  
+}
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/ParentArray.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/ParentArray.java
index 3f24d583..3e997c3e 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/ParentArray.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/ParentArray.java
@@ -1 +1,161 @@
   + native
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// getParent() needs to be extremely efficient, to the point that we need
+// to fetch all the data in advance into memory, and answer these calls
+// from memory. Currently we use a large integer array, which is
+// initialized when the taxonomy is opened, and potentially enlarged
+// when it is refresh()ed.
+/**
+ * @lucene.experimental
+ */
+class ParentArray {
+
+  // These arrays are not syncrhonized. Rather, the reference to the array
+  // is volatile, and the only writing operation (refreshPrefetchArrays)
+  // simply creates a new array and replaces the reference. The volatility
+  // of the reference ensures the correct atomic replacement and its
+  // visibility properties (the content of the array is visible when the
+  // new reference is visible).
+  private volatile int prefetchParentOrdinal[] = null;
+
+  public int[] getArray() {
+    return prefetchParentOrdinal;
+  }
+
+  /**
+   * refreshPrefetch() refreshes the parent array. Initially, it fills the
+   * array from the positions of an appropriate posting list. If called during
+   * a refresh(), when the arrays already exist, only values for new documents
+   * (those beyond the last one in the array) are read from the positions and
+   * added to the arrays (that are appropriately enlarged). We assume (and
+   * this is indeed a correct assumption in our case) that existing categories
+   * are never modified or deleted.
+   */
+  void refresh(IndexReader indexReader) throws IOException {
+    // Note that it is not necessary for us to obtain the read lock.
+    // The reason is that we are only called from refresh() (precluding
+    // another concurrent writer) or from the constructor (when no method
+    // could be running).
+    // The write lock is also not held during the following code, meaning
+    // that reads *can* happen while this code is running. The "volatile"
+    // property of the prefetchParentOrdinal and prefetchDepth array
+    // references ensure the correct visibility property of the assignment
+    // but other than that, we do *not* guarantee that a reader will not
+    // use an old version of one of these arrays (or both) while a refresh
+    // is going on. But we find this acceptable - until a refresh has
+    // finished, the reader should not expect to see new information
+    // (and the old information is the same in the old and new versions).
+    int first;
+    int num = indexReader.maxDoc();
+    if (prefetchParentOrdinal==null) {
+      prefetchParentOrdinal = new int[num];
+      // Starting Lucene 2.9, following the change LUCENE-1542, we can
+      // no longer reliably read the parent "-1" (see comment in
+      // LuceneTaxonomyWriter.SinglePositionTokenStream). We have no way
+      // to fix this in indexing without breaking backward-compatibility
+      // with existing indexes, so what we'll do instead is just
+      // hard-code the parent of ordinal 0 to be -1, and assume (as is
+      // indeed the case) that no other parent can be -1.
+      if (num>0) {
+        prefetchParentOrdinal[0] = TaxonomyReader.INVALID_ORDINAL;
+      }
+      first = 1;
+    } else {
+      first = prefetchParentOrdinal.length;
+      if (first==num) {
+        return; // nothing to do - no category was added
+      }
+      // In Java 6, we could just do Arrays.copyOf()...
+      int[] newarray = new int[num];
+      System.arraycopy(prefetchParentOrdinal, 0, newarray, 0,
+          prefetchParentOrdinal.length);
+      prefetchParentOrdinal = newarray;
+    }
+
+    // Read the new part of the parents array from the positions:
+    // TODO (Facet): avoid Multi*?
+    Bits liveDocs = MultiFields.getLiveDocs(indexReader);
+    DocsAndPositionsEnum positions = MultiFields.getTermPositionsEnum(indexReader, liveDocs,
+        Consts.FIELD_PAYLOADS, new BytesRef(Consts.PAYLOAD_PARENT));
+      if ((positions == null || positions.advance(first) == DocsAndPositionsEnum.NO_MORE_DOCS) && first < num) {
+        throw new CorruptIndexException("Missing parent data for category " + first);
+      }
+      for (int i=first; i<num; i++) {
+        // Note that we know positions.doc() >= i (this is an
+        // invariant kept throughout this loop)
+        if (positions.docID()==i) {
+          if (positions.freq() == 0) { // shouldn't happen
+            throw new CorruptIndexException(
+                "Missing parent data for category "+i);
+          }
+
+          // TODO (Facet): keep a local (non-volatile) copy of the prefetchParentOrdinal
+          // reference, because access to volatile reference is slower (?).
+          // Note: The positions we get here are one less than the position
+          // increment we added originally, so we get here the right numbers:
+          prefetchParentOrdinal[i] = positions.nextPosition();
+
+          if (positions.nextDoc() == DocsAndPositionsEnum.NO_MORE_DOCS) {
+            if ( i+1 < num ) {
+              throw new CorruptIndexException(
+                  "Missing parent data for category "+(i+1));
+            }
+            break;
+          }
+        } else { // this shouldn't happen
+        throw new CorruptIndexException(
+            "Missing parent data for category "+i);
+      }
+    }
+  }
+
+  /**
+   * add() is used in LuceneTaxonomyWriter, not in LuceneTaxonomyReader.
+   * It is only called from a synchronized method, so it is not reentrant,
+   * and also doesn't need to worry about reads happening at the same time.
+   * 
+   * NOTE: add() and refresh() CANNOT be used together. If you call add(),
+   * this changes the arrays and refresh() can no longer be used.
+   */
+  void add(int ordinal, int parentOrdinal) throws IOException {
+    if (ordinal >= prefetchParentOrdinal.length) {
+      // grow the array, if necessary.
+      // In Java 6, we could just do Arrays.copyOf()...
+      int[] newarray = new int[ordinal*2+1];
+      System.arraycopy(prefetchParentOrdinal, 0, newarray, 0,
+          prefetchParentOrdinal.length);
+      prefetchParentOrdinal = newarray;
+    }
+    prefetchParentOrdinal[ordinal] = parentOrdinal;
+  }
+
+}
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java
index 8d80ce1f..646bdc01 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java
@@ -1,7 +1,7 @@
 package org.apache.lucene.facet.taxonomy.writercache;
 
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -23,7 +23,7 @@
 /**
  * TaxonomyWriterCache is a relatively simple interface for a cache of
  * category->ordinal mappings, used in TaxonomyWriter implementations
- * (such as {@link LuceneTaxonomyWriter}).
+ * (such as {@link DirectoryTaxonomyWriter}).
  * <P>
  * It basically has put() methods for adding a mapping, and get() for looking
  * a mapping up the cache. The cache does <B>not</B> guarantee to hold
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java
index baecdbb3..917becdb 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java
@@ -119,7 +119,7 @@ String stats() {
    * If cache is full remove least recently used entries from cache.
    * Return true if anything was removed, false otherwise.
    * 
-   * See comment in {@link LuceneTaxonomyWriter#addToCache(String, Integer)}
+   * See comment in {@link DirectoryTaxonomyWriter#addToCache(String, Integer)}
    * for an explanation why we clean 2/3rds of the cache, and not just one
    * entry.
    */ 
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
index d5785398..30e03222 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
@@ -9,7 +9,6 @@
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.DocumentBuilder.DocumentBuilderException;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
@@ -44,8 +43,8 @@
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -138,7 +137,7 @@ protected final void initIndex(int partitionSize, boolean onDisk) throws Excepti
     }
     
     RandomIndexWriter iw = new RandomIndexWriter(random, indexDir, getIndexWriterConfig(getAnalyzer()));
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
     
     populateIndex(iw, taxo, getFacetIndexingParams(partitionSize));
     
@@ -149,7 +148,7 @@ protected final void initIndex(int partitionSize, boolean onDisk) throws Excepti
     iw.close();
     
     // prepare for searching
-    taxoReader = new LuceneTaxonomyReader(taxoDir);
+    taxoReader = new DirectoryTaxonomyReader(taxoDir);
     indexReader = IndexReader.open(indexDir);
     searcher = newSearcher(indexReader);
   }
@@ -191,7 +190,7 @@ protected FacetSearchParams getFacetedSearchParams(int partitionSize) {
    * <p>Subclasses can override this to test different scenarios
    */
   protected void populateIndex(RandomIndexWriter iw, TaxonomyWriter taxo, FacetIndexingParams iParams)
-      throws IOException, DocumentBuilderException, CorruptIndexException {
+      throws IOException, CorruptIndexException {
     // add test documents 
     int numDocsToIndex = numDocsToIndex();
     for (int doc=0; doc<numDocsToIndex; doc++) {
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
index 4dee45fd..4659eba4 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
@@ -32,8 +32,8 @@
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -70,7 +70,7 @@
       IndexTaxonomyReaderPair pair = new IndexTaxonomyReaderPair();
       pair.indexReader = IndexReader.open(dirs[i][0]);
       pair.indexSearcher = new IndexSearcher(pair.indexReader);
-      pair.taxReader = new LuceneTaxonomyReader(dirs[i][1]);
+      pair.taxReader = new DirectoryTaxonomyReader(dirs[i][1]);
       pairs[i] = pair;
     }
     return pairs;
@@ -84,7 +84,7 @@
       pair.indexWriter = new IndexWriter(dirs[i][0], new IndexWriterConfig(
           LuceneTestCase.TEST_VERSION_CURRENT, new StandardAnalyzer(
               LuceneTestCase.TEST_VERSION_CURRENT)));
-      pair.taxWriter = new LuceneTaxonomyWriter(dirs[i][1]);
+      pair.taxWriter = new DirectoryTaxonomyWriter(dirs[i][1]);
       pair.indexWriter.commit();
       pair.taxWriter.commit();
       pairs[i] = pair;
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/enhancements/TwoEnhancementsTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/enhancements/TwoEnhancementsTest.java
index 5a38ba23..866bb11f 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/enhancements/TwoEnhancementsTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/enhancements/TwoEnhancementsTest.java
@@ -21,7 +21,7 @@
 import org.apache.lucene.facet.search.DrillDown;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -58,7 +58,7 @@ public void testTwoEmptyAndNonEmptyByteArrays() throws Exception {
 
     RandomIndexWriter indexWriter = new RandomIndexWriter(random, indexDir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir);
 
     // a category document builder will add the categories to a document
     // once build() is called
@@ -103,7 +103,7 @@ public void testTwoNonEmptyByteArrays() throws Exception {
 
     RandomIndexWriter indexWriter = new RandomIndexWriter(random, indexDir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir);
 
     // a category document builder will add the categories to a document
     // once build() is called
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/enhancements/association/CustomAssociationPropertyTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/enhancements/association/CustomAssociationPropertyTest.java
index 5acace10..09bcab41 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/enhancements/association/CustomAssociationPropertyTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/enhancements/association/CustomAssociationPropertyTest.java
@@ -16,8 +16,8 @@
 import org.apache.lucene.facet.index.attributes.CategoryAttributeImpl;
 import org.apache.lucene.facet.index.attributes.CategoryProperty;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -59,7 +59,7 @@ public void merge(CategoryProperty other) {
     
     RandomIndexWriter w = new RandomIndexWriter(random, iDir, 
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
-    LuceneTaxonomyWriter taxoW = new LuceneTaxonomyWriter(tDir);
+    DirectoryTaxonomyWriter taxoW = new DirectoryTaxonomyWriter(tDir);
     
     CategoryContainer cc = new CategoryContainer();
     EnhancementsDocumentBuilder builder = new EnhancementsDocumentBuilder(taxoW, iParams);
@@ -75,7 +75,7 @@ public void merge(CategoryProperty other) {
     IndexReader reader = w.getReader();
     w.close();
     
-    LuceneTaxonomyReader taxo = new LuceneTaxonomyReader(tDir);
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(tDir);
     String field = iParams.getCategoryListParams(new CategoryPath("0")).getTerm().field();
     AssociationsPayloadIterator api = new AssociationsPayloadIterator(reader, field);
 
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java
index 86fef2c4..50730500 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java
@@ -23,8 +23,8 @@
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -68,7 +68,7 @@ public void testTaxonomyMergeUtils() throws Exception {
 
   private void verifyResults(Directory dir, Directory taxDir) throws IOException {
     IndexReader reader1 = IndexReader.open(dir);
-    LuceneTaxonomyReader taxReader = new LuceneTaxonomyReader(taxDir);
+    DirectoryTaxonomyReader taxReader = new DirectoryTaxonomyReader(taxDir);
     IndexSearcher searcher = newSearcher(reader1);
     FacetSearchParams fsp = new FacetSearchParams();
     fsp.addFacetRequest(new CountFacetRequest(new CategoryPath("tag"), NUM_DOCS));
@@ -94,7 +94,7 @@ private void buildIndexWithFacets(Directory dir, Directory taxDir, boolean asc)
         new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, config);
     
-    LuceneTaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(taxDir);
+    DirectoryTaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxDir);
     for (int i = 1; i <= NUM_DOCS; i++) {
       Document doc = new Document();
       List<CategoryPath> categoryPaths = new ArrayList<CategoryPath>(i + 1);
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/OrdinalPolicyTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/OrdinalPolicyTest.java
index 00b5c27d..7514143a 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/OrdinalPolicyTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/OrdinalPolicyTest.java
@@ -10,7 +10,7 @@
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -48,7 +48,7 @@ public void testDefaultOrdinalPolicy() {
   public void testNonTopLevelOrdinalPolicy() throws Exception {
     Directory dir = newDirectory();
     TaxonomyWriter taxonomy = null;
-    taxonomy = new LuceneTaxonomyWriter(dir);
+    taxonomy = new DirectoryTaxonomyWriter(dir);
 
     int[] topLevelOrdinals = new int[10];
     String[] topLevelStrings = new String[10];
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/PathPolicyTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/PathPolicyTest.java
index 7f7b6511..df7f9f3a 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/PathPolicyTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/PathPolicyTest.java
@@ -9,7 +9,7 @@
 import org.apache.lucene.facet.index.categorypolicy.PathPolicy;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -54,7 +54,7 @@ public void testDefaultPathPolicy() {
   public void testNonTopLevelPathPolicy() throws Exception {
     Directory dir = newDirectory();
     TaxonomyWriter taxonomy = null;
-    taxonomy = new LuceneTaxonomyWriter(dir);
+    taxonomy = new DirectoryTaxonomyWriter(dir);
 
     CategoryPath[] topLevelPaths = new CategoryPath[10];
     String[] topLevelStrings = new String[10];
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryParentsStreamTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryParentsStreamTest.java
index 4ac2bfee..ede92726 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryParentsStreamTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryParentsStreamTest.java
@@ -19,7 +19,7 @@
 import org.apache.lucene.facet.index.streaming.CategoryListTokenizer;
 import org.apache.lucene.facet.index.streaming.CategoryParentsStream;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -50,7 +50,7 @@
   @Test
   public void testStreamDefaultParams() throws IOException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
     CategoryParentsStream stream = new CategoryParentsStream(
         new CategoryAttributesStream(categoryContainer),
@@ -77,7 +77,7 @@ public void testStreamDefaultParams() throws IOException {
   @Test
   public void testStreamNonTopLevelParams() throws IOException {
     Directory directory = newDirectory();
-    final TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    final TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
     FacetIndexingParams indexingParams = new DefaultFacetIndexingParams() {
       @Override
@@ -118,7 +118,7 @@ protected PathPolicy fixedPathPolicy() {
   @Test
   public void testNoRetainableAttributes() throws IOException, FacetException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(directory);
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(directory);
 
     new CategoryParentsStream(new CategoryAttributesStream(categoryContainer),
         taxonomyWriter, new DefaultFacetIndexingParams());
@@ -152,7 +152,7 @@ public void testNoRetainableAttributes() throws IOException, FacetException {
   @Test
   public void testRetainableAttributes() throws IOException, FacetException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
 
     FacetIndexingParams indexingParams = new DefaultFacetIndexingParams();
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryTokenizerTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryTokenizerTest.java
index 14804e65..8d52ab15 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryTokenizerTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryTokenizerTest.java
@@ -17,7 +17,7 @@
 import org.apache.lucene.facet.index.streaming.CategoryTokenizer;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -47,7 +47,7 @@
   @Test
   public void testTokensDefaultParams() throws IOException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
     DefaultFacetIndexingParams indexingParams = new DefaultFacetIndexingParams();
     CategoryTokenizer tokenizer = new CategoryTokenizer(
@@ -86,7 +86,7 @@ public void testTokensDefaultParams() throws IOException {
   @Test
   public void testLongCategoryPath() throws IOException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
 
     List<CategoryPath> longCategory = new ArrayList<CategoryPath>();
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java
index 20ebcdd8..bfdffe49 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java
@@ -4,7 +4,6 @@
 import java.util.Arrays;
 import java.util.List;
 
-import org.apache.lucene.DocumentBuilder.DocumentBuilderException;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -52,8 +51,7 @@
 
   @Override
   protected void populateIndex(RandomIndexWriter iw, TaxonomyWriter taxo,
-      FacetIndexingParams iParams) throws IOException,
-      DocumentBuilderException, CorruptIndexException {
+      FacetIndexingParams iParams) throws IOException, CorruptIndexException {
     currDoc = -1;
     super.populateIndex(iw, taxo, iParams);
   }
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java
index 20fdc920..2cfa4387 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java
@@ -28,8 +28,8 @@
 import org.apache.lucene.facet.search.params.FacetSearchParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -53,7 +53,7 @@
   private FacetSearchParams defaultParams = new FacetSearchParams();
   private FacetSearchParams nonDefaultParams;
   private static IndexReader reader;
-  private static LuceneTaxonomyReader taxo;
+  private static DirectoryTaxonomyReader taxo;
   private static Directory dir;
   private static Directory taxoDir;
   
@@ -74,7 +74,7 @@ public static void createIndexes() throws CorruptIndexException, LockObtainFaile
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
     
     taxoDir = newDirectory();
-    TaxonomyWriter taxoWriter = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
     
     for (int i = 0; i < 100; i++) {
       ArrayList<CategoryPath> paths = new ArrayList<CategoryPath>();
@@ -100,7 +100,7 @@ public static void createIndexes() throws CorruptIndexException, LockObtainFaile
     reader = writer.getReader();
     writer.close();
     
-    taxo = new LuceneTaxonomyReader(taxoDir);
+    taxo = new DirectoryTaxonomyReader(taxoDir);
   }
   
   @Test
@@ -149,6 +149,8 @@ public void testQuery() throws IOException {
     Query q4 = DrillDown.query(defaultParams, fooQuery, new CategoryPath("b"));
     docs = searcher.search(q4, 100);
     assertEquals(10, docs.totalHits);
+    
+    searcher.close();
   }
   
   @Test
@@ -170,6 +172,8 @@ public void testQueryImplicitDefaultParams() throws IOException {
     Query q4 = DrillDown.query(fooQuery, new CategoryPath("b"));
     docs = searcher.search(q4, 100);
     assertEquals(10, docs.totalHits);
+    
+    searcher.close();
   }
   
   @AfterClass
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
index e5325199..560ebe0d 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
@@ -38,8 +38,8 @@
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -67,7 +67,7 @@ public void testDefault() throws Exception {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1], OpenMode.CREATE);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1], OpenMode.CREATE);
 
     /**
      * Configure with no custom counting lists
@@ -80,7 +80,7 @@ public void testDefault() throws Exception {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
@@ -109,7 +109,7 @@ public void testCustom() throws Exception {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1],
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1],
         OpenMode.CREATE);
 
     PerDimensionIndexingParams iParams = new PerDimensionIndexingParams();
@@ -121,7 +121,7 @@ public void testCustom() throws Exception {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
@@ -150,7 +150,7 @@ public void testTwoCustomsSameField() throws Exception {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1],
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1],
         OpenMode.CREATE);
 
     PerDimensionIndexingParams iParams = new PerDimensionIndexingParams();
@@ -164,7 +164,7 @@ public void testTwoCustomsSameField() throws Exception {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
@@ -199,7 +199,7 @@ public void testDifferentFieldsAndText() throws Exception {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1], OpenMode.CREATE);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1], OpenMode.CREATE);
 
     PerDimensionIndexingParams iParams = new PerDimensionIndexingParams();
     iParams.addCategoryListParams(new CategoryPath("Band"),
@@ -212,7 +212,7 @@ public void testDifferentFieldsAndText() throws Exception {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
@@ -240,7 +240,7 @@ public void testSomeSameSomeDifferent() throws Exception {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1],
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1],
         OpenMode.CREATE);
 
     PerDimensionIndexingParams iParams = new PerDimensionIndexingParams();
@@ -257,7 +257,7 @@ public void testSomeSameSomeDifferent() throws Exception {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
index d70d0eda..0583bc0c 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
@@ -29,8 +29,8 @@
 import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.facet.util.PartitionsUtils;
 
 /**
@@ -80,7 +80,7 @@ protected int fixedPartitionSize() {
       RandomIndexWriter iw = new RandomIndexWriter(random, iDir,
           newIndexWriterConfig(TEST_VERSION_CURRENT,
               new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE));
-      TaxonomyWriter tw = new LuceneTaxonomyWriter(tDir);
+      TaxonomyWriter tw = new DirectoryTaxonomyWriter(tDir);
       prvt_add(iParams, iw, tw, "a", "b");
       prvt_add(iParams, iw, tw, "a", "b", "1");
       prvt_add(iParams, iw, tw, "a", "b", "1");
@@ -104,7 +104,7 @@ protected int fixedPartitionSize() {
       tw.close();
 
       IndexSearcher is = newSearcher(ir);
-      LuceneTaxonomyReader tr = new LuceneTaxonomyReader(tDir);
+      DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(tDir);
 
       // Get all of the documents and run the query, then do different
       // facet counts and compare to control
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
index 8b54182d..ec1e4b38 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
@@ -34,8 +34,8 @@
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.SlowRAMDirectory;
 import org.apache.lucene.util._TestUtil;
@@ -67,12 +67,12 @@
    */
   private static class TFCThread extends Thread {
     private final IndexReader r;
-    private final LuceneTaxonomyReader tr;
+    private final DirectoryTaxonomyReader tr;
     private final FacetIndexingParams iParams;
     
     TotalFacetCounts tfc;
 
-    public TFCThread(IndexReader r, LuceneTaxonomyReader tr, FacetIndexingParams iParams) {
+    public TFCThread(IndexReader r, DirectoryTaxonomyReader tr, FacetIndexingParams iParams) {
       this.r = r;
       this.tr = tr;
       this.iParams = iParams;
@@ -156,7 +156,7 @@ private void doTestGeneralSynchronization(int numThreads, int sleepMillis,
     
     // Open the slow readers
     IndexReader slowIndexReader = IndexReader.open(indexDir);
-    TaxonomyReader slowTaxoReader = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader slowTaxoReader = new DirectoryTaxonomyReader(taxoDir);
 
     // Class to perform search and return results as threads
     class Multi extends Thread {
@@ -421,7 +421,7 @@ public void testMemoryCacheSynchronization() throws Exception {
     // Write index using 'normal' directories
     IndexWriter w = new IndexWriter(indexDir, new IndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
-    LuceneTaxonomyWriter tw = new LuceneTaxonomyWriter(taxoDir);
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
     DefaultFacetIndexingParams iParams = new DefaultFacetIndexingParams();
     // Add documents and facets
     for (int i = 0; i < 1000; i++) {
@@ -434,7 +434,7 @@ public void testMemoryCacheSynchronization() throws Exception {
     taxoDir.setSleepMillis(1);
 
     IndexReader r = IndexReader.open(indexDir);
-    LuceneTaxonomyReader tr = new LuceneTaxonomyReader(taxoDir);
+    DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
 
     // Create and start threads. Thread1 should lock the cache and calculate
     // the TFC array. The second thread should block until the first is
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/association/AssociationsFacetRequestTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/association/AssociationsFacetRequestTest.java
index 89dee04e..11cdeb1a 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/association/AssociationsFacetRequestTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/association/AssociationsFacetRequestTest.java
@@ -29,8 +29,8 @@
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -69,7 +69,7 @@ public static void beforeClassAssociationsFacetRequestTest() throws Exception {
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, 
         new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
     
-    TaxonomyWriter taxoWriter = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
     
     EnhancementsDocumentBuilder builder = new EnhancementsDocumentBuilder(
         taxoWriter, new DefaultEnhancementsIndexingParams(
@@ -106,7 +106,7 @@ public static void afterClassAssociationsFacetRequestTest() throws Exception {
   
   @Test
   public void testIntSumAssociation() throws Exception {
-    LuceneTaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
 
     // facet requests for two facets
     FacetSearchParams fsp = new FacetSearchParams();
@@ -132,7 +132,7 @@ public void testIntSumAssociation() throws Exception {
   
   @Test
   public void testFloatSumAssociation() throws Exception {
-    LuceneTaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
 
     // facet requests for two facets
     FacetSearchParams fsp = new FacetSearchParams();
@@ -161,7 +161,7 @@ public void testDifferentAggregatorsSameCategoryList() throws Exception {
     // Same category list cannot be aggregated by two different aggregators. If
     // you want to do that, you need to separate the categories into two
     // category list (you'll still have one association list).
-    LuceneTaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
 
     // facet requests for two facets
     FacetSearchParams fsp = new FacetSearchParams();
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java
index ff2e0609..f86d36b8 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java
@@ -9,7 +9,7 @@
 import org.apache.lucene.facet.search.FacetResultsHandler;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -58,8 +58,8 @@ public void testGetFacetResultHandlerDifferentTaxonomy() throws Exception {
     // create empty indexes, so that LTR ctor won't complain about a missing index.
     new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, null)).close();
     new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, null)).close();
-    TaxonomyReader tr1 = new LuceneTaxonomyReader(dir1);
-    TaxonomyReader tr2 = new LuceneTaxonomyReader(dir2);
+    TaxonomyReader tr1 = new DirectoryTaxonomyReader(dir1);
+    TaxonomyReader tr2 = new DirectoryTaxonomyReader(dir2);
     FacetResultsHandler frh1 = fr.createFacetResultsHandler(tr1);
     FacetResultsHandler frh2 = fr.createFacetResultsHandler(tr2);
     assertTrue("should not return the same FacetResultHandler instance for different TaxonomyReader instances", frh1 != frh2);
@@ -77,7 +77,7 @@ public void testImmutability() throws Exception {
     Directory dir = newDirectory();
     // create empty indexes, so that LTR ctor won't complain about a missing index.
     new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, null)).close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(dir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dir);
     FacetResultsHandler frh = fr.createFacetResultsHandler(tr);
     fr.setDepth(10);
     assertEquals(FacetRequest.DEFAULT_DEPTH, frh.getFacetRequest().getDepth());
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java
index 59163664..40df7ba1 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java
@@ -8,8 +8,8 @@
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.facet.util.PartitionsUtils;
 
 /**
@@ -37,8 +37,8 @@ public void testDefaultSettings() throws Exception {
     assertEquals("unexpected default facet indexing params class", DefaultFacetIndexingParams.class.getName(), fsp.getFacetIndexingParams().getClass().getName());
     assertEquals("no facet requests should be added by default", 0, fsp.getFacetRequests().size());
     Directory dir = newDirectory();
-    new LuceneTaxonomyWriter(dir).close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(dir);
+    new DirectoryTaxonomyWriter(dir).close();
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dir);
     assertEquals("unexpected partition offset for 0 categories", 1, PartitionsUtils.partitionOffset(fsp, 1, tr));
     assertEquals("unexpected partition size for 0 categories", 1, PartitionsUtils.partitionSize(fsp,tr));
     tr.close();
@@ -56,11 +56,11 @@ public void testAddFacetRequest() throws Exception {
   public void testPartitionSizeWithCategories() throws Exception {
     FacetSearchParams fsp = new FacetSearchParams();
     Directory dir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dir);
     tw.addCategory(new CategoryPath("a"));
     tw.commit();
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(dir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dir);
     assertEquals("unexpected partition offset for 1 categories", 2, PartitionsUtils.partitionOffset(fsp, 1, tr));
     assertEquals("unexpected partition size for 1 categories", 2, PartitionsUtils.partitionSize(fsp,tr));
     tr.close();
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java
index 73f3f83a..0353dbc3 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java
@@ -32,8 +32,8 @@
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.facet.util.ScoredDocIdsUtils;
 
 /**
@@ -93,7 +93,7 @@ private void doTestCLParamMultiIteratorsByRequest(boolean cacheCLI) throws Excep
     Directory taxoDir = newDirectory();
     populateIndex(iParams, indexDir, taxoDir);
 
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     IndexReader reader = IndexReader.open(indexDir);
 
     CategoryListCache clCache = null;
@@ -168,7 +168,7 @@ private void populateIndex(FacetIndexingParams iParams, Directory indexDir,
       Directory taxoDir) throws Exception {
     RandomIndexWriter writer = new RandomIndexWriter(random, indexDir, 
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
-    TaxonomyWriter taxoWriter = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
 
     for (CategoryPath[] categories : perDocCategories) {
       writer.addDocument(new CategoryDocumentBuilder(taxoWriter, iParams)
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
index a92dbad8..52f0f88e 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
@@ -14,8 +14,8 @@
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader.ChildrenArrays;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.util.SlowRAMDirectory;
 
 /**
@@ -159,7 +159,7 @@ private String showcat(CategoryPath path) {
   @Test
   public void testWriter() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     // Also check TaxonomyWriter.getSize() - see that the taxonomy's size
     // is what we expect it to be.
@@ -175,7 +175,7 @@ public void testWriter() throws Exception {
   @Test
   public void testWriterTwice() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     // run fillTaxonomy again - this will try to add the same categories
     // again, and check that we see the same ordinal paths again, not
@@ -197,10 +197,10 @@ public void testWriterTwice() throws Exception {
   @Test
   public void testWriterTwice2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    tw = new LuceneTaxonomyWriter(indexDir);
+    tw = new DirectoryTaxonomyWriter(indexDir);
     // run fillTaxonomy again - this will try to add the same categories
     // again, and check that we see the same ordinals again, not different
     // ones, and that the number of categories hasn't grown by the new
@@ -222,7 +222,7 @@ public void testWriterTwice2() throws Exception {
   public void testWriterTwice3() throws Exception {
     Directory indexDir = newDirectory();
     // First, create and fill the taxonomy
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
     // Now, open the same taxonomy and add the same categories again.
@@ -231,7 +231,7 @@ public void testWriterTwice3() throws Exception {
     // all into memory and close it's reader. The bug was that it closed
     // the reader, but forgot that it did (because it didn't set the reader
     // reference to null).
-    tw = new LuceneTaxonomyWriter(indexDir);
+    tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     // Add one new category, just to make commit() do something:
     tw.addCategory(new CategoryPath("hi"));
@@ -253,7 +253,7 @@ public void testWriterTwice3() throws Exception {
   @Test
   public void testWriterSimpler() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     assertEquals(1, tw.getSize()); // the root only
     // Test that adding a new top-level category works
     assertEquals(1, tw.addCategory(new CategoryPath("a")));
@@ -297,12 +297,12 @@ public void testWriterSimpler() throws Exception {
   @Test
   public void testRootOnly() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     // right after opening the index, it should already contain the
     // root, so have size 1:
     assertEquals(1, tw.getSize());
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     assertEquals(1, tr.getSize());
     assertEquals(0, tr.getPath(0).length());
     assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getParent(0));
@@ -319,9 +319,9 @@ public void testRootOnly() throws Exception {
   @Test
   public void testRootOnly2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     assertEquals(1, tr.getSize());
     assertEquals(0, tr.getPath(0).length());
     assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getParent(0));
@@ -339,10 +339,10 @@ public void testRootOnly2() throws Exception {
   @Test
   public void testReaderBasic() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     // test TaxonomyReader.getSize():
     assertEquals(expectedCategories.length, tr.getSize());
@@ -398,10 +398,10 @@ Note that after testReaderBasic(), we already know we can trust the
   @Test
   public void testReaderParent() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     // check that the parent of the root ordinal is the invalid ordinal:
     assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getParent(0));
@@ -463,11 +463,11 @@ public void testReaderParent() throws Exception {
   @Test
   public void testWriterParent1() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    tw = new LuceneTaxonomyWriter(indexDir);
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    tw = new DirectoryTaxonomyWriter(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     
     checkWriterParent(tr, tw);
     
@@ -479,10 +479,10 @@ public void testWriterParent1() throws Exception {
   @Test
   public void testWriterParent2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     
     checkWriterParent(tr, tw);
     
@@ -542,10 +542,10 @@ other methods (which we have already tested above).
   @Test
   public void testReaderParentArray() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     int[] parents = tr.getParentArray();
     assertEquals(tr.getSize(), parents.length);
     for (int i=0; i<tr.getSize(); i++) {
@@ -563,10 +563,10 @@ public void testReaderParentArray() throws Exception {
   @Test
   public void testChildrenArrays() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     ChildrenArrays ca = tr.getChildrenArrays();
     int[] youngestChildArray = ca.getYoungestChildArray();
     assertEquals(tr.getSize(), youngestChildArray.length);
@@ -627,10 +627,10 @@ public void testChildrenArrays() throws Exception {
   @Test
   public void testChildrenArraysInvariants() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     ChildrenArrays ca = tr.getChildrenArrays();
     int[] youngestChildArray = ca.getYoungestChildArray();
     assertEquals(tr.getSize(), youngestChildArray.length);
@@ -707,10 +707,10 @@ public void testChildrenArraysInvariants() throws Exception {
   @Test
   public void testChildrenArraysGrowth() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.addCategory(new CategoryPath("hi", "there"));
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     ChildrenArrays ca = tr.getChildrenArrays();
     assertEquals(3, tr.getSize());
     assertEquals(3, ca.getOlderSiblingArray().length);
@@ -747,12 +747,12 @@ public void testChildrenArraysGrowth() throws Exception {
   public void testTaxonomyReaderRefreshRaces() throws Exception {
     // compute base child arrays - after first chunk, and after the other
     Directory indexDirBase =  newDirectory();
-    TaxonomyWriter twBase = new LuceneTaxonomyWriter(indexDirBase);
+    TaxonomyWriter twBase = new DirectoryTaxonomyWriter(indexDirBase);
     twBase.addCategory(new CategoryPath("a", "0"));
     final CategoryPath abPath = new CategoryPath("a", "b");
     twBase.addCategory(abPath);
     twBase.commit();
-    TaxonomyReader trBase = new LuceneTaxonomyReader(indexDirBase);
+    TaxonomyReader trBase = new DirectoryTaxonomyReader(indexDirBase);
 
     final ChildrenArrays ca1 = trBase.getChildrenArrays();
     
@@ -779,12 +779,12 @@ private void assertConsistentYoungestChild(final CategoryPath abPath,
       final int abOrd, final int abYoungChildBase1, final int abYoungChildBase2, final int retry)
       throws Exception {
     SlowRAMDirectory indexDir =  new SlowRAMDirectory(-1,null); // no slowness for intialization
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.addCategory(new CategoryPath("a", "0"));
     tw.addCategory(abPath);
     tw.commit();
     
-    final TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    final TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     for (int i=0; i < 1<<10; i++) { //1024 facets
       final CategoryPath cp = new CategoryPath("a", "b", Integer.toString(i));
       tw.addCategory(cp);
@@ -865,9 +865,9 @@ its own object (with obviously no connection between the objects) using
   @Test
   public void testSeparateReaderAndWriter() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     int author = 1;
 
@@ -932,9 +932,9 @@ public void testSeparateReaderAndWriter() throws Exception {
   @Test
   public void testSeparateReaderAndWriter2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     // Test getOrdinal():
     CategoryPath author = new CategoryPath("Author");
@@ -968,26 +968,26 @@ public void testSeparateReaderAndWriter2() throws Exception {
   public void testWriterLock() throws Exception {
     // native fslock impl gets angry if we use it, so use RAMDirectory explicitly.
     Directory indexDir = new RAMDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.addCategory(new CategoryPath("hi", "there"));
     tw.commit();
     // we deliberately not close the write now, and keep it open and
     // locked.
     // Verify that the writer worked:
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     assertEquals(2, tr.getOrdinal(new CategoryPath("hi", "there")));
     // Try to open a second writer, with the first one locking the directory.
     // We expect to get a LockObtainFailedException.
     try {
-      new LuceneTaxonomyWriter(indexDir);
+      new DirectoryTaxonomyWriter(indexDir);
       fail("should have failed to write in locked directory");
     } catch (LockObtainFailedException e) {
       // this is what we expect to happen.
     }
     // Remove the lock, and now the open should succeed, and we can
     // write to the new writer.
-    LuceneTaxonomyWriter.unlock(indexDir);
-    TaxonomyWriter tw2 = new LuceneTaxonomyWriter(indexDir);
+    DirectoryTaxonomyWriter.unlock(indexDir);
+    TaxonomyWriter tw2 = new DirectoryTaxonomyWriter(indexDir);
     tw2.addCategory(new CategoryPath("hey"));
     tw2.close();
     // See that the writer indeed wrote:
@@ -1054,7 +1054,7 @@ public static void checkPaths(TaxonomyWriter tw) throws IOException {
   @Test
   public void testWriterCheckPaths() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomyCheckPaths(tw);
     // Also check TaxonomyWriter.getSize() - see that the taxonomy's size
     // is what we expect it to be.
@@ -1073,14 +1073,14 @@ public void testWriterCheckPaths() throws Exception {
   @Test
   public void testWriterCheckPaths2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     checkPaths(tw);
     fillTaxonomy(tw);
     checkPaths(tw);
     tw.close();
 
-    tw = new LuceneTaxonomyWriter(indexDir);
+    tw = new DirectoryTaxonomyWriter(indexDir);
     checkPaths(tw);
     fillTaxonomy(tw);
     checkPaths(tw);
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomies.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomies.java
index 3f24d583..4935de3b 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomies.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomies.java
@@ -1 +1,255 @@
   + native
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.File;
+
+import org.apache.lucene.store.Directory;
+import org.junit.Test;
+
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.DiskOrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestAddTaxonomies extends LuceneTestCase {
+
+  @Test
+  public void test1() throws Exception {
+    Directory dir1 = newDirectory();
+    DirectoryTaxonomyWriter tw1 = new DirectoryTaxonomyWriter(dir1);
+    tw1.addCategory(new CategoryPath("Author", "Mark Twain"));
+    tw1.addCategory(new CategoryPath("Animals", "Dog"));
+    Directory dir2 = newDirectory();
+    DirectoryTaxonomyWriter tw2 = new DirectoryTaxonomyWriter(dir2);
+    tw2.addCategory(new CategoryPath("Author", "Rob Pike"));
+    tw2.addCategory(new CategoryPath("Aardvarks", "Bob"));
+    tw2.close();
+    Directory dir3 = newDirectory();
+    DirectoryTaxonomyWriter tw3 = new DirectoryTaxonomyWriter(dir3);
+    tw3.addCategory(new CategoryPath("Author", "Zebra Smith"));
+    tw3.addCategory(new CategoryPath("Aardvarks", "Bob"));
+    tw3.addCategory(new CategoryPath("Aardvarks", "Aaron"));
+    tw3.close();
+
+    MemoryOrdinalMap[] maps = new MemoryOrdinalMap[2];
+    maps[0] = new MemoryOrdinalMap();
+    maps[1] = new MemoryOrdinalMap();
+
+    tw1.addTaxonomies(new Directory[] { dir2, dir3 }, maps);
+    tw1.close();
+
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dir1);
+
+    // Test that the merged taxonomy now contains what we expect:
+    // First all the categories of the original taxonomy, in their original order:
+    assertEquals(tr.getPath(0).toString(), "");
+    assertEquals(tr.getPath(1).toString(), "Author");
+    assertEquals(tr.getPath(2).toString(), "Author/Mark Twain");
+    assertEquals(tr.getPath(3).toString(), "Animals");
+    assertEquals(tr.getPath(4).toString(), "Animals/Dog");
+    // Then the categories new in the new taxonomy, in alphabetical order: 
+    assertEquals(tr.getPath(5).toString(), "Aardvarks");
+    assertEquals(tr.getPath(6).toString(), "Aardvarks/Aaron");
+    assertEquals(tr.getPath(7).toString(), "Aardvarks/Bob");
+    assertEquals(tr.getPath(8).toString(), "Author/Rob Pike");
+    assertEquals(tr.getPath(9).toString(), "Author/Zebra Smith");
+    assertEquals(tr.getSize(), 10);
+
+    // Test that the maps contain what we expect
+    int[] map0 = maps[0].getMap();
+    assertEquals(5, map0.length);
+    assertEquals(0, map0[0]);
+    assertEquals(1, map0[1]);
+    assertEquals(8, map0[2]);
+    assertEquals(5, map0[3]);
+    assertEquals(7, map0[4]);
+
+    int[] map1 = maps[1].getMap();
+    assertEquals(6, map1.length);
+    assertEquals(0, map1[0]);
+    assertEquals(1, map1[1]);
+    assertEquals(9, map1[2]);
+    assertEquals(5, map1[3]);
+    assertEquals(7, map1[4]);
+    assertEquals(6, map1[5]);
+    
+    tr.close();
+    dir1.close();
+    dir2.close();
+    dir3.close();
+  }
+
+  // a reasonable random test
+  public void testmedium() throws Exception {
+    int numTests = atLeast(3);
+    for (int i = 0; i < numTests; i++) {
+      dotest(_TestUtil.nextInt(random, 1, 10), 
+             _TestUtil.nextInt(random, 1, 100), 
+             _TestUtil.nextInt(random, 100, 1000),
+             random.nextBoolean());
+    }
+  }
+
+  // A more comprehensive and big random test.
+  @Test @Nightly
+  public void testbig() throws Exception {
+    dotest(2, 1000, 5000, false);
+    dotest(10, 10000, 100, false);
+    dotest(50, 20, 100, false);
+    dotest(10, 1000, 10000, false);
+    dotest(50, 20, 10000, false);
+    dotest(1, 20, 10000, false);
+    dotest(10, 1, 10000, false);
+    dotest(10, 1000, 20000, true);
+  }
+
+  private void dotest(int ntaxonomies, int ncats, int range, boolean disk) throws Exception {
+    Directory dirs[] = new Directory[ntaxonomies];
+    Directory copydirs[] = new Directory[ntaxonomies];
+
+    for (int i=0; i<ntaxonomies; i++) {
+      dirs[i] = newDirectory();
+      copydirs[i] = newDirectory();
+      DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[i]);
+      DirectoryTaxonomyWriter copytw = new DirectoryTaxonomyWriter(copydirs[i]);
+      for (int j=0; j<ncats; j++) {
+        String cat = Integer.toString(random.nextInt(range));
+        tw.addCategory(new CategoryPath("a",cat));
+        copytw.addCategory(new CategoryPath("a",cat));
+      }
+      // System.err.println("Taxonomy "+i+": "+tw.getSize());
+      tw.close();
+      copytw.close();
+    }
+
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0]);
+    Directory otherdirs[] = new Directory[ntaxonomies-1];
+    System.arraycopy(dirs, 1, otherdirs, 0, ntaxonomies-1);
+
+    OrdinalMap[] maps = new OrdinalMap[ntaxonomies-1];
+    if (ntaxonomies>1) {
+      for (int i=0; i<ntaxonomies-1; i++) {
+        if (disk) {
+          // TODO: use a LTC tempfile
+          maps[i] = new DiskOrdinalMap(new File(System.getProperty("java.io.tmpdir"),
+              "tmpmap"+i));
+        } else {
+          maps[i] = new MemoryOrdinalMap();
+        }
+      }
+    }
+
+    tw.addTaxonomies(otherdirs, maps);
+    // System.err.println("Merged axonomy: "+tw.getSize());
+    tw.close();
+
+    // Check that all original categories in the main taxonomy remain in
+    // unchanged, and the rest of the taxonomies are completely unchanged.
+    for (int i=0; i<ntaxonomies; i++) {
+      TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[i]);
+      TaxonomyReader copytr = new DirectoryTaxonomyReader(copydirs[i]);
+      if (i==0) {
+        assertTrue(tr.getSize() >= copytr.getSize());
+      } else {
+        assertEquals(copytr.getSize(), tr.getSize());
+      }
+      for (int j=0; j<copytr.getSize(); j++) {
+        String expected = copytr.getPath(j).toString();
+        String got = tr.getPath(j).toString();
+        assertTrue("Comparing category "+j+" of taxonomy "+i+": expected "+expected+", got "+got,
+            expected.equals(got));
+      }
+      tr.close();
+      copytr.close();
+    }
+
+    // Check that all the new categories in the main taxonomy are in
+    // lexicographic order. This isn't a requirement of our API, but happens
+    // this way in our current implementation.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0]);
+    TaxonomyReader copytr = new DirectoryTaxonomyReader(copydirs[0]);
+    if (tr.getSize() > copytr.getSize()) {
+      String prev = tr.getPath(copytr.getSize()).toString();
+      for (int j=copytr.getSize()+1; j<tr.getSize(); j++) {
+        String n = tr.getPath(j).toString();
+        assertTrue(prev.compareTo(n)<0);
+        prev=n;
+      }
+    }
+    int oldsize = copytr.getSize(); // remember for later
+    tr.close();
+    copytr.close();
+
+    // Check that all the categories from other taxonomies exist in the new
+    // taxonomy.
+    TaxonomyReader main = new DirectoryTaxonomyReader(dirs[0]);
+    for (int i=1; i<ntaxonomies; i++) {
+      TaxonomyReader other = new DirectoryTaxonomyReader(dirs[i]);
+      for (int j=0; j<other.getSize(); j++) {
+        int otherord = main.getOrdinal(other.getPath(j));
+        assertTrue(otherord != TaxonomyReader.INVALID_ORDINAL);
+      }
+      other.close();
+    }
+
+    // Check that all the new categories in the merged taxonomy exist in
+    // one of the added taxonomies.
+    TaxonomyReader[] others = new TaxonomyReader[ntaxonomies-1]; 
+    for (int i=1; i<ntaxonomies; i++) {
+      others[i-1] = new DirectoryTaxonomyReader(dirs[i]);
+    }
+    for (int j=oldsize; j<main.getSize(); j++) {
+      boolean found=false;
+      CategoryPath path = main.getPath(j);
+      for (int i=1; i<ntaxonomies; i++) {
+        if (others[i-1].getOrdinal(path) != TaxonomyReader.INVALID_ORDINAL) {
+          found=true;
+          break;
+        }
+      }
+      if (!found) {
+        fail("Found category "+j+" ("+path+") in merged taxonomy not in any of the separate ones");
+      }
+    }
+
+    // Check that all the maps are correct
+    for (int i=0; i<ntaxonomies-1; i++) {
+      int[] map = maps[i].getMap();
+      for (int j=0; j<map.length; j++) {
+        assertEquals(map[j], main.getOrdinal(others[i].getPath(j)));
+      }
+    }
+
+    for (int i=1; i<ntaxonomies; i++) {
+      others[i-1].close();
+    }
+
+    main.close();
+    IOUtils.close(dirs);
+    IOUtils.close(copydirs);
+  }
+
+}
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java
index 3f24d583..32a02cd7 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java
@@ -1 +1,81 @@
   + native
+package org.apache.lucene.facet.taxonomy.directory;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestDirectoryTaxonomyReader extends LuceneTestCase {
+
+  @Test
+  public void testCloseAfterIncRef() throws Exception {
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir);
+    ltw.addCategory(new CategoryPath("a"));
+    ltw.close();
+    
+    DirectoryTaxonomyReader ltr = new DirectoryTaxonomyReader(dir);
+    ltr.incRef();
+    ltr.close();
+    
+    // should not fail as we incRef() before close
+    ltr.getSize();
+    ltr.decRef();
+    
+    dir.close();
+  }
+  
+  @Test
+  public void testCloseTwice() throws Exception {
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir);
+    ltw.addCategory(new CategoryPath("a"));
+    ltw.close();
+    
+    DirectoryTaxonomyReader ltr = new DirectoryTaxonomyReader(dir);
+    ltr.close();
+    ltr.close(); // no exception should be thrown
+    
+    dir.close();
+  }
+  
+  @Test
+  public void testAlreadyClosed() throws Exception {
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir);
+    ltw.addCategory(new CategoryPath("a"));
+    ltw.close();
+    
+    DirectoryTaxonomyReader ltr = new DirectoryTaxonomyReader(dir);
+    ltr.close();
+    try {
+      ltr.getSize();
+      fail("An AlreadyClosedException should have been thrown here");
+    } catch (AlreadyClosedException ace) {
+      // good!
+    }
+    dir.close();
+  }
+  
+}
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
index 3f24d583..c37b6076 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
@@ -1 +1,90 @@
   + native
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.store.Directory;
+import org.junit.Test;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestDirectoryTaxonomyWriter extends LuceneTestCase {
+
+  // A No-Op TaxonomyWriterCache which always discards all given categories, and
+  // always returns true in put(), to indicate some cache entries were cleared.
+  private static class NoOpCache implements TaxonomyWriterCache {
+
+    NoOpCache() { }
+    
+    public void close() {}
+    public int get(CategoryPath categoryPath) { return -1; }
+    public int get(CategoryPath categoryPath, int length) { return get(categoryPath); }
+    public boolean put(CategoryPath categoryPath, int ordinal) { return true; }
+    public boolean put(CategoryPath categoryPath, int prefixLen, int ordinal) { return true; }
+    public boolean hasRoom(int numberOfEntries) { return false; }
+    
+  }
+  
+  @Test
+  public void testCommit() throws Exception {
+    // Verifies that nothing is committed to the underlying Directory, if
+    // commit() wasn't called.
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, new NoOpCache());
+    assertFalse(IndexReader.indexExists(dir));
+    ltw.commit(); // first commit, so that an index will be created
+    ltw.addCategory(new CategoryPath("a"));
+    
+    IndexReader r = IndexReader.open(dir);
+    assertEquals("No categories should have been committed to the underlying directory", 1, r.numDocs());
+    r.close();
+    ltw.close();
+    dir.close();
+  }
+  
+  @Test
+  public void testCommitUserData() throws Exception {
+    // Verifies that committed data is retrievable
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, new NoOpCache());
+    assertFalse(IndexReader.indexExists(dir));
+    ltw.commit(); // first commit, so that an index will be created
+    ltw.addCategory(new CategoryPath("a"));
+    ltw.addCategory(new CategoryPath("b"));
+    Map <String, String> userCommitData = new HashMap<String, String>();
+    userCommitData.put("testing", "1 2 3");
+    ltw.commit(userCommitData);
+    ltw.close();
+    IndexReader r = IndexReader.open(dir);
+    assertEquals("2 categories plus root should have been committed to the underlying directory", 3, r.numDocs());
+    Map <String, String> readUserCommitData = r.getCommitUserData();
+    assertTrue("wrong value extracted from commit data", 
+        "1 2 3".equals(readUserCommitData.get("testing")));
+    r.close();
+    dir.close();
+  }
+  
+}
diff --git a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java
index e69de29b..c912f681 100644
--- a/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java
+++ b/lucene/dev/branches/solrcloud/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java
@@ -0,0 +1,204 @@
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FilterIndexReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.junit.Test;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * This test case attempts to catch index "leaks" in LuceneTaxonomyReader/Writer,
+ * i.e., cases where an index has been opened, but never closed; In that case,
+ * Java would eventually collect this object and close the index, but leaving
+ * the index open might nevertheless cause problems - e.g., on Windows it prevents
+ * deleting it.
+ */
+public class TestIndexClose extends LuceneTestCase {
+
+  @Test
+  public void testLeaks() throws Exception {
+    LeakChecker checker = new LeakChecker();
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter tw = checker.openWriter(dir);
+    tw.close();
+    assertEquals(0, checker.nopen());
+
+    tw = checker.openWriter(dir);
+    tw.addCategory(new CategoryPath("animal", "dog"));
+    tw.close();
+    assertEquals(0, checker.nopen());
+
+    DirectoryTaxonomyReader tr = checker.openReader(dir);
+    tr.getPath(1);
+    tr.refresh();
+    tr.close();
+    assertEquals(0, checker.nopen());
+
+    tr = checker.openReader(dir);
+    tw = checker.openWriter(dir);
+    tw.addCategory(new CategoryPath("animal", "cat"));
+    tr.refresh();
+    tw.commit();
+    tw.close();
+    tr.refresh();
+    tr.close();
+    assertEquals(0, checker.nopen());
+
+    tw = checker.openWriter(dir);
+    for (int i=0; i<10000; i++) {
+      tw.addCategory(new CategoryPath("number", Integer.toString(i)));
+    }
+    tw.close();
+    assertEquals(0, checker.nopen());
+    tw = checker.openWriter(dir);
+    for (int i=0; i<10000; i++) {
+      tw.addCategory(new CategoryPath("number", Integer.toString(i*2)));
+    }
+    tw.close();
+    assertEquals(0, checker.nopen());
+    dir.close();
+  }
+
+  private static class LeakChecker {
+    int ireader=0;
+    Set<Integer> openReaders = new HashSet<Integer>();
+
+    int iwriter=0;
+    Set<Integer> openWriters = new HashSet<Integer>();
+
+    LeakChecker() { }
+    
+    public DirectoryTaxonomyWriter openWriter(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
+      return new InstrumentedTaxonomyWriter(dir);
+    }
+
+    public DirectoryTaxonomyReader openReader(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
+      return new InstrumentedTaxonomyReader(dir);
+    }
+
+    public int nopen() {
+      int ret=0;
+      for (int i: openReaders) {
+        System.err.println("reader "+i+" still open");
+        ret++;
+      }
+      for (int i: openWriters) {
+        System.err.println("writer "+i+" still open");
+        ret++;
+      }
+      return ret;
+    }
+
+    private class InstrumentedTaxonomyWriter extends DirectoryTaxonomyWriter {
+      public InstrumentedTaxonomyWriter(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
+        super(dir);
+      }    
+      @Override
+      protected IndexReader openReader() throws IOException {
+        return new InstrumentedIndexReader(super.openReader()); 
+      }
+      @Override
+      protected IndexWriter openIndexWriter (Directory directory, OpenMode openMode) throws IOException {
+        return new InstrumentedIndexWriter(directory,
+            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))
+                .setOpenMode(openMode));
+      }
+
+    }
+
+    private class InstrumentedTaxonomyReader extends DirectoryTaxonomyReader {
+      public InstrumentedTaxonomyReader(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
+        super(dir);
+      }  
+      @Override
+      protected IndexReader openIndexReader(Directory dir) throws CorruptIndexException, IOException {
+        return new InstrumentedIndexReader(IndexReader.open(dir,true)); 
+      }
+
+    }
+
+    private class InstrumentedIndexReader extends FilterIndexReader {
+      int mynum;
+      public InstrumentedIndexReader(IndexReader in) {
+        super(in);
+        this.in = in;
+        mynum = ireader++;
+        openReaders.add(mynum);
+        //        System.err.println("opened "+mynum);
+      }
+      @Override
+      protected synchronized IndexReader doOpenIfChanged() throws CorruptIndexException, IOException {
+        IndexReader n = IndexReader.openIfChanged(in);
+        if (n == null) {
+          return null;
+        }
+        return new InstrumentedIndexReader(n);
+      }
+
+      // Unfortunately, IndexReader.close() is marked final so we can't
+      // change it! Fortunately, close() calls (if the object wasn't
+      // already closed) doClose() so we can override it to do our thing -
+      // just like FilterIndexReader does.
+      @Override
+      public void doClose() throws IOException {
+        in.close();
+        if (!openReaders.contains(mynum)) { // probably can't happen...
+          fail("Reader #"+mynum+" was closed twice!");
+        }
+        openReaders.remove(mynum);
+        //        System.err.println("closed "+mynum);
+      }
+    }
+    private class InstrumentedIndexWriter extends IndexWriter {
+      int mynum;
+      public InstrumentedIndexWriter(Directory d, IndexWriterConfig conf) throws CorruptIndexException, LockObtainFailedException, IOException {
+        super(d, conf);
+        mynum = iwriter++;
+        openWriters.add(mynum);
+        //        System.err.println("openedw "+mynum);
+      }
+
+      @Override
+      public void close() throws IOException {
+        super.close();
+        if (!openWriters.contains(mynum)) { // probably can't happen...
+          fail("Writer #"+mynum+" was closed twice!");
+        }
+        openWriters.remove(mynum);
+        //        System.err.println("closedw "+mynum);
+      }
+    }
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/modules/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java b/lucene/dev/branches/solrcloud/modules/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java
index 6f38fa6b..5ed5566c 100644
--- a/lucene/dev/branches/solrcloud/modules/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java
+++ b/lucene/dev/branches/solrcloud/modules/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java
@@ -505,7 +505,7 @@ public void setNextReader(AtomicReaderContext readerContext) throws IOException
     subDocUpto = 0;
     docBase = readerContext.docBase;
     //System.out.println("setNextReader base=" + docBase + " r=" + readerContext.reader);
-    lastDocPerGroupBits = lastDocPerGroup.getDocIdSet(readerContext).iterator();
+    lastDocPerGroupBits = lastDocPerGroup.getDocIdSet(readerContext, readerContext.reader.getLiveDocs()).iterator();
     groupEndDocID = -1;
 
     currentReaderContext = readerContext;
diff --git a/lucene/dev/branches/solrcloud/modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java b/lucene/dev/branches/solrcloud/modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
index 0094170a..8d243ce7 100644
--- a/lucene/dev/branches/solrcloud/modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
+++ b/lucene/dev/branches/solrcloud/modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
@@ -1221,11 +1221,11 @@ public ShardSearcher(IndexReader.AtomicReaderContext ctx, IndexReader.ReaderCont
     }
 
     public void search(Weight weight, Collector collector) throws IOException {
-      search(ctx, weight, null, collector);
+      search(ctx, weight, collector);
     }
 
     public TopDocs search(Weight weight, int topN) throws IOException {
-      return search(ctx, weight, null, null, topN);
+      return search(ctx, weight, null, topN);
     }
 
     @Override
diff --git a/lucene/dev/branches/solrcloud/modules/join/src/java/org/apache/lucene/search/join/BlockJoinCollector.java b/lucene/dev/branches/solrcloud/modules/join/src/java/org/apache/lucene/search/join/BlockJoinCollector.java
index 9d731d9b..38300b3e 100644
--- a/lucene/dev/branches/solrcloud/modules/join/src/java/org/apache/lucene/search/join/BlockJoinCollector.java
+++ b/lucene/dev/branches/solrcloud/modules/join/src/java/org/apache/lucene/search/join/BlockJoinCollector.java
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.LinkedList;
@@ -387,15 +386,17 @@ private void sortQueue() {
     // unbox once
     final int slot = _slot;
 
+    if (sortedGroups == null) {
     if (offset >= queue.size()) {
       return null;
     }
-    int totalGroupedHitCount = 0;
-
-    if (sortedGroups == null) {
       sortQueue();
+    } else if (offset > sortedGroups.length) {
+      return null;
     }
 
+    int totalGroupedHitCount = 0;
+
     final FakeScorer fakeScorer = new FakeScorer();
 
     final GroupDocs<Integer>[] groups = new GroupDocs[sortedGroups.length - offset];
diff --git a/lucene/dev/branches/solrcloud/modules/join/src/java/org/apache/lucene/search/join/BlockJoinQuery.java b/lucene/dev/branches/solrcloud/modules/join/src/java/org/apache/lucene/search/join/BlockJoinQuery.java
index 37dc1f92..b8978893 100644
--- a/lucene/dev/branches/solrcloud/modules/join/src/java/org/apache/lucene/search/join/BlockJoinQuery.java
+++ b/lucene/dev/branches/solrcloud/modules/join/src/java/org/apache/lucene/search/join/BlockJoinQuery.java
@@ -163,7 +163,7 @@ public Scorer scorer(AtomicReaderContext readerContext, boolean scoreDocsInOrder
         return null;
       }
 
-      final DocIdSet parents = parentsFilter.getDocIdSet(readerContext);
+      final DocIdSet parents = parentsFilter.getDocIdSet(readerContext, readerContext.reader.getLiveDocs());
       // TODO: once we do random-access filters we can
       // generalize this:
       if (parents == null) {
diff --git a/lucene/dev/branches/solrcloud/modules/join/src/test/org/apache/lucene/search/TestBlockJoin.java b/lucene/dev/branches/solrcloud/modules/join/src/test/org/apache/lucene/search/TestBlockJoin.java
index 2551449c..69da7c33 100644
--- a/lucene/dev/branches/solrcloud/modules/join/src/test/org/apache/lucene/search/TestBlockJoin.java
+++ b/lucene/dev/branches/solrcloud/modules/join/src/test/org/apache/lucene/search/TestBlockJoin.java
@@ -57,6 +57,14 @@ private Document makeJob(String skill, int year) {
     return job;
   }
 
+  // ... has multiple qualifications
+  private Document makeQualification(String qualification, int year) {
+    Document job = new Document();
+    job.add(newField("qualification", qualification, StringField.TYPE_STORED));
+    job.add(new NumericField("year").setIntValue(year));
+    return job;
+  }
+
   public void testSimple() throws Exception {
 
     final Directory dir = newDirectory();
@@ -492,4 +500,94 @@ private void compareHits(IndexReader r, IndexReader joinR, TopDocs results, TopG
       }
     }
   }
+
+  public void testMultiChildTypes() throws Exception {
+
+    final Directory dir = newDirectory();
+    final RandomIndexWriter w = new RandomIndexWriter(random, dir);
+
+    final List<Document> docs = new ArrayList<Document>();
+
+    docs.add(makeJob("java", 2007));
+    docs.add(makeJob("python", 2010));
+    docs.add(makeQualification("maths", 1999));
+    docs.add(makeResume("Lisa", "United Kingdom"));
+    w.addDocuments(docs);
+
+    IndexReader r = w.getReader();
+    w.close();
+    IndexSearcher s = new IndexSearcher(r);
+
+    // Create a filter that defines "parent" documents in the index - in this case resumes
+    Filter parentsFilter = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("docType", "resume"))));
+
+    // Define child document criteria (finds an example of relevant work experience)
+    BooleanQuery childJobQuery = new BooleanQuery();
+    childJobQuery.add(new BooleanClause(new TermQuery(new Term("skill", "java")), Occur.MUST));
+    childJobQuery.add(new BooleanClause(NumericRangeQuery.newIntRange("year", 2006, 2011, true, true), Occur.MUST));
+
+    BooleanQuery childQualificationQuery = new BooleanQuery();
+    childQualificationQuery.add(new BooleanClause(new TermQuery(new Term("qualification", "maths")), Occur.MUST));
+    childQualificationQuery.add(new BooleanClause(NumericRangeQuery.newIntRange("year", 1980, 2000, true, true), Occur.MUST));
+
+
+    // Define parent document criteria (find a resident in the UK)
+    Query parentQuery = new TermQuery(new Term("country", "United Kingdom"));
+
+    // Wrap the child document query to 'join' any matches
+    // up to corresponding parent:
+    BlockJoinQuery childJobJoinQuery = new BlockJoinQuery(childJobQuery, parentsFilter, BlockJoinQuery.ScoreMode.Avg);
+    BlockJoinQuery childQualificationJoinQuery = new BlockJoinQuery(childQualificationQuery, parentsFilter, BlockJoinQuery.ScoreMode.Avg);
+
+    // Combine the parent and nested child queries into a single query for a candidate
+    BooleanQuery fullQuery = new BooleanQuery();
+    fullQuery.add(new BooleanClause(parentQuery, Occur.MUST));
+    fullQuery.add(new BooleanClause(childJobJoinQuery, Occur.MUST));
+    fullQuery.add(new BooleanClause(childQualificationJoinQuery, Occur.MUST));
+
+    //????? How do I control volume of jobs vs qualifications per parent?
+    BlockJoinCollector c = new BlockJoinCollector(Sort.RELEVANCE, 10, true, false);
+
+    s.search(fullQuery, c);
+
+    //Examine "Job" children
+    boolean showNullPointerIssue=true;
+    if (showNullPointerIssue) {
+      TopGroups<Integer> jobResults = c.getTopGroups(childJobJoinQuery, null, 0, 10, 0, true);
+
+      //assertEquals(1, results.totalHitCount);
+      assertEquals(1, jobResults.totalGroupedHitCount);
+      assertEquals(1, jobResults.groups.length);
+
+      final GroupDocs<Integer> group = jobResults.groups[0];
+      assertEquals(1, group.totalHits);
+
+      Document childJobDoc = s.doc(group.scoreDocs[0].doc);
+      //System.out.println("  doc=" + group.scoreDocs[0].doc);
+      assertEquals("java", childJobDoc.get("skill"));
+      assertNotNull(group.groupValue);
+      Document parentDoc = s.doc(group.groupValue);
+      assertEquals("Lisa", parentDoc.get("name"));
+    }
+
+    //Now Examine qualification children
+    TopGroups<Integer> qualificationResults = c.getTopGroups(childQualificationJoinQuery, null, 0, 10, 0, true);
+
+    //!!!!! This next line can null pointer - but only if prior "jobs" section called first
+    assertEquals(1, qualificationResults.totalGroupedHitCount);
+    assertEquals(1, qualificationResults.groups.length);
+
+    final GroupDocs<Integer> qGroup = qualificationResults.groups[0];
+    assertEquals(1, qGroup.totalHits);
+
+    Document childQualificationDoc = s.doc(qGroup.scoreDocs[0].doc);
+    assertEquals("maths", childQualificationDoc.get("qualification"));
+    assertNotNull(qGroup.groupValue);
+    Document parentDoc = s.doc(qGroup.groupValue);
+    assertEquals("Lisa", parentDoc.get("name"));
+
+
+    r.close();
+    dir.close();
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/BooleanFilter.java b/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
index d74fc4f4..f5ef73f3 100644
--- a/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
+++ b/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
@@ -24,10 +24,12 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.FixedBitSet;
 
 /**
@@ -48,7 +50,7 @@
    * of the filters that have been added.
    */
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
     FixedBitSet res = null;
     final IndexReader reader = context.reader;
     
@@ -91,12 +93,13 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
       }
     }
 
-    return res != null ? res : DocIdSet.EMPTY_DOCIDSET;
+    return res != null ? BitsFilteredDocIdSet.wrap(res, acceptDocs) : DocIdSet.EMPTY_DOCIDSET;
   }
 
   private static DocIdSetIterator getDISI(Filter filter, AtomicReaderContext context)
       throws IOException {
-    final DocIdSet set = filter.getDocIdSet(context);
+    // we dont pass acceptDocs, we will filter at the end using an additional filter
+    final DocIdSet set = filter.getDocIdSet(context, null);
     return (set == null || set == DocIdSet.EMPTY_DOCIDSET) ? null : set.iterator();
   }
 
diff --git a/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/ChainedFilter.java b/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
index 2352e132..5cea4982 100644
--- a/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
+++ b/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
@@ -19,9 +19,11 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.OpenBitSet;
 import org.apache.lucene.util.OpenBitSetDISI;
 
@@ -97,21 +99,22 @@ public ChainedFilter(Filter[] chain, int logic) {
    * {@link Filter#getDocIdSet}.
    */
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
     int[] index = new int[1]; // use array as reference to modifiable int;
     index[0] = 0;             // an object attribute would not be thread safe.
     if (logic != -1) {
-      return getDocIdSet(context, logic, index);
+      return BitsFilteredDocIdSet.wrap(getDocIdSet(context, logic, index), acceptDocs);
     } else if (logicArray != null) {
-      return getDocIdSet(context, logicArray, index);
+      return BitsFilteredDocIdSet.wrap(getDocIdSet(context, logicArray, index), acceptDocs);
     }
 
-    return getDocIdSet(context, DEFAULT, index);
+    return BitsFilteredDocIdSet.wrap(getDocIdSet(context, DEFAULT, index), acceptDocs);
   }
 
   private DocIdSetIterator getDISI(Filter filter, AtomicReaderContext context)
       throws IOException {
-    DocIdSet docIdSet = filter.getDocIdSet(context);
+    // we dont pass acceptDocs, we will filter at the end using an additional filter
+    DocIdSet docIdSet = filter.getDocIdSet(context, null);
     if (docIdSet == null) {
       return DocIdSet.EMPTY_DOCIDSET.iterator();
     } else {
@@ -156,7 +159,8 @@ private DocIdSet getDocIdSet(AtomicReaderContext context, int logic, int[] index
       throws IOException {
     OpenBitSetDISI result = initialResult(context, logic, index);
     for (; index[0] < chain.length; index[0]++) {
-      doChain(result, logic, chain[index[0]].getDocIdSet(context));
+      // we dont pass acceptDocs, we will filter at the end using an additional filter
+      doChain(result, logic, chain[index[0]].getDocIdSet(context, null));
     }
     return result;
   }
@@ -176,7 +180,8 @@ private DocIdSet getDocIdSet(AtomicReaderContext context, int[] logic, int[] ind
 
     OpenBitSetDISI result = initialResult(context, logic[0], index);
     for (; index[0] < chain.length; index[0]++) {
-      doChain(result, logic[index[0]], chain[index[0]].getDocIdSet(context));
+      // we dont pass acceptDocs, we will filter at the end using an additional filter
+      doChain(result, logic[index[0]], chain[index[0]].getDocIdSet(context, null));
     }
     return result;
   }
diff --git a/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java b/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java
index cb27f23e..bfd7a96c 100644
--- a/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java
+++ b/lucene/dev/branches/solrcloud/modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java
@@ -54,7 +54,7 @@ public void addTerm(Term term) {
    */
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
     IndexReader reader = context.reader;
     FixedBitSet result = new FixedBitSet(reader.maxDoc());
     Fields fields = reader.fields();
@@ -64,7 +64,6 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
     }
 
     BytesRef br = new BytesRef();
-    Bits liveDocs = reader.getLiveDocs();
     String lastField = null;
     Terms termsC = null;
     TermsEnum termsEnum = null;
@@ -72,6 +71,9 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
     for (Term term : terms) {
       if (!term.field().equals(lastField)) {
         termsC = fields.terms(term.field());
+        if (termsC == null) {
+          return result;
+        }
         termsEnum = termsC.iterator();
         lastField = term.field();
       }
@@ -79,7 +81,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
       if (terms != null) { // TODO this check doesn't make sense, decide which variable its supposed to be for
         br.copy(term.bytes());
         if (termsEnum.seekCeil(br) == TermsEnum.SeekStatus.FOUND) {
-          docs = termsEnum.docs(liveDocs, docs);
+          docs = termsEnum.docs(acceptDocs, docs);
           while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {
             result.set(docs.docID());
           }
diff --git a/lucene/dev/branches/solrcloud/modules/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java b/lucene/dev/branches/solrcloud/modules/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java
index 87ea97cc..6f682223 100644
--- a/lucene/dev/branches/solrcloud/modules/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java
+++ b/lucene/dev/branches/solrcloud/modules/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java
@@ -35,6 +35,7 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.QueryWrapperFilter;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.LuceneTestCase;
 
 import java.io.IOException;
@@ -94,7 +95,7 @@ private Filter getWrappedTermQuery(String field, String text) {
   private Filter getNullDISFilter() {
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
         return null;
       }
     };
@@ -103,7 +104,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) {
   private Filter getNullDISIFilter() {
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
         return new DocIdSet() {
           @Override
           public DocIdSetIterator iterator() {
@@ -122,7 +123,7 @@ public boolean isCacheable() {
   private void tstFilterCard(String mes, int expected, Filter filt)
       throws Exception {
     // BooleanFilter never returns null DIS or null DISI!
-    DocIdSetIterator disi = filt.getDocIdSet(new AtomicReaderContext(reader)).iterator();
+    DocIdSetIterator disi = filt.getDocIdSet(new AtomicReaderContext(reader), reader.getLiveDocs()).iterator();
     int actual = 0;
     while (disi.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
       actual++;
diff --git a/lucene/dev/branches/solrcloud/modules/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java b/lucene/dev/branches/solrcloud/modules/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
index cd485b66..ef13c0e5 100644
--- a/lucene/dev/branches/solrcloud/modules/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
+++ b/lucene/dev/branches/solrcloud/modules/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
@@ -28,6 +28,7 @@
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.MultiReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.index.Term;
@@ -35,6 +36,7 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.ReaderUtil;
 
 public class TermsFilterTest extends LuceneTestCase {
 
@@ -73,22 +75,57 @@ public void testMissingTerms() throws Exception {
 
     TermsFilter tf = new TermsFilter();
     tf.addTerm(new Term(fieldName, "19"));
-    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context);
+    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
     assertEquals("Must match nothing", 0, bits.cardinality());
 
     tf.addTerm(new Term(fieldName, "20"));
-    bits = (FixedBitSet) tf.getDocIdSet(context);
+    bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
     assertEquals("Must match 1", 1, bits.cardinality());
 
     tf.addTerm(new Term(fieldName, "10"));
-    bits = (FixedBitSet) tf.getDocIdSet(context);
+    bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
     assertEquals("Must match 2", 2, bits.cardinality());
 
     tf.addTerm(new Term(fieldName, "00"));
-    bits = (FixedBitSet) tf.getDocIdSet(context);
+    bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
     assertEquals("Must match 2", 2, bits.cardinality());
 
     reader.close();
     rd.close();
   }
+  
+  public void testMissingField() throws Exception {
+    String fieldName = "field1";
+    Directory rd1 = newDirectory();
+    RandomIndexWriter w1 = new RandomIndexWriter(random, rd1);
+    Document doc = new Document();
+    doc.add(newField(fieldName, "content1", StringField.TYPE_STORED));
+    w1.addDocument(doc);
+    IndexReader reader1 = w1.getReader();
+    w1.close();
+    
+    fieldName = "field2";
+    Directory rd2 = newDirectory();
+    RandomIndexWriter w2 = new RandomIndexWriter(random, rd2);
+    doc = new Document();
+    doc.add(newField(fieldName, "content2", StringField.TYPE_STORED));
+    w2.addDocument(doc);
+    IndexReader reader2 = w2.getReader();
+    w2.close();
+    
+    TermsFilter tf = new TermsFilter();
+    tf.addTerm(new Term(fieldName, "content1"));
+    
+    MultiReader multi = new MultiReader(reader1, reader2);
+    for (IndexReader.AtomicReaderContext context : ReaderUtil.leaves(multi.getTopReaderContext())) {
+      FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
+      assertTrue("Must be >= 0", bits.cardinality() >= 0);      
+    }
+    multi.close();
+    reader1.close();
+    reader2.close();
+    rd1.close();
+    rd2.close();
+  }
+
 }
diff --git a/lucene/dev/branches/solrcloud/modules/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java b/lucene/dev/branches/solrcloud/modules/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
index 384adf63..39dca3aa 100644
--- a/lucene/dev/branches/solrcloud/modules/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
+++ b/lucene/dev/branches/solrcloud/modules/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
@@ -203,12 +203,8 @@ protected QueryNode postProcessNode(QueryNode node) throws QueryNodeException {
             children.add(new FieldQueryNode(field, term, -1, -1));
 
           }
-          if (positionCount == 1)
             return new GroupQueryNode(
-              new StandardBooleanQueryNode(children, true));
-          else
-            return new StandardBooleanQueryNode(children, false);
-
+            new StandardBooleanQueryNode(children, positionCount==1));
         } else {
           // phrase query:
           MultiPhraseQueryNode mpq = new MultiPhraseQueryNode();
diff --git a/lucene/dev/branches/solrcloud/modules/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java b/lucene/dev/branches/solrcloud/modules/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
index e368eecd..c02a9659 100644
--- a/lucene/dev/branches/solrcloud/modules/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
+++ b/lucene/dev/branches/solrcloud/modules/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
@@ -21,6 +21,7 @@
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.NumericRangeFilter;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.queryparser.xml.DOMUtils;
 import org.apache.lucene.queryparser.xml.FilterBuilder;
@@ -155,7 +156,7 @@ public Filter getFilter(Element e) throws ParserException {
   static class NoMatchFilter extends Filter {
 
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
       return null;
 		}
 
diff --git a/lucene/dev/branches/solrcloud/modules/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java b/lucene/dev/branches/solrcloud/modules/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java
index a132341c..55d155b8 100644
--- a/lucene/dev/branches/solrcloud/modules/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java
+++ b/lucene/dev/branches/solrcloud/modules/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java
@@ -359,8 +359,16 @@ public void testCJKTerm() throws Exception {
     BooleanQuery expected = new BooleanQuery();
     expected.add(new TermQuery(new Term("field", "中")), BooleanClause.Occur.SHOULD);
     expected.add(new TermQuery(new Term("field", "国")), BooleanClause.Occur.SHOULD);
-    
     assertEquals(expected, getQuery("中国", analyzer));
+    
+    expected = new BooleanQuery();
+    expected.add(new TermQuery(new Term("field", "中")), BooleanClause.Occur.MUST);
+    BooleanQuery inner = new BooleanQuery();
+    inner.add(new TermQuery(new Term("field", "中")), BooleanClause.Occur.SHOULD);
+    inner.add(new TermQuery(new Term("field", "国")), BooleanClause.Occur.SHOULD);
+    expected.add(inner, BooleanClause.Occur.MUST);
+    assertEquals(expected, getQuery("中 AND 中国", new SimpleCJKAnalyzer()));
+
   }
   
   public void testCJKBoostedTerm() throws Exception {
@@ -609,7 +617,7 @@ public void testQPA() throws Exception {
 
     assertQueryEquals("drop AND stop AND roll", qpAnalyzer, "+drop +roll");
     assertQueryEquals("term phrase term", qpAnalyzer,
-        "term phrase1 phrase2 term");
+        "term (phrase1 phrase2) term");
 
     assertQueryEquals("term AND NOT phrase term", qpAnalyzer,
         "+term -(phrase1 phrase2) term");
diff --git a/lucene/dev/branches/solrcloud/modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java b/lucene/dev/branches/solrcloud/modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
index 1fce8274..e409fa34 100644
--- a/lucene/dev/branches/solrcloud/modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
+++ b/lucene/dev/branches/solrcloud/modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
@@ -65,7 +65,7 @@ public void testGetFilterHandleNumericParseError() throws Exception {
     try {
       IndexReader reader = new SlowMultiReaderWrapper(IndexReader.open(ramDir, true));
       try {
-        assertNull(filter.getDocIdSet((AtomicReaderContext) reader.getTopReaderContext()));
+        assertNull(filter.getDocIdSet((AtomicReaderContext) reader.getTopReaderContext(), reader.getLiveDocs()));
       }
       finally {
         reader.close();
diff --git a/lucene/dev/branches/solrcloud/modules/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java b/lucene/dev/branches/solrcloud/modules/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
index 6f3b9f32..3476c8e8 100644
--- a/lucene/dev/branches/solrcloud/modules/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
+++ b/lucene/dev/branches/solrcloud/modules/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
@@ -71,10 +71,7 @@
    *         shortest of the two terms instead of the longest.
    *  </ul>
    */
-  public static final StringDistance INTERNAL_LEVENSHTEIN = new StringDistance() {
-    public float getDistance(String s1, String s2) {
-      throw new UnsupportedOperationException("Not for external use.");
-    }};
+  public static final StringDistance INTERNAL_LEVENSHTEIN = new LuceneLevenshteinDistance();
 
   /** maximum edit distance for candidate terms */
   private int maxEdits = LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE;
diff --git a/lucene/dev/branches/solrcloud/modules/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java b/lucene/dev/branches/solrcloud/modules/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java
index e69de29b..054b718c 100644
--- a/lucene/dev/branches/solrcloud/modules/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java
+++ b/lucene/dev/branches/solrcloud/modules/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java
@@ -0,0 +1,104 @@
+package org.apache.lucene.search.spell;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.IntsRef;
+
+/**
+ *  Levenshtein implemented in a consistent way as Lucene's FuzzyTermsEnum.
+ *  
+ *  Note also that this metric differs in subtle ways from {@link LevensteinDistance}:
+ *  <ul>
+ *    <li> This metric treats full unicode codepoints as characters, but
+ *         LevenshteinDistance calculates based on UTF-16 code units.
+ *    <li> This metric scales raw edit distances into a floating point score
+ *         differently than LevenshteinDistance: the scaling is based upon the
+ *         shortest of the two terms instead of the longest.
+ *  </ul>
+ */
+public final class LuceneLevenshteinDistance implements StringDistance {
+
+  @Override
+  public float getDistance(String target, String other) {
+    IntsRef targetPoints;
+    IntsRef otherPoints;
+    int n;
+    int p[]; //'previous' cost array, horizontally
+    int d[]; // cost array, horizontally
+    int _d[]; //placeholder to assist in swapping p and d
+    
+    // cheaper to do this up front once
+    targetPoints = toIntsRef(target);
+    otherPoints = toIntsRef(other);
+    n = targetPoints.length;
+    p = new int[n+1]; 
+    d = new int[n+1]; 
+    
+    final int m = otherPoints.length;
+    if (n == 0 || m == 0) {
+      if (n == m) {
+        return 1;
+      }
+      else {
+        return 0;
+      }
+    } 
+
+
+    // indexes into strings s and t
+    int i; // iterates through s
+    int j; // iterates through t
+
+    int t_j; // jth character of t
+
+    int cost; // cost
+
+    for (i = 0; i <= n; i++) {
+      p[i] = i;
+    }
+
+    for (j = 1; j <= m; j++) {
+      t_j = otherPoints.ints[j - 1];
+      d[0] = j;
+
+      for (i=1; i <= n; i++) {
+        cost = targetPoints.ints[i - 1] == t_j ? 0 : 1;
+        // minimum of cell to the left+1, to the top+1, diagonally left and up +cost
+        d[i] = Math.min(Math.min(d[i - 1] + 1, p[i] + 1),  p[i - 1] + cost);
+      }
+
+      // copy current distance counts to 'previous row' distance counts
+      _d = p;
+       p = d;
+       d = _d;
+    }
+
+    // our last action in the above loop was to switch d and p, so p now
+    // actually has the most recent cost counts
+    return 1.0f - ((float) p[n] / Math.min(m, n));
+  }
+  
+  private static IntsRef toIntsRef(String s) {
+    IntsRef ref = new IntsRef(s.length()); // worst case
+    int utf16Len = s.length();
+    for (int i = 0, cp = 0; i < utf16Len; i += Character.charCount(cp)) {
+      cp = ref.ints[ref.length++] = Character.codePointAt(s, i);
+    }
+    return ref;
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/modules/suggest/src/test/org/apache/lucene/search/spell/TestDirectSpellChecker.java b/lucene/dev/branches/solrcloud/modules/suggest/src/test/org/apache/lucene/search/spell/TestDirectSpellChecker.java
index 38461acb..53c0458d 100644
--- a/lucene/dev/branches/solrcloud/modules/suggest/src/test/org/apache/lucene/search/spell/TestDirectSpellChecker.java
+++ b/lucene/dev/branches/solrcloud/modules/suggest/src/test/org/apache/lucene/search/spell/TestDirectSpellChecker.java
@@ -30,6 +30,35 @@
 
 public class TestDirectSpellChecker extends LuceneTestCase {
 
+  public void testInternalLevenshteinDistance() throws Exception {
+    DirectSpellChecker spellchecker = new DirectSpellChecker();
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random, dir, 
+        new MockAnalyzer(random, MockTokenizer.KEYWORD, true));
+
+    String[] termsToAdd = { "metanoia", "metanoian", "metanoiai", "metanoias", "metanoi𐑍" };
+    for (int i = 0; i < termsToAdd.length; i++) {
+      Document doc = new Document();
+      doc.add(newField("repentance", termsToAdd[i], TextField.TYPE_UNSTORED));
+      writer.addDocument(doc);
+    }
+
+    IndexReader ir = writer.getReader();
+    String misspelled = "metanoix";
+    SuggestWord[] similar = spellchecker.suggestSimilar(new Term("repentance", misspelled), 4, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX);
+    assertTrue(similar.length == 4);
+    
+    StringDistance sd = spellchecker.getDistance();
+    assertTrue(sd instanceof LuceneLevenshteinDistance);
+    for(SuggestWord word : similar) {
+      assertTrue(word.score==sd.getDistance(word.string, misspelled));
+      assertTrue(word.score==sd.getDistance(misspelled, word.string));
+    }
+    
+    ir.close();
+    writer.close();
+    dir.close();
+  }
   public void testSimpleExamples() throws Exception {
     DirectSpellChecker spellChecker = new DirectSpellChecker();
     spellChecker.setMinQueryLength(0);
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngine.java b/lucene/dev/branches/solrcloud/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngine.java
index 064e1b33..24dc398f 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngine.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngine.java
@@ -20,8 +20,7 @@
  * limitations under the License.
  */
 
-import java.io.IOException;
-import java.io.InputStream;
+import java.io.*;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.HashMap;
@@ -31,6 +30,7 @@
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.commons.io.IOUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.lucene.search.Query;
 import org.apache.solr.common.SolrDocument;
@@ -70,13 +70,14 @@
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import com.google.common.collect.Sets;
+import com.google.common.io.Closeables;
 
 /**
  * Search results clustering engine based on Carrot2 clustering algorithms.
  * <p/>
  * Output from this class is subject to change.
  *
- * @link http://project.carrot2.org
+ * @see "http://project.carrot2.org"
  */
 public class CarrotClusteringEngine extends SearchClusteringEngine {
 	private transient static Logger log = LoggerFactory
@@ -105,6 +106,90 @@
   private Controller controller = ControllerFactory.createPooling();
   private Class<? extends IClusteringAlgorithm> clusteringAlgorithmClass;
 
+  private static class SolrResourceLocator implements IResourceLocator {
+    private final SolrResourceLoader resourceLoader;
+    private final String carrot2ResourcesDir;
+
+    public SolrResourceLocator(SolrCore core, SolrParams initParams) {
+      resourceLoader = core.getResourceLoader();
+      carrot2ResourcesDir = initParams.get(
+          CarrotParams.LEXICAL_RESOURCES_DIR, CARROT_RESOURCES_PREFIX);
+    }
+
+    @Override
+    public IResource[] getAll(final String resource) {
+      final String resourceName = carrot2ResourcesDir + "/" + resource;
+      log.debug("Looking for Solr resource: " + resourceName);
+
+      InputStream resourceStream = null;
+      final byte [] asBytes;
+      try {
+        resourceStream = resourceLoader.openResource(resourceName);
+        asBytes = IOUtils.toByteArray(resourceStream);
+      } catch (RuntimeException e) {
+        log.debug("Resource not found in Solr's config: " + resourceName
+            + ". Using the default " + resource + " from Carrot JAR.");          
+        return new IResource[] {};
+      } catch (IOException e) {
+        log.warn("Could not read Solr resource " + resourceName);
+        return new IResource[] {};
+      } finally {
+        if (resourceStream != null) Closeables.closeQuietly(resourceStream);
+      }
+
+      log.info("Loaded Solr resource: " + resourceName);
+
+      final IResource foundResource = new IResource() {
+        @Override
+        public InputStream open() throws IOException {
+          return new ByteArrayInputStream(asBytes);
+        }
+        
+        @Override
+        public int hashCode() {
+          // In case multiple resources are found they will be deduped, but we don't use it in Solr,
+          // so simply rely on instance equivalence.
+          return super.hashCode();
+        }
+        
+        @Override
+        public boolean equals(Object obj) {
+          // In case multiple resources are found they will be deduped, but we don't use it in Solr,
+          // so simply rely on instance equivalence.
+          return super.equals(obj);
+        }
+
+        @Override
+        public String toString() {
+          return "Solr config resource: " + resourceName;
+        }
+      };
+
+      return new IResource[] { foundResource };
+    }
+
+    @Override
+    public int hashCode() {
+      // In case multiple locations are used locators will be deduped, but we don't use it in Solr,
+      // so simply rely on instance equivalence.
+      return super.hashCode();
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+      // In case multiple locations are used locators will be deduped, but we don't use it in Solr,
+      // so simply rely on instance equivalence.
+      return super.equals(obj);
+    }
+
+    @Override
+    public String toString() {
+      return "SolrResourceLocator, " 
+          + "configDir=" + new File(resourceLoader.getConfigDir()).getAbsolutePath()
+          + ", Carrot2 relative lexicalResourcesDir=";
+    }
+  }
+
   @Override
   @Deprecated
   public Object cluster(Query query, DocList docList, SolrQueryRequest sreq) {
@@ -171,38 +256,10 @@ public String init(NamedList config, final SolrCore core) {
 
     // Customize Carrot2's resource lookup to first look for resources
     // using Solr's resource loader. If that fails, try loading from the classpath.
-    DefaultLexicalDataFactoryDescriptor.attributeBuilder(initAttributes)
-        .resourceLookup(new ResourceLookup(new IResourceLocator() {
-          @Override
-          public IResource[] getAll(final String resource) {
-            final SolrResourceLoader resourceLoader = core.getResourceLoader();
-            final String carrot2ResourcesDir = initParams.get(
-                CarrotParams.LEXICAL_RESOURCES_DIR, CARROT_RESOURCES_PREFIX);
-            try {
-              log.debug("Looking for " + resource + " in "
-                  + carrot2ResourcesDir);
-              final InputStream resourceStream = resourceLoader
-                  .openResource(carrot2ResourcesDir + "/" + resource);
-
-              log.info(resource + " loaded from " + carrot2ResourcesDir);
-              final IResource foundResource = new IResource() {
-                @Override
-                public InputStream open() throws IOException {
-                  return resourceStream;
-                }
-              };
-              return new IResource[] { foundResource };
-            } catch (RuntimeException e) {
-              // No way to distinguish if the resource was found but failed
-              // to load or wasn't found at all, so we simply fall back
-              // to Carrot2 defaults here by returning an empty locations array.
-              log.debug(resource + " not found in " + carrot2ResourcesDir
-                  + ". Using the default " + resource + " from Carrot JAR.");
-              return new IResource[] {};
-            }
-          }
-        },
-
+    DefaultLexicalDataFactoryDescriptor.attributeBuilder(initAttributes).resourceLookup(
+      new ResourceLookup(
+        // Solr-specific resource loading.
+        new SolrResourceLocator(core, initParams),
         // Using the class loader directly because this time we want to omit the prefix
         new ClassLoaderLocator(core.getResourceLoader().getClassLoader())));
 
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingDocumentLoader.java b/lucene/dev/branches/solrcloud/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingDocumentLoader.java
index 756a8234..f9031aa5 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingDocumentLoader.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingDocumentLoader.java
@@ -144,10 +144,6 @@ public void load(SolrQueryRequest req, SolrQueryResponse rsp, ContentStream stre
     }
     if (parser != null) {
       Metadata metadata = new Metadata();
-      metadata.add(ExtractingMetadataConstants.STREAM_NAME, stream.getName());
-      metadata.add(ExtractingMetadataConstants.STREAM_SOURCE_INFO, stream.getSourceInfo());
-      metadata.add(ExtractingMetadataConstants.STREAM_SIZE, String.valueOf(stream.getSize()));
-      metadata.add(ExtractingMetadataConstants.STREAM_CONTENT_TYPE, stream.getContentType());
 
       // If you specify the resource name (the filename, roughly) with this parameter,
       // then Tika can make use of it in guessing the appropriate MIME type:
@@ -156,12 +152,16 @@ public void load(SolrQueryRequest req, SolrQueryResponse rsp, ContentStream stre
         metadata.add(Metadata.RESOURCE_NAME_KEY, resourceName);
       }
 
-      SolrContentHandler handler = factory.createSolrContentHandler(metadata, params, schema);
       InputStream inputStream = null;
       try {
         inputStream = stream.getStream();
+        metadata.add(ExtractingMetadataConstants.STREAM_NAME, stream.getName());
+        metadata.add(ExtractingMetadataConstants.STREAM_SOURCE_INFO, stream.getSourceInfo());
+        metadata.add(ExtractingMetadataConstants.STREAM_SIZE, String.valueOf(stream.getSize()));
+        metadata.add(ExtractingMetadataConstants.STREAM_CONTENT_TYPE, stream.getContentType());
         String xpathExpr = params.get(ExtractingParams.XPATH_EXPRESSION);
         boolean extractOnly = params.getBool(ExtractingParams.EXTRACT_ONLY, false);
+        SolrContentHandler handler = factory.createSolrContentHandler(metadata, params, schema);
         ContentHandler parsingHandler = handler;
 
         StringWriter writer = null;
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessor.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessor.java
index e69de29b..f0f3156d 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessor.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessor.java
@@ -0,0 +1,66 @@
+package org.apache.solr.update.processor;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.response.SolrQueryResponse;
+
+import com.cybozu.labs.langdetect.Detector;
+import com.cybozu.labs.langdetect.DetectorFactory;
+import com.cybozu.labs.langdetect.LangDetectException;
+import com.cybozu.labs.langdetect.Language;
+
+/**
+ * Identifies the language of a set of input fields using http://code.google.com/p/language-detection
+ * <p>
+ * See <a href="http://wiki.apache.org/solr/LanguageDetection">http://wiki.apache.org/solr/LanguageDetection</a>
+ * @since 3.5
+ */
+public class LangDetectLanguageIdentifierUpdateProcessor extends LanguageIdentifierUpdateProcessor {
+  
+  public LangDetectLanguageIdentifierUpdateProcessor(SolrQueryRequest req, 
+      SolrQueryResponse rsp, UpdateRequestProcessor next) {
+    super(req, rsp, next);
+  }
+
+  @Override
+  protected List<DetectedLanguage> detectLanguage(String content) {
+    if (content.trim().length() == 0) { // to be consistent with the tika impl?
+      log.debug("No input text to detect language from, returning empty list");
+      return Collections.emptyList();
+    }
+    
+    try {
+      Detector detector = DetectorFactory.create();
+      detector.append(content);
+      ArrayList<Language> langlist = detector.getProbabilities();
+      ArrayList<DetectedLanguage> solrLangList = new ArrayList<DetectedLanguage>();
+      for (Language l: langlist) {
+        solrLangList.add(new DetectedLanguage(l.lang, l.prob));
+      }
+      return solrLangList;
+    } catch (LangDetectException e) {
+      log.debug("Could not determine language, returning empty list: ", e);
+      return Collections.emptyList();
+    }
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactory.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactory.java
index 3f24d583..f545fd8b 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactory.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactory.java
@@ -1 +1,136 @@
   + native
+package org.apache.solr.update.processor;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.Charset;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.io.IOUtils;
+import org.apache.solr.common.params.SolrParams;
+import org.apache.solr.common.util.NamedList;
+import org.apache.solr.core.SolrCore;
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.response.SolrQueryResponse;
+import org.apache.solr.util.SolrPluginUtils;
+import org.apache.solr.util.plugin.SolrCoreAware;
+
+import com.cybozu.labs.langdetect.DetectorFactory;
+import com.cybozu.labs.langdetect.LangDetectException;
+
+/**
+ * Identifies the language of a set of input fields using 
+ * http://code.google.com/p/language-detection
+ * <p/>
+ * The UpdateProcessorChain config entry can take a number of parameters
+ * which may also be passed as HTTP parameters on the update request
+ * and override the defaults. Here is the simplest processor config possible:
+ * 
+ * <pre class="prettyprint" >
+ * &lt;processor class=&quot;org.apache.solr.update.processor.LangDetectLanguageIdentifierUpdateProcessorFactory&quot;&gt;
+ *   &lt;str name=&quot;langid.fl&quot;&gt;title,text&lt;/str&gt;
+ *   &lt;str name=&quot;langid.langField&quot;&gt;language_s&lt;/str&gt;
+ * &lt;/processor&gt;
+ * </pre>
+ * See <a href="http://wiki.apache.org/solr/LanguageDetection">http://wiki.apache.org/solr/LanguageDetection</a>
+ * @since 3.5
+ */
+public class LangDetectLanguageIdentifierUpdateProcessorFactory extends
+        UpdateRequestProcessorFactory implements SolrCoreAware, LangIdParams {
+
+  protected SolrParams defaults;
+  protected SolrParams appends;
+  protected SolrParams invariants;
+
+  public void inform(SolrCore core) {
+  }
+
+  /**
+   * The UpdateRequestProcessor may be initialized in solrconfig.xml similarly
+   * to a RequestHandler, with defaults, appends and invariants.
+   * @param args a NamedList with the configuration parameters 
+   */
+  @SuppressWarnings("rawtypes")
+  public void init( NamedList args )
+  {
+    try {
+      loadData();
+    } catch (Exception e) {
+      throw new RuntimeException("Couldn't load profile data, will return empty languages always!", e);
+    }
+    if (args != null) {
+      Object o;
+      o = args.get("defaults");
+      if (o != null && o instanceof NamedList) {
+        defaults = SolrParams.toSolrParams((NamedList) o);
+      } else {
+        defaults = SolrParams.toSolrParams(args);
+      }
+      o = args.get("appends");
+      if (o != null && o instanceof NamedList) {
+        appends = SolrParams.toSolrParams((NamedList) o);
+      }
+      o = args.get("invariants");
+      if (o != null && o instanceof NamedList) {
+        invariants = SolrParams.toSolrParams((NamedList) o);
+      }
+    }
+  }
+
+  @Override
+  public UpdateRequestProcessor getInstance(SolrQueryRequest req,
+                                            SolrQueryResponse rsp, UpdateRequestProcessor next) {
+    // Process defaults, appends and invariants if we got a request
+    if(req != null) {
+      SolrPluginUtils.setDefaults(req, defaults, appends, invariants);
+    }
+    return new LangDetectLanguageIdentifierUpdateProcessor(req, rsp, next);
+  }
+  
+  
+  // DetectorFactory is totally global, so we only want to do this once... ever!!!
+  static boolean loaded;
+  
+  // profiles we will load from classpath
+  static final String languages[] = {
+    "af", "ar", "bg", "bn", "cs", "da", "de", "el", "en", "es", "et", "fa", "fi", "fr", "gu",
+    "he", "hi", "hr", "hu", "id", "it", "ja", "kn", "ko", "lt", "lv", "mk", "ml", "mr", "ne",
+    "nl", "no", "pa", "pl", "pt", "ro", "ru", "sk", "sl", "so", "sq", "sv", "sw", "ta", "te",
+    "th", "tl", "tr", "uk", "ur", "vi", "zh-cn", "zh-tw"
+  };
+
+  public static synchronized void loadData() throws IOException, LangDetectException {
+    if (loaded)
+      return;
+    loaded = true;
+    List<String> profileData = new ArrayList<String>();
+    Charset encoding = Charset.forName("UTF-8");
+    for (String language : languages) {
+      InputStream stream = LangDetectLanguageIdentifierUpdateProcessor.class.getResourceAsStream("langdetect-profiles/" + language);
+      BufferedReader reader = new BufferedReader(new InputStreamReader(stream, encoding));
+      profileData.add(new String(IOUtils.toCharArray(reader)));
+      reader.close();
+    }
+    DetectorFactory.loadProfile(profileData);
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessor.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessor.java
index ddcd2438..eae53929 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessor.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessor.java
@@ -27,7 +27,6 @@
 import org.apache.solr.response.SolrQueryResponse;
 import org.apache.solr.schema.IndexSchema;
 import org.apache.solr.update.AddUpdateCommand;
-import org.apache.tika.language.LanguageIdentifier;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -41,15 +40,15 @@
 
 
 /**
- * Identifies the language of a set of input fields using Tika's
- * LanguageIdentifier. Also supports mapping of field names based
+ * Identifies the language of a set of input fields. 
+ * Also supports mapping of field names based
  * on detected language. 
- * The tika-core-x.y.jar must be on the classpath
  * <p>
  * See <a href="http://wiki.apache.org/solr/LanguageDetection">http://wiki.apache.org/solr/LanguageDetection</a>
  * @since 3.5
+ * @lucene.experimental
  */
-public class LanguageIdentifierUpdateProcessor extends UpdateRequestProcessor implements LangIdParams {
+public abstract class LanguageIdentifierUpdateProcessor extends UpdateRequestProcessor implements LangIdParams {
 
   protected final static Logger log = LoggerFactory
           .getLogger(LanguageIdentifierUpdateProcessor.class);
@@ -301,23 +300,7 @@ protected String concatFields(SolrInputDocument doc, String[] fields) {
    * @param content The content to identify
    * @return List of detected language(s) according to RFC-3066
    */
-  protected List<DetectedLanguage> detectLanguage(String content) {
-    List<DetectedLanguage> languages = new ArrayList<DetectedLanguage>();
-    if(content.trim().length() != 0) { 
-      LanguageIdentifier identifier = new LanguageIdentifier(content.toString());
-      // FIXME: Hack - we get the distance from toString and calculate our own certainty score
-      Double distance = Double.parseDouble(tikaSimilarityPattern.matcher(identifier.toString()).replaceFirst("$1"));
-      // This formula gives: 0.02 => 0.8, 0.1 => 0.5 which is a better sweetspot than isReasonablyCertain()
-      Double certainty = 1 - (5 * distance); 
-      certainty = (certainty < 0) ? 0 : certainty;
-      DetectedLanguage language = new DetectedLanguage(identifier.getLanguage(), certainty);
-      languages.add(language);
-      log.debug("Language detected as "+language+" with a certainty of "+language.getCertainty()+" (Tika distance="+identifier.toString()+")");
-    } else {
-      log.debug("No input text to detect language from, returning empty list");
-    }
-    return languages;
-  }
+  protected abstract List<DetectedLanguage> detectLanguage(String content);
 
   /**
    * Chooses a language based on the list of candidates detected 
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactory.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactory.java
index 43c94f4b..e69de29b 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactory.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactory.java
@@ -1,94 +0,0 @@
-package org.apache.solr.update.processor;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.solr.common.params.SolrParams;
-import org.apache.solr.common.util.NamedList;
-import org.apache.solr.core.SolrCore;
-import org.apache.solr.request.SolrQueryRequest;
-import org.apache.solr.response.SolrQueryResponse;
-import org.apache.solr.util.SolrPluginUtils;
-import org.apache.solr.util.plugin.SolrCoreAware;
-
-/**
- * Identifies the language of a set of input fields using Tika's
- * LanguageIdentifier. The tika-core-x.y.jar must be on the classpath
- * <p/>
- * The UpdateProcessorChain config entry can take a number of parameters
- * which may also be passed as HTTP parameters on the update request
- * and override the defaults. Here is the simplest processor config possible:
- * 
- * <pre class="prettyprint" >
- * &lt;processor class=&quot;org.apache.solr.update.processor.LanguageIdentifierUpdateProcessorFactory&quot;&gt;
- *   &lt;str name=&quot;langid.fl&quot;&gt;title,text&lt;/str&gt;
- *   &lt;str name=&quot;langid.langField&quot;&gt;language_s&lt;/str&gt;
- * &lt;/processor&gt;
- * </pre>
- * See <a href="http://wiki.apache.org/solr/LanguageDetection">http://wiki.apache.org/solr/LanguageDetection</a>
- * @since 3.5
- */
-public class LanguageIdentifierUpdateProcessorFactory extends
-        UpdateRequestProcessorFactory implements SolrCoreAware, LangIdParams {
-
-  protected SolrParams defaults;
-  protected SolrParams appends;
-  protected SolrParams invariants;
-
-  @Override
-  public void inform(SolrCore core) {
-  }
-
-  /**
-   * The UpdateRequestProcessor may be initialized in solrconfig.xml similarly
-   * to a RequestHandler, with defaults, appends and invariants.
-   * @param args a NamedList with the configuration parameters 
-   */
-  @SuppressWarnings("rawtypes")
-  public void init( NamedList args )
-  {
-    if (args != null) {
-      Object o;
-      o = args.get("defaults");
-      if (o != null && o instanceof NamedList) {
-        defaults = SolrParams.toSolrParams((NamedList) o);
-      } else {
-        defaults = SolrParams.toSolrParams(args);
-      }
-      o = args.get("appends");
-      if (o != null && o instanceof NamedList) {
-        appends = SolrParams.toSolrParams((NamedList) o);
-      }
-      o = args.get("invariants");
-      if (o != null && o instanceof NamedList) {
-        invariants = SolrParams.toSolrParams((NamedList) o);
-      }
-    }
-  }
-
-  @Override
-  public UpdateRequestProcessor getInstance(SolrQueryRequest req,
-                                            SolrQueryResponse rsp, UpdateRequestProcessor next) {
-    // Process defaults, appends and invariants if we got a request
-    if(req != null) {
-      SolrPluginUtils.setDefaults(req, defaults, appends, invariants);
-    }
-    return new LanguageIdentifierUpdateProcessor(req, rsp, next);
-  }
-
-
-}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessor.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessor.java
index 3f24d583..d07138a7 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessor.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessor.java
@@ -1 +1,61 @@
   + native
+package org.apache.solr.update.processor;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.response.SolrQueryResponse;
+import org.apache.tika.language.LanguageIdentifier;
+
+/**
+ * Identifies the language of a set of input fields using Tika's
+ * LanguageIdentifier.
+ * The tika-core-x.y.jar must be on the classpath
+ * <p>
+ * See <a href="http://wiki.apache.org/solr/LanguageDetection">http://wiki.apache.org/solr/LanguageDetection</a>
+ * @since 3.5
+ */
+public class TikaLanguageIdentifierUpdateProcessor extends LanguageIdentifierUpdateProcessor {
+
+  public TikaLanguageIdentifierUpdateProcessor(SolrQueryRequest req,
+      SolrQueryResponse rsp, UpdateRequestProcessor next) {
+    super(req, rsp, next);
+  }
+  
+  @Override
+  protected List<DetectedLanguage> detectLanguage(String content) {
+    List<DetectedLanguage> languages = new ArrayList<DetectedLanguage>();
+    if(content.trim().length() != 0) { 
+      LanguageIdentifier identifier = new LanguageIdentifier(content);
+      // FIXME: Hack - we get the distance from toString and calculate our own certainty score
+      Double distance = Double.parseDouble(tikaSimilarityPattern.matcher(identifier.toString()).replaceFirst("$1"));
+      // This formula gives: 0.02 => 0.8, 0.1 => 0.5 which is a better sweetspot than isReasonablyCertain()
+      Double certainty = 1 - (5 * distance); 
+      certainty = (certainty < 0) ? 0 : certainty;
+      DetectedLanguage language = new DetectedLanguage(identifier.getLanguage(), certainty);
+      languages.add(language);
+      log.debug("Language detected as "+language+" with a certainty of "+language.getCertainty()+" (Tika distance="+identifier.toString()+")");
+    } else {
+      log.debug("No input text to detect language from, returning empty list");
+    }
+    return languages;
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessorFactory.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessorFactory.java
index e69de29b..0c940b20 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessorFactory.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessorFactory.java
@@ -0,0 +1,93 @@
+package org.apache.solr.update.processor;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.solr.common.params.SolrParams;
+import org.apache.solr.common.util.NamedList;
+import org.apache.solr.core.SolrCore;
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.response.SolrQueryResponse;
+import org.apache.solr.util.SolrPluginUtils;
+import org.apache.solr.util.plugin.SolrCoreAware;
+
+/**
+ * Identifies the language of a set of input fields using Tika's
+ * LanguageIdentifier. The tika-core-x.y.jar must be on the classpath
+ * <p/>
+ * The UpdateProcessorChain config entry can take a number of parameters
+ * which may also be passed as HTTP parameters on the update request
+ * and override the defaults. Here is the simplest processor config possible:
+ * 
+ * <pre class="prettyprint" >
+ * &lt;processor class=&quot;org.apache.solr.update.processor.TikaLanguageIdentifierUpdateProcessorFactory&quot;&gt;
+ *   &lt;str name=&quot;langid.fl&quot;&gt;title,text&lt;/str&gt;
+ *   &lt;str name=&quot;langid.langField&quot;&gt;language_s&lt;/str&gt;
+ * &lt;/processor&gt;
+ * </pre>
+ * See <a href="http://wiki.apache.org/solr/LanguageDetection">http://wiki.apache.org/solr/LanguageDetection</a>
+ * @since 3.5
+ */
+public class TikaLanguageIdentifierUpdateProcessorFactory extends
+        UpdateRequestProcessorFactory implements SolrCoreAware, LangIdParams {
+
+  protected SolrParams defaults;
+  protected SolrParams appends;
+  protected SolrParams invariants;
+
+  public void inform(SolrCore core) {
+  }
+
+  /**
+   * The UpdateRequestProcessor may be initialized in solrconfig.xml similarly
+   * to a RequestHandler, with defaults, appends and invariants.
+   * @param args a NamedList with the configuration parameters 
+   */
+  @SuppressWarnings("rawtypes")
+  public void init( NamedList args )
+  {
+    if (args != null) {
+      Object o;
+      o = args.get("defaults");
+      if (o != null && o instanceof NamedList) {
+        defaults = SolrParams.toSolrParams((NamedList) o);
+      } else {
+        defaults = SolrParams.toSolrParams(args);
+      }
+      o = args.get("appends");
+      if (o != null && o instanceof NamedList) {
+        appends = SolrParams.toSolrParams((NamedList) o);
+      }
+      o = args.get("invariants");
+      if (o != null && o instanceof NamedList) {
+        invariants = SolrParams.toSolrParams((NamedList) o);
+      }
+    }
+  }
+
+  @Override
+  public UpdateRequestProcessor getInstance(SolrQueryRequest req,
+                                            SolrQueryResponse rsp, UpdateRequestProcessor next) {
+    // Process defaults, appends and invariants if we got a request
+    if(req != null) {
+      SolrPluginUtils.setDefaults(req, defaults, appends, invariants);
+    }
+    return new TikaLanguageIdentifierUpdateProcessor(req, rsp, next);
+  }
+
+
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactoryTest.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactoryTest.java
index 3f24d583..179086fb 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactoryTest.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactoryTest.java
@@ -1 +1,63 @@
   + native
+package org.apache.solr.update.processor;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.solr.common.SolrInputDocument;
+import org.apache.solr.common.params.ModifiableSolrParams;
+import org.junit.Test;
+
+public class LangDetectLanguageIdentifierUpdateProcessorFactoryTest extends LanguageIdentifierUpdateProcessorFactoryTestCase {
+  @Override
+  protected LanguageIdentifierUpdateProcessor createLangIdProcessor(ModifiableSolrParams parameters) throws Exception {
+    return new LangDetectLanguageIdentifierUpdateProcessor(_parser.buildRequestFrom(null, parameters, null), resp, null);
+  }
+  
+  // this one actually works better it seems with short docs
+  @Override
+  protected SolrInputDocument tooShortDoc() {
+    SolrInputDocument doc = new SolrInputDocument();
+    doc.addField("text", "");
+    return doc;
+  }
+  
+  /* we don't return 'un' for the super-short one (this detector things hungarian?).
+   * replace this with japanese
+   */
+  @Test @Override
+  public void testLangIdGlobal() throws Exception {
+    parameters = new ModifiableSolrParams();
+    parameters.add("langid.fl", "name,subject");
+    parameters.add("langid.langField", "language_s");
+    parameters.add("langid.fallback", "un");
+    liProcessor = createLangIdProcessor(parameters);
+    
+    assertLang("no", "id", "1no", "name", "Lucene", "subject", "Lucene er et fri/åpen kildekode programvarebibliotek for informasjonsgjenfinning, opprinnelig utviklet i programmeringsspråket Java av Doug Cutting. Lucene støttes av Apache Software Foundation og utgis under Apache-lisensen.");
+    assertLang("en", "id", "2en", "name", "Lucene", "subject", "Apache Lucene is a free/open source information retrieval software library, originally created in Java by Doug Cutting. It is supported by the Apache Software Foundation and is released under the Apache Software License.");
+    assertLang("sv", "id", "3sv", "name", "Maven", "subject", "Apache Maven är ett verktyg utvecklat av Apache Software Foundation och används inom systemutveckling av datorprogram i programspråket Java. Maven används för att automatiskt paketera (bygga) programfilerna till en distribuerbar enhet. Maven används inom samma område som Apache Ant men dess byggfiler är deklarativa till skillnad ifrån Ants skriptbaserade.");
+    assertLang("es", "id", "4es", "name", "Lucene", "subject", "Lucene es un API de código abierto para recuperación de información, originalmente implementada en Java por Doug Cutting. Está apoyado por el Apache Software Foundation y se distribuye bajo la Apache Software License. Lucene tiene versiones para otros lenguajes incluyendo Delphi, Perl, C#, C++, Python, Ruby y PHP.");
+    assertLang("ja", "id", "5ja", "name", "Japanese", "subject", "日本語（にほんご、にっぽんご）は主として、日本で使用されてきた言語である。日本国は法令上、公用語を明記していないが、事実上の公用語となっており、学校教育の「国語」で教えられる。");
+    assertLang("th", "id", "6th", "name", "บทความคัดสรรเดือนนี้", "subject", "อันเนอลีส มารี อันเนอ ฟรังค์ หรือมักรู้จักในภาษาไทยว่า แอนน์ แฟรงค์ เป็นเด็กหญิงชาวยิว เกิดที่เมืองแฟรงก์เฟิร์ต ประเทศเยอรมนี เธอมีชื่อเสียงโด่งดังในฐานะผู้เขียนบันทึกประจำวันซึ่งต่อมาได้รับการตีพิมพ์เป็นหนังสือ บรรยายเหตุการณ์ขณะหลบซ่อนตัวจากการล่าชาวยิวในประเทศเนเธอร์แลนด์ ระหว่างที่ถูกเยอรมนีเข้าครอบครองในช่วงสงครามโลกครั้งที่สอง");
+    assertLang("ru", "id", "7ru", "name", "Lucene", "subject", "The Apache Lucene — это свободная библиотека для высокоскоростного полнотекстового поиска, написанная на Java. Может быть использована для поиска в интернете и других областях компьютерной лингвистики (аналитическая философия).");
+    assertLang("de", "id", "8de", "name", "Lucene", "subject", "Lucene ist ein Freie-Software-Projekt der Apache Software Foundation, das eine Suchsoftware erstellt. Durch die hohe Leistungsfähigkeit und Skalierbarkeit können die Lucene-Werkzeuge für beliebige Projektgrößen und Anforderungen eingesetzt werden. So setzt beispielsweise Wikipedia Lucene für die Volltextsuche ein. Zudem verwenden die beiden Desktop-Suchprogramme Beagle und Strigi eine C#- bzw. C++- Portierung von Lucene als Indexer.");
+    assertLang("fr", "id", "9fr", "name", "Lucene", "subject", "Lucene est un moteur de recherche libre écrit en Java qui permet d'indexer et de rechercher du texte. C'est un projet open source de la fondation Apache mis à disposition sous licence Apache. Il est également disponible pour les langages Ruby, Perl, C++, PHP.");
+    assertLang("nl", "id", "10nl", "name", "Lucene", "subject", "Lucene is een gratis open source, tekst gebaseerde information retrieval API van origine geschreven in Java door Doug Cutting. Het wordt ondersteund door de Apache Software Foundation en is vrijgegeven onder de Apache Software Licentie. Lucene is ook beschikbaar in andere programeertalen zoals Perl, C#, C++, Python, Ruby en PHP.");
+    assertLang("it", "id", "11it", "name", "Lucene", "subject", "Lucene è una API gratuita ed open source per il reperimento di informazioni inizialmente implementata in Java da Doug Cutting. È supportata dall'Apache Software Foundation ed è resa disponibile con l'Apache License. Lucene è stata successivamente reimplementata in Perl, C#, C++, Python, Ruby e PHP.");
+    assertLang("pt", "id", "12pt", "name", "Lucene", "subject", "Apache Lucene, ou simplesmente Lucene, é um software de busca e uma API de indexação de documentos, escrito na linguagem de programação Java. É um software de código aberto da Apache Software Foundation licenciado através da licença Apache.");
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTest.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTest.java
index 3c2c90d9..b9d005f0 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTest.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTest.java
@@ -1,217 +1 @@
   + application/octet-stream
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.update.processor;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.common.SolrInputDocument;
-import org.apache.solr.common.params.ModifiableSolrParams;
-import org.apache.solr.core.SolrCore;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.solr.request.SolrQueryRequest;
-import org.apache.solr.response.SolrQueryResponse;
-import org.apache.solr.servlet.SolrRequestParsers;
-
-public class LanguageIdentifierUpdateProcessorFactoryTest extends SolrTestCaseJ4 {
-
-  protected static SolrRequestParsers _parser;
-  protected static SolrQueryRequest req;
-  protected static SolrQueryResponse resp = new SolrQueryResponse();
-  protected static LanguageIdentifierUpdateProcessor liProcessor;
-  protected static ModifiableSolrParams parameters;
-
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    initCore("solrconfig-languageidentifier.xml", "schema.xml", getFile("langid/solr").getAbsolutePath());
-    SolrCore core = h.getCore();
-    UpdateRequestProcessorChain chained = core.getUpdateProcessingChain("lang_id");
-    assertNotNull(chained);
-    _parser = new SolrRequestParsers(null);
-  }
-
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    clearIndex();
-    assertU(commit());
-  }
-
-  @Test
-  public void testLangIdGlobal() throws Exception {
-    parameters = new ModifiableSolrParams();
-    parameters.add("langid.fl", "name,subject");
-    parameters.add("langid.langField", "language_s");
-    parameters.add("langid.fallback", "un");
-    liProcessor = createLangIdProcessor(parameters);
-    
-    assertLang("no", "id", "1no", "name", "Lucene", "subject", "Lucene er et fri/åpen kildekode programvarebibliotek for informasjonsgjenfinning, opprinnelig utviklet i programmeringsspråket Java av Doug Cutting. Lucene støttes av Apache Software Foundation og utgis under Apache-lisensen.");
-    assertLang("en", "id", "2en", "name", "Lucene", "subject", "Apache Lucene is a free/open source information retrieval software library, originally created in Java by Doug Cutting. It is supported by the Apache Software Foundation and is released under the Apache Software License.");
-    assertLang("sv", "id", "3sv", "name", "Maven", "subject", "Apache Maven är ett verktyg utvecklat av Apache Software Foundation och används inom systemutveckling av datorprogram i programspråket Java. Maven används för att automatiskt paketera (bygga) programfilerna till en distribuerbar enhet. Maven används inom samma område som Apache Ant men dess byggfiler är deklarativa till skillnad ifrån Ants skriptbaserade.");
-    assertLang("es", "id", "4es", "name", "Lucene", "subject", "Lucene es un API de código abierto para recuperación de información, originalmente implementada en Java por Doug Cutting. Está apoyado por el Apache Software Foundation y se distribuye bajo la Apache Software License. Lucene tiene versiones para otros lenguajes incluyendo Delphi, Perl, C#, C++, Python, Ruby y PHP.");
-    assertLang("un", "id", "5un", "name", "a", "subject", "b");
-    assertLang("th", "id", "6th", "name", "บทความคัดสรรเดือนนี้", "subject", "อันเนอลีส มารี อันเนอ ฟรังค์ หรือมักรู้จักในภาษาไทยว่า แอนน์ แฟรงค์ เป็นเด็กหญิงชาวยิว เกิดที่เมืองแฟรงก์เฟิร์ต ประเทศเยอรมนี เธอมีชื่อเสียงโด่งดังในฐานะผู้เขียนบันทึกประจำวันซึ่งต่อมาได้รับการตีพิมพ์เป็นหนังสือ บรรยายเหตุการณ์ขณะหลบซ่อนตัวจากการล่าชาวยิวในประเทศเนเธอร์แลนด์ ระหว่างที่ถูกเยอรมนีเข้าครอบครองในช่วงสงครามโลกครั้งที่สอง");
-    assertLang("ru", "id", "7ru", "name", "Lucene", "subject", "The Apache Lucene — это свободная библиотека для высокоскоростного полнотекстового поиска, написанная на Java. Может быть использована для поиска в интернете и других областях компьютерной лингвистики (аналитическая философия).");
-    assertLang("de", "id", "8de", "name", "Lucene", "subject", "Lucene ist ein Freie-Software-Projekt der Apache Software Foundation, das eine Suchsoftware erstellt. Durch die hohe Leistungsfähigkeit und Skalierbarkeit können die Lucene-Werkzeuge für beliebige Projektgrößen und Anforderungen eingesetzt werden. So setzt beispielsweise Wikipedia Lucene für die Volltextsuche ein. Zudem verwenden die beiden Desktop-Suchprogramme Beagle und Strigi eine C#- bzw. C++- Portierung von Lucene als Indexer.");
-    assertLang("fr", "id", "9fr", "name", "Lucene", "subject", "Lucene est un moteur de recherche libre écrit en Java qui permet d'indexer et de rechercher du texte. C'est un projet open source de la fondation Apache mis à disposition sous licence Apache. Il est également disponible pour les langages Ruby, Perl, C++, PHP.");
-    assertLang("nl", "id", "10nl", "name", "Lucene", "subject", "Lucene is een gratis open source, tekst gebaseerde information retrieval API van origine geschreven in Java door Doug Cutting. Het wordt ondersteund door de Apache Software Foundation en is vrijgegeven onder de Apache Software Licentie. Lucene is ook beschikbaar in andere programeertalen zoals Perl, C#, C++, Python, Ruby en PHP.");
-    assertLang("it", "id", "11it", "name", "Lucene", "subject", "Lucene è una API gratuita ed open source per il reperimento di informazioni inizialmente implementata in Java da Doug Cutting. È supportata dall'Apache Software Foundation ed è resa disponibile con l'Apache License. Lucene è stata successivamente reimplementata in Perl, C#, C++, Python, Ruby e PHP.");
-    assertLang("pt", "id", "12pt", "name", "Lucene", "subject", "Apache Lucene, ou simplesmente Lucene, é um software de busca e uma API de indexação de documentos, escrito na linguagem de programação Java. É um software de código aberto da Apache Software Foundation licenciado através da licença Apache.");
-  }
-  
-  @Test
-  public void testMapFieldName() throws Exception {
-    parameters = new ModifiableSolrParams();
-    parameters.add("langid.fl", "name");
-    parameters.add("langid.map.lcmap", "jp:s zh:cjk ko:cjk");
-    parameters.add("langid.enforceSchema", "true");
-    liProcessor = createLangIdProcessor(parameters);
-    
-    assertEquals("test_no", liProcessor.getMappedField("test", "no"));
-    assertEquals("test_en", liProcessor.getMappedField("test", "en"));
-    assertEquals("test_s", liProcessor.getMappedField("test", "jp"));
-    assertEquals("test_cjk", liProcessor.getMappedField("test", "zh"));
-    assertEquals("test_cjk", liProcessor.getMappedField("test", "ko"));
-
-    // Prove support for other mapping regex
-    parameters.add("langid.map.pattern", "text_(.*?)_field");
-    parameters.add("langid.map.replace", "$1_{lang}Text");
-    liProcessor = createLangIdProcessor(parameters);
-
-    assertEquals("title_noText", liProcessor.getMappedField("text_title_field", "no"));
-    assertEquals("body_svText", liProcessor.getMappedField("text_body_field", "sv"));
-  }
-
-  @Test
-  public void testPreExisting() throws Exception {
-    SolrInputDocument doc;
-    parameters = new ModifiableSolrParams();
-    parameters.add("langid.fl", "text");
-    parameters.add("langid.langField", "language");
-    parameters.add("langid.langsField", "languages");
-    parameters.add("langid.enforceSchema", "false");
-    parameters.add("langid.map", "true");
-    liProcessor = createLangIdProcessor(parameters);
-    
-    doc = englishDoc();
-    assertEquals("en", liProcessor.process(doc).getFieldValue("language"));
-    assertEquals("en", liProcessor.process(doc).getFieldValue("languages"));
-    
-    doc = englishDoc();
-    doc.setField("language", "no");
-    assertEquals("no", liProcessor.process(doc).getFieldValue("language"));
-    assertEquals("no", liProcessor.process(doc).getFieldValue("languages"));
-    assertNotNull(liProcessor.process(doc).getFieldValue("text_no"));
-  }
-
-  @Test
-  public void testDefaultFallbackEmptyString() throws Exception {
-    SolrInputDocument doc;
-    parameters = new ModifiableSolrParams();
-    parameters.add("langid.fl", "text");
-    parameters.add("langid.langField", "language");
-    parameters.add("langid.enforceSchema", "false");
-    liProcessor = createLangIdProcessor(parameters);
-    
-    doc = tooShortDoc();
-    assertEquals("", liProcessor.process(doc).getFieldValue("language"));
-  }
-
-  @Test
-  public void testFallback() throws Exception {
-    SolrInputDocument doc;
-    parameters = new ModifiableSolrParams();
-    parameters.add("langid.fl", "text");
-    parameters.add("langid.langField", "language");
-    parameters.add("langid.fallbackFields", "noop,fb");
-    parameters.add("langid.fallback", "fbVal");
-    parameters.add("langid.enforceSchema", "false");
-    liProcessor = createLangIdProcessor(parameters);
-      
-    // Verify fallback to field fb (noop field does not exist and is skipped)
-    doc = tooShortDoc();
-    doc.addField("fb", "fbField");
-    assertEquals("fbField", liProcessor.process(doc).getFieldValue("language"));
-
-    // Verify fallback to fallback value since no fallback fields exist
-    doc = tooShortDoc();
-    assertEquals("fbVal", liProcessor.process(doc).getFieldValue("language"));  
-  }
-  
-  @Test
-  public void testResolveLanguage() throws Exception {
-    List<DetectedLanguage> langs;
-    parameters = new ModifiableSolrParams();
-    parameters.add("langid.fl", "text");
-    parameters.add("langid.langField", "language");
-    liProcessor = createLangIdProcessor(parameters);
-
-    // No detected languages
-    langs = new ArrayList<DetectedLanguage>();
-    assertEquals("", liProcessor.resolveLanguage(langs, null));
-    assertEquals("fallback", liProcessor.resolveLanguage(langs, "fallback"));
-
-    // One detected language
-    langs.add(new DetectedLanguage("one", 1.0));
-    assertEquals("one", liProcessor.resolveLanguage(langs, "fallback"));    
-
-    // One detected language under default threshold
-    langs = new ArrayList<DetectedLanguage>();
-    langs.add(new DetectedLanguage("under", 0.1));
-    assertEquals("fallback", liProcessor.resolveLanguage(langs, "fallback"));    
-  }
-  
-  
-  // Various utility methods
-  
-  private SolrInputDocument englishDoc() {
-    SolrInputDocument doc = new SolrInputDocument();
-    doc.addField("text", "Apache Lucene is a free/open source information retrieval software library, originally created in Java by Doug Cutting. It is supported by the Apache Software Foundation and is released under the Apache Software License.");
-    return doc;
-  }
-
-  private SolrInputDocument tooShortDoc() {
-    SolrInputDocument doc = new SolrInputDocument();
-    doc.addField("text", "This text is too short");
-    return doc;
-  }
-
-  private LanguageIdentifierUpdateProcessor createLangIdProcessor(ModifiableSolrParams parameters) throws Exception {
-    return new LanguageIdentifierUpdateProcessor(_parser.buildRequestFrom(null, parameters, null), resp, null);
-  }
-
-  private void assertLang(String langCode, String... fieldsAndValues) throws Exception {
-    if(liProcessor == null)
-      throw new Exception("Processor must be initialized before calling assertLang()");
-    SolrInputDocument doc = sid(fieldsAndValues);
-    assertEquals(langCode, liProcessor.process(doc).getFieldValue(liProcessor.langField));
-  }
-  
-  private SolrInputDocument sid(String... fieldsAndValues) {
-    SolrInputDocument doc = new SolrInputDocument();
-    for (int i = 0; i < fieldsAndValues.length; i+=2) {
-      doc.addField(fieldsAndValues[i], fieldsAndValues[i+1]);
-    }
-    return doc;
-  }
-}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTestCase.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTestCase.java
index 3f24d583..2a61dd51 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTestCase.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTestCase.java
@@ -1 +1,215 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.update.processor;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.solr.SolrTestCaseJ4;
+import org.apache.solr.common.SolrInputDocument;
+import org.apache.solr.common.params.ModifiableSolrParams;
+import org.apache.solr.core.SolrCore;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.response.SolrQueryResponse;
+import org.apache.solr.servlet.SolrRequestParsers;
+
+public abstract class LanguageIdentifierUpdateProcessorFactoryTestCase extends SolrTestCaseJ4 {
+
+  protected static SolrRequestParsers _parser;
+  protected static SolrQueryRequest req;
+  protected static SolrQueryResponse resp = new SolrQueryResponse();
+  protected static LanguageIdentifierUpdateProcessor liProcessor;
+  protected static ModifiableSolrParams parameters;
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    initCore("solrconfig-languageidentifier.xml", "schema.xml", getFile("langid/solr").getAbsolutePath());
+    SolrCore core = h.getCore();
+    UpdateRequestProcessorChain chained = core.getUpdateProcessingChain("lang_id");
+    assertNotNull(chained);
+    _parser = new SolrRequestParsers(null);
+  }
+
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    clearIndex();
+    assertU(commit());
+  }
+
+  @Test
+  public void testLangIdGlobal() throws Exception {
+    parameters = new ModifiableSolrParams();
+    parameters.add("langid.fl", "name,subject");
+    parameters.add("langid.langField", "language_s");
+    parameters.add("langid.fallback", "un");
+    liProcessor = createLangIdProcessor(parameters);
+    
+    assertLang("no", "id", "1no", "name", "Lucene", "subject", "Lucene er et fri/åpen kildekode programvarebibliotek for informasjonsgjenfinning, opprinnelig utviklet i programmeringsspråket Java av Doug Cutting. Lucene støttes av Apache Software Foundation og utgis under Apache-lisensen.");
+    assertLang("en", "id", "2en", "name", "Lucene", "subject", "Apache Lucene is a free/open source information retrieval software library, originally created in Java by Doug Cutting. It is supported by the Apache Software Foundation and is released under the Apache Software License.");
+    assertLang("sv", "id", "3sv", "name", "Maven", "subject", "Apache Maven är ett verktyg utvecklat av Apache Software Foundation och används inom systemutveckling av datorprogram i programspråket Java. Maven används för att automatiskt paketera (bygga) programfilerna till en distribuerbar enhet. Maven används inom samma område som Apache Ant men dess byggfiler är deklarativa till skillnad ifrån Ants skriptbaserade.");
+    assertLang("es", "id", "4es", "name", "Lucene", "subject", "Lucene es un API de código abierto para recuperación de información, originalmente implementada en Java por Doug Cutting. Está apoyado por el Apache Software Foundation y se distribuye bajo la Apache Software License. Lucene tiene versiones para otros lenguajes incluyendo Delphi, Perl, C#, C++, Python, Ruby y PHP.");
+    assertLang("un", "id", "5un", "name", "a", "subject", "b");
+    assertLang("th", "id", "6th", "name", "บทความคัดสรรเดือนนี้", "subject", "อันเนอลีส มารี อันเนอ ฟรังค์ หรือมักรู้จักในภาษาไทยว่า แอนน์ แฟรงค์ เป็นเด็กหญิงชาวยิว เกิดที่เมืองแฟรงก์เฟิร์ต ประเทศเยอรมนี เธอมีชื่อเสียงโด่งดังในฐานะผู้เขียนบันทึกประจำวันซึ่งต่อมาได้รับการตีพิมพ์เป็นหนังสือ บรรยายเหตุการณ์ขณะหลบซ่อนตัวจากการล่าชาวยิวในประเทศเนเธอร์แลนด์ ระหว่างที่ถูกเยอรมนีเข้าครอบครองในช่วงสงครามโลกครั้งที่สอง");
+    assertLang("ru", "id", "7ru", "name", "Lucene", "subject", "The Apache Lucene — это свободная библиотека для высокоскоростного полнотекстового поиска, написанная на Java. Может быть использована для поиска в интернете и других областях компьютерной лингвистики (аналитическая философия).");
+    assertLang("de", "id", "8de", "name", "Lucene", "subject", "Lucene ist ein Freie-Software-Projekt der Apache Software Foundation, das eine Suchsoftware erstellt. Durch die hohe Leistungsfähigkeit und Skalierbarkeit können die Lucene-Werkzeuge für beliebige Projektgrößen und Anforderungen eingesetzt werden. So setzt beispielsweise Wikipedia Lucene für die Volltextsuche ein. Zudem verwenden die beiden Desktop-Suchprogramme Beagle und Strigi eine C#- bzw. C++- Portierung von Lucene als Indexer.");
+    assertLang("fr", "id", "9fr", "name", "Lucene", "subject", "Lucene est un moteur de recherche libre écrit en Java qui permet d'indexer et de rechercher du texte. C'est un projet open source de la fondation Apache mis à disposition sous licence Apache. Il est également disponible pour les langages Ruby, Perl, C++, PHP.");
+    assertLang("nl", "id", "10nl", "name", "Lucene", "subject", "Lucene is een gratis open source, tekst gebaseerde information retrieval API van origine geschreven in Java door Doug Cutting. Het wordt ondersteund door de Apache Software Foundation en is vrijgegeven onder de Apache Software Licentie. Lucene is ook beschikbaar in andere programeertalen zoals Perl, C#, C++, Python, Ruby en PHP.");
+    assertLang("it", "id", "11it", "name", "Lucene", "subject", "Lucene è una API gratuita ed open source per il reperimento di informazioni inizialmente implementata in Java da Doug Cutting. È supportata dall'Apache Software Foundation ed è resa disponibile con l'Apache License. Lucene è stata successivamente reimplementata in Perl, C#, C++, Python, Ruby e PHP.");
+    assertLang("pt", "id", "12pt", "name", "Lucene", "subject", "Apache Lucene, ou simplesmente Lucene, é um software de busca e uma API de indexação de documentos, escrito na linguagem de programação Java. É um software de código aberto da Apache Software Foundation licenciado através da licença Apache.");
+  }
+  
+  @Test
+  public void testMapFieldName() throws Exception {
+    parameters = new ModifiableSolrParams();
+    parameters.add("langid.fl", "name");
+    parameters.add("langid.map.lcmap", "jp:s zh:cjk ko:cjk");
+    parameters.add("langid.enforceSchema", "true");
+    liProcessor = createLangIdProcessor(parameters);
+    
+    assertEquals("test_no", liProcessor.getMappedField("test", "no"));
+    assertEquals("test_en", liProcessor.getMappedField("test", "en"));
+    assertEquals("test_s", liProcessor.getMappedField("test", "jp"));
+    assertEquals("test_cjk", liProcessor.getMappedField("test", "zh"));
+    assertEquals("test_cjk", liProcessor.getMappedField("test", "ko"));
+
+    // Prove support for other mapping regex
+    parameters.add("langid.map.pattern", "text_(.*?)_field");
+    parameters.add("langid.map.replace", "$1_{lang}Text");
+    liProcessor = createLangIdProcessor(parameters);
+
+    assertEquals("title_noText", liProcessor.getMappedField("text_title_field", "no"));
+    assertEquals("body_svText", liProcessor.getMappedField("text_body_field", "sv"));
+  }
+
+  @Test
+  public void testPreExisting() throws Exception {
+    SolrInputDocument doc;
+    parameters = new ModifiableSolrParams();
+    parameters.add("langid.fl", "text");
+    parameters.add("langid.langField", "language");
+    parameters.add("langid.langsField", "languages");
+    parameters.add("langid.enforceSchema", "false");
+    parameters.add("langid.map", "true");
+    liProcessor = createLangIdProcessor(parameters);
+    
+    doc = englishDoc();
+    assertEquals("en", liProcessor.process(doc).getFieldValue("language"));
+    assertEquals("en", liProcessor.process(doc).getFieldValue("languages"));
+    
+    doc = englishDoc();
+    doc.setField("language", "no");
+    assertEquals("no", liProcessor.process(doc).getFieldValue("language"));
+    assertEquals("no", liProcessor.process(doc).getFieldValue("languages"));
+    assertNotNull(liProcessor.process(doc).getFieldValue("text_no"));
+  }
+
+  @Test
+  public void testDefaultFallbackEmptyString() throws Exception {
+    SolrInputDocument doc;
+    parameters = new ModifiableSolrParams();
+    parameters.add("langid.fl", "text");
+    parameters.add("langid.langField", "language");
+    parameters.add("langid.enforceSchema", "false");
+    liProcessor = createLangIdProcessor(parameters);
+    
+    doc = tooShortDoc();
+    assertEquals("", liProcessor.process(doc).getFieldValue("language"));
+  }
+
+  @Test
+  public void testFallback() throws Exception {
+    SolrInputDocument doc;
+    parameters = new ModifiableSolrParams();
+    parameters.add("langid.fl", "text");
+    parameters.add("langid.langField", "language");
+    parameters.add("langid.fallbackFields", "noop,fb");
+    parameters.add("langid.fallback", "fbVal");
+    parameters.add("langid.enforceSchema", "false");
+    liProcessor = createLangIdProcessor(parameters);
+      
+    // Verify fallback to field fb (noop field does not exist and is skipped)
+    doc = tooShortDoc();
+    doc.addField("fb", "fbField");
+    assertEquals("fbField", liProcessor.process(doc).getFieldValue("language"));
+
+    // Verify fallback to fallback value since no fallback fields exist
+    doc = tooShortDoc();
+    assertEquals("fbVal", liProcessor.process(doc).getFieldValue("language"));  
+  }
+  
+  @Test
+  public void testResolveLanguage() throws Exception {
+    List<DetectedLanguage> langs;
+    parameters = new ModifiableSolrParams();
+    parameters.add("langid.fl", "text");
+    parameters.add("langid.langField", "language");
+    liProcessor = createLangIdProcessor(parameters);
+
+    // No detected languages
+    langs = new ArrayList<DetectedLanguage>();
+    assertEquals("", liProcessor.resolveLanguage(langs, null));
+    assertEquals("fallback", liProcessor.resolveLanguage(langs, "fallback"));
+
+    // One detected language
+    langs.add(new DetectedLanguage("one", 1.0));
+    assertEquals("one", liProcessor.resolveLanguage(langs, "fallback"));    
+
+    // One detected language under default threshold
+    langs = new ArrayList<DetectedLanguage>();
+    langs.add(new DetectedLanguage("under", 0.1));
+    assertEquals("fallback", liProcessor.resolveLanguage(langs, "fallback"));    
+  }
+  
+  
+  // Various utility methods
+  
+  private SolrInputDocument englishDoc() {
+    SolrInputDocument doc = new SolrInputDocument();
+    doc.addField("text", "Apache Lucene is a free/open source information retrieval software library, originally created in Java by Doug Cutting. It is supported by the Apache Software Foundation and is released under the Apache Software License.");
+    return doc;
+  }
+
+  protected SolrInputDocument tooShortDoc() {
+    SolrInputDocument doc = new SolrInputDocument();
+    doc.addField("text", "This text is too short");
+    return doc;
+  }
+
+  protected abstract LanguageIdentifierUpdateProcessor createLangIdProcessor(ModifiableSolrParams parameters) throws Exception;
+
+  protected void assertLang(String langCode, String... fieldsAndValues) throws Exception {
+    if(liProcessor == null)
+      throw new Exception("Processor must be initialized before calling assertLang()");
+    SolrInputDocument doc = sid(fieldsAndValues);
+    assertEquals(langCode, liProcessor.process(doc).getFieldValue(liProcessor.langField));
+  }
+  
+  private SolrInputDocument sid(String... fieldsAndValues) {
+    SolrInputDocument doc = new SolrInputDocument();
+    for (int i = 0; i < fieldsAndValues.length; i+=2) {
+      doc.addField(fieldsAndValues[i], fieldsAndValues[i+1]);
+    }
+    return doc;
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessorFactoryTest.java b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessorFactoryTest.java
index e69de29b..d65c66d7 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessorFactoryTest.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/langid/src/test/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessorFactoryTest.java
@@ -0,0 +1,27 @@
+package org.apache.solr.update.processor;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.solr.common.params.ModifiableSolrParams;
+
+public class TikaLanguageIdentifierUpdateProcessorFactoryTest extends LanguageIdentifierUpdateProcessorFactoryTestCase {
+  @Override
+  protected LanguageIdentifierUpdateProcessor createLangIdProcessor(ModifiableSolrParams parameters) throws Exception {
+    return new TikaLanguageIdentifierUpdateProcessor(_parser.buildRequestFrom(null, parameters, null), resp, null);
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java
index b9d005f0..f1170bdc 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java
@@ -1 +1,96 @@
   + application/octet-stream
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.response;
+
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.response.SolrQueryResponse;
+import org.apache.solr.search.DocList;
+import org.apache.solr.search.DocSlice;
+import org.apache.solr.common.SolrDocumentList;
+import org.apache.solr.common.SolrException;
+
+/**
+ * This class is used by the Velocity response writer to provide a consistent paging tool for use by templates.
+ *
+ * TODO: add more details
+ */
+public class PageTool {
+  private long start;
+  private int results_per_page = 10;
+  private long results_found;
+  private int page_count;
+  private int current_page_number;
+
+  public PageTool(SolrQueryRequest request, SolrQueryResponse response) {
+    String rows = request.getParams().get("rows");
+
+    if (rows != null) {
+      results_per_page = new Integer(rows);
+    }
+    //TODO: Handle group by results
+    Object docs = response.getValues().get("response");
+    if (docs != null) {
+      if (docs instanceof DocSlice) {
+        DocSlice doc_slice = (DocSlice) docs;
+        results_found = doc_slice.matches();
+        start = doc_slice.offset();
+      } else if(docs instanceof ResultContext) {
+        DocList dl = ((ResultContext) docs).docs;
+        results_found = dl.matches();
+        start = dl.offset();
+      } else if(docs instanceof SolrDocumentList) {
+        SolrDocumentList doc_list = (SolrDocumentList) docs;
+        results_found = doc_list.getNumFound();
+        start = doc_list.getStart();
+      } else {
+	  throw new SolrException(SolrException.ErrorCode.UNKNOWN, "Unknown response type "+docs+". Expected one of DocSlice, ResultContext or SolrDocumentList");
+      }
+    }
+
+    page_count = (int) Math.ceil(results_found / (double) results_per_page);
+    current_page_number = (int) Math.ceil(start / (double) results_per_page) + (page_count > 0 ? 1 : 0);
+  }
+
+  public long getStart() {
+    return start;
+  }
+
+  public int getResults_per_page() {
+    return results_per_page;
+  }
+
+  public long getResults_found() {
+    return results_found;
+  }
+
+  public int getPage_count() {
+    return page_count;
+  }
+
+  public int getCurrent_page_number() {
+    return current_page_number;
+  }
+
+  @Override
+  public String toString() {
+    return "Found " + results_found +
+           " Page " + current_page_number + " of " + page_count +
+           " Starting at " + start + " per page " + results_per_page;
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/SolrParamResourceLoader.java b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/SolrParamResourceLoader.java
index 6962a789..43a32c8b 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/SolrParamResourceLoader.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/SolrParamResourceLoader.java
@@ -1,2 +1,78 @@
   + Date Author Id Revision HeadURL
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.response;
+
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.velocity.runtime.resource.loader.ResourceLoader;
+import org.apache.velocity.runtime.resource.Resource;
+import org.apache.velocity.exception.ResourceNotFoundException;
+import org.apache.commons.collections.ExtendedProperties;
+
+import java.io.ByteArrayInputStream;
+import java.io.InputStream;
+import java.io.UnsupportedEncodingException;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+
+public class SolrParamResourceLoader extends ResourceLoader {
+  private Map<String,String> templates = new HashMap<String,String>();
+  public SolrParamResourceLoader(SolrQueryRequest request) {
+    super();
+
+    // TODO: Consider using content streams, but need a template name associated with each stream
+    // for now, a custom param convention of template.<name>=<template body> is a nice example
+    // of per-request overrides of templates
+
+    org.apache.solr.common.params.SolrParams params = request.getParams();
+    Iterator<String> names = params.getParameterNamesIterator();
+    while (names.hasNext()) {
+      String name = names.next();
+      
+      if (name.startsWith("v.template.")) {
+        templates.put(name.substring(11) + ".vm",params.get(name));
+      }
+    }
+  }
+
+  @Override
+  public void init(ExtendedProperties extendedProperties) {
+  }
+
+  @Override
+  public InputStream getResourceStream(String s) throws ResourceNotFoundException {
+    String template = templates.get(s);
+    try {
+      return template == null ? null : new ByteArrayInputStream(template.getBytes("UTF-8"));
+    } catch (UnsupportedEncodingException e) {
+      throw new RuntimeException(e); // may not happen
+    }
+  }
+
+  @Override
+  public boolean isSourceModified(Resource resource) {
+    return false;
+  }
+
+  @Override
+  public long getLastModified(Resource resource) {
+    return 0;
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/SolrVelocityResourceLoader.java b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/SolrVelocityResourceLoader.java
index 6962a789..a3786633 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/SolrVelocityResourceLoader.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/SolrVelocityResourceLoader.java
@@ -1,2 +1,56 @@
   + Date Author Id Revision HeadURL
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.solr.response;
+
+import org.apache.velocity.runtime.resource.loader.ResourceLoader;
+import org.apache.velocity.runtime.resource.Resource;
+import org.apache.velocity.exception.ResourceNotFoundException;
+import org.apache.commons.collections.ExtendedProperties;
+import org.apache.solr.core.SolrResourceLoader;
+
+import java.io.InputStream;
+
+// TODO: the name of this class seems ridiculous
+public class SolrVelocityResourceLoader extends ResourceLoader {
+  private SolrResourceLoader loader;
+
+  public SolrVelocityResourceLoader(SolrResourceLoader loader) {
+    super();
+    this.loader = loader;
+  }
+
+  @Override
+  public void init(ExtendedProperties extendedProperties) {
+  }
+
+  @Override
+  public InputStream getResourceStream(String template_name) throws ResourceNotFoundException {
+    return loader.openResource(template_name);
+  }
+
+  @Override
+  public boolean isSourceModified(Resource resource) {
+    return false;
+  }
+
+  @Override
+  public long getLastModified(Resource resource) {
+    return 0;
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/VelocityResponseWriter.java b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/VelocityResponseWriter.java
index 6962a789..aa024f16 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/VelocityResponseWriter.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/java/org/apache/solr/response/VelocityResponseWriter.java
@@ -1,2 +1,192 @@
   + Date Author Id Revision HeadURL
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.response;
+
+import org.apache.solr.client.solrj.SolrResponse;
+import org.apache.solr.client.solrj.response.QueryResponse;
+import org.apache.solr.client.solrj.response.SolrResponseBase;
+import org.apache.solr.common.util.NamedList;
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.velocity.Template;
+import org.apache.velocity.VelocityContext;
+import org.apache.velocity.app.VelocityEngine;
+import org.apache.velocity.tools.generic.*;
+
+import java.io.*;
+import java.util.Properties;
+
+public class VelocityResponseWriter implements QueryResponseWriter {
+
+  // TODO: maybe pass this Logger to the template for logging from there?
+//  private static final Logger log = LoggerFactory.getLogger(VelocityResponseWriter.class);
+
+  public void write(Writer writer, SolrQueryRequest request, SolrQueryResponse response) throws IOException {
+    VelocityEngine engine = getEngine(request);  // TODO: have HTTP headers available for configuring engine
+
+    Template template = getTemplate(engine, request);
+
+    VelocityContext context = new VelocityContext();
+
+    context.put("request", request);
+
+    // Turn the SolrQueryResponse into a SolrResponse.
+    // QueryResponse has lots of conveniences suitable for a view
+    // Problem is, which SolrResponse class to use?
+    // One patch to SOLR-620 solved this by passing in a class name as
+    // as a parameter and using reflection and Solr's class loader to
+    // create a new instance.  But for now the implementation simply
+    // uses QueryResponse, and if it chokes in a known way, fall back
+    // to bare bones SolrResponseBase.
+    // TODO: Can this writer know what the handler class is?  With echoHandler=true it can get its string name at least
+    SolrResponse rsp = new QueryResponse();
+    NamedList<Object> parsedResponse = BinaryResponseWriter.getParsedResponse(request, response);
+    try {
+      rsp.setResponse(parsedResponse);
+
+      // page only injected if QueryResponse works
+      context.put("page", new PageTool(request, response));  // page tool only makes sense for a SearchHandler request... *sigh*
+    } catch (ClassCastException e) {
+      // known edge case where QueryResponse's extraction assumes "response" is a SolrDocumentList
+      // (AnalysisRequestHandler emits a "response")
+      e.printStackTrace();
+      rsp = new SolrResponseBase();
+      rsp.setResponse(parsedResponse);
+    }
+    context.put("response", rsp);
+
+    // Velocity context tools - TODO: make these pluggable
+    context.put("esc", new EscapeTool());
+    context.put("date", new ComparisonDateTool());
+    context.put("list", new ListTool());
+    context.put("math", new MathTool());
+    context.put("number", new NumberTool());
+    context.put("sort", new SortTool());
+
+    context.put("engine", engine);  // for $engine.resourceExists(...)
+
+    String layout_template = request.getParams().get("v.layout");
+    String json_wrapper = request.getParams().get("v.json");
+    boolean wrap_response = (layout_template != null) || (json_wrapper != null);
+
+    // create output, optionally wrap it into a json object
+    if (wrap_response) {
+      StringWriter stringWriter = new StringWriter();
+      template.merge(context, stringWriter);
+
+      if (layout_template != null) {
+        context.put("content", stringWriter.toString());
+        stringWriter = new StringWriter();
+        try {
+          engine.getTemplate(layout_template + ".vm").merge(context, stringWriter);
+        } catch (Exception e) {
+          throw new IOException(e.getMessage());
+        }
+      }
+
+      if (json_wrapper != null) {
+        writer.write(request.getParams().get("v.json") + "(");
+        writer.write(getJSONWrap(stringWriter.toString()));
+        writer.write(')');
+      } else {  // using a layout, but not JSON wrapping
+        writer.write(stringWriter.toString());
+      }
+    } else {
+      template.merge(context, writer);
+    }
+  }
+
+  private VelocityEngine getEngine(SolrQueryRequest request) {
+    VelocityEngine engine = new VelocityEngine();
+    String template_root = request.getParams().get("v.base_dir");
+    File baseDir = new File(request.getCore().getResourceLoader().getConfigDir(), "velocity");
+    if (template_root != null) {
+      baseDir = new File(template_root);
+    }
+    engine.setProperty(VelocityEngine.FILE_RESOURCE_LOADER_PATH, baseDir.getAbsolutePath());
+    engine.setProperty("params.resource.loader.instance", new SolrParamResourceLoader(request));
+    SolrVelocityResourceLoader resourceLoader =
+        new SolrVelocityResourceLoader(request.getCore().getSolrConfig().getResourceLoader());
+    engine.setProperty("solr.resource.loader.instance", resourceLoader);
+
+    // TODO: Externalize Velocity properties
+    engine.setProperty(VelocityEngine.RESOURCE_LOADER, "params,file,solr");
+    String propFile = request.getParams().get("v.properties");
+    try {
+      if (propFile == null)
+        engine.init();
+      else {
+        InputStream is = null;
+        try {
+          is = resourceLoader.getResourceStream(propFile);
+          Properties props = new Properties();
+          props.load(is);
+          engine.init(props);
+        }
+        finally {
+          if (is != null) is.close();
+        }
+      }
+    }
+    catch (Exception e) {
+      throw new RuntimeException(e);
+    }
+
+    return engine;
+  }
+
+  private Template getTemplate(VelocityEngine engine, SolrQueryRequest request) throws IOException {
+    Template template;
+
+    String template_name = request.getParams().get("v.template");
+    String qt = request.getParams().get("qt");
+    String path = (String) request.getContext().get("path");
+    if (template_name == null && path != null) {
+      template_name = path;
+    }  // TODO: path is never null, so qt won't get picked up  maybe special case for '/select' to use qt, otherwise use path?
+    if (template_name == null && qt != null) {
+      template_name = qt;
+    }
+    if (template_name == null) template_name = "index";
+    try {
+      template = engine.getTemplate(template_name + ".vm");
+    } catch (Exception e) {
+      throw new IOException(e.getMessage());
+    }
+
+    return template;
+  }
+
+  public String getContentType(SolrQueryRequest request, SolrQueryResponse response) {
+    return request.getParams().get("v.contentType", "text/html;charset=UTF-8");
+  }
+
+  private String getJSONWrap(String xmlResult) {  // TODO: maybe noggit or Solr's JSON utilities can make this cleaner?
+    // escape the double quotes and backslashes
+    String replace1 = xmlResult.replaceAll("\\\\", "\\\\\\\\");
+    replace1 = replace1.replaceAll("\\n", "\\\\n");
+    replace1 = replace1.replaceAll("\\r", "\\\\r");
+    String replaced = replace1.replaceAll("\"", "\\\\\"");
+    // wrap it in a JSON object
+    return "{\"result\":\"" + replaced + "\"}";
+  }
+
+  public void init(NamedList args) {
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/test/org/apache/solr/velocity/VelocityResponseWriterTest.java b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/test/org/apache/solr/velocity/VelocityResponseWriterTest.java
index e69de29b..79aeb42f 100644
--- a/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/test/org/apache/solr/velocity/VelocityResponseWriterTest.java
+++ b/lucene/dev/branches/solrcloud/solr/contrib/velocity/src/test/org/apache/solr/velocity/VelocityResponseWriterTest.java
@@ -0,0 +1,57 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.velocity;
+
+import org.apache.solr.SolrTestCaseJ4;
+import org.apache.solr.response.SolrQueryResponse;
+import org.apache.solr.response.VelocityResponseWriter;
+import org.apache.solr.request.SolrQueryRequest;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import java.io.StringWriter;
+import java.io.IOException;
+
+public class VelocityResponseWriterTest extends SolrTestCaseJ4 {
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    initCore("solrconfig.xml", "schema.xml", getFile("velocity/solr").getAbsolutePath());
+  }
+
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    clearIndex();
+    assertU(commit());
+  }
+
+  @Test
+  public void testTemplateName() throws IOException {
+    org.apache.solr.response.VelocityResponseWriter vrw = new VelocityResponseWriter();
+    SolrQueryRequest req = req("v.template","custom", "v.template.custom","$response.response.response_data");
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    StringWriter buf = new StringWriter();
+    rsp.add("response_data", "testing");
+    vrw.write(buf, req, rsp);
+    assertEquals("testing", buf.toString());
+  }
+
+  // TODO: add test that works with true Solr requests and wt=velocity to ensure the test tests that it's registered properly, etc
+}
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/analysis/PhoneticFilterFactory.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/analysis/PhoneticFilterFactory.java
index 624fbbf7..6ae0bf3a 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/analysis/PhoneticFilterFactory.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/analysis/PhoneticFilterFactory.java
@@ -21,9 +21,12 @@
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
 
 import org.apache.commons.codec.Encoder;
 import org.apache.commons.codec.language.Caverphone;
+import org.apache.commons.codec.language.ColognePhonetic;
 import org.apache.commons.codec.language.DoubleMetaphone;
 import org.apache.commons.codec.language.Metaphone;
 import org.apache.commons.codec.language.RefinedSoundex;
@@ -59,16 +62,18 @@
 {
   public static final String ENCODER = "encoder";
   public static final String INJECT = "inject"; // boolean
+  private static final String PACKAGE_CONTAINING_ENCODERS = "org.apache.commons.codec.language.";
   
-  private static final Map<String, Class<? extends Encoder>> registry;
-  static {
-    registry = new HashMap<String, Class<? extends Encoder>>();
-    registry.put( "DoubleMetaphone".toUpperCase(Locale.ENGLISH), DoubleMetaphone.class );
-    registry.put( "Metaphone".toUpperCase(Locale.ENGLISH),       Metaphone.class );
-    registry.put( "Soundex".toUpperCase(Locale.ENGLISH),         Soundex.class );
-    registry.put( "RefinedSoundex".toUpperCase(Locale.ENGLISH),  RefinedSoundex.class );
-    registry.put( "Caverphone".toUpperCase(Locale.ENGLISH),      Caverphone.class );
-  }
+  private static final Map<String, Class<? extends Encoder>> registry = new HashMap<String, Class<? extends Encoder>>()
+  {{
+    put( "DoubleMetaphone".toUpperCase(Locale.ENGLISH), DoubleMetaphone.class );
+    put( "Metaphone".toUpperCase(Locale.ENGLISH),       Metaphone.class );
+    put( "Soundex".toUpperCase(Locale.ENGLISH),         Soundex.class );
+    put( "RefinedSoundex".toUpperCase(Locale.ENGLISH),  RefinedSoundex.class );
+    put( "Caverphone".toUpperCase(Locale.ENGLISH),      Caverphone.class );
+    put( "ColognePhonetic".toUpperCase(Locale.ENGLISH), ColognePhonetic.class );
+  }};
+  private static final Lock lock = new ReentrantLock();
   
   protected boolean inject = true;
   protected String name = null;
@@ -87,7 +92,12 @@ public void init(Map<String,String> args) {
     }
     Class<? extends Encoder> clazz = registry.get(name.toUpperCase(Locale.ENGLISH));
     if( clazz == null ) {
-      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, "Unknown encoder: "+name +" ["+registry.keySet()+"]" );
+      lock.lock();
+      try {
+        clazz = resolveEncoder(name);
+      } finally {
+        lock.unlock();
+      }
     }
     
     try {
@@ -105,6 +115,30 @@ public void init(Map<String,String> args) {
     }
   }
   
+  private Class<? extends Encoder> resolveEncoder(String name) {
+    Class<? extends Encoder> clazz = null;
+    try {
+      clazz = lookupEncoder(PACKAGE_CONTAINING_ENCODERS+name);
+    } catch (ClassNotFoundException e) {
+      try {
+        clazz = lookupEncoder(name);
+      } catch (ClassNotFoundException cnfe) {
+        throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, "Unknown encoder: "+name +" ["+registry.keySet()+"]" );
+      }
+    }
+    catch (ClassCastException e) {
+      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, "Not an encoder: "+name +" ["+registry.keySet()+"]" );
+    }
+    return clazz;
+  }
+  
+  private Class<? extends Encoder> lookupEncoder(String name)
+      throws ClassNotFoundException {
+    Class<? extends Encoder> clazz = Class.forName(name).asSubclass(Encoder.class);
+    registry.put( name.toUpperCase(Locale.ENGLISH), clazz );
+    return clazz;
+  }
+
   public PhoneticFilter create(TokenStream input) {
     return new PhoneticFilter(input,encoder,inject);
   }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/core/RequestHandlers.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/core/RequestHandlers.java
index 80c7c04d..93def299 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/core/RequestHandlers.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/core/RequestHandlers.java
@@ -177,7 +177,7 @@ void initHandlersFromConfig(SolrConfig config ){
       }
     }
 
-    // we've now registered all handlers, time ot init them in the same order
+    // we've now registered all handlers, time to init them in the same order
     for (Map.Entry<PluginInfo,SolrRequestHandler> entry : handlers.entrySet()) {
       PluginInfo info = entry.getKey();
       SolrRequestHandler requestHandler = entry.getValue();
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/core/SolrCore.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/core/SolrCore.java
index d3b86c07..9b55edfc 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/core/SolrCore.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/core/SolrCore.java
@@ -48,14 +48,12 @@
 import org.apache.solr.util.plugin.SolrCoreAware;
 import org.apache.solr.util.plugin.PluginInfoInitialized;
 import org.apache.commons.io.IOUtils;
+import org.eclipse.jdt.core.dom.ThisExpression;
 import org.xml.sax.SAXException;
 
 import javax.xml.parsers.ParserConfigurationException;
 
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.InputStream;
+import java.io.*;
 import java.util.*;
 import java.util.concurrent.*;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -151,11 +149,13 @@ public String getDataDir() {
   }
 
   public String getIndexDir() {
+    synchronized (searcherLock) {
     if (_searcher == null)
       return dataDir + "index/";
     SolrIndexSearcher searcher = _searcher.get();
     return searcher.getIndexDir() == null ? dataDir + "index/" : searcher.getIndexDir();
   }
+  }
 
 
   /**
@@ -289,8 +289,8 @@ public void registerNewSearcherListener( SolrEventListener listener )
    * 
    * @see SolrCoreAware
    */
-  public void registerResponseWriter( String name, QueryResponseWriter responseWriter ){
-    responseWriters.put(name, responseWriter);
+  public QueryResponseWriter registerResponseWriter( String name, QueryResponseWriter responseWriter ){
+    return responseWriters.put(name, responseWriter);
   }
   
   public SolrCore reload(SolrResourceLoader resourceLoader) throws IOException,
@@ -475,6 +475,10 @@ private UpdateHandler createUpdateHandler(String className, UpdateHandler update
     return createReloadedUpdateHandler(className, UpdateHandler.class, "Update Handler", updateHandler);
   }
   
+  private QueryResponseWriter createQueryResponseWriter(String className) {
+    return createInstance(className, QueryResponseWriter.class, "Query Response Writer");
+  }
+  
   /**
    * 
    * @param dataDir
@@ -813,6 +817,21 @@ public void addCloseHook( CloseHook hook )
      closeHooks.add( hook );
    }
 
+  /** @lucene.internal
+   *  Debugging aid only.  No non-test code should be released with uncommented verbose() calls.  */
+  public static boolean VERBOSE = Boolean.parseBoolean(System.getProperty("tests.verbose","false"));
+  public static void verbose(Object... args) {
+    if (!VERBOSE) return;
+    StringBuilder sb = new StringBuilder("VERBOSE:");
+    sb.append(Thread.currentThread().getName());
+    sb.append(':');
+    for (Object o : args) {
+      sb.append(' ');
+      sb.append(o==null ? "(null)" : o.toString());
+    }
+    System.out.println(sb.toString());
+  }
+
 
   ////////////////////////////////////////////////////////////////////////////////
   // Request Handler
@@ -942,7 +961,8 @@ public UpdateHandler getUpdateHandler() {
 
   // The current searcher used to service queries.
   // Don't access this directly!!!! use getSearcher() to
-  // get it (and it will increment the ref count at the same time)
+  // get it (and it will increment the ref count at the same time).
+  // This reference is protected by searcherLock.
   private RefCounted<SolrIndexSearcher> _searcher;
 
   // All of the open searchers.  Don't access this directly.
@@ -1107,13 +1127,15 @@ public UpdateHandler getUpdateHandler() {
         if (updateHandlerReopens) {
           
           tmp = getUpdateHandler().reopenSearcher(newestSearcher.get());
-          
         } else {
           
           IndexReader currentReader = newestSearcher.get().getIndexReader();
           IndexReader newReader;
           
+          // verbose("start reopen without writer, reader=", currentReader);
           newReader = IndexReader.openIfChanged(currentReader);
+          // verbose("reopen result", newReader);
+
           
           if (newReader == null) {
             currentReader.incRef();
@@ -1123,8 +1145,11 @@ public UpdateHandler getUpdateHandler() {
           tmp = new SolrIndexSearcher(this, schema, "main", newReader, true, true, true, directoryFactory);
         }
 
+
       } else {
+        // verbose("non-reopen START:");
         tmp = new SolrIndexSearcher(this, newIndexDir, schema, getSolrConfig().mainIndexConfig, "main", true, true, directoryFactory);
+        // verbose("non-reopen DONE: searcher=",tmp);
       }
     } catch (Throwable th) {
       synchronized(searcherLock) {
@@ -1157,6 +1182,7 @@ public UpdateHandler getUpdateHandler() {
       boolean alreadyRegistered = false;
       synchronized (searcherLock) {
         _searchers.add(newSearchHolder);
+        // verbose("added searcher ",newSearchHolder.get()," to _searchers");
 
         if (_searcher == null) {
           // if there isn't a current searcher then we may
@@ -1523,7 +1549,6 @@ final public static void log(Throwable e) {
     m.put("ruby", new RubyResponseWriter());
     m.put("raw", new RawResponseWriter());
     m.put("javabin", new BinaryResponseWriter());
-    m.put("velocity", new VelocityResponseWriter());
     m.put("csv", new CSVResponseWriter());
     DEFAULT_RESPONSE_WRITERS = Collections.unmodifiableMap(m);
   }
@@ -1531,7 +1556,54 @@ final public static void log(Throwable e) {
   /** Configure the query response writers. There will always be a default writer; additional
    * writers may also be configured. */
   private void initWriters() {
-    defaultResponseWriter = initPlugins(responseWriters, QueryResponseWriter.class);
+    // use link map so we iterate in the same order
+    Map<PluginInfo,QueryResponseWriter> writers = new LinkedHashMap<PluginInfo,QueryResponseWriter>();
+    for (PluginInfo info : solrConfig.getPluginInfos(QueryResponseWriter.class.getName())) {
+      try {
+        QueryResponseWriter writer;
+        String startup = info.attributes.get("startup") ;
+        if( startup != null ) {
+          if( "lazy".equals(startup) ) {
+            log.info("adding lazy queryResponseWriter: " + info.className);
+            writer = new LazyQueryResponseWriterWrapper(this, info.className, info.initArgs );
+          } else {
+            throw new Exception( "Unknown startup value: '"+startup+"' for: "+info.className );
+          }
+        } else {
+          writer = createQueryResponseWriter(info.className);
+        }
+        writers.put(info,writer);
+        QueryResponseWriter old = registerResponseWriter(info.name, writer);
+        if(old != null) {
+          log.warn("Multiple queryResponseWriter registered to the same name: " + info.name + " ignoring: " + old.getClass().getName());
+        }
+        if(info.isDefault()){
+          defaultResponseWriter = writer;
+          if(defaultResponseWriter != null)
+            log.warn("Multiple default queryResponseWriter registered ignoring: " + old.getClass().getName());
+        }
+        log.info("created "+info.name+": " + info.className);
+      } catch (Exception ex) {
+          SolrConfig.severeErrors.add( ex );
+          SolrException e = new SolrException
+            (SolrException.ErrorCode.SERVER_ERROR, "QueryResponseWriter init failure", ex);
+          SolrException.logOnce(log,null,e);
+          throw e;
+      }
+    }
+
+    // we've now registered all handlers, time to init them in the same order
+    for (Map.Entry<PluginInfo,QueryResponseWriter> entry : writers.entrySet()) {
+      PluginInfo info = entry.getKey();
+      QueryResponseWriter writer = entry.getValue();
+      responseWriters.put(info.name, writer);
+      if (writer instanceof PluginInfoInitialized) {
+        ((PluginInfoInitialized) writer).init(info);
+      } else{
+        writer.init(info.initArgs);
+      }
+    }
+
     for (Map.Entry<String, QueryResponseWriter> entry : DEFAULT_RESPONSE_WRITERS.entrySet()) {
       if(responseWriters.get(entry.getKey()) == null) responseWriters.put(entry.getKey(), entry.getValue());
     }
@@ -1783,6 +1855,50 @@ public CodecProvider getCodecProvider() {
     return codecProvider;
   }
 
+  public final class LazyQueryResponseWriterWrapper implements QueryResponseWriter {
+    private SolrCore _core;
+    private String _className;
+    private NamedList _args;
+    private QueryResponseWriter _writer;
+
+    public LazyQueryResponseWriterWrapper(SolrCore core, String className, NamedList args) {
+      _core = core;
+      _className = className;
+      _args = args;
+      _writer = null;
+    }
+
+    public synchronized QueryResponseWriter getWrappedWriter()
+    {
+      if( _writer == null ) {
+        try {
+          QueryResponseWriter writer = createQueryResponseWriter(_className);
+          writer.init( _args );
+          _writer = writer;
+        }
+        catch( Exception ex ) {
+          throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, "lazy loading error", ex );
+        }
+      }
+      return _writer;
+    }
+
+
+    @Override
+    public void init(NamedList args) {
+      // do nothing
+    }
+
+    @Override
+    public void write(Writer writer, SolrQueryRequest request, SolrQueryResponse response) throws IOException {
+      getWrappedWriter().write(writer, request, response);
+    }
+
+    @Override
+    public String getContentType(SolrQueryRequest request, SolrQueryResponse response) {
+      return getWrappedWriter().getContentType(request, response);
+    }
+  }
 }
 
 
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
index 610a043c..dd74fe9d 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
@@ -345,19 +345,17 @@ SolrInputDocument readDocument(XMLStreamReader reader, IndexSchema schema) throw
    */
   private ContentStream extractSingleContentStream(SolrQueryRequest req) {
     Iterable<ContentStream> streams = req.getContentStreams();
+    String exceptionMsg = "DocumentAnalysisRequestHandler expects a single content stream with documents to analyze";
     if (streams == null) {
-      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
-              "DocumentAnlysisRequestHandler expects a single content stream with documents to analyze");
+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, exceptionMsg);
     }
     Iterator<ContentStream> iter = streams.iterator();
     if (!iter.hasNext()) {
-      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
-              "DocumentAnlysisRequestHandler expects a single content stream with documents to analyze");
+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, exceptionMsg);
     }
     ContentStream stream = iter.next();
     if (iter.hasNext()) {
-      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
-              "DocumentAnlysisRequestHandler expects a single content stream with documents to analyze");
+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, exceptionMsg);
     }
     return stream;
   }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
index fc5b5125..75f3f326 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
@@ -27,8 +27,10 @@
 import org.apache.solr.common.SolrInputDocument;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.util.StrUtils;
+import org.apache.solr.core.SolrCore;
 import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.response.SolrQueryResponse;
+import org.apache.solr.response.transform.DocTransformer;
 import org.apache.solr.schema.FieldType;
 import org.apache.solr.schema.IndexSchema;
 import org.apache.solr.schema.SchemaField;
@@ -100,6 +102,7 @@ public void process(ResponseBuilder rb) throws IOException
 
     RefCounted<SolrIndexSearcher> searcherHolder = null;
 
+    DocTransformer transformer = rsp.getReturnFields().getTransformer();
    try {
      SolrIndexSearcher searcher = null;
 
@@ -115,7 +118,11 @@ public void process(ResponseBuilder rb) throws IOException
            int oper = (Integer)entry.get(0);
            switch (oper) {
              case UpdateLog.ADD:
-              docList.add(toSolrDoc((SolrInputDocument)entry.get(entry.size()-1), req.getSchema()));
+               SolrDocument doc = toSolrDoc((SolrInputDocument)entry.get(entry.size()-1), req.getSchema());
+               if(transformer!=null) {
+                 transformer.transform(doc, -1); // unknown docID
+               }
+              docList.add(doc);
               break;
              case UpdateLog.DELETE:
               break;
@@ -132,10 +139,16 @@ public void process(ResponseBuilder rb) throws IOException
          searcher = searcherHolder.get();
        }
 
+       // SolrCore.verbose("RealTimeGet using searcher ", searcher);
+
        int docid = searcher.getFirstMatch(new Term(idField.getName(), idBytes));
        if (docid < 0) continue;
        Document luceneDocument = searcher.doc(docid);
-       docList.add(toSolrDoc(luceneDocument,  req.getSchema()));
+       SolrDocument doc = toSolrDoc(luceneDocument,  req.getSchema());
+       if( transformer != null ) {
+         transformer.transform(doc, docid);
+       }
+       docList.add(doc);
      }
 
    } finally {
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
index 4b0acc22..24d982b3 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
@@ -257,7 +257,7 @@ void countTerms() throws IOException {
         // count collection array only needs to be as big as the number of terms we are
         // going to collect counts for.
         final int[] counts = this.counts = new int[nTerms];
-        DocIdSet idSet = baseSet.getDocIdSet(context);
+        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs
         DocIdSetIterator iter = idSet.iterator();
 
 
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/BinaryResponseWriter.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/BinaryResponseWriter.java
index 870c0997..35c9c3a4 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/BinaryResponseWriter.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/BinaryResponseWriter.java
@@ -87,7 +87,18 @@ public Object resolve(Object o, JavaBinCodec codec) throws IOException {
         writeResults(ctx, codec);
         return null; // null means we completely handled it
       }
+      if( o instanceof IndexableField ) {
+        if(schema == null) schema = solrQueryRequest.getSchema(); 
 
+        IndexableField f = (IndexableField)o;
+        SchemaField sf = schema.getFieldOrNull(f.name());
+        try {
+          o = getValue(sf, f);
+        } 
+        catch (Exception e) {
+          LOG.warn("Error reading a field : " + o, e);
+        }
+      }
       if (o instanceof SolrDocument) {
         // Remove any fields that were not requested.
         // This typically happens when distributed search adds 
@@ -163,28 +174,11 @@ public SolrDocument getDoc(Document doc) {
         String fieldName = f.name();
         if( !returnFields.wantsField(fieldName) ) 
           continue;
+        
         SchemaField sf = schema.getFieldOrNull(fieldName);
-        FieldType ft = null;
-        if(sf != null) ft =sf.getType();
-        Object val;
-        if (ft == null) {  // handle fields not in the schema
-          BytesRef bytesRef = f.binaryValue();
-          if (bytesRef != null) {
-            if (bytesRef.offset == 0 && bytesRef.length == bytesRef.bytes.length) {
-              val = bytesRef.bytes;
-            } else {
-              final byte[] bytes = new byte[bytesRef.length];
-              val = bytes;
-              System.arraycopy(bytesRef.bytes, bytesRef.offset, bytes, 0, bytesRef.length);
-            }
-          } else val = f.stringValue();
-        } else {
+        Object val = null;
           try {
-            if (useFieldObjects && KNOWN_TYPES.contains(ft.getClass())) {
-              val = ft.toObject(f);
-            } else {
-              val = ft.toExternal(f);
-            }
+          val = getValue(sf,f);
           } catch (Exception e) {
             // There is a chance of the underlying field not really matching the
             // actual field type . So ,it can throw exception
@@ -192,7 +186,7 @@ public SolrDocument getDoc(Document doc) {
             //if it happens log it and continue
             continue;
           }
-        }
+          
         if(sf != null && sf.multiValued() && !solrDoc.containsKey(fieldName)){
           ArrayList l = new ArrayList();
           l.add(val);
@@ -204,6 +198,29 @@ public SolrDocument getDoc(Document doc) {
       return solrDoc;
     }
 
+    public Object getValue(SchemaField sf, IndexableField f) throws Exception {
+      FieldType ft = null;
+      if(sf != null) ft =sf.getType();
+      
+      if (ft == null) {  // handle fields not in the schema
+        BytesRef bytesRef = f.binaryValue();
+        if (bytesRef != null) {
+          if (bytesRef.offset == 0 && bytesRef.length == bytesRef.bytes.length) {
+            return bytesRef.bytes;
+          } else {
+            final byte[] bytes = new byte[bytesRef.length];
+            System.arraycopy(bytesRef.bytes, bytesRef.offset, bytes, 0, bytesRef.length);
+            return bytes;
+          }
+        } else return f.stringValue();
+      } else {
+        if (useFieldObjects && KNOWN_TYPES.contains(ft.getClass())) {
+          return ft.toObject(f);
+        } else {
+          return ft.toExternal(f);
+        }
+      }
+    }
   }
 
 
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/PageTool.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/PageTool.java
index 42bf31e5..e69de29b 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/PageTool.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/PageTool.java
@@ -1,90 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.response;
-
-import org.apache.solr.request.SolrQueryRequest;
-import org.apache.solr.response.SolrQueryResponse;
-import org.apache.solr.search.DocList;
-import org.apache.solr.search.DocSlice;
-import org.apache.solr.common.SolrDocumentList;
-import org.apache.solr.common.SolrException;
-
-public class PageTool {
-  private long start;
-  private int results_per_page = 10;
-  private long results_found;
-  private int page_count;
-  private int current_page_number;
-
-  public PageTool(SolrQueryRequest request, SolrQueryResponse response) {
-    String rows = request.getParams().get("rows");
-
-    if (rows != null) {
-      results_per_page = new Integer(rows);
-    }
-    //TODO: Handle group by results
-    Object docs = response.getValues().get("response");
-    if (docs != null) {
-      if (docs instanceof DocSlice) {
-        DocSlice doc_slice = (DocSlice) docs;
-        results_found = doc_slice.matches();
-        start = doc_slice.offset();
-      } else if(docs instanceof ResultContext) {
-        DocList dl = ((ResultContext) docs).docs;
-        results_found = dl.matches();
-        start = dl.offset();
-      } else if(docs instanceof SolrDocumentList) {
-        SolrDocumentList doc_list = (SolrDocumentList) docs;
-        results_found = doc_list.getNumFound();
-        start = doc_list.getStart();
-      } else {
-	  throw new SolrException(SolrException.ErrorCode.UNKNOWN, "Unknown response type "+docs+". Expected one of DocSlice, ResultContext or SolrDocumentList");
-      }
-    }
-
-    page_count = (int) Math.ceil(results_found / (double) results_per_page);
-    current_page_number = (int) Math.ceil(start / (double) results_per_page) + (page_count > 0 ? 1 : 0);
-  }
-
-  public long getStart() {
-    return start;
-  }
-
-  public int getResults_per_page() {
-    return results_per_page;
-  }
-
-  public long getResults_found() {
-    return results_found;
-  }
-
-  public int getPage_count() {
-    return page_count;
-  }
-
-  public int getCurrent_page_number() {
-    return current_page_number;
-  }
-
-  @Override
-  public String toString() {
-    return "Found " + results_found +
-           " Page " + current_page_number + " of " + page_count +
-           " Starting at " + start + " per page " + results_per_page;
-  }
-}
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/SolrParamResourceLoader.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/SolrParamResourceLoader.java
index 6319a602..e69de29b 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/SolrParamResourceLoader.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/SolrParamResourceLoader.java
@@ -1,76 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.response;
-
-import org.apache.solr.request.SolrQueryRequest;
-import org.apache.velocity.runtime.resource.loader.ResourceLoader;
-import org.apache.velocity.runtime.resource.Resource;
-import org.apache.velocity.exception.ResourceNotFoundException;
-import org.apache.commons.collections.ExtendedProperties;
-
-import java.io.ByteArrayInputStream;
-import java.io.InputStream;
-import java.io.UnsupportedEncodingException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-
-public class SolrParamResourceLoader extends ResourceLoader {
-  private Map<String,String> templates = new HashMap<String,String>();
-  public SolrParamResourceLoader(SolrQueryRequest request) {
-    super();
-
-    // TODO: Consider using content streams, but need a template name associated with each stream
-    // for now, a custom param convention of template.<name>=<template body> is a nice example
-    // of per-request overrides of templates
-
-    org.apache.solr.common.params.SolrParams params = request.getParams();
-    Iterator<String> names = params.getParameterNamesIterator();
-    while (names.hasNext()) {
-      String name = names.next();
-      
-      if (name.startsWith("v.template.")) {
-        templates.put(name.substring(11) + ".vm",params.get(name));
-      }
-    }
-  }
-
-  @Override
-  public void init(ExtendedProperties extendedProperties) {
-  }
-
-  @Override
-  public InputStream getResourceStream(String s) throws ResourceNotFoundException {
-    String template = templates.get(s);
-    try {
-      return template == null ? null : new ByteArrayInputStream(template.getBytes("UTF-8"));
-    } catch (UnsupportedEncodingException e) {
-      throw new RuntimeException(e); // may not happen
-    }
-  }
-
-  @Override
-  public boolean isSourceModified(Resource resource) {
-    return false;
-  }
-
-  @Override
-  public long getLastModified(Resource resource) {
-    return 0;
-  }
-}
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/SolrVelocityResourceLoader.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/SolrVelocityResourceLoader.java
index 67feca0a..e69de29b 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/SolrVelocityResourceLoader.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/SolrVelocityResourceLoader.java
@@ -1,54 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.solr.response;
-
-import org.apache.velocity.runtime.resource.loader.ResourceLoader;
-import org.apache.velocity.runtime.resource.Resource;
-import org.apache.velocity.exception.ResourceNotFoundException;
-import org.apache.commons.collections.ExtendedProperties;
-import org.apache.solr.core.SolrResourceLoader;
-
-import java.io.InputStream;
-
-// TODO: the name of this class seems ridiculous
-public class SolrVelocityResourceLoader extends ResourceLoader {
-  private SolrResourceLoader loader;
-
-  public SolrVelocityResourceLoader(SolrResourceLoader loader) {
-    super();
-    this.loader = loader;
-  }
-
-  @Override
-  public void init(ExtendedProperties extendedProperties) {
-  }
-
-  @Override
-  public InputStream getResourceStream(String template_name) throws ResourceNotFoundException {
-    return loader.openResource(template_name);
-  }
-
-  @Override
-  public boolean isSourceModified(Resource resource) {
-    return false;
-  }
-
-  @Override
-  public long getLastModified(Resource resource) {
-    return 0;
-  }
-}
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/TextResponseWriter.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/TextResponseWriter.java
index 7ebaa8b7..3c7d2939 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/TextResponseWriter.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/TextResponseWriter.java
@@ -146,6 +146,9 @@ public final void writeVal(String name, Object val) throws IOException {
       writeDouble(name, ((Double)val).doubleValue());
     } else if (val instanceof Document) {
       SolrDocument doc = toSolrDocument( (Document)val );
+      if( returnFields.getTransformer() != null ) {
+        returnFields.getTransformer().transform( doc, -1 );
+      }
       writeSolrDocument(name, doc, returnFields, 0 );
     } else if (val instanceof SolrDocument) {
       writeSolrDocument(name, (SolrDocument)val, returnFields, 0);
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/VelocityResponseWriter.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/VelocityResponseWriter.java
index c3a2d5ae..e69de29b 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/VelocityResponseWriter.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/response/VelocityResponseWriter.java
@@ -1,190 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.response;
-
-import org.apache.solr.client.solrj.SolrResponse;
-import org.apache.solr.client.solrj.response.QueryResponse;
-import org.apache.solr.client.solrj.response.SolrResponseBase;
-import org.apache.solr.common.util.NamedList;
-import org.apache.solr.request.SolrQueryRequest;
-import org.apache.velocity.Template;
-import org.apache.velocity.VelocityContext;
-import org.apache.velocity.app.VelocityEngine;
-import org.apache.velocity.tools.generic.*;
-
-import java.io.*;
-import java.util.Properties;
-
-public class VelocityResponseWriter implements QueryResponseWriter {
-
-  // TODO: maybe pass this Logger to the template for logging from there?
-//  private static final Logger log = LoggerFactory.getLogger(VelocityResponseWriter.class);
-
-  public void write(Writer writer, SolrQueryRequest request, SolrQueryResponse response) throws IOException {
-    VelocityEngine engine = getEngine(request);  // TODO: have HTTP headers available for configuring engine
-
-    Template template = getTemplate(engine, request);
-
-    VelocityContext context = new VelocityContext();
-
-    context.put("request", request);
-
-    // Turn the SolrQueryResponse into a SolrResponse.
-    // QueryResponse has lots of conveniences suitable for a view
-    // Problem is, which SolrResponse class to use?
-    // One patch to SOLR-620 solved this by passing in a class name as
-    // as a parameter and using reflection and Solr's class loader to
-    // create a new instance.  But for now the implementation simply
-    // uses QueryResponse, and if it chokes in a known way, fall back
-    // to bare bones SolrResponseBase.
-    // TODO: Can this writer know what the handler class is?  With echoHandler=true it can get its string name at least
-    SolrResponse rsp = new QueryResponse();
-    NamedList<Object> parsedResponse = BinaryResponseWriter.getParsedResponse(request, response);
-    try {
-      rsp.setResponse(parsedResponse);
-
-      // page only injected if QueryResponse works
-      context.put("page", new PageTool(request, response));  // page tool only makes sense for a SearchHandler request... *sigh*
-    } catch (ClassCastException e) {
-      // known edge case where QueryResponse's extraction assumes "response" is a SolrDocumentList
-      // (AnalysisRequestHandler emits a "response")
-      e.printStackTrace();
-      rsp = new SolrResponseBase();
-      rsp.setResponse(parsedResponse);
-    }
-    context.put("response", rsp);
-
-    // Velocity context tools - TODO: make these pluggable
-    context.put("esc", new EscapeTool());
-    context.put("date", new ComparisonDateTool());
-    context.put("list", new ListTool());
-    context.put("math", new MathTool());
-    context.put("number", new NumberTool());
-    context.put("sort", new SortTool());
-
-    context.put("engine", engine);  // for $engine.resourceExists(...)
-
-    String layout_template = request.getParams().get("v.layout");
-    String json_wrapper = request.getParams().get("v.json");
-    boolean wrap_response = (layout_template != null) || (json_wrapper != null);
-
-    // create output, optionally wrap it into a json object
-    if (wrap_response) {
-      StringWriter stringWriter = new StringWriter();
-      template.merge(context, stringWriter);
-
-      if (layout_template != null) {
-        context.put("content", stringWriter.toString());
-        stringWriter = new StringWriter();
-        try {
-          engine.getTemplate(layout_template + ".vm").merge(context, stringWriter);
-        } catch (Exception e) {
-          throw new IOException(e.getMessage());
-        }
-      }
-
-      if (json_wrapper != null) {
-        writer.write(request.getParams().get("v.json") + "(");
-        writer.write(getJSONWrap(stringWriter.toString()));
-        writer.write(')');
-      } else {  // using a layout, but not JSON wrapping
-        writer.write(stringWriter.toString());
-      }
-    } else {
-      template.merge(context, writer);
-    }
-  }
-
-  private VelocityEngine getEngine(SolrQueryRequest request) {
-    VelocityEngine engine = new VelocityEngine();
-    String template_root = request.getParams().get("v.base_dir");
-    File baseDir = new File(request.getCore().getResourceLoader().getConfigDir(), "velocity");
-    if (template_root != null) {
-      baseDir = new File(template_root);
-    }
-    engine.setProperty(VelocityEngine.FILE_RESOURCE_LOADER_PATH, baseDir.getAbsolutePath());
-    engine.setProperty("params.resource.loader.instance", new SolrParamResourceLoader(request));
-    SolrVelocityResourceLoader resourceLoader =
-        new SolrVelocityResourceLoader(request.getCore().getSolrConfig().getResourceLoader());
-    engine.setProperty("solr.resource.loader.instance", resourceLoader);
-
-    // TODO: Externalize Velocity properties
-    engine.setProperty(VelocityEngine.RESOURCE_LOADER, "params,file,solr");
-    String propFile = request.getParams().get("v.properties");
-    try {
-      if (propFile == null)
-        engine.init();
-      else {
-        InputStream is = null;
-        try {
-          is = resourceLoader.getResourceStream(propFile);
-          Properties props = new Properties();
-          props.load(is);
-          engine.init(props);
-        }
-        finally {
-          if (is != null) is.close();
-        }
-      }
-    }
-    catch (Exception e) {
-      throw new RuntimeException(e);
-    }
-
-    return engine;
-  }
-
-  private Template getTemplate(VelocityEngine engine, SolrQueryRequest request) throws IOException {
-    Template template;
-
-    String template_name = request.getParams().get("v.template");
-    String qt = request.getParams().get("qt");
-    String path = (String) request.getContext().get("path");
-    if (template_name == null && path != null) {
-      template_name = path;
-    }  // TODO: path is never null, so qt won't get picked up  maybe special case for '/select' to use qt, otherwise use path?
-    if (template_name == null && qt != null) {
-      template_name = qt;
-    }
-    if (template_name == null) template_name = "index";
-    try {
-      template = engine.getTemplate(template_name + ".vm");
-    } catch (Exception e) {
-      throw new IOException(e.getMessage());
-    }
-
-    return template;
-  }
-
-  public String getContentType(SolrQueryRequest request, SolrQueryResponse response) {
-    return request.getParams().get("v.contentType", "text/html;charset=UTF-8");
-  }
-
-  private String getJSONWrap(String xmlResult) {  // TODO: maybe noggit or Solr's JSON utilities can make this cleaner?
-    // escape the double quotes and backslashes
-    String replace1 = xmlResult.replaceAll("\\\\", "\\\\\\\\");
-    replace1 = replace1.replaceAll("\\n", "\\\\n");
-    replace1 = replace1.replaceAll("\\r", "\\\\r");
-    String replaced = replace1.replaceAll("\"", "\\\\\"");
-    // wrap it in a JSON object
-    return "{\"result\":\"" + replaced + "\"}";
-  }
-
-  public void init(NamedList args) {
-  }
-}
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/BitDocSet.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/BitDocSet.java
index 2ffe6f08..8ff265c0 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/BitDocSet.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/BitDocSet.java
@@ -17,10 +17,17 @@
 
 package org.apache.solr.search;
 
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.BitsFilteredDocIdSet;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.OpenBitSet;
 import org.apache.lucene.util.OpenBitSetIterator;
 import org.apache.lucene.search.DocIdSetIterator;
 
+import java.io.IOException;
+
 /**
  * <code>BitDocSet</code> represents an unordered set of Lucene Document Ids
  * using a BitSet.  A set bit represents inclusion in the set for that document.
@@ -231,4 +238,75 @@ public long memSize() {
   protected BitDocSet clone() {
     return new BitDocSet((OpenBitSet)bits.clone(), size);
   }
+
+  @Override
+  public Filter getTopFilter() {
+    final OpenBitSet bs = bits;
+    // TODO: if cardinality isn't cached, do a quick measure of sparseness
+    // and return null from bits() if too sparse.
+
+    return new Filter() {
+      @Override
+      public DocIdSet getDocIdSet(final IndexReader.AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+        IndexReader reader = context.reader;
+
+        if (context.isTopLevel) {
+          return BitsFilteredDocIdSet.wrap(bs, acceptDocs);
+        }
+
+        final int base = context.docBase;
+        final int maxDoc = reader.maxDoc();
+        final int max = base + maxDoc;   // one past the max doc in this segment.
+
+        return BitsFilteredDocIdSet.wrap(new DocIdSet() {
+          @Override
+          public DocIdSetIterator iterator() throws IOException {
+            return new DocIdSetIterator() {
+              int pos=base-1;
+              int adjustedDoc=-1;
+
+              @Override
+              public int docID() {
+                return adjustedDoc;
+              }
+
+              @Override
+              public int nextDoc() throws IOException {
+                pos = bs.nextSetBit(pos+1);
+                return adjustedDoc = (pos>=0 && pos<max) ? pos-base : NO_MORE_DOCS;
+              }
+
+              @Override
+              public int advance(int target) throws IOException {
+                if (target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;
+                pos = bs.nextSetBit(target+base);
+                return adjustedDoc = (pos>=0 && pos<max) ? pos-base : NO_MORE_DOCS;
+              }
+            };
+          }
+
+          @Override
+          public boolean isCacheable() {
+            return true;
+          }
+
+          @Override
+          public Bits bits() throws IOException {
+            return new Bits() {
+              @Override
+              public boolean get(int index) {
+                return bs.fastGet(index + base);
+              }
+
+              @Override
+              public int length() {
+                return maxDoc;
+              }
+            };
+          }
+
+        }, acceptDocs);
+      }
+    };
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/DocSet.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/DocSet.java
index dd69c976..08ce2a88 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/DocSet.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/DocSet.java
@@ -18,10 +18,12 @@
 package org.apache.solr.search;
 
 import org.apache.solr.common.SolrException;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.OpenBitSet;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 
@@ -270,18 +272,18 @@ public Filter getTopFilter() {
 
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
         IndexReader reader = context.reader;
 
         if (context.isTopLevel) {
-          return bs;
+          return BitsFilteredDocIdSet.wrap(bs, acceptDocs);
         }
 
         final int base = context.docBase;
         final int maxDoc = reader.maxDoc();
         final int max = base + maxDoc;   // one past the max doc in this segment.
 
-        return new DocIdSet() {
+        return BitsFilteredDocIdSet.wrap(new DocIdSet() {
           @Override
           public DocIdSetIterator iterator() throws IOException {
             return new DocIdSetIterator() {
@@ -313,7 +315,13 @@ public boolean isCacheable() {
             return true;
           }
 
-        };
+          @Override
+          public Bits bits() throws IOException {
+            // sparse filters should not use random access
+            return null;
+          }
+
+        }, acceptDocs);
       }
     };
   }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
index c00f22d4..a179c33f 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
@@ -219,7 +219,7 @@ public Scorer scorer(IndexReader.AtomicReaderContext context, boolean scoreDocsI
         filter = resultSet.getTopFilter();
       }
 
-      DocIdSet readerSet = filter.getDocIdSet(context);
+      DocIdSet readerSet = filter.getDocIdSet(context, null);  // this set only includes live docs
       if (readerSet == null) readerSet=DocIdSet.EMPTY_DOCIDSET;
       return new JoinScorer(this, readerSet.iterator(), getBoost());
     }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java
index f6cea937..c54285b4 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java
@@ -121,13 +121,13 @@ public void normalize(float norm, float topLevelBoost) {
     @Override
     public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
         boolean topScorer, Bits acceptDocs) throws IOException {
-      return new ConstantScorer(context, this, queryWeight);
+      return new ConstantScorer(context, this, queryWeight, acceptDocs);
     }
 
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
 
-      ConstantScorer cs = new ConstantScorer(context, this, queryWeight);
+      ConstantScorer cs = new ConstantScorer(context, this, queryWeight, context.reader.getLiveDocs());
       boolean exists = cs.docIdSetIterator.advance(doc) == doc;
 
       ComplexExplanation result = new ComplexExplanation();
@@ -152,12 +152,14 @@ public Explanation explain(AtomicReaderContext context, int doc) throws IOExcept
   protected class ConstantScorer extends Scorer {
     final DocIdSetIterator docIdSetIterator;
     final float theScore;
+    final Bits acceptDocs;
     int doc = -1;
 
-    public ConstantScorer(AtomicReaderContext context, ConstantWeight w, float theScore) throws IOException {
+    public ConstantScorer(AtomicReaderContext context, ConstantWeight w, float theScore, Bits acceptDocs) throws IOException {
       super(w);
       this.theScore = theScore;
-      DocIdSet docIdSet = filter instanceof SolrFilter ? ((SolrFilter)filter).getDocIdSet(w.context, context) : filter.getDocIdSet(context);
+      this.acceptDocs = acceptDocs;
+      DocIdSet docIdSet = filter instanceof SolrFilter ? ((SolrFilter)filter).getDocIdSet(w.context, context, acceptDocs) : filter.getDocIdSet(context, acceptDocs);
       if (docIdSet == null) {
         docIdSetIterator = DocIdSet.EMPTY_DOCIDSET.iterator();
       } else {
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrFilter.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrFilter.java
index 3b57d612..33069a31 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrFilter.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrFilter.java
@@ -20,6 +20,7 @@
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
@@ -39,10 +40,10 @@
    * The context object will be passed to getDocIdSet() where this info can be retrieved. */
   public abstract void createWeight(Map context, IndexSearcher searcher) throws IOException;
   
-  public abstract DocIdSet getDocIdSet(Map context, AtomicReaderContext readerContext) throws IOException;
+  public abstract DocIdSet getDocIdSet(Map context, AtomicReaderContext readerContext, Bits acceptDocs) throws IOException;
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
-    return getDocIdSet(null, context);
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    return getDocIdSet(null, context, acceptDocs);
   }
 }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
index 74ddfcf6..9e929728 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
@@ -184,7 +184,7 @@ public SolrIndexSearcher(SolrCore core, IndexSchema schema, String name, IndexRe
 
   @Override
   public String toString() {
-    return name;
+    return name + "{" + reader + "}";
   }
 
   public SolrCore getCore() {
@@ -631,9 +631,10 @@ public DocSet getDocSet(List<Query> queries) throws IOException {
     for (int i=0; i<leaves.length; i++) {
       final AtomicReaderContext leaf = leaves[i];
       final IndexReader reader = leaf.reader;
+      final Bits liveDocs = reader.getLiveDocs();   // TODO: the filter may already only have liveDocs...
       DocIdSet idSet = null;
       if (pf.filter != null) {
-        idSet = pf.filter.getDocIdSet(leaf);
+        idSet = pf.filter.getDocIdSet(leaf, liveDocs);
         if (idSet == null) continue;
       }
       DocIdSetIterator idIter = null;
@@ -643,7 +644,6 @@ public DocSet getDocSet(List<Query> queries) throws IOException {
       }
 
       collector.setNextReader(leaf);
-      Bits liveDocs = reader.getLiveDocs();
       int max = reader.maxDoc();
 
       if (idIter == null) {
@@ -2056,8 +2056,8 @@ public FilterImpl(DocSet filter, List<Weight> weights) {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
-    DocIdSet sub = topFilter == null ? null : topFilter.getDocIdSet(context);
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    DocIdSet sub = topFilter == null ? null : topFilter.getDocIdSet(context, acceptDocs);
     if (weights.size() == 0) return sub;
     return new FilterSet(sub, context);
   }
@@ -2089,6 +2089,11 @@ public DocIdSetIterator iterator() throws IOException {
       if (iterators.size()==2) return new DualFilterIterator(iterators.get(0), iterators.get(1));
       return new FilterIterator(iterators.toArray(new DocIdSetIterator[iterators.size()]));
     }
+
+    @Override
+    public Bits bits() throws IOException {
+      return null;  // don't use random access
+    }
   }
 
   private static class FilterIterator extends DocIdSetIterator {
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java
index 0d306d88..7a6839ee 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java
@@ -17,7 +17,9 @@
 
 package org.apache.solr.search;
 
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.OpenBitSet;
+import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
@@ -655,7 +657,7 @@ public Filter getTopFilter() {
       int lastEndIdx = 0;
 
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
+      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
         IndexReader reader = context.reader;
 
         final int base = context.docBase;
@@ -688,7 +690,7 @@ public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
         lastEndIdx = endIdx;
 
 
-        return new DocIdSet() {
+        return BitsFilteredDocIdSet.wrap(new DocIdSet() {
           @Override
           public DocIdSetIterator iterator() throws IOException {
             return new DocIdSetIterator() {
@@ -751,7 +753,13 @@ public boolean isCacheable() {
             return true;
           }
 
-        };
+          @Override
+          public Bits bits() throws IOException {
+            // random access is expensive for this set
+            return null;
+          }
+
+        }, acceptDocs);
       }
     };
   }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java
index 5faae7cd..dd6d6289 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java
@@ -21,7 +21,9 @@
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.util.Bits;
 import org.apache.solr.search.SolrFilter;
 
 import java.io.IOException;
@@ -72,13 +74,17 @@ public boolean isIncludeUpper() {
 
 
   @Override
-  public DocIdSet getDocIdSet(final Map context, final AtomicReaderContext readerContext) throws IOException {
-     return new DocIdSet() {
+  public DocIdSet getDocIdSet(final Map context, final AtomicReaderContext readerContext, Bits acceptDocs) throws IOException {
+     return BitsFilteredDocIdSet.wrap(new DocIdSet() {
        @Override
       public DocIdSetIterator iterator() throws IOException {
          return valueSource.getValues(context, readerContext).getRangeScorer(readerContext.reader, lowerVal, upperVal, includeLower, includeUpper);
        }
-     };
+       @Override
+       public Bits bits() throws IOException {
+         return null;  // don't use random access
+       }
+     }, acceptDocs);
   }
 
   @Override
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
index 1bb23259..1bba1eaf 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
@@ -29,6 +29,7 @@
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 
+import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -157,7 +158,11 @@ public int addDoc(AddUpdateCommand cmd) throws IOException {
           updateTerm = cmd.updateTerm;
         }
 
-        writer.updateDocument(updateTerm, cmd.getLuceneDocument());
+        Document luceneDocument = cmd.getLuceneDocument();
+        // SolrCore.verbose("updateDocument",updateTerm,luceneDocument,writer);
+        writer.updateDocument(updateTerm, luceneDocument);
+        // SolrCore.verbose("updateDocument",updateTerm,"DONE");
+
         if(del) { // ensure id remains unique
           BooleanQuery bq = new BooleanQuery();
           bq.add(new BooleanClause(new TermQuery(updateTerm), Occur.MUST_NOT));
@@ -195,7 +200,12 @@ public void delete(DeleteUpdateCommand cmd) throws IOException {
     deleteByIdCommands.incrementAndGet();
     deleteByIdCommandsCumulative.incrementAndGet();
 
-    solrCoreState.getIndexWriter(core).deleteDocuments(new Term(idField.getName(), cmd.getIndexedId()));
+    IndexWriter writer = solrCoreState.getIndexWriter(core);
+    Term deleteTerm = new Term(idField.getName(), cmd.getIndexedId());
+
+    // SolrCore.verbose("deleteDocuments",deleteTerm,writer);
+    writer.deleteDocuments(deleteTerm);
+    // SolrCore.verbose("deleteDocuments",deleteTerm,"DONE");
 
     ulog.delete(cmd);
  
@@ -312,7 +322,9 @@ public void commit(CommitUpdateCommand cmd) throws IOException {
           ulog.preCommit(cmd);
         }
 
+        // SolrCore.verbose("writer.commit() start writer=",writer);
         writer.commit();
+        // SolrCore.verbose("writer.commit() end");
         numDocsPending.set(0);
         callPostCommitCallbacks();
       } else {
@@ -385,8 +397,10 @@ public SolrIndexSearcher reopenSearcher(SolrIndexSearcher previousSearcher) thro
     IndexReader currentReader = previousSearcher.getIndexReader();
     IndexReader newReader;
 
-    newReader = IndexReader.openIfChanged(currentReader, solrCoreState.getIndexWriter(core), true);
-  
+    IndexWriter writer = solrCoreState.getIndexWriter(core);
+    // SolrCore.verbose("start reopen from",previousSearcher,"writer=",writer);
+    newReader = IndexReader.openIfChanged(currentReader, writer, true);
+    // SolrCore.verbose("reopen result", newReader);
     
     if (newReader == null) {
       currentReader.incRef();
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/update/FSUpdateLog.java b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/update/FSUpdateLog.java
index 552a9f44..a52fb08c 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/update/FSUpdateLog.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/java/org/apache/solr/update/FSUpdateLog.java
@@ -188,7 +188,7 @@ public void add(AddUpdateCommand cmd) {
       long pos = tlog.write(cmd);
       LogPtr ptr = new LogPtr(pos, cmd.getVersion());
       map.put(cmd.getIndexedId(), ptr);
-      // System.out.println("TLOG: added id " + cmd.getPrintableId() + " to " + tlog + " " + ptr + " map=" + System.identityHashCode(map));
+      // SolrCore.verbose("TLOG: added id " + cmd.getPrintableId() + " to " + tlog + " " + ptr + " map=" + System.identityHashCode(map));
     }
   }
 
@@ -201,7 +201,7 @@ public void delete(DeleteUpdateCommand cmd) {
       long pos = tlog.writeDelete(cmd);
       LogPtr ptr = new LogPtr(pos, cmd.version);
       map.put(br, ptr);
-      // System.out.println("TLOG: added delete for id " + cmd.id + " to " + tlog + " " + ptr + " map=" + System.identityHashCode(map));
+      // SolrCore.verbose("TLOG: added delete for id " + cmd.id + " to " + tlog + " " + ptr + " map=" + System.identityHashCode(map));
     }
   }
 
@@ -215,7 +215,7 @@ public void deleteByQuery(DeleteUpdateCommand cmd) {
       // optimistic concurrency? Maybe we shouldn't support deleteByQuery w/ optimistic concurrency
       long pos = tlog.writeDeleteByQuery(cmd);
       LogPtr ptr = new LogPtr(pos, cmd.getVersion());
-      // System.out.println("TLOG: added deleteByQuery " + cmd.query + " to " + tlog + " " + ptr + " map=" + System.identityHashCode(map));
+      // SolrCore.verbose("TLOG: added deleteByQuery " + cmd.query + " to " + tlog + " " + ptr + " map=" + System.identityHashCode(map));
     }
   }
 
@@ -275,7 +275,7 @@ public void preSoftCommit(CommitUpdateCommand cmd) {
       // But we do know that any updates already added will definitely
       // show up in the latest reader after the commit succeeds.
       map = new HashMap<BytesRef, LogPtr>();
-      // System.out.println("TLOG: preSoftCommit: prevMap="+ System.identityHashCode(prevMap) + " new map=" + System.identityHashCode(map));
+      // SolrCore.verbose("TLOG: preSoftCommit: prevMap="+ System.identityHashCode(prevMap) + " new map=" + System.identityHashCode(map));
     }
   }
 
@@ -288,7 +288,7 @@ public void postSoftCommit(CommitUpdateCommand cmd) {
       // If this DUH2 synchronization were to be removed, preSoftCommit should
       // record what old maps were created and only remove those.
       clearOldMaps();
-      // System.out.println("TLOG: postSoftCommit: disposing of prevMap="+ System.identityHashCode(prevMap));
+      // SolrCore.verbose("TLOG: postSoftCommit: disposing of prevMap="+ System.identityHashCode(prevMap));
     }
   }
 
@@ -300,18 +300,18 @@ public Object lookup(BytesRef indexedId) {
     synchronized (this) {
       entry = map.get(indexedId);
       lookupLog = tlog;  // something found in "map" will always be in "tlog"
-      // System.out.println("TLOG: lookup: for id " + indexedId.utf8ToString() + " in map " +  System.identityHashCode(map) + " got " + entry + " lookupLog=" + lookupLog);
+      // SolrCore.verbose("TLOG: lookup: for id " + indexedId.utf8ToString() + " in map " +  System.identityHashCode(map) + " got " + entry + " lookupLog=" + lookupLog);
       if (entry == null && prevMap != null) {
         entry = prevMap.get(indexedId);
         // something found in prevMap will always be found in preMapLog (which could be tlog or prevTlog)
         lookupLog = prevMapLog;
-        // System.out.println("TLOG: lookup: for id " + indexedId.utf8ToString() + " in prevMap " +  System.identityHashCode(prevMap) + " got " + entry + " lookupLog="+lookupLog);
+        // SolrCore.verbose("TLOG: lookup: for id " + indexedId.utf8ToString() + " in prevMap " +  System.identityHashCode(prevMap) + " got " + entry + " lookupLog="+lookupLog);
       }
       if (entry == null && prevMap2 != null) {
         entry = prevMap2.get(indexedId);
         // something found in prevMap2 will always be found in preMapLog2 (which could be tlog or prevTlog)
         lookupLog = prevMapLog2;
-        // System.out.println("TLOG: lookup: for id " + indexedId.utf8ToString() + " in prevMap2 " +  System.identityHashCode(prevMap) + " got " + entry + " lookupLog="+lookupLog);
+        // SolrCore.verbose("TLOG: lookup: for id " + indexedId.utf8ToString() + " in prevMap2 " +  System.identityHashCode(prevMap) + " got " + entry + " lookupLog="+lookupLog);
       }
 
       if (entry == null) {
@@ -485,6 +485,7 @@ public long writeData(Object o) {
       this.tlogFile = tlogFile;
       raf = new RandomAccessFile(this.tlogFile, "rw");
       start = raf.length();
+      // System.out.println("###start= "+start);
       channel = raf.getChannel();
       os = Channels.newOutputStream(channel);
       fos = FastOutputStream.wrap(os);
@@ -535,12 +536,22 @@ public long write(AddUpdateCommand cmd) {
           pos = start + fos.size();
         }
 
+        /***
+        System.out.println("###writing at " + pos + " fos.size()=" + fos.size() + " raf.length()=" + raf.length());
+         if (pos != fos.size()) {
+          throw new RuntimeException("ERROR" + "###writing at " + pos + " fos.size()=" + fos.size() + " raf.length()=" + raf.length());
+        }
+         ***/
+
         codec.init(fos);
         codec.writeTag(JavaBinCodec.ARR, 3);
         codec.writeInt(UpdateLog.ADD);  // should just take one byte
         codec.writeLong(cmd.getVersion());
         codec.writeSolrInputDocument(cmd.getSolrInputDocument());
         // fos.flushBuffer();  // flush later
+
+
+
         return pos;
       } catch (IOException e) {
         throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);
@@ -600,6 +611,12 @@ public Object lookup(long pos) {
       synchronized (fos) {
         // TODO: optimize this by keeping track of what we have flushed up to
         fos.flushBuffer();
+        /***
+         System.out.println("###flushBuffer to " + fos.size() + " raf.length()=" + raf.length() + " pos="+pos);
+        if (fos.size() != raf.length() || pos >= fos.size() ) {
+          throw new RuntimeException("ERROR" + "###flushBuffer to " + fos.size() + " raf.length()=" + raf.length() + " pos="+pos);
+        }
+        ***/
       }
 
       ChannelFastInputStream fis = new ChannelFastInputStream(channel, pos);
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/OutputWriterTest.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/OutputWriterTest.java
index 03cfe298..59e3b7af 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/OutputWriterTest.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/OutputWriterTest.java
@@ -19,12 +19,16 @@
 
 import java.io.IOException;
 import java.io.Writer;
+import java.util.List;
 
 import org.apache.solr.common.params.CommonParams;
 import org.apache.solr.common.util.NamedList;
+import org.apache.solr.core.PluginInfo;
+import org.apache.solr.core.SolrCore;
 import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.response.QueryResponseWriter;
 import org.apache.solr.response.SolrQueryResponse;
+import org.apache.solr.response.XMLResponseWriter;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
@@ -89,6 +93,14 @@ public void testTrivialXsltWriterInclude() throws Exception {
         assertTrue(out.contains("DUMMY"));
     }
     
+    public void testLazy() {
+      QueryResponseWriter qrw = h.getCore().getQueryResponseWriter("useless");
+      assertTrue("Should be a lazy class", qrw instanceof SolrCore.LazyQueryResponseWriterWrapper);
+
+      qrw = h.getCore().getQueryResponseWriter("xml");
+      assertTrue("Should not be a lazy class", qrw instanceof XMLResponseWriter);
+
+    }
     
     ////////////////////////////////////////////////////////////////////////////
     /** An output writer that doesn't do anything useful. */
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/analysis/TestPhoneticFilterFactory.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/analysis/TestPhoneticFilterFactory.java
index 02e27e81..52ff99bf 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/analysis/TestPhoneticFilterFactory.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/analysis/TestPhoneticFilterFactory.java
@@ -32,10 +32,34 @@
  */
 public class TestPhoneticFilterFactory extends BaseTokenTestCase {
   
+  private static final int REPEATS = 100000;
+
+  /**
+   * Case: default
+   */
   public void testFactory()
   {
     Map<String,String> args = new HashMap<String, String>();
     
+    PhoneticFilterFactory ff = new PhoneticFilterFactory();
+    
+    args.put( PhoneticFilterFactory.ENCODER, "Metaphone" );
+    ff.init( args );
+    assertTrue( ff.encoder instanceof Metaphone );
+    assertTrue( ff.inject ); // default
+
+    args.put( PhoneticFilterFactory.INJECT, "false" );
+    ff.init( args );
+    assertFalse( ff.inject );
+  }
+  
+  /**
+   * Case: Failures and Exceptions
+   */
+  public void testFactoryCaseFailure()
+  {
+    Map<String,String> args = new HashMap<String, String>();
+    
     PhoneticFilterFactory ff = new PhoneticFilterFactory();
     try {
       ff.init( args );
@@ -48,15 +72,27 @@ public void testFactory()
       fail( "unknown encoder parameter" );
     }
     catch( Exception ex ) {}
+    args.put( PhoneticFilterFactory.ENCODER, "org.apache.commons.codec.language.NonExistence" );
+    try {
+      ff.init( args );
+      fail( "unknown encoder parameter" );
+    }
+    catch( Exception ex ) {}
+  }
     
-    args.put( PhoneticFilterFactory.ENCODER, "Metaphone" );
+  /**
+   * Case: Reflection
+   */
+  public void testFactoryCaseReflection()
+  {
+    Map<String,String> args = new HashMap<String, String>();
+    
+    PhoneticFilterFactory ff = new PhoneticFilterFactory();
+
+    args.put( PhoneticFilterFactory.ENCODER, "org.apache.commons.codec.language.Metaphone" );
     ff.init( args );
     assertTrue( ff.encoder instanceof Metaphone );
     assertTrue( ff.inject ); // default
-
-    args.put( PhoneticFilterFactory.INJECT, "false" );
-    ff.init( args );
-    assertFalse( ff.inject );
   }
   
   public void testAlgorithms() throws Exception {
@@ -85,6 +121,12 @@ public void testAlgorithms() throws Exception {
           "TTA1111111", "Datha", "KLN1111111", "Carlene" });
     assertAlgorithm("Caverphone", "false", "Darda Karleen Datha Carlene",
         new String[] { "TTA1111111", "KLN1111111", "TTA1111111", "KLN1111111" });
+    
+    assertAlgorithm("ColognePhonetic", "true", "Meier Schmitt Meir Schmidt",
+        new String[] { "67", "Meier", "862", "Schmitt", 
+          "67", "Meir", "862", "Schmidt" });
+    assertAlgorithm("ColognePhonetic", "false", "Meier Schmitt Meir Schmidt",
+        new String[] { "67", "862", "67", "862" });
   }
   
   static void assertAlgorithm(String algName, String inject, String input,
@@ -98,4 +140,25 @@ static void assertAlgorithm(String algName, String inject, String input,
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, expected);
   }
+  
+  public void testSpeed() throws Exception {
+	  checkSpeedEncoding("Metaphone", "easgasg", "ESKS");
+	  checkSpeedEncoding("DoubleMetaphone", "easgasg", "ASKS");
+	  checkSpeedEncoding("Soundex", "easgasg", "E220");
+	  checkSpeedEncoding("RefinedSoundex", "easgasg", "E034034");
+	  checkSpeedEncoding("Caverphone", "Carlene", "KLN1111111");
+	  checkSpeedEncoding("ColognePhonetic", "Schmitt", "862");
+  }
+  
+  private void checkSpeedEncoding(String encoder, String toBeEncoded, String estimated) throws Exception {
+	  long start = System.currentTimeMillis();
+	  for ( int i=0; i<REPEATS; i++) {
+		    assertAlgorithm(encoder, "false", toBeEncoded,
+		            new String[] { estimated });
+	  }
+	  long duration = System.currentTimeMillis()-start;
+	  if (VERBOSE)
+	    System.out.println(encoder + " encodings per msec: "+(REPEATS/duration));
+  }
+  
 }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/core/TestJmxIntegration.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/core/TestJmxIntegration.java
index 6032d91a..d0c8dca7 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/core/TestJmxIntegration.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/core/TestJmxIntegration.java
@@ -20,6 +20,7 @@
 import org.apache.solr.util.AbstractSolrTestCase;
 import org.junit.After;
 import org.junit.Before;
+import org.junit.Ignore;
 import org.junit.Test;
 
 import javax.management.*;
@@ -108,7 +109,7 @@ public void testJmxUpdate() throws Exception {
             numDocs > oldNumDocs);
   }
 
-  @Test
+  @Test @Ignore("timing problem? https://issues.apache.org/jira/browse/SOLR-2715")
   public void testJmxOnCoreReload() throws Exception {
     List<MBeanServer> servers = MBeanServerFactory.findMBeanServer(null);
     MBeanServer mbeanServer = servers.get(0);
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/handler/admin/ShowFileRequestHandlerTest.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/handler/admin/ShowFileRequestHandlerTest.java
index df9772ec..d4731869 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/handler/admin/ShowFileRequestHandlerTest.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/handler/admin/ShowFileRequestHandlerTest.java
@@ -34,7 +34,7 @@
 
 /**
  * Extend SolrJettyTestBase because the SOLR-2535 bug only manifested itself when
- * the {@link SolrDispatchFilter} is used, which isn't for embedded Solr use.
+ * the {@link org.apache.solr.servlet.SolrDispatchFilter} is used, which isn't for embedded Solr use.
  */
 public class ShowFileRequestHandlerTest extends SolrJettyTestBase {
 
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/request/TestRemoteStreaming.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/request/TestRemoteStreaming.java
index e69de29b..f6633202 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/request/TestRemoteStreaming.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/request/TestRemoteStreaming.java
@@ -0,0 +1,116 @@
+package org.apache.solr.request;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.commons.io.IOUtils;
+import org.apache.solr.SolrJettyTestBase;
+import org.apache.solr.client.solrj.SolrQuery;
+import org.apache.solr.client.solrj.SolrServer;
+import org.apache.solr.client.solrj.SolrServerException;
+import org.apache.solr.client.solrj.impl.CommonsHttpSolrServer;
+import org.apache.solr.client.solrj.response.QueryResponse;
+import org.apache.solr.common.SolrInputDocument;
+import org.apache.solr.util.ExternalPaths;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.StringWriter;
+import java.io.UnsupportedEncodingException;
+import java.net.URL;
+import java.net.URLEncoder;
+
+/**
+ * See SOLR-2854.
+ */
+public class TestRemoteStreaming extends SolrJettyTestBase {
+
+  @BeforeClass
+  public static void beforeTest() throws Exception {
+    createJetty(ExternalPaths.EXAMPLE_HOME, null, null);
+  }
+
+  @Before
+  public void doBefore() throws IOException, SolrServerException {
+    //add document and commit, and ensure it's there
+    SolrServer server1 = getSolrServer();
+    SolrInputDocument doc = new SolrInputDocument();
+    doc.addField( "id", "xxxx" );
+    server1.add(doc);
+    server1.commit();
+    assertTrue(searchFindsIt());
+  }
+
+  @Test
+  public void testMakeDeleteAllUrl() throws Exception {
+    getUrlForString(makeDeleteAllUrl());
+    assertFalse(searchFindsIt());
+  }
+
+  @Test
+  public void testStreamUrl() throws Exception {
+    CommonsHttpSolrServer solrServer = (CommonsHttpSolrServer) getSolrServer();
+    String streamUrl = solrServer.getBaseURL()+"/select?q=*:*&fl=id&wt=csv";
+
+    String getUrl = solrServer.getBaseURL()+"/debug/dump?wt=xml&stream.url="+URLEncoder.encode(streamUrl,"UTF-8");
+    String content = getUrlForString(getUrl);
+    assertTrue(content.contains("xxxx"));
+    //System.out.println(content);
+  }
+
+  private String getUrlForString(String getUrl) throws IOException {
+    Object obj = new URL(getUrl).getContent();
+    if (obj instanceof InputStream) {
+      InputStream inputStream = (InputStream) obj;
+      try {
+        StringWriter strWriter = new StringWriter();
+        IOUtils.copy(inputStream,strWriter);
+        return strWriter.toString();
+      } finally {
+        IOUtils.closeQuietly(inputStream);
+      }
+    }
+    return null;
+  }
+
+  /** Do a select query with the stream.url. Solr should NOT access that URL, and so the data should be there. */
+  @Test
+  public void testNoUrlAccess() throws Exception {
+    SolrQuery query = new SolrQuery();
+    query.setQuery( "*:*" );//for anything
+    query.add("stream.url",makeDeleteAllUrl());
+    getSolrServer().query(query);
+    assertTrue(searchFindsIt());//still there
+  }
+
+  /** Compose a url that if you get it, it will delete all the data. */
+  private String makeDeleteAllUrl() throws UnsupportedEncodingException {
+    CommonsHttpSolrServer solrServer = (CommonsHttpSolrServer) getSolrServer();
+    String deleteQuery = "<delete><query>*:*</query></delete>";
+    return solrServer.getBaseURL()+"/update?commit=true&stream.body="+ URLEncoder.encode(deleteQuery, "UTF-8");
+  }
+
+  private boolean searchFindsIt() throws SolrServerException {
+    SolrQuery query = new SolrQuery();
+    query.setQuery( "id:xxxx" );
+    QueryResponse rsp = getSolrServer().query(query);
+    return rsp.getResults().getNumFound() != 0;
+  }
+}
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestDocSet.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestDocSet.java
index e643b8b1..d392ac11 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestDocSet.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestDocSet.java
@@ -427,8 +427,8 @@ public void doFilterTest(IndexReader reader) throws IOException {
 
     // first test in-sequence sub readers
     for (AtomicReaderContext readerContext : ReaderUtil.leaves(topLevelContext)) {
-      da = fa.getDocIdSet(readerContext);
-      db = fb.getDocIdSet(readerContext);
+      da = fa.getDocIdSet(readerContext, null);
+      db = fb.getDocIdSet(readerContext, null);
       doTestIteratorEqual(da, db);
     }  
 
@@ -437,8 +437,8 @@ public void doFilterTest(IndexReader reader) throws IOException {
     // now test out-of-sequence sub readers
     for (int i=0; i<nReaders; i++) {
       AtomicReaderContext readerContext = leaves[rand.nextInt(nReaders)];
-      da = fa.getDocIdSet(readerContext);
-      db = fb.getDocIdSet(readerContext);
+      da = fa.getDocIdSet(readerContext, null);
+      db = fb.getDocIdSet(readerContext, null);
       doTestIteratorEqual(da, db);
     }
   }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java
index 76faa643..e3b2a14f 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java
@@ -21,15 +21,23 @@
 package org.apache.solr.search;
 
 
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.index.*;
+import org.apache.lucene.search.*;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
 import org.apache.noggit.ObjectBuilder;
 import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.common.SolrException;
-import org.apache.solr.common.SolrInputDocument;
 import org.apache.solr.request.SolrQueryRequest;
 import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.Ignore;
 
+import java.io.IOException;
 import java.util.*;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -37,6 +45,8 @@
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 
+import static  org.apache.solr.core.SolrCore.verbose;
+
 public class TestRealTimeGet extends SolrTestCaseJ4 {
 
   @BeforeClass
@@ -53,10 +63,10 @@ public void testGetRealtime() throws Exception {
     assertJQ(req("q","id:1")
         ,"/response/numFound==0"
     );
-    assertJQ(req("qt","/get", "id","1", "fl","id")
+    assertJQ(req("qt","/get","id","1")
         ,"=={'doc':{'id':'1'}}"
     );
-    assertJQ(req("qt","/get","ids","1", "fl","id")
+    assertJQ(req("qt","/get","ids","1")
         ,"=={" +
         "  'response':{'numFound':1,'start':0,'docs':[" +
         "      {" +
@@ -69,10 +79,10 @@ public void testGetRealtime() throws Exception {
     assertJQ(req("q","id:1")
         ,"/response/numFound==1"
     );
-    assertJQ(req("qt","/get","id","1", "fl","id")
+    assertJQ(req("qt","/get","id","1")
         ,"=={'doc':{'id':'1'}}"
     );
-    assertJQ(req("qt","/get","ids","1", "fl","id")
+    assertJQ(req("qt","/get","ids","1")
         ,"=={" +
         "  'response':{'numFound':1,'start':0,'docs':[" +
         "      {" +
@@ -129,34 +139,8 @@ public void testGetRealtime() throws Exception {
   ***/
 
 
-  public static void verbose(Object... args) {
-    if (!VERBOSE) return;
-    StringBuilder sb = new StringBuilder("TEST:");
-    sb.append(Thread.currentThread().getName());
-    sb.append(':');
-    for (Object o : args) {
-      sb.append(' ');
-      sb.append(o.toString());
-    }
-    System.out.println(sb.toString());
-  }
-
-  static class DocInfo {
-    long version;
-    long val;
-
-    public DocInfo(long version, long val) {
-      this.version = version;
-      this.val = val;
-    }
-
-    public String toString() {
-      return "{version="+version+",val="+val+"\"";
-    }
-  }
-
-  final ConcurrentHashMap<Integer,DocInfo> model = new ConcurrentHashMap<Integer,DocInfo>();
-  Map<Integer,DocInfo> committedModel = new HashMap<Integer,DocInfo>();
+  final ConcurrentHashMap<Integer,Long> model = new ConcurrentHashMap<Integer,Long>();
+  Map<Integer,Long> committedModel = new HashMap<Integer,Long>();
   long snapshotCount;
   long committedModelClock;
   volatile int lastId;
@@ -171,20 +155,21 @@ private void initModel(int ndocs) {
     syncArr = new Object[ndocs];
 
     for (int i=0; i<ndocs; i++) {
-      model.put(i, new DocInfo(0, -1L));
+      model.put(i, -1L);
       syncArr[i] = new Object();
     }
     committedModel.putAll(model);
   }
 
-
   @Test
   public void testStressGetRealtime() throws Exception {
     clearIndex();
     assertU(commit());
 
+    // req().getCore().getUpdateHandler().getIndexWriterProvider().getIndexWriter(req().getCore()).setInfoStream(System.out);
+
     final int commitPercent = 5 + random.nextInt(20);
-    final int softCommitPercent = 30+random.nextInt(60); // what percent of the commits are soft
+    final int softCommitPercent = 30+random.nextInt(75); // what percent of the commits are soft
     final int deletePercent = 4+random.nextInt(25);
     final int deleteByQueryPercent = 0;  // real-time get isn't currently supported with delete-by-query
     final int ndocs = 5 + (random.nextBoolean() ? random.nextInt(25) : random.nextInt(200));
@@ -194,10 +179,22 @@ public void testStressGetRealtime() throws Exception {
 
         // query variables
     final int percentRealtimeQuery = 60;
+    // final AtomicLong operations = new AtomicLong(50000);  // number of query operations to perform in total
     final AtomicLong operations = new AtomicLong(50000);  // number of query operations to perform in total
     int nReadThreads = 5 + random.nextInt(25);
 
 
+    verbose("commitPercent=", commitPercent);
+    verbose("softCommitPercent=",softCommitPercent);
+    verbose("deletePercent=",deletePercent);
+    verbose("deleteByQueryPercent=", deleteByQueryPercent);
+    verbose("ndocs=", ndocs);
+    verbose("nWriteThreads=", nWriteThreads);
+    verbose("nReadThreads=", nReadThreads);
+    verbose("percentRealtimeQuery=", percentRealtimeQuery);
+    verbose("maxConcurrentCommits=", maxConcurrentCommits);
+    verbose("operations=", operations);
+
 
     initModel(ndocs);
 
@@ -217,12 +214,13 @@ public void run() {
 
             if (oper < commitPercent) {
               if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {
-                Map<Integer,DocInfo> newCommittedModel;
+                Map<Integer,Long> newCommittedModel;
                 long version;
 
                 synchronized(TestRealTimeGet.this) {
-                  newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot
+                  newCommittedModel = new HashMap<Integer,Long>(model);  // take a snapshot
                   version = snapshotCount++;
+                  verbose("took snapshot version=",version);
                 }
 
                 if (rand.nextInt(100) < softCommitPercent) {
@@ -230,9 +228,9 @@ public void run() {
                   assertU(h.commit("softCommit","true"));
                   verbose("softCommit end");
                 } else {
-                  verbose("commit start");
+                  verbose("hardCommit start");
                   assertU(commit());
-                  verbose("commit end");
+                  verbose("hardCommit end");
                 }
 
                 synchronized(TestRealTimeGet.this) {
@@ -263,11 +261,8 @@ public void run() {
 
             // We can't concurrently update the same document and retain our invariants of increasing values
             // since we can't guarantee what order the updates will be executed.
-            // Even with versions, we can't remove the sync because increasing versions does not mean increasing vals.
            synchronized (sync) {
-              DocInfo info = model.get(id);
-
-              long val = info.val;
+              Long val = model.get(id);
               long nextVal = Math.abs(val)+1;
 
               if (oper < commitPercent + deletePercent) {
@@ -275,11 +270,8 @@ public void run() {
                   verbose("deleting id",id,"val=",nextVal);
                 }
 
-                // assertU("<delete><id>" + id + "</id></delete>");
-                Long version = deleteAndGetVersion(Integer.toString(id));
-
-                model.put(id, new DocInfo(version, -nextVal));
-
+                assertU("<delete><id>" + id + "</id></delete>");
+                model.put(id, -nextVal);
                 if (VERBOSE) {
                   verbose("deleting id", id, "val=",nextVal,"DONE");
                 }
@@ -289,7 +281,7 @@ public void run() {
                 }
 
                 assertU("<delete><query>id:" + id + "</query></delete>");
-                model.put(id, new DocInfo(-1L, -nextVal));
+                model.put(id, -nextVal);
                 if (VERBOSE) {
                   verbose("deleteByQuery id",id, "val=",nextVal,"DONE");
                 }
@@ -298,16 +290,15 @@ public void run() {
                   verbose("adding id", id, "val=", nextVal);
                 }
 
-                // assertU(adoc("id",Integer.toString(id), field, Long.toString(nextVal)));
-                Long version = addAndGetVersion(sdoc("id", Integer.toString(id), field, Long.toString(nextVal)));
-                model.put(id, new DocInfo(version, nextVal));
+                assertU(adoc("id",Integer.toString(id), field, Long.toString(nextVal)));
+                model.put(id, nextVal);
 
                 if (VERBOSE) {
                   verbose("adding id", id, "val=", nextVal,"DONE");
                 }
 
               }
-            }   // end sync
+            }
 
             if (!before) {
               lastId = id;
@@ -340,13 +331,13 @@ public void run() {
               // so when querying, we should first check the model, and then the index
 
               boolean realTime = rand.nextInt(100) < percentRealtimeQuery;
-              DocInfo info;
+              long val;
 
               if (realTime) {
-                info = model.get(id);
+                val = model.get(id);
               } else {
                 synchronized(TestRealTimeGet.this) {
-                  info = committedModel.get(id);
+                  val = committedModel.get(id);
                 }
               }
 
@@ -368,11 +359,9 @@ public void run() {
               } else {
                 assertEquals(1, doclist.size());
                 long foundVal = (Long)(((Map)doclist.get(0)).get(field));
-                long foundVer = (Long)(((Map)doclist.get(0)).get("_version_"));
-                if (foundVal < Math.abs(info.val)
-                    || (foundVer == info.version && foundVal != info.val) ) {    // if the version matches, the val must
-                  verbose("ERROR, id=", id, "found=",response,"model",info);
-                  assertTrue(false);
+                if (foundVal < Math.abs(val)) {
+                  verbose("ERROR, id", id, "foundVal=",foundVal,"model val=",val,"realTime=",realTime);
+                  assertTrue(foundVal >= Math.abs(val));
                 }
               }
             }
@@ -401,13 +390,13 @@ public void run() {
 
 
 
-  @Test
-  public void testStressGetRealtimeVersions() throws Exception {
-    clearIndex();
-    assertU(commit());
 
+  // The purpose of this test is to roughly model how solr uses lucene
+  IndexReader reader;
+  @Test
+  public void testStressLuceneNRT() throws Exception {
     final int commitPercent = 5 + random.nextInt(20);
-    final int softCommitPercent = 30+random.nextInt(60); // what percent of the commits are soft
+    final int softCommitPercent = 30+random.nextInt(75); // what percent of the commits are soft
     final int deletePercent = 4+random.nextInt(25);
     final int deleteByQueryPercent = 0;  // real-time get isn't currently supported with delete-by-query
     final int ndocs = 5 + (random.nextBoolean() ? random.nextInt(25) : random.nextInt(200));
@@ -415,12 +404,22 @@ public void testStressGetRealtimeVersions() throws Exception {
 
     final int maxConcurrentCommits = nWriteThreads;   // number of committers at a time... it should be <= maxWarmingSearchers
 
-        // query variables
-    final int percentRealtimeQuery = 60;
-    final AtomicLong operations = new AtomicLong(50000);  // number of query operations to perform in total
+    final AtomicLong operations = new AtomicLong(10000);  // number of query operations to perform in total - crank up if
     int nReadThreads = 5 + random.nextInt(25);
-
-
+    final boolean tombstones = random.nextBoolean();
+    final boolean syncCommits = random.nextBoolean();
+
+    verbose("commitPercent=", commitPercent);
+    verbose("softCommitPercent=",softCommitPercent);
+    verbose("deletePercent=",deletePercent);
+    verbose("deleteByQueryPercent=", deleteByQueryPercent);
+    verbose("ndocs=", ndocs);
+    verbose("nWriteThreads=", nWriteThreads);
+    verbose("nReadThreads=", nReadThreads);
+    verbose("maxConcurrentCommits=", maxConcurrentCommits);
+    verbose("operations=", operations);
+    verbose("tombstones=", tombstones);
+    verbose("syncCommits=", syncCommits);
 
     initModel(ndocs);
 
@@ -428,6 +427,41 @@ public void testStressGetRealtimeVersions() throws Exception {
 
     List<Thread> threads = new ArrayList<Thread>();
 
+
+    final FieldType idFt = new FieldType();
+    idFt.setIndexed(true);
+    idFt.setStored(true);
+    idFt.setOmitNorms(true);
+    idFt.setTokenized(false);
+    idFt.setIndexOptions(FieldInfo.IndexOptions.DOCS_ONLY);
+
+    final FieldType ft2 = new FieldType();
+    ft2.setIndexed(false);
+    ft2.setStored(true);
+
+
+    // model how solr does locking - only allow one thread to do a hard commit at once, and only one thread to do a soft commit, but
+    // a hard commit in progress does not stop a soft commit.
+    final Lock hardCommitLock = syncCommits ? new ReentrantLock() : null;
+    final Lock reopenLock = syncCommits ? new ReentrantLock() : null;
+
+
+    // RAMDirectory dir = new RAMDirectory();
+    // final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, new WhitespaceAnalyzer(Version.LUCENE_40)));
+
+    Directory dir = newDirectory();
+
+    final RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
+    writer.setDoRandomOptimizeAssert(false);
+    writer.w.setInfoStream(VERBOSE ? System.out : null);
+    writer.w.setInfoStream(null);
+
+    // writer.commit();
+    // reader = IndexReader.open(dir);
+    // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged
+    // to only opening at the last commit point.
+    reader = IndexReader.open(writer.w, true);
+
     for (int i=0; i<nWriteThreads; i++) {
       Thread thread = new Thread("WRITER"+i) {
         Random rand = new Random(random.nextInt());
@@ -440,34 +474,81 @@ public void run() {
 
             if (oper < commitPercent) {
               if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {
-                Map<Integer,DocInfo> newCommittedModel;
+                  Map<Integer,Long> newCommittedModel;
                 long version;
+                  IndexReader oldReader;
+
+                  boolean softCommit = rand.nextInt(100) < softCommitPercent;
+
+                  if (!softCommit) {
+                    // only allow one hard commit to proceed at once
+                    if (hardCommitLock != null) hardCommitLock.lock();
+                    verbose("hardCommit start");
+
+                    writer.commit();
+                  }
+
+                  if (reopenLock != null) reopenLock.lock();
 
                 synchronized(TestRealTimeGet.this) {
-                  newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot
+                    newCommittedModel = new HashMap<Integer,Long>(model);  // take a snapshot
                   version = snapshotCount++;
+                    oldReader = reader;
+                    oldReader.incRef();  // increment the reference since we will use this for reopening
                 }
 
-                if (rand.nextInt(100) < softCommitPercent) {
-                  verbose("softCommit start");
-                  assertU(h.commit("softCommit","true"));
-                  verbose("softCommit end");
+                  if (!softCommit) {
+                    // must commit after taking a snapshot of the model
+                    // writer.commit();
+                  }
+
+                  verbose("reopen start using", oldReader);
+
+                  IndexReader newReader;
+                  if (softCommit) {
+                    newReader = IndexReader.openIfChanged(oldReader, writer.w, true);
                 } else {
-                  verbose("commit start");
-                  assertU(commit());
-                  verbose("commit end");
+                    // will only open to last commit
+                   newReader = IndexReader.openIfChanged(oldReader);
                 }
 
+
+                  if (newReader == null) {
+                    oldReader.incRef();
+                    newReader = oldReader;
+                  }
+                  oldReader.decRef();
+
+                  verbose("reopen result", newReader);
+
                 synchronized(TestRealTimeGet.this) {
-                  // install this model snapshot only if it's newer than the current one
+                    assert newReader.getRefCount() > 0;
+                    assert reader.getRefCount() > 0;
+
+                    // install the new reader if it's newest (and check the current version since another reader may have already been installed)
+                    if (newReader.getVersion() > reader.getVersion()) {
+                      reader.decRef();
+                      reader = newReader;
+
+                      // install this snapshot only if it's newer than the current one
                   if (version >= committedModelClock) {
-                    if (VERBOSE) {
-                      verbose("installing new committedModel version="+committedModelClock);
-                    }
                     committedModel = newCommittedModel;
                     committedModelClock = version;
                   }
+
+                    } else {
+                      // close if unused
+                      newReader.decRef();
+                    }
+
                 }
+
+                  if (reopenLock != null) reopenLock.unlock();
+
+                  if (!softCommit) {
+                    if (hardCommitLock != null) hardCommitLock.unlock();
+                  }
+
               }
               numCommitting.decrementAndGet();
               continue;
@@ -486,75 +567,68 @@ public void run() {
 
             // We can't concurrently update the same document and retain our invariants of increasing values
             // since we can't guarantee what order the updates will be executed.
-            // Even with versions, we can't remove the sync because increasing versions does not mean increasing vals.
-           // synchronized (sync) {
-              DocInfo info = model.get(id);
-
-              long val = info.val;
+              synchronized (sync) {
+                Long val = model.get(id);
               long nextVal = Math.abs(val)+1;
 
               if (oper < commitPercent + deletePercent) {
-                if (VERBOSE) {
-                  verbose("deleting id",id,"val=",nextVal);
+                  // add tombstone first
+                  if (tombstones) {
+                    Document d = new Document();
+                    d.add(new Field("id","-"+Integer.toString(id), idFt));
+                    d.add(new Field(field, Long.toString(nextVal), ft2));
+                    verbose("adding tombstone for id",id,"val=",nextVal);
+                    writer.updateDocument(new Term("id", "-"+Integer.toString(id)), d);
                 }
 
-                // assertU("<delete><id>" + id + "</id></delete>");
-                Long version = deleteAndGetVersion(Integer.toString(id));
-                assertTrue(version < 0);
-
-                // only update model if the version is newer
-                synchronized (model) {
-                  DocInfo currInfo = model.get(id);
-                  if (Math.abs(version) > Math.abs(currInfo.version)) {
-                    model.put(id, new DocInfo(version, -nextVal));
-                  }
-                }
+                  verbose("deleting id",id,"val=",nextVal);
+                  writer.deleteDocuments(new Term("id",Integer.toString(id)));
+                  model.put(id, -nextVal);
+                  verbose("deleting id",id,"val=",nextVal,"DONE");
 
-                if (VERBOSE) {
-                  verbose("deleting id", id, "val=",nextVal,"DONE");
-                }
               } else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {
-                if (VERBOSE) {
-                  verbose("deleteByQuery id ",id, "val=",nextVal);
+                  //assertU("<delete><query>id:" + id + "</query></delete>");
+
+                  // add tombstone first
+                  if (tombstones) {
+                    Document d = new Document();
+                    d.add(new Field("id","-"+Integer.toString(id), idFt));
+                    d.add(new Field(field, Long.toString(nextVal), ft2));
+                    verbose("adding tombstone for id",id,"val=",nextVal);
+                    writer.updateDocument(new Term("id", "-"+Integer.toString(id)), d);
                 }
 
-                assertU("<delete><query>id:" + id + "</query></delete>");
-                model.put(id, new DocInfo(-1L, -nextVal));
-                if (VERBOSE) {
-                  verbose("deleteByQuery id",id, "val=",nextVal,"DONE");
-                }
+                  verbose("deleteByQuery",id,"val=",nextVal);
+                  writer.deleteDocuments(new TermQuery(new Term("id", Integer.toString(id))));
+                  model.put(id, -nextVal);
+                  verbose("deleteByQuery",id,"val=",nextVal,"DONE");
               } else {
-                if (VERBOSE) {
-                  verbose("adding id", id, "val=", nextVal);
-                }
+                  // model.put(id, nextVal);   // uncomment this and this test should fail.
 
                 // assertU(adoc("id",Integer.toString(id), field, Long.toString(nextVal)));
-                Long version = addAndGetVersion(sdoc("id", Integer.toString(id), field, Long.toString(nextVal)));
-                assertTrue(version > 0);
-
-                // only update model if the version is newer
-                synchronized (model) {
-                  DocInfo currInfo = model.get(id);
-                  if (version > currInfo.version) {
-                    model.put(id, new DocInfo(version, nextVal));
-                  }
+                  Document d = new Document();
+                  d.add(new Field("id",Integer.toString(id), idFt));
+                  d.add(new Field(field, Long.toString(nextVal), ft2));
+                  verbose("adding id",id,"val=",nextVal);
+                  writer.updateDocument(new Term("id", Integer.toString(id)), d);
+                  if (tombstones) {
+                    // remove tombstone after new addition (this should be optional?)
+                    verbose("deleting tombstone for id",id);
+                    writer.deleteDocuments(new Term("id","-"+Integer.toString(id)));
+                    verbose("deleting tombstone for id",id,"DONE");
                 }
 
-                if (VERBOSE) {
-                  verbose("adding id", id, "val=", nextVal,"DONE");
+                  model.put(id, nextVal);
+                  verbose("adding id",id,"val=",nextVal,"DONE");
                 }
-
               }
-            // }   // end sync
 
             if (!before) {
               lastId = id;
             }
           }
-        } catch (Throwable e) {
-          operations.set(-1L);
-          SolrException.log(log, e);
-          fail(e.getMessage());
+          } catch (Exception  ex) {
+            throw new RuntimeException(ex);
         }
         }
       };
@@ -577,48 +651,57 @@ public void run() {
               // when indexing, we update the index, then the model
               // so when querying, we should first check the model, and then the index
 
-              boolean realTime = rand.nextInt(100) < percentRealtimeQuery;
-              DocInfo info;
+              long val;
 
-              if (realTime) {
-                info = model.get(id);
-              } else {
                 synchronized(TestRealTimeGet.this) {
-                  info = committedModel.get(id);
+                val = committedModel.get(id);
                 }
+
+
+              IndexReader r;
+              synchronized(TestRealTimeGet.this) {
+                r = reader;
+                r.incRef();
               }
 
-              if (VERBOSE) {
-                verbose("querying id", id);
+              int docid = getFirstMatch(r, new Term("id",Integer.toString(id)));
+
+              if (docid < 0 && tombstones) {
+                // if we couldn't find the doc, look for it's tombstone
+                docid = getFirstMatch(r, new Term("id","-"+Integer.toString(id)));
+                if (docid < 0) {
+                  if (val == -1L) {
+                    // expected... no doc was added yet
+                    r.decRef();
+                    continue;
+                  }
+                  verbose("ERROR: Couldn't find a doc  or tombstone for id", id, "using reader",r,"expected value",val);
+                  fail("No documents or tombstones found for id " + id + ", expected at least " + val);
               }
-              SolrQueryRequest sreq;
-              if (realTime) {
-                sreq = req("wt","json", "qt","/get", "ids",Integer.toString(id));
-              } else {
-                sreq = req("wt","json", "q","id:"+Integer.toString(id), "omitHeader","true");
               }
 
-              String response = h.query(sreq);
-              Map rsp = (Map)ObjectBuilder.fromJSON(response);
-              List doclist = (List)(((Map)rsp.get("response")).get("docs"));
-              if (doclist.size() == 0) {
-                // there's no info we can get back with a delete, so not much we can check without further synchronization
+              if (docid < 0 && !tombstones) {
+                // nothing to do - we can't tell anything from a deleted doc without tombstones
               } else {
-                assertEquals(1, doclist.size());
-                long foundVal = (Long)(((Map)doclist.get(0)).get(field));
-                long foundVer = (Long)(((Map)doclist.get(0)).get("_version_"));
-                if (foundVer < Math.abs(info.version)
-                    || (foundVer == info.version && foundVal != info.val) ) {    // if the version matches, the val must
-                  verbose("ERROR, id=", id, "found=",response,"model",info);
-                  assertTrue(false);
+                if (docid < 0) {
+                  verbose("ERROR: Couldn't find a doc for id", id, "using reader",r);
                 }
+                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone
+                Document doc = r.document(docid);
+                long foundVal = Long.parseLong(doc.get(field));
+                if (foundVal < Math.abs(val)) {
+                  verbose("ERROR: id",id,"model_val=",val," foundVal=",foundVal,"reader=",reader);
+              }
+                assertTrue(foundVal >= Math.abs(val));
               }
+
+              r.decRef();
             }
           }
           catch (Throwable e) {
             operations.set(-1L);
-            SolrException.log(log, e);
-            fail(e.getMessage());
+            SolrException.log(log,e);
+            fail(e.toString());
           }
         }
       };
@@ -635,20 +718,26 @@ public void run() {
       thread.join();
     }
 
+    writer.close();
+    reader.close();
+    dir.close();
   }
 
 
-  private Long addAndGetVersion(SolrInputDocument sdoc) throws Exception {
-    String response = updateJ(jsonAdd(sdoc), null);
-    Map rsp = (Map)ObjectBuilder.fromJSON(response);
-    return (Long) ((List)rsp.get("adds")).get(1);
+  public int getFirstMatch(IndexReader r, Term t) throws IOException {
+    Fields fields = MultiFields.getFields(r);
+    if (fields == null) return -1;
+    Terms terms = fields.terms(t.field());
+    if (terms == null) return -1;
+    BytesRef termBytes = t.bytes();
+    DocsEnum docs = terms.docs(MultiFields.getLiveDocs(r), termBytes, null);
+    if (docs == null) return -1;
+    int id = docs.nextDoc();
+    if (id != DocIdSetIterator.NO_MORE_DOCS) {
+      int next = docs.nextDoc();
+      assertEquals(DocIdSetIterator.NO_MORE_DOCS, next);
   }
-
-  private Long deleteAndGetVersion(String id) throws Exception {
-    String response = updateJ("{\"delete\":{\"id\":\""+id+"\"}}", null);
-    Map rsp = (Map)ObjectBuilder.fromJSON(response);
-    return (Long) ((List)rsp.get("deletes")).get(1);
+    return id == DocIdSetIterator.NO_MORE_DOCS ? -1 : id;
   }
 
-
 }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestSort.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestSort.java
index feb6f401..f6602656 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestSort.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/search/TestSort.java
@@ -31,6 +31,7 @@
 import org.apache.lucene.search.SortField.Type;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.OpenBitSet;
 import org.apache.lucene.util._TestUtil;
 import org.apache.solr.SolrTestCaseJ4;
@@ -199,8 +200,8 @@ public void testSort() throws Exception {
       for (int i=0; i<qiter; i++) {
         Filter filt = new Filter() {
           @Override
-          public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {
-            return randSet(context.reader.maxDoc());
+          public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+            return BitsFilteredDocIdSet.wrap(randSet(context.reader.maxDoc()), acceptDocs);
           }
         };
 
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/update/AutoCommitTest.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/update/AutoCommitTest.java
index cae75e2e..ace3f8ae 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/update/AutoCommitTest.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/update/AutoCommitTest.java
@@ -32,7 +32,6 @@
 import org.apache.solr.request.SolrQueryRequestBase;
 import org.apache.solr.response.SolrQueryResponse;
 import org.apache.solr.search.SolrIndexSearcher;
-import org.apache.solr.update.NewSearcherListener.TriggerOn;
 import org.apache.solr.util.AbstractSolrTestCase;
 import org.apache.solr.util.RefCounted;
 
@@ -261,269 +260,4 @@ public void testMaxTime() throws Exception {
     assertQ("now it should", req("id:500") ,"//result[@numFound=1]" );
   }
 
-  public void testSoftCommitMaxDocs() throws Exception {
-
-    SolrCore core = h.getCore();
-    NewSearcherListener trigger = new NewSearcherListener(TriggerOn.Hard);
-
-    core.registerNewSearcherListener(trigger);
-    
-    DirectUpdateHandler2 updateHandler = (DirectUpdateHandler2)core.getUpdateHandler();
-    updateHandler.registerCommitCallback(trigger);
-    
-    CommitTracker tracker = updateHandler.commitTracker;
-    tracker.setTimeUpperBound(-1);
-    tracker.setDocsUpperBound(8);
- 
-    
-    NewSearcherListener softTrigger = new NewSearcherListener(TriggerOn.Soft);
-    updateHandler.registerSoftCommitCallback(softTrigger);
-    core.registerNewSearcherListener(softTrigger);
-    
-    CommitTracker softTracker = updateHandler.softCommitTracker;
-    softTracker.setTimeUpperBound(-1);
-    softTracker.setDocsUpperBound(4);
-
-    
-    XmlUpdateRequestHandler handler = new XmlUpdateRequestHandler();
-    handler.init( null );
-    
-    MapSolrParams params = new MapSolrParams( new HashMap<String, String>() );
-    
-    // Add documents
-    SolrQueryResponse rsp = new SolrQueryResponse();
-    SolrQueryRequestBase req = new SolrQueryRequestBase( core, params ) {};
-    for( int i=0; i<4; i++ ) {
-      req.setContentStreams( toContentStreams(
-        adoc("id", Integer.toString(i), "subject", "info" ), null ) );
-      handler.handleRequest( req, rsp );
-    }
-    // It should not be there right away
-    assertQ("shouldn't find any", req("id:1") ,"//result[@numFound=0]" );
-    assertEquals( 0, tracker.getCommitCount());
-
-    req.setContentStreams( toContentStreams(
-        adoc("id", "4", "subject", "info" ), null ) );
-    handler.handleRequest( req, rsp );
-
-    assertTrue(softTrigger.waitForNewSearcher(10000));
-    softTrigger.reset();
-    
-    assertQ("should find 5", req("*:*") ,"//result[@numFound=5]" );
-    assertEquals( 1, softTracker.getCommitCount());
-    assertEquals( 0, tracker.getCommitCount());
-    
-    req.setContentStreams( toContentStreams(
-        adoc("id", "5", "subject", "info" ), null ) );
-    handler.handleRequest( req, rsp );
-      
-    // Now make sure we can find it
-    assertQ("should find one", req("id:4") ,"//result[@numFound=1]" );
-    assertEquals( 1, softTracker.getCommitCount());
-    // But not the one added afterward
-    assertQ("should not find one", req("id:5") ,"//result[@numFound=0]" );
-    assertEquals( 1, softTracker.getCommitCount());
-    
-    for( int i=6; i<10; i++ ) {
-      req.setContentStreams( toContentStreams(
-        adoc("id", Integer.toString(i), "subject", "info" ), null ) );
-      handler.handleRequest( req, rsp );
-    }
-    req.close();
-    
-    assertTrue(softTrigger.waitForNewSearcher(30000));
-    softTrigger.reset();
-    
-    assertTrue(trigger.waitForNewSearcher(10000));
-    assertQ("should find 10", req("*:*") ,"//result[@numFound=10]" );
-    assertEquals( 2, softTracker.getCommitCount());
-    assertEquals( 1, tracker.getCommitCount());
-  }
-  
-  public void testSoftCommitMaxTime() throws Exception {
-    SolrCore core = h.getCore();
-    NewSearcherListener trigger = new NewSearcherListener();    
-    core.registerNewSearcherListener(trigger);
-    DirectUpdateHandler2 updater = (DirectUpdateHandler2) core.getUpdateHandler();
-    CommitTracker tracker = updater.commitTracker;
-    CommitTracker softTracker = updater.softCommitTracker;
-    
-    // too low of a number can cause a slow host to commit before the test code checks that it
-    // isn't there... causing a failure at "shouldn't find any"
-    softTracker.setTimeUpperBound(2000);
-    softTracker.setDocsUpperBound(-1);
-    // updater.commitCallbacks.add(trigger);
-    
-    XmlUpdateRequestHandler handler = new XmlUpdateRequestHandler();
-    handler.init( null );
-    
-    MapSolrParams params = new MapSolrParams( new HashMap<String, String>() );
-    
-    // Add a single document
-    SolrQueryResponse rsp = new SolrQueryResponse();
-    SolrQueryRequestBase req = new SolrQueryRequestBase( core, params ) {};
-    req.setContentStreams( toContentStreams(
-      adoc("id", "529", "field_t", "what's inside?", "subject", "info"), null ) );
-    trigger.reset();
-    handler.handleRequest( req, rsp );
-
-    // Check it it is in the index
-    assertQ("shouldn't find any", req("id:529") ,"//result[@numFound=0]" );
-    assertEquals(0, softTracker.getCommitCount());
-
-    // Wait longer than the autocommit time
-    assertTrue(trigger.waitForNewSearcher(30000));
-    trigger.reset();
-    req.setContentStreams( toContentStreams(
-      adoc("id", "530", "field_t", "what's inside?", "subject", "info"), null ) );
-    handler.handleRequest( req, rsp );
-      
-    // Now make sure we can find it
-    assertQ("should find one", req("id:529") ,"//result[@numFound=1]" );
-    // But not this one
-    assertQ("should find none", req("id:530") ,"//result[@numFound=0]" );
-    verbose("###about to delete 529");
-    // Delete the document
-    assertU(delI("529"));
-    assertQ("deleted, but should still be there", req("id:529") ,"//result[@numFound=1]" );
-    // Wait longer than the autocommit time
-    verbose("###starting to wait for new searcher.  softTracker.getCommitCount()==",softTracker.getCommitCount());
-    assertTrue(trigger.waitForNewSearcher(15000));
-    trigger.reset();
-    verbose("###done waiting for new searcher.  softTracker.getCommitCount()==",softTracker.getCommitCount());
-
-    // what's the point of this update?
-    req.setContentStreams( toContentStreams(
-      adoc("id", "550", "field_t", "what's inside?", "subject", "info"), null ) );
-    handler.handleRequest( req, rsp );
-
-
-    assertEquals( 2, softTracker.getCommitCount() );
-    assertQ("deleted and time has passed", req("id:529") ,"//result[@numFound=0]" );
-    
-    // now make the call 5 times really fast and make sure it 
-    // only commits once
-    req.setContentStreams( toContentStreams(
-        adoc("id", "500" ), null ) );
-    for( int i=0;i<5; i++ ) {
-      handler.handleRequest( req, rsp );
-    }
-    assertQ("should not be there yet", req("id:500") ,"//result[@numFound=0]" );
-    
-    // Wait longer than the autocommit time
-    assertTrue(trigger.waitForNewSearcher(15000));
-    trigger.reset();
-    
-    req.setContentStreams( toContentStreams(
-      adoc("id", "531", "field_t", "what's inside?", "subject", "info"), null ) );
-    handler.handleRequest( req, rsp );
-    assertEquals( 3, softTracker.getCommitCount() );
-    assertEquals( 0, tracker.getCommitCount() );
-    
-    assertQ("now it should", req("id:500") ,"//result[@numFound=1]" );
-    assertQ("but not this", req("id:531") ,"//result[@numFound=0]" );
-  }
-  
-  public void testSoftAndHardCommitMaxTime() throws Exception {
-    SolrCore core = h.getCore();
-    NewSearcherListener softTrigger = new NewSearcherListener(TriggerOn.Soft);  
-    NewSearcherListener hardTrigger = new NewSearcherListener(TriggerOn.Hard); 
-    core.registerNewSearcherListener(softTrigger);
-    core.registerNewSearcherListener(hardTrigger);
-    DirectUpdateHandler2 updater = (DirectUpdateHandler2) core.getUpdateHandler();
-    
-    updater.registerSoftCommitCallback(softTrigger);
-    updater.registerCommitCallback(hardTrigger);
-    
-    CommitTracker hardTracker = updater.commitTracker;
-    CommitTracker softTracker = updater.softCommitTracker;
-    
-    // too low of a number can cause a slow host to commit before the test code checks that it
-    // isn't there... causing a failure at "shouldn't find any"
-    softTracker.setTimeUpperBound(500);
-    softTracker.setDocsUpperBound(-1);
-    hardTracker.setTimeUpperBound(1200);
-    hardTracker.setDocsUpperBound(-1);
-    // updater.commitCallbacks.add(trigger);
-    
-    XmlUpdateRequestHandler handler = new XmlUpdateRequestHandler();
-    handler.init( null );
-    
-    MapSolrParams params = new MapSolrParams( new HashMap<String, String>() );
-    
-    // Add a single document
-    SolrQueryResponse rsp = new SolrQueryResponse();
-    SolrQueryRequestBase req = new SolrQueryRequestBase( core, params ) {};
-    req.setContentStreams( toContentStreams(
-      adoc("id", "529", "field_t", "what's inside?", "subject", "info"), null ) );
-
-    handler.handleRequest( req, rsp );
-
-    // Check if it is in the index
-    assertQ("shouldn't find any", req("id:529") ,"//result[@numFound=0]" );
-
-    // Wait longer than the autocommit time
-    assertTrue(softTrigger.waitForNewSearcher(30000));
-    softTrigger.reset();
-    req.setContentStreams( toContentStreams(
-      adoc("id", "530", "field_t", "what's inside?", "subject", "info"), null ) );
-    handler.handleRequest( req, rsp );
-      
-    // Now make sure we can find it
-    assertQ("should find one", req("id:529") ,"//result[@numFound=1]" );
-    // But not this one
-    assertQ("should find none", req("id:530") ,"//result[@numFound=0]" );
-    
-    // Delete the document
-    assertU( delI("529") );
-    assertQ("deleted, but should still be there", req("id:529") ,"//result[@numFound=1]" );
-    
-    // Wait longer than the autocommit time
-    assertTrue(softTrigger.waitForNewSearcher(30000));
-    softTrigger.reset();
-
-    
-    req.setContentStreams( toContentStreams(
-      adoc("id", "550", "field_t", "what's inside?", "subject", "info"), null ) );
-    handler.handleRequest( req, rsp );
-    int totalCommits = softTracker.getCommitCount() + hardTracker.getCommitCount();
-    assertTrue("expected:>=2 but got " + totalCommits, totalCommits >= 2);
-    assertQ("deleted and time has passed", req("id:529") ,"//result[@numFound=0]" );
-    
-    // now make the call 2 times really fast and make sure id:500
-    // is not visible right away
-    req.setContentStreams( toContentStreams(
-        adoc("id", "500" ), null ) );
-    for( int i=0;i<2; i++ ) {
-      handler.handleRequest( req, rsp );
-    }
-    assertQ("should not be there yet", req("id:500") ,"//result[@numFound=0]" );
-    
-    // Wait longer than the autocommit time
-    assertTrue(softTrigger.waitForNewSearcher(30000));
-    softTrigger.reset();
-    
-    req.setContentStreams( toContentStreams(
-      adoc("id", "531", "field_t", "what's inside?", "subject", "info"), null ) );
-    handler.handleRequest( req, rsp );
-    
-    // depending on timing, you might see 2 or 3 soft commits
-    int softCommitCnt = softTracker.getCommitCount();
-    assertTrue("commit cnt:" + softCommitCnt, softCommitCnt == 2
-        || softCommitCnt == 3);
-    
-    // depending on timing, you might see 1 or 2 hard commits
-    assertTrue(hardTrigger.waitForNewSearcher(30000));
-    hardTrigger.reset();
-    
-    int hardCommitCnt = hardTracker.getCommitCount();
-    assertTrue("commit cnt:" + hardCommitCnt, hardCommitCnt == 1
-        || hardCommitCnt == 2);
-    
-    assertTrue(softTrigger.waitForNewSearcher(30000));
-    softTrigger.reset();
-    
-    assertQ("now it should", req("id:500") ,"//result[@numFound=1]" );
-    assertQ("but not this", req("id:531") ,"//result[@numFound=1]" );
-  }
 }
diff --git a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/update/SoftAutoCommitTest.java b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/update/SoftAutoCommitTest.java
index e69de29b..bb645bc2 100644
--- a/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/update/SoftAutoCommitTest.java
+++ b/lucene/dev/branches/solrcloud/solr/core/src/test/org/apache/solr/update/SoftAutoCommitTest.java
@@ -0,0 +1,370 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.update;
+
+import static java.util.concurrent.TimeUnit.MILLISECONDS;
+import static java.util.concurrent.TimeUnit.SECONDS;
+import static org.junit.Assert.assertEquals;
+
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+
+import org.apache.solr.common.util.NamedList;
+import org.apache.solr.core.SolrCore;
+import org.apache.solr.core.SolrEventListener;
+import org.apache.solr.search.SolrIndexSearcher;
+import org.apache.solr.util.AbstractSolrTestCase;
+import org.junit.Before;
+
+/**
+ * Test auto commit functionality in a way that doesn't suck.
+ * <p>
+ * AutoCommitTest is an abomination that is way to brittle in how it 
+ * tries to check that commits happened, and when they happened.
+ * The goal of this test class is to (ultimately) completely replace all 
+ * of the functionality of that test class using:
+ * </p>
+ * <ul>
+ *   <li>A more robust monitor of commit/newSearcher events that records 
+ *       the times of those events in a queue that can be polled.  
+ *       Multiple events in rapid succession are not lost.
+ *   </li>
+ *   <li>Timing checks that are forgiving of slow machines and use 
+ *       knowledge of how slow A-&gt;B was to affect the expectation of 
+ *       how slow B-&gt;C will be
+ *   </li>
+ * </ul>
+ */
+public class SoftAutoCommitTest extends AbstractSolrTestCase {
+
+  @Override
+  public String getSchemaFile() { return "schema.xml"; }
+  @Override
+  public String getSolrConfigFile() { return "solrconfig.xml"; }
+
+  private MockEventListener monitor;
+  private DirectUpdateHandler2 updater;
+    
+  @Before
+  public void createMonitor() throws Exception {
+    SolrCore core = h.getCore();
+
+    updater = (DirectUpdateHandler2) core.getUpdateHandler();
+    monitor = new MockEventListener();
+
+    core.registerNewSearcherListener(monitor);
+    updater.registerSoftCommitCallback(monitor);
+    updater.registerCommitCallback(monitor);
+  }
+
+  public void testSoftAndHardCommitMaxTimeMixedAdds() throws Exception {
+
+    final int softCommitWaitMillis = 500;
+    final int hardCommitWaitMillis = 1200;
+
+    CommitTracker hardTracker = updater.commitTracker;
+    CommitTracker softTracker = updater.softCommitTracker;
+    
+    softTracker.setTimeUpperBound(softCommitWaitMillis);
+    softTracker.setDocsUpperBound(-1);
+    hardTracker.setTimeUpperBound(hardCommitWaitMillis);
+    hardTracker.setDocsUpperBound(-1);
+    
+    // Add a single document
+    long add529 = System.currentTimeMillis();
+    assertU(adoc("id", "529", "subject", "the doc we care about in this test"));
+
+    monitor.assertSaneOffers();
+
+    // Wait for the soft commit with some fudge
+    Long soft529 = monitor.soft.poll(softCommitWaitMillis * 2, MILLISECONDS);
+    assertNotNull("soft529 wasn't fast enough", soft529);
+    monitor.assertSaneOffers();
+
+    // check for the searcher, should have happend right after soft commit
+    Long searcher529 = monitor.searcher.poll(softCommitWaitMillis, MILLISECONDS);
+    assertNotNull("searcher529 wasn't fast enough", searcher529);
+    monitor.assertSaneOffers();
+
+    // toss in another doc, shouldn't affect first hard commit time we poll
+    assertU(adoc("id", "530", "subject", "just for noise/activity"));
+
+    // wait for the hard commit
+    Long hard529 = monitor.hard.poll(hardCommitWaitMillis * 5, MILLISECONDS);
+    assertNotNull("hard529 wasn't fast enough", hard529);
+    monitor.assertSaneOffers();
+    
+    assertTrue("soft529 occured too fast: " + 
+               add529 + " + " + softCommitWaitMillis + " !<= " + soft529,
+               add529 + softCommitWaitMillis <= soft529);
+    assertTrue("hard529 occured too fast: " + 
+               add529 + " + " + hardCommitWaitMillis + " !<= " + hard529,
+               add529 + hardCommitWaitMillis <= hard529);
+
+    // however slow the machine was to do the soft commit compared to expected,
+    // assume newSearcher had some magnitude of that much overhead as well 
+    long slowTestFudge = Math.max(100, 6 * (soft529 - add529 - softCommitWaitMillis));
+    assertTrue("searcher529 wasn't soon enough after soft529: " +
+               searcher529 + " !< " + soft529 + " + " + slowTestFudge + " (fudge)",
+               searcher529 < soft529 + slowTestFudge );
+
+    assertTrue("hard529 was before searcher529: " + 
+               searcher529 + " !<= " + hard529,
+               searcher529 <= hard529);
+
+    monitor.assertSaneOffers();
+
+    // there may have been (or will be) a second hard commit for 530
+    Long hard530 = monitor.hard.poll(hardCommitWaitMillis, MILLISECONDS);
+    assertEquals("Tracker reports too many hard commits",
+                 (null == hard530 ? 1 : 2), 
+                 hardTracker.getCommitCount());
+
+    // there may have been a second soft commit for 530, 
+    // but if so it must have already happend
+    Long soft530 = monitor.soft.poll(0, MILLISECONDS);
+    if (null != soft530) {
+      assertEquals("Tracker reports too many soft commits",
+                   2, softTracker.getCommitCount());
+      if (null != hard530) {
+        assertTrue("soft530 after hard530: " +
+                   soft530 + " !<= " + hard530,
+                   soft530 <= hard530);
+      } else {
+        assertTrue("soft530 after hard529 but no hard530: " +
+                   soft530 + " !<= " + hard529,
+                   soft530 <= hard529);
+      }
+    } else {
+      assertEquals("Tracker reports too many soft commits",
+                   1, softTracker.getCommitCount());
+    }
+      
+    if (null != soft530 || null != hard530) {
+      assertNotNull("at least one extra commit for 530, but no searcher",
+                    monitor.searcher.poll(0, MILLISECONDS));
+    }
+
+    monitor.assertSaneOffers();
+
+    // wait a bit, w/o other action we definitley shouldn't see any 
+    // new hard/soft commits 
+    assertNull("Got a hard commit we weren't expecting",
+               monitor.hard.poll(2, SECONDS));
+    assertNull("Got a soft commit we weren't expecting",
+               monitor.soft.poll(0, MILLISECONDS));
+
+    monitor.assertSaneOffers();
+  }
+
+  public void testSoftAndHardCommitMaxTimeDelete() throws Exception {
+
+    final int softCommitWaitMillis = 500;
+    final int hardCommitWaitMillis = 1200;
+
+    CommitTracker hardTracker = updater.commitTracker;
+    CommitTracker softTracker = updater.softCommitTracker;
+    
+    softTracker.setTimeUpperBound(softCommitWaitMillis);
+    softTracker.setDocsUpperBound(-1);
+    hardTracker.setTimeUpperBound(hardCommitWaitMillis);
+    hardTracker.setDocsUpperBound(-1);
+    
+    // add a doc and force a commit
+    assertU(adoc("id", "529", "subject", "the doc we care about in this test"));
+    assertU(commit());
+    long postAdd529 = System.currentTimeMillis();
+
+    // wait for first hard/soft commit
+    Long soft529 = monitor.soft.poll(softCommitWaitMillis * 2, MILLISECONDS);
+    assertNotNull("soft529 wasn't fast enough", soft529);
+    Long manCommit = monitor.hard.poll(0, MILLISECONDS);
+
+    assertNotNull("manCommit wasn't fast enough", manCommit);
+    assertTrue("forced manCommit didn't happen when it should have: " + 
+        manCommit + " !< " + postAdd529, 
+        manCommit < postAdd529);
+    
+    Long hard529 = monitor.hard.poll(hardCommitWaitMillis, MILLISECONDS);
+    assertNotNull("hard529 wasn't fast enough", hard529);
+
+    monitor.assertSaneOffers();
+    monitor.clear();
+
+    // Delete the document
+    long del529 = System.currentTimeMillis();
+    assertU( delI("529") );
+
+    monitor.assertSaneOffers();
+
+    // Wait for the soft commit with some fudge
+    soft529 = monitor.soft.poll(softCommitWaitMillis * 2, MILLISECONDS);
+    assertNotNull("soft529 wasn't fast enough", soft529);
+    monitor.assertSaneOffers();
+ 
+    // check for the searcher, should have happened right after soft commit
+    Long searcher529 = monitor.searcher.poll(softCommitWaitMillis, MILLISECONDS);
+    assertNotNull("searcher529 wasn't fast enough", searcher529);
+    monitor.assertSaneOffers();
+
+    // toss in another doc, shouldn't affect first hard commit time we poll
+    assertU(adoc("id", "550", "subject", "just for noise/activity"));
+
+    // wait for the hard commit
+    hard529 = monitor.hard.poll(hardCommitWaitMillis * 3, MILLISECONDS);
+    assertNotNull("hard529 wasn't fast enough", hard529);
+    monitor.assertSaneOffers();
+    
+    assertTrue("soft529 occured too fast: " + 
+               del529 + " + " + softCommitWaitMillis + " !<= " + soft529,
+               del529 + softCommitWaitMillis <= soft529);
+    assertTrue("hard529 occured too fast: " + 
+               del529 + " + " + hardCommitWaitMillis + " !<= " + hard529,
+               del529 + hardCommitWaitMillis <= hard529);
+
+    // however slow the machine was to do the soft commit compared to expected,
+    // assume newSearcher had some magnitude of that much overhead as well 
+    long slowTestFudge = Math.max(100, 3 * (soft529 - del529 - softCommitWaitMillis));
+    assertTrue("searcher529 wasn't soon enough after soft529: " +
+               searcher529 + " !< " + soft529 + " + " + slowTestFudge + " (fudge)",
+               searcher529 < soft529 + slowTestFudge );
+
+    assertTrue("hard529 was before searcher529: " + 
+               searcher529 + " !<= " + hard529,
+               searcher529 <= hard529);
+
+    // clear commmits
+    monitor.hard.clear();
+    monitor.soft.clear();
+    
+    // wait a bit, w/o other action we definitely shouldn't see any 
+    // new hard/soft commits 
+    assertNull("Got a hard commit we weren't expecting",
+               monitor.hard.poll(2, SECONDS));
+    assertNull("Got a soft commit we weren't expecting",
+               monitor.soft.poll(0, MILLISECONDS));
+
+    monitor.assertSaneOffers();
+  }
+
+  public void testSoftAndHardCommitMaxTimeRapidAdds() throws Exception {
+ 
+    final int softCommitWaitMillis = 500;
+    final int hardCommitWaitMillis = 1200;
+
+    CommitTracker hardTracker = updater.commitTracker;
+    CommitTracker softTracker = updater.softCommitTracker;
+    
+    softTracker.setTimeUpperBound(softCommitWaitMillis);
+    softTracker.setDocsUpperBound(-1);
+    hardTracker.setTimeUpperBound(hardCommitWaitMillis);
+    hardTracker.setDocsUpperBound(-1);
+    
+    // try to add 5 docs really fast
+    long fast5start = System.currentTimeMillis();
+    for( int i=0;i<5; i++ ) {
+      assertU(adoc("id", ""+500 + i, "subject", "five fast docs"));
+    }
+    long fast5end = System.currentTimeMillis() - 100; // minus a tad of slop
+    long fast5time = 1 + fast5end - fast5start;
+
+    // total time for all 5 adds determines the number of soft to expect
+    long expectedSoft = (long)Math.ceil(fast5time / softCommitWaitMillis);
+    long expectedHard = (long)Math.ceil(fast5time / hardCommitWaitMillis);
+    
+    // note: counting from 1 for multiplication
+    for (int i = 1; i <= expectedSoft; i++) {
+      // Wait for the soft commit with some fudge
+      Long soft = monitor.soft.poll(softCommitWaitMillis * 2, MILLISECONDS);
+      assertNotNull(i + ": soft wasn't fast enough", soft);
+      monitor.assertSaneOffers();
+
+      // have to assume none of the docs were added until
+      // very end of the add window
+      assertTrue(i + ": soft occured too fast: " + 
+                 fast5end + " + (" + softCommitWaitMillis + " * " + i +
+                 ") !<= " + soft,
+                 fast5end + (softCommitWaitMillis * i) <= soft);
+    }
+
+    // note: counting from 1 for multiplication
+    for (int i = 1; i <= expectedHard; i++) {
+      // wait for the hard commit, shouldn't need any fudge given 
+      // other actions already taken
+      Long hard = monitor.hard.poll(hardCommitWaitMillis, MILLISECONDS);
+      assertNotNull(i + ": hard wasn't fast enough", hard);
+      monitor.assertSaneOffers();
+      
+      // have to assume none of the docs were added until
+      // very end of the add window
+      assertTrue(i + ": soft occured too fast: " + 
+                 fast5end + " + (" + hardCommitWaitMillis + " * " + i +
+                 ") !<= " + hard,
+                 fast5end + (hardCommitWaitMillis * i) <= hard);
+    }
+ 
+  }
+}
+
+class MockEventListener implements SolrEventListener {
+
+  // use capacity bound Queues just so we're sure we don't OOM 
+  public final BlockingQueue<Long> soft = new LinkedBlockingQueue<Long>(1000);
+  public final BlockingQueue<Long> hard = new LinkedBlockingQueue<Long>(1000);
+  public final BlockingQueue<Long> searcher = new LinkedBlockingQueue<Long>(1000);
+
+  // if non enpty, then at least one offer failed (queues full)
+  private StringBuffer fail = new StringBuffer();
+
+  public MockEventListener() { /* NOOP */ }
+  
+  @Override
+  public void init(NamedList args) {}
+  
+  @Override
+  public void newSearcher(SolrIndexSearcher newSearcher,
+                          SolrIndexSearcher currentSearcher) {
+    Long now = System.currentTimeMillis();
+    if (!searcher.offer(now)) fail.append(", newSearcher @ " + now);
+  }
+  
+  @Override
+  public void postCommit() {
+    Long now = System.currentTimeMillis();
+    if (!hard.offer(now)) fail.append(", hardCommit @ " + now);
+  }
+  
+  @Override
+  public void postSoftCommit() {
+    Long now = System.currentTimeMillis();
+    if (!soft.offer(now)) fail.append(", softCommit @ " + now);
+  }
+  
+  public void clear() {
+    soft.clear();
+    hard.clear();
+    searcher.clear();
+    fail.setLength(0);
+  }
+  
+  public void assertSaneOffers() {
+    assertEquals("Failure of MockEventListener" + fail.toString(), 
+                 0, fail.length());
+  }
+}
+
diff --git a/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CommonsHttpSolrServer.java b/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CommonsHttpSolrServer.java
index bad6a016..616de71c 100644
--- a/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CommonsHttpSolrServer.java
+++ b/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CommonsHttpSolrServer.java
@@ -282,7 +282,7 @@ public CommonsHttpSolrServer(URL baseURL, HttpClient client, ResponseParser pars
     int tries = _maxRetries + 1;
     try {
       while( tries-- > 0 ) {
-        // Note: since we aren't do intermittent time keeping
+        // Note: since we aren't doing intermittent time keeping
         // ourselves, the potential non-timeout latency could be as
         // much as tries-times (plus scheduling effects) the given
         // timeAllowed.
diff --git a/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/common/util/ContentStream.java b/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/common/util/ContentStream.java
index de8d48bc..d416c5c7 100644
--- a/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/common/util/ContentStream.java
+++ b/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/common/util/ContentStream.java
@@ -50,6 +50,10 @@
    *  
    * Only the first call to <code>getStream()</code> or <code>getReader()</code>
    * is guaranteed to work.  The runtime behavior for additional calls is undefined.
+   *
+   * Note: you must call <code>getStream()</code> or <code>getReader()</code> before
+   * the attributes (name, contentType, etc) are guaranteed to be set.  Streams may be
+   * lazy loaded only when this method is called.
    */
   InputStream getStream() throws IOException;
 
@@ -68,6 +72,10 @@
    *  
    * Only the first call to <code>getStream()</code> or <code>getReader()</code>
    * is guaranteed to work.  The runtime behavior for additional calls is undefined.
+   *
+   * Note: you must call <code>getStream()</code> or <code>getReader()</code> before
+   * the attributes (name, contentType, etc) are guaranteed to be set.  Streams may be
+   * lazy loaded only when this method is called.
    */
   Reader getReader() throws IOException;
 }
diff --git a/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/common/util/ContentStreamBase.java b/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/common/util/ContentStreamBase.java
index 9d4861d3..abcf2c89 100644
--- a/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/common/util/ContentStreamBase.java
+++ b/lucene/dev/branches/solrcloud/solr/solrj/src/java/org/apache/solr/common/util/ContentStreamBase.java
@@ -72,19 +72,18 @@ public static String getCharsetFromContentType( String contentType )
   public static class URLStream extends ContentStreamBase
   {
     private final URL url;
-    final URLConnection conn;
     
     public URLStream( URL url ) throws IOException {
       this.url = url; 
-      this.conn = this.url.openConnection();
-      
-      contentType = conn.getContentType();
-      name = url.toExternalForm();
-      size = new Long( conn.getContentLength() );
       sourceInfo = "url";
     }
 
     public InputStream getStream() throws IOException {
+      URLConnection conn = this.url.openConnection();
+      
+      contentType = conn.getContentType();
+      name = url.toExternalForm();
+      size = new Long( conn.getContentLength() );
       return conn.getInputStream();
     }
   }
diff --git a/lucene/dev/branches/solrcloud/solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTests.java b/lucene/dev/branches/solrcloud/solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTests.java
index 2f069874..50d14230 100644
--- a/lucene/dev/branches/solrcloud/solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTests.java
+++ b/lucene/dev/branches/solrcloud/solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTests.java
@@ -35,6 +35,7 @@
 import org.apache.solr.client.solrj.impl.XMLResponseParser;
 import org.apache.solr.client.solrj.request.DirectXmlRequest;
 import org.apache.solr.client.solrj.request.LukeRequest;
+import org.apache.solr.client.solrj.request.QueryRequest;
 import org.apache.solr.client.solrj.request.SolrPing;
 import org.apache.solr.client.solrj.response.FieldStatsInfo;
 import org.apache.solr.client.solrj.request.UpdateRequest;
@@ -992,4 +993,43 @@ public void testChineseDefaults() throws Exception {
     QueryResponse rsp = server.query( query );
     assertEquals(1, rsp.getResults().getNumFound());
   }
+  
+
+  @Test
+  public void testRealtimeGet() throws Exception
+  {    
+    SolrServer server = getSolrServer();
+    
+    // Empty the database...
+    server.deleteByQuery( "*:*" );// delete everything!
+    
+    // Now add something...
+    SolrInputDocument doc = new SolrInputDocument();
+    doc.addField( "id", "DOCID", 1.0f );
+    doc.addField( "name", "hello", 1.0f );
+    server.add( doc );
+    server.commit();  // Since the transaction log is disabled in the example, we need to commit
+    
+    SolrQuery q = new SolrQuery();
+    q.setQueryType("/get");
+    q.set("id", "DOCID");
+    q.set("fl", "id,name,aaa:[value v=aaa]");
+    
+    // First Try with the BinaryResponseParser
+    QueryRequest req = new QueryRequest( q );
+    req.setResponseParser(new BinaryResponseParser());
+    QueryResponse rsp = req.process(server);
+    SolrDocument out = (SolrDocument)rsp.getResponse().get("doc");
+    assertEquals("DOCID", out.get("id"));
+    assertEquals("hello", out.get("name"));
+    assertEquals("aaa", out.get("aaa"));
+
+    // Then with the XMLResponseParser
+    req.setResponseParser(new XMLResponseParser());
+    rsp = req.process(server);
+    out = (SolrDocument)rsp.getResponse().get("doc");
+    assertEquals("DOCID", out.get("id"));
+    assertEquals("hello", out.get("name"));
+    assertEquals("aaa", out.get("aaa"));
+  }
 }
diff --git a/lucene/dev/branches/solrcloud/solr/solrj/src/test/org/apache/solr/common/util/ContentStreamTest.java b/lucene/dev/branches/solrcloud/solr/solrj/src/test/org/apache/solr/common/util/ContentStreamTest.java
index 903f0467..48c41405 100644
--- a/lucene/dev/branches/solrcloud/solr/solrj/src/test/org/apache/solr/common/util/ContentStreamTest.java
+++ b/lucene/dev/branches/solrcloud/solr/solrj/src/test/org/apache/solr/common/util/ContentStreamTest.java
@@ -92,10 +92,9 @@ public void testURLStream() throws IOException
     
     
     ContentStreamBase stream = new ContentStreamBase.URLStream( url );
+    in = stream.getStream();  // getStream is needed before getSize is valid
     assertEquals( content.length, stream.getSize().intValue() );
     
-    // Test the stream
-    in = stream.getStream();
     try {
       assertTrue( IOUtils.contentEquals( 
           new ByteArrayInputStream(content), in ) );
diff --git a/lucene/dev/branches/solrcloud/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java b/lucene/dev/branches/solrcloud/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
index 7d553a18..37fb81fa 100644
--- a/lucene/dev/branches/solrcloud/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
+++ b/lucene/dev/branches/solrcloud/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
@@ -41,7 +41,6 @@
 import org.apache.solr.schema.SchemaField;
 import org.apache.solr.search.SolrIndexSearcher;
 import org.apache.solr.servlet.DirectSolrConnection;
-import org.apache.solr.update.SolrIndexWriter;
 import org.apache.solr.util.TestHarness;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
@@ -65,7 +64,6 @@
   @BeforeClass
   public static void beforeClassSolrTestCase() throws Exception {
     startTrackingSearchers();
-    startTrackingWriters();
     ignoreException("ignore_exception");
   }
 
@@ -74,7 +72,6 @@ public static void afterClassSolrTestCase() throws Exception {
     deleteCore();
     resetExceptionIgnores();
     endTrackingSearchers();
-    endTrackingWriters();
   }
 
   @Override
@@ -130,28 +127,6 @@ public static void endTrackingSearchers() {
      }
   }
   
-  static long numWriterOpens;
-  static long numWriterCloses;
-  public static void startTrackingWriters() {
-    numOpens = SolrIndexWriter.numOpens.get();
-    numCloses = SolrIndexWriter.numCloses.get();
-  }
-
-  public static void endTrackingWriters() {
-     long endNumOpens = SolrIndexWriter.numOpens.get();
-     long endNumCloses = SolrIndexWriter.numCloses.get();
-     
-     SolrIndexWriter.numOpens.getAndSet(0);
-     SolrIndexWriter.numCloses.getAndSet(0);
-
-     if (endNumOpens-numOpens != endNumCloses-numCloses) {
-       String msg = "ERROR: SolrIndexWriter opens=" + (endNumOpens-numWriterOpens) + " closes=" + (endNumCloses-numWriterCloses);
-       log.error(msg);
-       testsFailed = true;
-       fail(msg);
-     }
-  }
-
   /** Causes an exception matching the regex pattern to not be logged. */
   public static void ignoreException(String pattern) {
     if (SolrException.ignorePatterns == null)
@@ -674,7 +649,7 @@ public static String updateJ(String json, SolrParams args) throws Exception {
     SolrCore core = h.getCore();
     if (args == null) args = params("wt","json","indent","true");
     DirectSolrConnection connection = new DirectSolrConnection(core);
-    SolrRequestHandler handler = core.getRequestHandler("/udate/json");
+    SolrRequestHandler handler = core.getRequestHandler("/update/json");
     if (handler == null) {
       handler = new JsonUpdateRequestHandler();
       handler.init(null);
diff --git a/lucene/dev/branches/solrcloud/solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java b/lucene/dev/branches/solrcloud/solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java
index 513c3698..8f0382e1 100644
--- a/lucene/dev/branches/solrcloud/solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java
+++ b/lucene/dev/branches/solrcloud/solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java
@@ -97,13 +97,11 @@ public String getSolrHome() {
   @BeforeClass
   public static void beforeClassAbstractSolrTestCase() throws Exception {
     SolrTestCaseJ4.startTrackingSearchers();
-    SolrTestCaseJ4.startTrackingWriters();
   }
   
   @AfterClass
   public static void afterClassAbstractSolrTestCase() throws Exception {
     SolrTestCaseJ4.endTrackingSearchers();
-    SolrTestCaseJ4.endTrackingWriters();
   }
   
   /**
