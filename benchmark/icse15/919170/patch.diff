diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtable.java
index 6858478b..55250091 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/BinaryMemtable.java
@@ -30,8 +30,8 @@
 
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.io.SSTableReader;
-import org.apache.cassandra.io.SSTableWriter;
+import org.apache.cassandra.io.sstable.SSTableWriter;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.WrappedRunnable;
 import org.cliffc.high_scale_lib.NonBlockingHashMap;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java
index 7ee1adf2..fb2a4207 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java
@@ -27,7 +27,7 @@
 import java.util.Collection;
 
 import org.apache.cassandra.io.ICompactSerializer2;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.db.marshal.AbstractType;
 
 public class ColumnFamilySerializer implements ICompactSerializer2<ColumnFamily>
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
index 501256ed..36ea631d 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
@@ -49,10 +49,11 @@
 import org.apache.cassandra.dht.Bounds;
 import org.apache.cassandra.dht.Range;
 import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.io.SSTable;
-import org.apache.cassandra.io.SSTableReader;
-import org.apache.cassandra.io.SSTableScanner;
-import org.apache.cassandra.io.SSTableTracker;
+import org.apache.cassandra.io.*;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableScanner;
+import org.apache.cassandra.io.sstable.SSTableTracker;
 import org.apache.cassandra.io.util.FileUtils;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.thrift.SliceRange;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnIndexer.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnIndexer.java
index a90cb2e6..9b53b0e1 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnIndexer.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/ColumnIndexer.java
@@ -27,7 +27,7 @@
 
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.io.util.DataOutputBuffer;
-import org.apache.cassandra.io.IndexHelper;
+import org.apache.cassandra.io.sstable.IndexHelper;
 import org.apache.cassandra.utils.BloomFilter;
 import org.apache.cassandra.db.marshal.AbstractType;
 
@@ -35,7 +35,6 @@
 /**
  * Help to create an index for a column family based on size of columns
  */
-
 public class ColumnIndexer
 {
 	/**
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CompactionManager.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CompactionManager.java
index 4b0bb31f..34ba31a9 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CompactionManager.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/CompactionManager.java
@@ -32,6 +32,7 @@
 import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
 import org.apache.cassandra.dht.Range;
 import org.apache.cassandra.io.*;
+import org.apache.cassandra.io.sstable.*;
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.service.AntiEntropyService;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
index 0e9d08ea..f4cc6a8e 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
@@ -33,8 +33,8 @@
 import org.apache.cassandra.db.filter.*;
 import org.apache.cassandra.db.marshal.AbstractType;
 import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.io.SSTableReader;
-import org.apache.cassandra.io.SSTableWriter;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableWriter;
 import org.apache.cassandra.io.util.DataOutputBuffer;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.WrappedRunnable;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Table.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Table.java
index 1573c454..654a1c91 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Table.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/Table.java
@@ -30,8 +30,8 @@
 import org.apache.cassandra.db.commitlog.CommitLog;
 import org.apache.cassandra.db.commitlog.CommitLogSegment;
 import org.apache.cassandra.dht.Range;
-import org.apache.cassandra.io.SSTableDeletingReference;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableDeletingReference;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.io.util.FileUtils;
 
 import java.net.InetAddress;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/NamesQueryFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/NamesQueryFilter.java
index 913e9c4e..f5921974 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/NamesQueryFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/NamesQueryFilter.java
@@ -24,7 +24,7 @@
 import java.io.IOException;
 import java.util.*;
 
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.utils.ReducingIterator;
 import org.apache.cassandra.db.*;
 import org.apache.cassandra.db.marshal.AbstractType;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/QueryFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/QueryFilter.java
index f2afeeb1..71b7150f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/QueryFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/QueryFilter.java
@@ -26,7 +26,7 @@
 import java.util.Iterator;
 import java.util.Arrays;
 
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.utils.ReducingIterator;
 import org.apache.cassandra.db.*;
 import org.apache.cassandra.db.marshal.AbstractType;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SSTableNamesIterator.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SSTableNamesIterator.java
index baedce3a..e9e54453 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SSTableNamesIterator.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SSTableNamesIterator.java
@@ -28,7 +28,8 @@
 import org.apache.cassandra.db.DecoratedKey;
 import org.apache.cassandra.db.IColumn;
 import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.io.*;
+import org.apache.cassandra.io.sstable.IndexHelper;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.io.util.BufferedRandomAccessFile;
 import org.apache.cassandra.io.util.FileDataInput;
 import org.apache.cassandra.config.DatabaseDescriptor;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SSTableSliceIterator.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SSTableSliceIterator.java
index 90bf6ca9..f9b23977 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SSTableSliceIterator.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SSTableSliceIterator.java
@@ -28,7 +28,8 @@
 import org.apache.cassandra.db.IColumn;
 import org.apache.cassandra.db.ColumnFamily;
 import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.io.*;
+import org.apache.cassandra.io.sstable.IndexHelper;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.io.util.FileDataInput;
 import org.apache.cassandra.config.DatabaseDescriptor;
 
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java
index aecb6239..88cfc734 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java
@@ -30,7 +30,7 @@
 import org.apache.commons.collections.IteratorUtils;
 
 import com.google.common.collect.Collections2;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.db.*;
 import org.apache.cassandra.db.marshal.AbstractType;
 
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/CompactionIterator.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/CompactionIterator.java
index d875528b..adf8b5eb 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/CompactionIterator.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/CompactionIterator.java
@@ -36,6 +36,8 @@
 import org.apache.cassandra.db.ColumnFamily;
 import org.apache.cassandra.db.DecoratedKey;
 import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableScanner;
 import org.apache.cassandra.io.util.DataOutputBuffer;
 
 public class CompactionIterator extends ReducingIterator<IteratingRow, CompactionIterator.CompactedRow> implements Closeable
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IndexHelper.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IndexHelper.java
index fc628609..e69de29b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IndexHelper.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IndexHelper.java
@@ -1,158 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.*;
-import java.util.*;
-
-import org.apache.cassandra.db.ColumnSerializer;
-import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.utils.BloomFilter;
-import org.apache.cassandra.io.util.FileDataInput;
-
-
-/**
- * Provides helper to serialize, deserialize and use column indexes.
- */
-
-public class IndexHelper
-{
-
-    /**
-     * Skip the bloom filter
-     * @param in the data input from which the bloom filter should be skipped
-     * @throws IOException
-     */
-    public static void skipBloomFilter(DataInput in) throws IOException
-    {
-        /* size of the bloom filter */
-        int size = in.readInt();
-        /* skip the serialized bloom filter */
-        int skipped = in.skipBytes(size);
-        if (skipped != size)
-            throw new EOFException("attempted to skip " + size + " bytes but only skipped " + skipped);
-    }
-
-	/**
-	 * Skip the index
-	 * @param file the data input from which the index should be skipped
-	 * @throws IOException
-	 */
-	public static void skipIndex(DataInput file) throws IOException
-	{
-        /* read only the column index list */
-        int columnIndexSize = file.readInt();
-        /* skip the column index data */
-        if (file.skipBytes(columnIndexSize) != columnIndexSize)
-            throw new EOFException();
-	}
-    
-    /**
-     * Deserialize the index into a structure and return it
-     * @throws IOException
-     */
-	public static ArrayList<IndexInfo> deserializeIndex(FileDataInput in) throws IOException
-	{
-        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();
-
-		int columnIndexSize = in.readInt();
-        in.mark();
-        while (in.bytesPastMark() < columnIndexSize)
-        {
-            indexList.add(IndexInfo.deserialize(in));
-        }
-        assert in.bytesPastMark() == columnIndexSize;
-
-        return indexList;
-	}
-
-    /**
-     * Defreeze the bloom filter.
-     *
-     * @return bloom filter summarizing the column information
-     * @throws java.io.IOException
-     */
-    public static BloomFilter defreezeBloomFilter(FileDataInput file) throws IOException
-    {
-        int size = file.readInt();
-        byte[] bytes = new byte[size];
-        file.readFully(bytes);
-        
-        ByteArrayInputStream bufIn = new ByteArrayInputStream(bytes);
-        return BloomFilter.serializer().deserialize(new DataInputStream(bufIn));
-    }
-
-    /**
-     * the index of the IndexInfo in which @name will be found.
-     * If the index is @indexList.size(), the @name appears nowhere.
-     */
-    public static int indexFor(byte[] name, List<IndexInfo> indexList, AbstractType comparator, boolean reversed)
-    {
-        if (name.length == 0 && reversed)
-            return indexList.size() - 1;
-        IndexInfo target = new IndexInfo(name, name, 0, 0);
-        int index = Collections.binarySearch(indexList, target, getComparator(comparator));
-        return index < 0 ? -1 * (index + 1) : index;
-    }
-
-    public static Comparator<IndexInfo> getComparator(final AbstractType nameComparator)
-    {
-        return new Comparator<IndexInfo>()
-        {
-            public int compare(IndexInfo o1, IndexInfo o2)
-            {
-                return nameComparator.compare(o1.lastName, o2.lastName);
-            }
-        };
-    }
-
-    public static class IndexInfo
-    {
-        public final long width;
-        public final byte[] lastName;
-        public final byte[] firstName;
-        public final long offset;
-
-        public IndexInfo(byte[] firstName, byte[] lastName, long offset, long width)
-        {
-            this.firstName = firstName;
-            this.lastName = lastName;
-            this.offset = offset;
-            this.width = width;
-        }
-
-        public void serialize(DataOutput dos) throws IOException
-        {
-            ColumnSerializer.writeName(firstName, dos);
-            ColumnSerializer.writeName(lastName, dos);
-            dos.writeLong(offset);
-            dos.writeLong(width);
-        }
-
-        public int serializedSize()
-        {
-            return 2 + firstName.length + 2 + lastName.length + 8 + 8;
-        }
-
-        public static IndexInfo deserialize(FileDataInput dis) throws IOException
-        {
-            return new IndexInfo(ColumnSerializer.readName(dis), ColumnSerializer.readName(dis), dis.readLong(), dis.readLong());
-        }
-    }
-}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IteratingRow.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IteratingRow.java
index 595ca9e6..e657e0cb 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IteratingRow.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/IteratingRow.java
@@ -28,9 +28,12 @@
 import org.apache.cassandra.db.DecoratedKey;
 import org.apache.cassandra.db.IColumn;
 import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.sstable.IndexHelper;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.io.util.BufferedRandomAccessFile;
-import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.io.util.FileDataInput;
+import org.apache.cassandra.service.StorageService;
+
 import com.google.common.collect.AbstractIterator;
 
 public class IteratingRow extends AbstractIterator<IColumn> implements Comparable<IteratingRow>
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTable.java
index 786dd95f..e69de29b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTable.java
@@ -1,375 +0,0 @@
-package org.apache.cassandra.io;
-/*
- * 
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * 
- *   http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- * 
- */
-
-
-import java.io.File;
-import java.io.IOException;
-import java.util.List;
-import java.util.Arrays;
-import java.util.Map;
-import java.util.StringTokenizer;
-
-import org.apache.log4j.Logger;
-import org.apache.commons.lang.StringUtils;
-
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.utils.BloomFilter;
-import org.apache.cassandra.io.util.FileUtils;
-import org.apache.cassandra.db.DecoratedKey;
-
-import com.google.common.base.Objects;
-
-/**
- * This class is built on top of the SequenceFile. It stores
- * data on disk in sorted fashion. However the sorting is upto
- * the application. This class expects keys to be handed to it
- * in sorted order.
- *
- * A separate index file is maintained as well, containing the
- * SSTable keys and the offset into the SSTable at which they are found.
- * Every 1/indexInterval key is read into memory when the SSTable is opened.
- *
- * Finally, a bloom filter file is also kept for the keys in each SSTable.
- */
-public abstract class SSTable
-{
-    static final Logger logger = Logger.getLogger(SSTable.class);
-
-    public static final int FILES_ON_DISK = 3; // data, index, and bloom filter
-    public static final String COMPONENT_DATA = "Data.db";
-    public static final String COMPONENT_INDEX = "Index.db";
-    public static final String COMPONENT_FILTER = "Filter.db";
-
-    public static final String COMPONENT_COMPACTED = "Compacted";
-
-    protected Descriptor desc;
-    protected IPartitioner partitioner;
-    protected BloomFilter bf;
-    protected List<KeyPosition> indexPositions;
-    protected Map<KeyPosition, PositionSize> spannedIndexDataPositions; // map of index position, to data position, for index entries spanning mmap segments
-
-    /* Every 128th index entry is loaded into memory so we know where to start looking for the actual key w/o seeking */
-    public static final int INDEX_INTERVAL = 128;/* Required extension for temporary files created during compactions. */
-    public static final String TEMPFILE_MARKER = "tmp";
-
-    protected SSTable(String filename, IPartitioner partitioner)
-    {
-        assert filename.endsWith("-" + COMPONENT_DATA);
-        this.desc = Descriptor.fromFilename(filename);
-        this.partitioner = partitioner;
-    }
-
-    public Descriptor getDescriptor()
-    {
-        return desc;
-    }
-
-    protected static String parseColumnFamilyName(String filename)
-    {
-        return new File(filename).getName().split("-")[0];
-    }
-
-    public static String indexFilename(String dataFile)
-    {
-        return Descriptor.fromFilename(dataFile).filenameFor(COMPONENT_INDEX);
-    }
-
-    public String indexFilename()
-    {
-        return desc.filenameFor(COMPONENT_INDEX);
-    }
-
-    protected static String compactedFilename(String dataFile)
-    {
-        return Descriptor.fromFilename(dataFile).filenameFor(COMPONENT_COMPACTED);
-    }
-
-    /**
-     * We use a ReferenceQueue to manage deleting files that have been compacted
-     * and for which no more SSTable references exist.  But this is not guaranteed
-     * to run for each such file because of the semantics of the JVM gc.  So,
-     * we write a marker to `compactedFilename` when a file is compacted;
-     * if such a marker exists on startup, the file should be removed.
-     *
-     * @return true if the file was deleted
-     */
-    public static boolean deleteIfCompacted(String dataFilename) throws IOException
-    {
-        if (new File(compactedFilename(dataFilename)).exists())
-        {
-            FileUtils.deleteWithConfirm(new File(dataFilename));
-            FileUtils.deleteWithConfirm(new File(SSTable.indexFilename(dataFilename)));
-            FileUtils.deleteWithConfirm(new File(SSTable.filterFilename(dataFilename)));
-            FileUtils.deleteWithConfirm(new File(SSTable.compactedFilename(dataFilename)));
-            logger.info("Deleted " + dataFilename);
-            return true;
-        }
-        return false;
-    }
-
-    protected String compactedFilename()
-    {
-        return desc.filenameFor(COMPONENT_COMPACTED);
-    }
-
-    protected static String filterFilename(String dataFile)
-    {
-        return Descriptor.fromFilename(dataFile).filenameFor(COMPONENT_FILTER);
-    }
-
-    public String filterFilename()
-    {
-        return desc.filenameFor(COMPONENT_FILTER);
-    }
-
-    public String getFilename()
-    {
-        return desc.filenameFor(COMPONENT_DATA);
-    }
-
-    /** @return component names for files associated w/ this SSTable */
-    public List<String> getAllComponents()
-    {
-        // TODO streaming relies on the -Data (getFilename) file to be last, this is clunky
-        return Arrays.asList(COMPONENT_FILTER, COMPONENT_INDEX, COMPONENT_DATA);
-    }
-
-    public String getColumnFamilyName()
-    {
-        return desc.cfname;
-    }
-
-    public String getTableName()
-    {
-        return desc.ksname;
-    }
-
-    public static String parseTableName(String filename)
-    {
-        return Descriptor.fromFilename(filename).ksname;        
-    }
-
-    public static long getTotalBytes(Iterable<SSTableReader> sstables)
-    {
-        long sum = 0;
-        for (SSTableReader sstable : sstables)
-        {
-            sum += sstable.length();
-        }
-        return sum;
-    }
-
-    /**
-     * This is a simple container for the index Key and its corresponding position
-     * in the data file. Binary search is performed on a list of these objects
-     * to lookup keys within the SSTable data file.
-     */
-    public static class KeyPosition implements Comparable<KeyPosition>
-    {
-        public final DecoratedKey key;
-        public final long position;
-
-        public KeyPosition(DecoratedKey key, long position)
-        {
-            this.key = key;
-            this.position = position;
-        }
-
-        public int compareTo(KeyPosition kp)
-        {
-            return key.compareTo(kp.key);
-        }
-
-        public String toString()
-        {
-            return key + ":" + position;
-        }
-    }
-
-    public long bytesOnDisk()
-    {
-        long bytes = 0;
-        for (String cname : getAllComponents())
-        {
-            bytes += new File(desc.filenameFor(cname)).length();
-        }
-        return bytes;
-    }
-
-    @Override
-    public String toString()
-    {
-        return getClass().getName() + "(" +
-               "path='" + getFilename() + '\'' +
-               ')';
-    }
-
-    public static class PositionSize
-    {
-        public final long position;
-        public final long size;
-
-        public PositionSize(long position, long size)
-        {
-            this.position = position;
-            this.size = size;
-        }
-    }
-
-    /**
-     * A SSTable is described by the keyspace and column family it contains data
-     * for, a generation (where higher generations contain more recent data) and
-     * an alphabetic version string.
-     *
-     * A descriptor can be marked as temporary, which influences generated filenames.
-     */
-    public static class Descriptor
-    {
-        public static final String LEGACY_VERSION = "a";
-        public static final String CURRENT_VERSION = "b";
-
-        public final File directory;
-        public final String version;
-        public final String ksname;
-        public final String cfname;
-        public final int generation;
-        public final boolean temporary;
-        private final int hashCode;
-
-        /**
-         * A descriptor that assumes CURRENT_VERSION.
-         */
-        public Descriptor(File directory, String ksname, String cfname, int generation, boolean temp)
-        {
-            this(CURRENT_VERSION, directory, ksname, cfname, generation, temp);
-        }
-
-        public Descriptor(String version, File directory, String ksname, String cfname, int generation, boolean temp)
-        {
-            assert version != null && directory != null && ksname != null && cfname != null;
-            this.version = version;
-            this.directory = directory;
-            this.ksname = ksname;
-            this.cfname = cfname;
-            this.generation = generation;
-            temporary = temp;
-            hashCode = Objects.hashCode(directory, generation, ksname, cfname);
-        }
-
-        /**
-         * @param suffix A component suffix, such as 'Data.db'/'Index.db'/etc
-         * @return A filename for this descriptor with the given suffix.
-         */
-        public String filenameFor(String suffix)
-        {
-            StringBuilder buff = new StringBuilder();
-            buff.append(directory).append(File.separatorChar);
-            buff.append(cfname).append("-");
-            if (temporary)
-                buff.append(TEMPFILE_MARKER).append("-");
-            if (!LEGACY_VERSION.equals(version))
-                buff.append(version).append("-");
-            buff.append(generation).append("-");
-            buff.append(suffix);
-            return buff.toString();
-        }
-
-        /**
-         * Filename of the form "<ksname>/<cfname>-[tmp-][<version>-]<gen>-*"
-         * @param filename A full SSTable filename, including the directory.
-         * @return A SSTable.Descriptor for the filename. 
-         */
-        public static Descriptor fromFilename(String filename)
-        {
-            int separatorPos = filename.lastIndexOf(File.separatorChar);
-            assert separatorPos != -1 : "Filename must include parent directory.";
-            File directory = new File(filename.substring(0, separatorPos));
-            String name = filename.substring(separatorPos+1, filename.length());
-
-            // name of parent directory is keyspace name
-            String ksname = directory.getName();
-
-            // tokenize the filename
-            StringTokenizer st = new StringTokenizer(name, "-");
-            String nexttok = null;
-            
-            // all filenames must start with a column family
-            String cfname = st.nextToken();
-
-            // optional temporary marker
-            nexttok = st.nextToken();
-            boolean temporary = false;
-            if (nexttok.equals(TEMPFILE_MARKER))
-            {
-                temporary = true;
-                nexttok = st.nextToken();
-            }
-
-            // optional version string
-            String version = LEGACY_VERSION;
-            if (versionValidate(nexttok))
-            {
-                version = nexttok;
-                nexttok = st.nextToken();
-            }
-            int generation = Integer.parseInt(nexttok);
-
-            return new Descriptor(version, directory, ksname, cfname, generation, temporary);
-        }
-        
-        /**
-         * @return True if the given version string is not empty, and
-         * contains all lowercase letters, as defined by java.lang.Character.
-         */
-        private static boolean versionValidate(String ver)
-        {
-            if (ver.length() < 1) return false;
-            for (char ch : ver.toCharArray())
-                if (!Character.isLetter(ch) || !Character.isLowerCase(ch))
-                    return false;
-            return true;
-        }
-
-        @Override
-        public String toString()
-        {
-            return this.filenameFor("<>");
-        }
-
-        @Override
-        public boolean equals(Object o)
-        {
-            if (o == this)
-                return true;
-            if (!(o instanceof Descriptor))
-                return false;
-            Descriptor that = (Descriptor)o;
-            return that.directory.equals(this.directory) && that.generation == this.generation && that.ksname.equals(this.ksname) && that.cfname.equals(this.cfname);
-        }
-
-        @Override
-        public int hashCode()
-        {
-            return hashCode;
-        }
-    }
-}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableDeletingReference.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableDeletingReference.java
index a7f2b38a..e69de29b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableDeletingReference.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableDeletingReference.java
@@ -1,107 +0,0 @@
-package org.apache.cassandra.io;
-/*
- * 
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * 
- *   http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- * 
- */
-
-
-import java.io.File;
-import java.io.IOError;
-import java.io.IOException;
-import java.lang.ref.PhantomReference;
-import java.lang.ref.ReferenceQueue;
-import java.util.Timer;
-import java.util.TimerTask;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.io.util.FileUtils;
-
-public class SSTableDeletingReference extends PhantomReference<SSTableReader>
-{
-    private static final Logger logger = Logger.getLogger(SSTableDeletingReference.class);
-
-    private static final Timer timer = new Timer("SSTABLE-CLEANUP-TIMER");
-    public static final int RETRY_DELAY = 10000;
-
-    private final SSTableTracker tracker;
-    public final String path;
-    private final long size;
-    private boolean deleteOnCleanup;
-
-    SSTableDeletingReference(SSTableTracker tracker, SSTableReader referent, ReferenceQueue<? super SSTableReader> q)
-    {
-        super(referent, q);
-        this.tracker = tracker;
-        this.path = referent.getFilename();
-        this.size = referent.bytesOnDisk();
-    }
-
-    public void deleteOnCleanup()
-    {
-        deleteOnCleanup = true;
-    }
-
-    public void cleanup() throws IOException
-    {
-        if (deleteOnCleanup)
-        {
-            // this is tricky because the mmapping might not have been finalized yet,
-            // and delete will fail until it is.  additionally, we need to make sure to
-            // delete the data file first, so on restart the others will be recognized as GCable
-            // even if the compaction marker gets deleted next.
-            timer.schedule(new CleanupTask(), RETRY_DELAY);
-        }
-    }
-
-    private class CleanupTask extends TimerTask
-    {
-        int attempts = 0;
-
-        @Override
-        public void run()
-        {
-            File datafile = new File(path);
-            if (!datafile.delete())
-            {
-                if (attempts++ < DeletionService.MAX_RETRIES)
-                {
-                    timer.schedule(this, RETRY_DELAY);
-                    return;
-                }
-                else
-                {
-                    throw new RuntimeException("Unable to delete " + path);
-                }
-            }
-            try
-            {
-                FileUtils.deleteWithConfirm(new File(SSTable.indexFilename(path)));
-                FileUtils.deleteWithConfirm(new File(SSTable.filterFilename(path)));
-                FileUtils.deleteWithConfirm(new File(SSTable.compactedFilename(path)));
-            }
-            catch (IOException e)
-            {
-                throw new IOError(e);
-            }
-            tracker.spaceReclaimed(size);
-            logger.info("Deleted " + path);
-        }
-    }
-}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableReader.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableReader.java
index 7a335000..e69de29b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableReader.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableReader.java
@@ -1,569 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.*;
-import java.util.*;
-import java.lang.ref.ReferenceQueue;
-import java.lang.ref.Reference;
-import java.nio.channels.FileChannel;
-import java.nio.MappedByteBuffer;
-
-import org.apache.log4j.Logger;
-
-import org.apache.commons.lang.StringUtils;
-
-import org.apache.cassandra.cache.InstrumentedCache;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.utils.BloomFilter;
-import org.apache.cassandra.utils.Pair;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.*;
-import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.io.util.BufferedRandomAccessFile;
-import org.apache.cassandra.io.util.FileDataInput;
-import org.apache.cassandra.io.util.MappedFileDataInput;
-
-import org.cliffc.high_scale_lib.NonBlockingHashMap;
-
-/**
- * SSTableReaders are open()ed by Table.onStart; after that they are created by SSTableWriter.renameAndOpen.
- * Do not re-call open() on existing SSTable files; use the references kept by ColumnFamilyStore post-start instead.
- */
-public class SSTableReader extends SSTable implements Comparable<SSTableReader>
-{
-    private static final Logger logger = Logger.getLogger(SSTableReader.class);
-
-    // `finalizers` is required to keep the PhantomReferences alive after the enclosing SSTR is itself
-    // unreferenced.  otherwise they will never get enqueued.
-    private static final Set<Reference<SSTableReader>> finalizers = new HashSet<Reference<SSTableReader>>();
-    private static final ReferenceQueue<SSTableReader> finalizerQueue = new ReferenceQueue<SSTableReader>()
-    {{
-        Runnable runnable = new Runnable()
-        {
-            public void run()
-            {
-                while (true)
-                {
-                    SSTableDeletingReference r = null;
-                    try
-                    {
-                        r = (SSTableDeletingReference) finalizerQueue.remove();
-                        finalizers.remove(r);
-                    }
-                    catch (InterruptedException e)
-                    {
-                        throw new RuntimeException(e);
-                    }
-                    try
-                    {
-                        r.cleanup();
-                    }
-                    catch (IOException e)
-                    {
-                        logger.error("Error deleting " + r.path, e);
-                    }
-                }
-            }
-        };
-        new Thread(runnable, "SSTABLE-DELETER").start();
-    }};
-    private static final long BUFFER_SIZE = Integer.MAX_VALUE;
-
-    public static int indexInterval()
-    {
-        return INDEX_INTERVAL;
-    }
-
-    public static long getApproximateKeyCount(Iterable<SSTableReader> sstables)
-    {
-        long count = 0;
-
-        for (SSTableReader sstable : sstables)
-        {
-            int indexKeyCount = sstable.getIndexPositions().size();
-            count = count + (indexKeyCount + 1) * INDEX_INTERVAL;
-            if (logger.isDebugEnabled())
-                logger.debug("index size for bloom filter calc for file  : " + sstable.getFilename() + "   : " + count);
-        }
-
-        return count;
-    }
-
-    public static SSTableReader open(Descriptor desc) throws IOException
-    {
-        return open(desc.filenameFor(COMPONENT_DATA));
-    }
-
-    public static SSTableReader open(String dataFileName) throws IOException
-    {
-        return open(dataFileName, StorageService.getPartitioner());
-    }
-
-    public static SSTableReader open(String dataFileName, IPartitioner partitioner) throws IOException
-    {
-        assert partitioner != null;
-
-        long start = System.currentTimeMillis();
-        SSTableReader sstable = new SSTableReader(dataFileName, partitioner);
-        logger.info("Sampling index for " + dataFileName);
-        sstable.loadIndexFile();
-        sstable.loadBloomFilter();
-
-        if (logger.isDebugEnabled())
-            logger.debug("INDEX LOAD TIME for "  + dataFileName + ": " + (System.currentTimeMillis() - start) + " ms.");
-
-        return sstable;
-    }
-
-    private volatile SSTableDeletingReference phantomReference;
-    // jvm can only map up to 2GB at a time, so we split index/data into segments of that size when using mmap i/o
-    private final MappedByteBuffer[] indexBuffers;
-    private final MappedByteBuffer[] buffers;
-
-    private InstrumentedCache<Pair<Descriptor,DecoratedKey>,PositionSize> keyCache;
-
-    SSTableReader(String filename,
-                  IPartitioner partitioner,
-                  List<KeyPosition> indexPositions, Map<KeyPosition, PositionSize> spannedIndexDataPositions,
-                  BloomFilter bloomFilter)
-    throws IOException
-    {
-        super(filename, partitioner);
-
-        if (DatabaseDescriptor.getIndexAccessMode() == DatabaseDescriptor.DiskAccessMode.mmap)
-        {
-            long indexLength = new File(indexFilename()).length();
-            int bufferCount = 1 + (int) (indexLength / BUFFER_SIZE);
-            indexBuffers = new MappedByteBuffer[bufferCount];
-            long remaining = indexLength;
-            for (int i = 0; i < bufferCount; i++)
-            {
-                indexBuffers[i] = mmap(indexFilename(), i * BUFFER_SIZE, (int) Math.min(remaining, BUFFER_SIZE));
-                remaining -= BUFFER_SIZE;
-            }
-        }
-        else
-        {
-            assert DatabaseDescriptor.getIndexAccessMode() == DatabaseDescriptor.DiskAccessMode.standard;
-            indexBuffers = null;
-        }
-
-        if (DatabaseDescriptor.getDiskAccessMode() == DatabaseDescriptor.DiskAccessMode.mmap)
-        {
-            int bufferCount = 1 + (int) (new File(getFilename()).length() / BUFFER_SIZE);
-            buffers = new MappedByteBuffer[bufferCount];
-            long remaining = length();
-            for (int i = 0; i < bufferCount; i++)
-            {
-                buffers[i] = mmap(getFilename(), i * BUFFER_SIZE, (int) Math.min(remaining, BUFFER_SIZE));
-                remaining -= BUFFER_SIZE;
-            }
-        }
-        else
-        {
-            assert DatabaseDescriptor.getDiskAccessMode() == DatabaseDescriptor.DiskAccessMode.standard;
-            buffers = null;
-        }
-
-        this.indexPositions = indexPositions;
-        this.spannedIndexDataPositions = spannedIndexDataPositions;
-        this.bf = bloomFilter;
-    }
-
-    public void setTrackedBy(SSTableTracker tracker)
-    {
-        phantomReference = new SSTableDeletingReference(tracker, this, finalizerQueue);
-        finalizers.add(phantomReference);
-        keyCache = tracker.getKeyCache();
-    }
-
-    private static MappedByteBuffer mmap(String filename, long start, int size) throws IOException
-    {
-        RandomAccessFile raf;
-        try
-        {
-            raf = new RandomAccessFile(filename, "r");
-        }
-        catch (FileNotFoundException e)
-        {
-            throw new IOError(e);
-        }
-
-        try
-        {
-            return raf.getChannel().map(FileChannel.MapMode.READ_ONLY, start, size);
-        }
-        finally
-        {
-            raf.close();
-        }
-    }
-
-    private SSTableReader(String filename, IPartitioner partitioner) throws IOException
-    {
-        this(filename, partitioner, null, null, null);
-    }
-
-    public List<KeyPosition> getIndexPositions()
-    {
-        return indexPositions;
-    }
-
-    public long estimatedKeys()
-    {
-        return indexPositions.size() * INDEX_INTERVAL;
-    }
-
-    void loadBloomFilter() throws IOException
-    {
-        DataInputStream stream = new DataInputStream(new FileInputStream(filterFilename()));
-        try
-        {
-            bf = BloomFilter.serializer().deserialize(stream);
-        }
-        finally
-        {
-            stream.close();
-        }
-    }
-
-    void loadIndexFile() throws IOException
-    {
-        indexPositions = new ArrayList<KeyPosition>();
-        // we read the positions in a BRAF so we don't have to worry about an entry spanning a mmap boundary.
-        // any entries that do, we force into the in-memory sample so key lookup can always bsearch within
-        // a single mmapped segment.
-        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(), "r");
-        try
-        {
-            int i = 0;
-            long indexSize = input.length();
-            while (true)
-            {
-                long indexPosition = input.getFilePointer();
-                if (indexPosition == indexSize)
-                {
-                    break;
-                }
-                DecoratedKey decoratedKey = partitioner.convertFromDiskFormat(input.readUTF());
-                long dataPosition = input.readLong();
-                long nextIndexPosition = input.getFilePointer();
-                boolean spannedEntry = bufferIndex(indexPosition) != bufferIndex(nextIndexPosition);
-                if (i++ % INDEX_INTERVAL == 0 || spannedEntry)
-                {
-                    KeyPosition info;
-                    info = new KeyPosition(decoratedKey, indexPosition);
-                    indexPositions.add(info);
-
-                    if (spannedEntry)
-                    {
-                        if (spannedIndexDataPositions == null)
-                        {
-                            spannedIndexDataPositions = new HashMap<KeyPosition, PositionSize>();
-                        }
-                        // read the next index entry to see how big the row is corresponding to the current, mmap-segment-spanning one
-                        input.readUTF();
-                        long nextDataPosition = input.readLong();
-                        input.seek(nextIndexPosition);
-                        spannedIndexDataPositions.put(info, new PositionSize(dataPosition, nextDataPosition - dataPosition));
-                    }
-                }
-            }
-        }
-        finally
-        {
-            input.close();
-        }
-    }
-
-    /** get the position in the index file to start scanning to find the given key (at most indexInterval keys away) */
-    private KeyPosition getIndexScanPosition(DecoratedKey decoratedKey)
-    {
-        assert indexPositions != null && indexPositions.size() > 0;
-        int index = Collections.binarySearch(indexPositions, new KeyPosition(decoratedKey, -1));
-        if (index < 0)
-        {
-            // binary search gives us the first index _greater_ than the key searched for,
-            // i.e., its insertion position
-            int greaterThan = (index + 1) * -1;
-            if (greaterThan == 0)
-                return null;
-            return indexPositions.get(greaterThan - 1);
-        }
-        else
-        {
-            return indexPositions.get(index);
-        }
-    }
-
-    /**
-     * returns the position in the data file to find the given key, or -1 if the key is not present
-     */
-    public PositionSize getPosition(DecoratedKey decoratedKey) throws IOException
-    {
-        // first, check bloom filter
-        if (!bf.isPresent(partitioner.convertToDiskFormat(decoratedKey)))
-            return null;
-
-        // next, the key cache
-        Pair<Descriptor, DecoratedKey> unifiedKey = new Pair<Descriptor, DecoratedKey>(desc, decoratedKey);
-        if (keyCache != null && keyCache.getCapacity() > 0)
-        {
-            PositionSize cachedPosition = keyCache.get(unifiedKey);
-            if (cachedPosition != null)
-            {
-                return cachedPosition;
-            }
-        }
-
-        // next, see if the sampled index says it's impossible for the key to be present
-        KeyPosition sampledPosition = getIndexScanPosition(decoratedKey);
-        if (sampledPosition == null)
-        {
-            return null;
-        }
-
-        // handle exact sampled index hit
-        if (spannedIndexDataPositions != null)
-        {
-            PositionSize info = spannedIndexDataPositions.get(sampledPosition);
-            if (info != null)
-                return info;
-        }
-
-        // scan the on-disk index, starting at the nearest sampled position
-        long p = sampledPosition.position;
-        FileDataInput input;
-        if (indexBuffers == null)
-        {
-            input = new BufferedRandomAccessFile(indexFilename(), "r");
-            ((BufferedRandomAccessFile)input).seek(p);
-        }
-        else
-        {
-            input = new MappedFileDataInput(indexBuffers[bufferIndex(p)], indexFilename(), (int)(p % BUFFER_SIZE));
-        }
-        try
-        {
-            int i = 0;
-            do
-            {
-                DecoratedKey indexDecoratedKey;
-                try
-                {
-                    indexDecoratedKey = partitioner.convertFromDiskFormat(input.readUTF());
-                }
-                catch (EOFException e)
-                {
-                    return null;
-                }
-                long position = input.readLong();
-                int v = indexDecoratedKey.compareTo(decoratedKey);
-                if (v == 0)
-                {
-                    PositionSize info;
-                    if (!input.isEOF())
-                    {
-                        int utflen = input.readUnsignedShort();
-                        if (utflen != input.skipBytes(utflen))
-                            throw new EOFException();
-                        info = new PositionSize(position, input.readLong() - position);
-                    }
-                    else
-                    {
-                        info = new PositionSize(position, length() - position);
-                    }
-                    if (keyCache != null && keyCache.getCapacity() > 0)
-                        keyCache.put(unifiedKey, info);
-                    return info;
-                }
-                if (v > 0)
-                    return null;
-            } while  (++i < INDEX_INTERVAL);
-        }
-        finally
-        {
-            input.close();
-        }
-        return null;
-    }
-
-    /** like getPosition, but if key is not found will return the location of the first key _greater_ than the desired one, or -1 if no such key exists. */
-    public long getNearestPosition(DecoratedKey decoratedKey) throws IOException
-    {
-        KeyPosition sampledPosition = getIndexScanPosition(decoratedKey);
-        if (sampledPosition == null)
-        {
-            return 0;
-        }
-
-        // can't use a MappedFileDataInput here, since we might cross a segment boundary while scanning
-        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(), "r");
-        input.seek(sampledPosition.position);
-        try
-        {
-            while (true)
-            {
-                DecoratedKey indexDecoratedKey;
-                try
-                {
-                    indexDecoratedKey = partitioner.convertFromDiskFormat(input.readUTF());
-                }
-                catch (EOFException e)
-                {
-                    return -1;
-                }
-                long position = input.readLong();
-                int v = indexDecoratedKey.compareTo(decoratedKey);
-                if (v >= 0)
-                    return position;
-            }
-        }
-        finally
-        {
-            input.close();
-        }
-    }
-
-    public long length()
-    {
-        return new File(getFilename()).length();
-    }
-
-    public int compareTo(SSTableReader o)
-    {
-        return desc.generation - o.desc.generation;
-    }
-
-    public void markCompacted() throws IOException
-    {
-        if (logger.isDebugEnabled())
-            logger.debug("Marking " + getFilename() + " compacted");
-        if (!new File(compactedFilename()).createNewFile())
-        {
-            throw new IOException("Unable to create compaction marker");
-        }
-        phantomReference.deleteOnCleanup();
-    }
-
-    /** obviously only for testing */
-    public void forceBloomFilterFailures()
-    {
-        bf = BloomFilter.alwaysMatchingBloomFilter();
-    }
-
-    public IPartitioner getPartitioner()
-    {
-        return partitioner;
-    }
-
-    public SSTableScanner getScanner(int bufferSize) throws IOException
-    {
-        return new SSTableScanner(this, bufferSize);
-    }
-
-    public FileDataInput getFileDataInput(DecoratedKey decoratedKey, int bufferSize) throws IOException
-    {
-        PositionSize info = getPosition(decoratedKey);
-        if (info == null)
-            return null;
-
-        if (buffers == null || (bufferIndex(info.position) != bufferIndex(info.position + info.size)))
-        {
-            BufferedRandomAccessFile file = new BufferedRandomAccessFile(getFilename(), "r", bufferSize);
-            file.seek(info.position);
-            return file;
-        }
-        return new MappedFileDataInput(buffers[bufferIndex(info.position)], getFilename(), (int) (info.position % BUFFER_SIZE));
-    }
-
-    static int bufferIndex(long position)
-    {
-        return (int) (position / BUFFER_SIZE);
-    }
-
-    public AbstractType getColumnComparator()
-    {
-        return DatabaseDescriptor.getComparator(getTableName(), getColumnFamilyName());
-    }
-
-    public ColumnFamily makeColumnFamily()
-    {
-        return ColumnFamily.create(getTableName(), getColumnFamilyName());
-    }
-
-    public ICompactSerializer2<IColumn> getColumnSerializer()
-    {
-        return DatabaseDescriptor.getColumnFamilyType(getTableName(), getColumnFamilyName()).equals("Standard")
-               ? Column.serializer()
-               : SuperColumn.serializer(getColumnComparator());
-    }
-}
-
-class FileSSTableMap
-{
-    private final Map<String, SSTableReader> map = new NonBlockingHashMap<String, SSTableReader>();
-
-    public SSTableReader get(String filename)
-    {
-        try
-        {
-            return map.get(new File(filename).getCanonicalPath());
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    public SSTableReader put(String filename, SSTableReader value)
-    {
-        try
-        {
-            return map.put(new File(filename).getCanonicalPath(), value);
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    public Collection<SSTableReader> values()
-    {
-        return map.values();
-    }
-
-    public void clear()
-    {
-        map.clear();
-    }
-
-    public void remove(String filename) throws IOException
-    {
-        map.remove(new File(filename).getCanonicalPath());
-    }
-
-    @Override
-    public String toString()
-    {
-        return "FileSSTableMap {" + StringUtils.join(map.keySet(), ", ") + "}";
-    }
-}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableScanner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableScanner.java
index 7442e364..e69de29b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableScanner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableScanner.java
@@ -1,149 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.IOException;
-import java.io.Closeable;
-import java.io.IOError;
-import java.util.Iterator;
-import java.util.Arrays;
-
-import org.apache.cassandra.db.DecoratedKey;
-import org.apache.cassandra.io.util.BufferedRandomAccessFile;
-import org.apache.cassandra.io.util.FileDataInput;
-
-import org.apache.log4j.Logger;
-
-
-public class SSTableScanner implements Iterator<IteratingRow>, Closeable
-{
-    private static Logger logger = Logger.getLogger(SSTableScanner.class);
-
-    private final BufferedRandomAccessFile file;
-    private final SSTableReader sstable;
-    private IteratingRow row;
-    private boolean exhausted = false;
-    private Iterator<IteratingRow> iterator;
-
-    /**
-     * @param sstable SSTable to scan.
-     */
-    SSTableScanner(SSTableReader sstable, int bufferSize) throws IOException
-    {
-        this.file = new BufferedRandomAccessFile(sstable.getFilename(), "r", bufferSize);
-        this.sstable = sstable;
-    }
-
-    public void close() throws IOException
-    {
-        file.close();
-    }
-
-    public void seekTo(DecoratedKey seekKey)
-    {
-        try
-        {
-            long position = sstable.getNearestPosition(seekKey);
-            if (position < 0)
-            {
-                exhausted = true;
-                return;
-            }
-            file.seek(position);
-            row = null;
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException("corrupt sstable", e);
-        }
-    }
-
-    public long getFileLength()
-    {
-        try
-        {
-            return file.length();
-        }
-        catch (IOException e)
-        {
-            throw new IOError(e);
-        }
-    }
-
-    public long getFilePointer()
-    {
-        return file.getFilePointer();
-    }
-
-    public boolean hasNext()
-    {
-        if (iterator == null)
-            iterator = exhausted ? Arrays.asList(new IteratingRow[0]).iterator() : new KeyScanningIterator();
-        return iterator.hasNext();
-    }
-
-    public IteratingRow next()
-    {
-        if (iterator == null)
-            iterator = exhausted ? Arrays.asList(new IteratingRow[0]).iterator() : new KeyScanningIterator();
-        return iterator.next();
-    }
-
-    public void remove()
-    {
-        throw new UnsupportedOperationException();
-    }
-
-    private class KeyScanningIterator implements Iterator<IteratingRow>
-    {
-        public boolean hasNext()
-        {
-            try
-            {
-                if (row == null)
-                    return !file.isEOF();
-                return row.getEndPosition() < file.length();
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-
-        public IteratingRow next()
-        {
-            try
-            {
-                if (row != null)
-                    row.skipRemaining();
-                assert !file.isEOF();
-                return row = new IteratingRow(file, sstable);
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-
-        public void remove()
-        {
-            throw new UnsupportedOperationException();
-        }
-    }
-}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableTracker.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableTracker.java
index ac5fbcb9..e69de29b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableTracker.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableTracker.java
@@ -1,185 +0,0 @@
-package org.apache.cassandra.io;
-/*
- * 
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * 
- *   http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- * 
- */
-
-
-import java.util.*;
-import java.io.IOException;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.cassandra.cache.JMXInstrumentedCache;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.ColumnFamily;
-import org.apache.cassandra.db.DecoratedKey;
-import org.apache.cassandra.utils.Pair;
-
-import org.apache.log4j.Logger;
-
-public class SSTableTracker implements Iterable<SSTableReader>
-{
-    private static final Logger logger = Logger.getLogger(SSTableTracker.class);
-
-    private volatile Set<SSTableReader> sstables;
-    private final AtomicLong liveSize = new AtomicLong();
-    private final AtomicLong totalSize = new AtomicLong();
-
-    private final String ksname;
-    private final String cfname;
-
-    private final JMXInstrumentedCache<Pair<SSTable.Descriptor,DecoratedKey>,SSTable.PositionSize> keyCache;
-    private final JMXInstrumentedCache<String, ColumnFamily> rowCache;
-
-    public SSTableTracker(String ksname, String cfname)
-    {
-        this.ksname = ksname;
-        this.cfname = cfname;
-        sstables = Collections.emptySet();
-        keyCache = new JMXInstrumentedCache<Pair<SSTable.Descriptor,DecoratedKey>,SSTable.PositionSize>(ksname, cfname + "KeyCache", 0);
-        rowCache = new JMXInstrumentedCache<String, ColumnFamily>(ksname, cfname + "RowCache", 0);
-    }
-
-    public synchronized void replace(Collection<SSTableReader> oldSSTables, Iterable<SSTableReader> replacements) throws IOException
-    {
-        Set<SSTableReader> sstablesNew = new HashSet<SSTableReader>(sstables);
-
-        for (SSTableReader sstable : replacements)
-        {
-            assert sstable.getIndexPositions() != null;
-            sstablesNew.add(sstable);
-            long size = sstable.bytesOnDisk();
-            liveSize.addAndGet(size);
-            totalSize.addAndGet(size);
-            sstable.setTrackedBy(this);
-        }
-
-        for (SSTableReader sstable : oldSSTables)
-        {
-            boolean removed = sstablesNew.remove(sstable);
-            assert removed;
-            sstable.markCompacted();
-            liveSize.addAndGet(-sstable.bytesOnDisk());
-        }
-
-        sstables = Collections.unmodifiableSet(sstablesNew);
-        updateCacheSizes();
-    }
-
-    public synchronized void add(Iterable<SSTableReader> sstables)
-    {
-        assert sstables != null;
-        try
-        {
-            replace(Collections.<SSTableReader>emptyList(), sstables);
-        }
-        catch (IOException e)
-        {
-            throw new AssertionError(e);
-        }
-    }
-
-    public synchronized void markCompacted(Collection<SSTableReader> compacted) throws IOException
-    {
-        replace(compacted, Collections.<SSTableReader>emptyList());
-    }
-
-    /**
-     * Resizes the key and row caches based on the current key estimate.
-     */
-    public synchronized void updateCacheSizes()
-    {
-        long keys = estimatedKeys();
-        
-        int keyCacheSize = DatabaseDescriptor.getKeysCachedFor(ksname, cfname, keys);
-        if (keyCacheSize != keyCache.getCapacity())
-        {
-            // update cache size for the new key volume
-            if (logger.isDebugEnabled())
-                logger.debug("key cache capacity for " + cfname + " is " + keyCacheSize);
-            keyCache.setCapacity(keyCacheSize);
-        }
-
-        int rowCacheSize = DatabaseDescriptor.getRowsCachedFor(ksname, cfname, keys);
-        if (rowCacheSize != rowCache.getCapacity())
-        {   
-            if (logger.isDebugEnabled())
-                logger.debug("row cache capacity for " + cfname + " is " + rowCacheSize);
-            rowCache.setCapacity(rowCacheSize);
-        }
-    }
-
-    // the modifiers create new, unmodifiable objects each time; the volatile fences the assignment
-    // so we don't need any further synchronization for the common case here
-    public Set<SSTableReader> getSSTables()
-    {
-        return sstables;
-    }
-
-    public int size()
-    {
-        return sstables.size();
-    }
-
-    public Iterator<SSTableReader> iterator()
-    {
-        return sstables.iterator();
-    }
-
-    public synchronized void clearUnsafe()
-    {
-        sstables = Collections.emptySet();
-    }
-
-    public JMXInstrumentedCache<String, ColumnFamily> getRowCache()
-    {
-        return rowCache;
-    }
-
-    public long estimatedKeys()
-    {
-        long n = 0;
-        for (SSTableReader sstable : this)
-        {
-            n += sstable.estimatedKeys();
-        }
-        return n;
-    }
-
-    public long getLiveSize()
-    {
-        return liveSize.get();
-    }
-
-    public long getTotalSize()
-    {
-        return totalSize.get();
-    }
-
-    public void spaceReclaimed(long size)
-    {
-        totalSize.addAndGet(-size);
-    }
-
-    public JMXInstrumentedCache<Pair<SSTable.Descriptor, DecoratedKey>, SSTable.PositionSize> getKeyCache()
-    {
-        return keyCache;
-    }
-}
-
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableWriter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableWriter.java
index 154891a1..e69de29b 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableWriter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/SSTableWriter.java
@@ -1,185 +0,0 @@
-package org.apache.cassandra.io;
-/*
- * 
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * 
- *   http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- * 
- */
-
-
-import java.io.DataOutputStream;
-import java.io.FileOutputStream;
-import java.io.IOError;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.DecoratedKey;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.io.util.BufferedRandomAccessFile;
-import org.apache.cassandra.io.util.DataOutputBuffer;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.BloomFilter;
-import org.apache.cassandra.utils.FBUtilities;
-
-public class SSTableWriter extends SSTable
-{
-    private static Logger logger = Logger.getLogger(SSTableWriter.class);
-
-    private long keysWritten;
-    private BufferedRandomAccessFile dataFile;
-    private BufferedRandomAccessFile indexFile;
-    private DecoratedKey lastWrittenKey;
-    private BloomFilter bf;
-
-    public SSTableWriter(String filename, long keyCount, IPartitioner partitioner) throws IOException
-    {
-        super(filename, partitioner);
-        dataFile = new BufferedRandomAccessFile(getFilename(), "rw", (int)(DatabaseDescriptor.getFlushDataBufferSizeInMB() * 1024 * 1024));
-        indexFile = new BufferedRandomAccessFile(indexFilename(), "rw", (int)(DatabaseDescriptor.getFlushIndexBufferSizeInMB() * 1024 * 1024));
-        bf = BloomFilter.getFilter(keyCount, 15);
-    }
-
-    private long beforeAppend(DecoratedKey decoratedKey) throws IOException
-    {
-        if (decoratedKey == null)
-        {
-            throw new IOException("Keys must not be null.");
-        }
-        if (lastWrittenKey != null && lastWrittenKey.compareTo(decoratedKey) > 0)
-        {
-            logger.info("Last written key : " + lastWrittenKey);
-            logger.info("Current key : " + decoratedKey);
-            logger.info("Writing into file " + getFilename());
-            throw new IOException("Keys must be written in ascending order.");
-        }
-        return (lastWrittenKey == null) ? 0 : dataFile.getFilePointer();
-    }
-
-    private void afterAppend(DecoratedKey decoratedKey, long dataPosition, int dataSize) throws IOException
-    {
-        String diskKey = partitioner.convertToDiskFormat(decoratedKey);
-        bf.add(diskKey);
-        lastWrittenKey = decoratedKey;
-        long indexPosition = indexFile.getFilePointer();
-        indexFile.writeUTF(diskKey);
-        indexFile.writeLong(dataPosition);
-        if (logger.isTraceEnabled())
-            logger.trace("wrote " + decoratedKey + " at " + dataPosition);
-        if (logger.isTraceEnabled())
-            logger.trace("wrote index of " + decoratedKey + " at " + indexPosition);
-
-        boolean spannedEntry = SSTableReader.bufferIndex(indexPosition) != SSTableReader.bufferIndex(indexFile.getFilePointer());
-        if (keysWritten++ % INDEX_INTERVAL == 0 || spannedEntry)
-        {
-            if (indexPositions == null)
-            {
-                indexPositions = new ArrayList<KeyPosition>();
-            }
-            KeyPosition info = new KeyPosition(decoratedKey, indexPosition);
-            indexPositions.add(info);
-
-            if (spannedEntry)
-            {
-                if (spannedIndexDataPositions == null)
-                {
-                    spannedIndexDataPositions = new HashMap<KeyPosition, PositionSize>();
-                }
-                spannedIndexDataPositions.put(info, new PositionSize(dataPosition, dataSize));
-            }
-        }
-    }
-
-    // TODO make this take a DataOutputStream and wrap the byte[] version to combine them
-    public void append(DecoratedKey decoratedKey, DataOutputBuffer buffer) throws IOException
-    {
-        long currentPosition = beforeAppend(decoratedKey);
-        dataFile.writeUTF(partitioner.convertToDiskFormat(decoratedKey));
-        int length = buffer.getLength();
-        assert length > 0;
-        dataFile.writeInt(length);
-        dataFile.write(buffer.getData(), 0, length);
-        afterAppend(decoratedKey, currentPosition, length);
-    }
-
-    public void append(DecoratedKey decoratedKey, byte[] value) throws IOException
-    {
-        long currentPosition = beforeAppend(decoratedKey);
-        dataFile.writeUTF(partitioner.convertToDiskFormat(decoratedKey));
-        assert value.length > 0;
-        dataFile.writeInt(value.length);
-        dataFile.write(value);
-        afterAppend(decoratedKey, currentPosition, value.length);
-    }
-
-    public SSTableReader closeAndOpenReader() throws IOException
-    {
-        // bloom filter
-        FileOutputStream fos = new FileOutputStream(filterFilename());
-        DataOutputStream stream = new DataOutputStream(fos);
-        BloomFilter.serializer().serialize(bf, stream);
-        stream.flush();
-        fos.getFD().sync();
-        stream.close();
-
-        // index
-        indexFile.getChannel().force(true);
-        indexFile.close();
-
-        // main data
-        dataFile.close(); // calls force
-
-        String newpath = getFilename();
-        rename(indexFilename());
-        rename(filterFilename());
-        newpath = rename(newpath); // important to do this last since index & filter file names are derived from it
-
-        return new SSTableReader(newpath, partitioner, indexPositions, spannedIndexDataPositions, bf);
-    }
-
-    static String rename(String tmpFilename)
-    {
-        String filename = tmpFilename.replace("-" + SSTable.TEMPFILE_MARKER, "");
-        try
-        {
-            FBUtilities.renameWithConfirm(tmpFilename, filename);
-        }
-        catch (IOException e)
-        {
-            throw new IOError(e);
-        }
-        return filename;
-    }
-
-    public long getFilePointer()
-    {
-        return dataFile.getFilePointer();
-    }
-    
-    public static SSTableReader renameAndOpen(String dataFileName) throws IOException
-    {
-        SSTableWriter.rename(indexFilename(dataFileName));
-        SSTableWriter.rename(filterFilename(dataFileName));
-        dataFileName = SSTableWriter.rename(dataFileName);
-        return SSTableReader.open(dataFileName,
-                                  StorageService.getPartitioner());
-    }
-
-}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/IndexHelper.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/IndexHelper.java
index 3f24d583..019237e4 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/IndexHelper.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/IndexHelper.java
@@ -1 +1,158 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.*;
+import java.util.*;
+
+import org.apache.cassandra.db.ColumnSerializer;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.io.util.FileDataInput;
+
+
+/**
+ * Provides helper to serialize, deserialize and use column indexes.
+ */
+public class IndexHelper
+{
+
+    /**
+     * Skip the bloom filter
+     * @param in the data input from which the bloom filter should be skipped
+     * @throws IOException
+     */
+    public static void skipBloomFilter(DataInput in) throws IOException
+    {
+        /* size of the bloom filter */
+        int size = in.readInt();
+        /* skip the serialized bloom filter */
+        int skipped = in.skipBytes(size);
+        if (skipped != size)
+            throw new EOFException("attempted to skip " + size + " bytes but only skipped " + skipped);
+    }
+
+	/**
+	 * Skip the index
+	 * @param file the data input from which the index should be skipped
+	 * @throws IOException
+	 */
+	public static void skipIndex(DataInput file) throws IOException
+	{
+        /* read only the column index list */
+        int columnIndexSize = file.readInt();
+        /* skip the column index data */
+        if (file.skipBytes(columnIndexSize) != columnIndexSize)
+            throw new EOFException();
+	}
+    
+    /**
+     * Deserialize the index into a structure and return it
+     * @throws IOException
+     */
+	public static ArrayList<IndexInfo> deserializeIndex(FileDataInput in) throws IOException
+	{
+        ArrayList<IndexInfo> indexList = new ArrayList<IndexInfo>();
+
+		int columnIndexSize = in.readInt();
+        in.mark();
+        while (in.bytesPastMark() < columnIndexSize)
+        {
+            indexList.add(IndexInfo.deserialize(in));
+        }
+        assert in.bytesPastMark() == columnIndexSize;
+
+        return indexList;
+	}
+
+    /**
+     * Defreeze the bloom filter.
+     *
+     * @return bloom filter summarizing the column information
+     * @throws java.io.IOException
+     */
+    public static BloomFilter defreezeBloomFilter(FileDataInput file) throws IOException
+    {
+        int size = file.readInt();
+        byte[] bytes = new byte[size];
+        file.readFully(bytes);
+        
+        ByteArrayInputStream bufIn = new ByteArrayInputStream(bytes);
+        return BloomFilter.serializer().deserialize(new DataInputStream(bufIn));
+    }
+
+    /**
+     * the index of the IndexInfo in which @name will be found.
+     * If the index is @indexList.size(), the @name appears nowhere.
+     */
+    public static int indexFor(byte[] name, List<IndexInfo> indexList, AbstractType comparator, boolean reversed)
+    {
+        if (name.length == 0 && reversed)
+            return indexList.size() - 1;
+        IndexInfo target = new IndexInfo(name, name, 0, 0);
+        int index = Collections.binarySearch(indexList, target, getComparator(comparator));
+        return index < 0 ? -1 * (index + 1) : index;
+    }
+
+    public static Comparator<IndexInfo> getComparator(final AbstractType nameComparator)
+    {
+        return new Comparator<IndexInfo>()
+        {
+            public int compare(IndexInfo o1, IndexInfo o2)
+            {
+                return nameComparator.compare(o1.lastName, o2.lastName);
+            }
+        };
+    }
+
+    public static class IndexInfo
+    {
+        public final long width;
+        public final byte[] lastName;
+        public final byte[] firstName;
+        public final long offset;
+
+        public IndexInfo(byte[] firstName, byte[] lastName, long offset, long width)
+        {
+            this.firstName = firstName;
+            this.lastName = lastName;
+            this.offset = offset;
+            this.width = width;
+        }
+
+        public void serialize(DataOutput dos) throws IOException
+        {
+            ColumnSerializer.writeName(firstName, dos);
+            ColumnSerializer.writeName(lastName, dos);
+            dos.writeLong(offset);
+            dos.writeLong(width);
+        }
+
+        public int serializedSize()
+        {
+            return 2 + firstName.length + 2 + lastName.length + 8 + 8;
+        }
+
+        public static IndexInfo deserialize(FileDataInput dis) throws IOException
+        {
+            return new IndexInfo(ColumnSerializer.readName(dis), ColumnSerializer.readName(dis), dis.readLong(), dis.readLong());
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTable.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTable.java
index 3f24d583..92e40a93 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTable.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTable.java
@@ -1 +1,374 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.List;
+import java.util.Arrays;
+import java.util.Map;
+import java.util.StringTokenizer;
+
+import org.apache.log4j.Logger;
+import org.apache.commons.lang.StringUtils;
+
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.db.DecoratedKey;
+
+import com.google.common.base.Objects;
+
+/**
+ * This class is built on top of the SequenceFile. It stores
+ * data on disk in sorted fashion. However the sorting is upto
+ * the application. This class expects keys to be handed to it
+ * in sorted order.
+ *
+ * A separate index file is maintained as well, containing the
+ * SSTable keys and the offset into the SSTable at which they are found.
+ * Every 1/indexInterval key is read into memory when the SSTable is opened.
+ *
+ * Finally, a bloom filter file is also kept for the keys in each SSTable.
+ */
+public abstract class SSTable
+{
+    static final Logger logger = Logger.getLogger(SSTable.class);
+
+    public static final int FILES_ON_DISK = 3; // data, index, and bloom filter
+    public static final String COMPONENT_DATA = "Data.db";
+    public static final String COMPONENT_INDEX = "Index.db";
+    public static final String COMPONENT_FILTER = "Filter.db";
+
+    public static final String COMPONENT_COMPACTED = "Compacted";
+
+    protected Descriptor desc;
+    protected IPartitioner partitioner;
+    protected BloomFilter bf;
+    protected List<KeyPosition> indexPositions;
+    protected Map<KeyPosition, PositionSize> spannedIndexDataPositions; // map of index position, to data position, for index entries spanning mmap segments
+
+    /* Every 128th index entry is loaded into memory so we know where to start looking for the actual key w/o seeking */
+    public static final int INDEX_INTERVAL = 128;/* Required extension for temporary files created during compactions. */
+    public static final String TEMPFILE_MARKER = "tmp";
+
+    protected SSTable(String filename, IPartitioner partitioner)
+    {
+        assert filename.endsWith("-" + COMPONENT_DATA);
+        this.desc = Descriptor.fromFilename(filename);
+        this.partitioner = partitioner;
+    }
+
+    public Descriptor getDescriptor()
+    {
+        return desc;
+    }
+
+    protected static String parseColumnFamilyName(String filename)
+    {
+        return new File(filename).getName().split("-")[0];
+    }
+
+    public static String indexFilename(String dataFile)
+    {
+        return Descriptor.fromFilename(dataFile).filenameFor(COMPONENT_INDEX);
+    }
+
+    public String indexFilename()
+    {
+        return desc.filenameFor(COMPONENT_INDEX);
+    }
+
+    protected static String compactedFilename(String dataFile)
+    {
+        return Descriptor.fromFilename(dataFile).filenameFor(COMPONENT_COMPACTED);
+    }
+
+    /**
+     * We use a ReferenceQueue to manage deleting files that have been compacted
+     * and for which no more SSTable references exist.  But this is not guaranteed
+     * to run for each such file because of the semantics of the JVM gc.  So,
+     * we write a marker to `compactedFilename` when a file is compacted;
+     * if such a marker exists on startup, the file should be removed.
+     *
+     * @return true if the file was deleted
+     */
+    public static boolean deleteIfCompacted(String dataFilename) throws IOException
+    {
+        if (new File(compactedFilename(dataFilename)).exists())
+        {
+            FileUtils.deleteWithConfirm(new File(dataFilename));
+            FileUtils.deleteWithConfirm(new File(SSTable.indexFilename(dataFilename)));
+            FileUtils.deleteWithConfirm(new File(SSTable.filterFilename(dataFilename)));
+            FileUtils.deleteWithConfirm(new File(SSTable.compactedFilename(dataFilename)));
+            logger.info("Deleted " + dataFilename);
+            return true;
+        }
+        return false;
+    }
+
+    protected String compactedFilename()
+    {
+        return desc.filenameFor(COMPONENT_COMPACTED);
+    }
+
+    protected static String filterFilename(String dataFile)
+    {
+        return Descriptor.fromFilename(dataFile).filenameFor(COMPONENT_FILTER);
+    }
+
+    public String filterFilename()
+    {
+        return desc.filenameFor(COMPONENT_FILTER);
+    }
+
+    public String getFilename()
+    {
+        return desc.filenameFor(COMPONENT_DATA);
+    }
+
+    /** @return component names for files associated w/ this SSTable */
+    public List<String> getAllComponents()
+    {
+        // TODO streaming relies on the -Data (getFilename) file to be last, this is clunky
+        return Arrays.asList(COMPONENT_FILTER, COMPONENT_INDEX, COMPONENT_DATA);
+    }
+
+    public String getColumnFamilyName()
+    {
+        return desc.cfname;
+    }
+
+    public String getTableName()
+    {
+        return desc.ksname;
+    }
+
+    public static String parseTableName(String filename)
+    {
+        return Descriptor.fromFilename(filename).ksname;        
+    }
+
+    public static long getTotalBytes(Iterable<SSTableReader> sstables)
+    {
+        long sum = 0;
+        for (SSTableReader sstable : sstables)
+        {
+            sum += sstable.length();
+        }
+        return sum;
+    }
+
+    /**
+     * This is a simple container for the index Key and its corresponding position
+     * in the data file. Binary search is performed on a list of these objects
+     * to lookup keys within the SSTable data file.
+     */
+    public static class KeyPosition implements Comparable<KeyPosition>
+    {
+        public final DecoratedKey key;
+        public final long position;
+
+        public KeyPosition(DecoratedKey key, long position)
+        {
+            this.key = key;
+            this.position = position;
+        }
+
+        public int compareTo(KeyPosition kp)
+        {
+            return key.compareTo(kp.key);
+        }
+
+        public String toString()
+        {
+            return key + ":" + position;
+        }
+    }
+
+    public long bytesOnDisk()
+    {
+        long bytes = 0;
+        for (String cname : getAllComponents())
+        {
+            bytes += new File(desc.filenameFor(cname)).length();
+        }
+        return bytes;
+    }
+
+    @Override
+    public String toString()
+    {
+        return getClass().getName() + "(" +
+               "path='" + getFilename() + '\'' +
+               ')';
+    }
+
+    public static class PositionSize
+    {
+        public final long position;
+        public final long size;
+
+        public PositionSize(long position, long size)
+        {
+            this.position = position;
+            this.size = size;
+        }
+    }
+
+    /**
+     * A SSTable is described by the keyspace and column family it contains data
+     * for, a generation (where higher generations contain more recent data) and
+     * an alphabetic version string.
+     *
+     * A descriptor can be marked as temporary, which influences generated filenames.
+     */
+    public static class Descriptor
+    {
+        public static final String LEGACY_VERSION = "a";
+        public static final String CURRENT_VERSION = "b";
+
+        public final File directory;
+        public final String version;
+        public final String ksname;
+        public final String cfname;
+        public final int generation;
+        public final boolean temporary;
+        private final int hashCode;
+
+        /**
+         * A descriptor that assumes CURRENT_VERSION.
+         */
+        public Descriptor(File directory, String ksname, String cfname, int generation, boolean temp)
+        {
+            this(CURRENT_VERSION, directory, ksname, cfname, generation, temp);
+        }
+
+        public Descriptor(String version, File directory, String ksname, String cfname, int generation, boolean temp)
+        {
+            assert version != null && directory != null && ksname != null && cfname != null;
+            this.version = version;
+            this.directory = directory;
+            this.ksname = ksname;
+            this.cfname = cfname;
+            this.generation = generation;
+            temporary = temp;
+            hashCode = Objects.hashCode(directory, generation, ksname, cfname);
+        }
+
+        /**
+         * @param suffix A component suffix, such as 'Data.db'/'Index.db'/etc
+         * @return A filename for this descriptor with the given suffix.
+         */
+        public String filenameFor(String suffix)
+        {
+            StringBuilder buff = new StringBuilder();
+            buff.append(directory).append(File.separatorChar);
+            buff.append(cfname).append("-");
+            if (temporary)
+                buff.append(TEMPFILE_MARKER).append("-");
+            if (!LEGACY_VERSION.equals(version))
+                buff.append(version).append("-");
+            buff.append(generation).append("-");
+            buff.append(suffix);
+            return buff.toString();
+        }
+
+        /**
+         * Filename of the form "<ksname>/<cfname>-[tmp-][<version>-]<gen>-*"
+         * @param filename A full SSTable filename, including the directory.
+         * @return A SSTable.Descriptor for the filename. 
+         */
+        public static Descriptor fromFilename(String filename)
+        {
+            int separatorPos = filename.lastIndexOf(File.separatorChar);
+            assert separatorPos != -1 : "Filename must include parent directory.";
+            File directory = new File(filename.substring(0, separatorPos));
+            String name = filename.substring(separatorPos+1, filename.length());
+
+            // name of parent directory is keyspace name
+            String ksname = directory.getName();
+
+            // tokenize the filename
+            StringTokenizer st = new StringTokenizer(name, "-");
+            String nexttok = null;
+            
+            // all filenames must start with a column family
+            String cfname = st.nextToken();
+
+            // optional temporary marker
+            nexttok = st.nextToken();
+            boolean temporary = false;
+            if (nexttok.equals(TEMPFILE_MARKER))
+            {
+                temporary = true;
+                nexttok = st.nextToken();
+            }
+
+            // optional version string
+            String version = LEGACY_VERSION;
+            if (versionValidate(nexttok))
+            {
+                version = nexttok;
+                nexttok = st.nextToken();
+            }
+            int generation = Integer.parseInt(nexttok);
+
+            return new Descriptor(version, directory, ksname, cfname, generation, temporary);
+        }
+        
+        /**
+         * @return True if the given version string is not empty, and
+         * contains all lowercase letters, as defined by java.lang.Character.
+         */
+        private static boolean versionValidate(String ver)
+        {
+            if (ver.length() < 1) return false;
+            for (char ch : ver.toCharArray())
+                if (!Character.isLetter(ch) || !Character.isLowerCase(ch))
+                    return false;
+            return true;
+        }
+
+        @Override
+        public String toString()
+        {
+            return this.filenameFor("<>");
+        }
+
+        @Override
+        public boolean equals(Object o)
+        {
+            if (o == this)
+                return true;
+            if (!(o instanceof Descriptor))
+                return false;
+            Descriptor that = (Descriptor)o;
+            return that.directory.equals(this.directory) && that.generation == this.generation && that.ksname.equals(this.ksname) && that.cfname.equals(this.cfname);
+        }
+
+        @Override
+        public int hashCode()
+        {
+            return hashCode;
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableDeletingReference.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableDeletingReference.java
index 3f24d583..a52bdfe2 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableDeletingReference.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableDeletingReference.java
@@ -1 +1,107 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.File;
+import java.io.IOError;
+import java.io.IOException;
+import java.lang.ref.PhantomReference;
+import java.lang.ref.ReferenceQueue;
+import java.util.Timer;
+import java.util.TimerTask;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.io.DeletionService;
+import org.apache.cassandra.io.util.FileUtils;
+
+public class SSTableDeletingReference extends PhantomReference<SSTableReader>
+{
+    private static final Logger logger = Logger.getLogger(SSTableDeletingReference.class);
+
+    private static final Timer timer = new Timer("SSTABLE-CLEANUP-TIMER");
+    public static final int RETRY_DELAY = 10000;
+
+    private final SSTableTracker tracker;
+    public final String path;
+    private final long size;
+    private boolean deleteOnCleanup;
+
+    SSTableDeletingReference(SSTableTracker tracker, SSTableReader referent, ReferenceQueue<? super SSTableReader> q)
+    {
+        super(referent, q);
+        this.tracker = tracker;
+        this.path = referent.getFilename();
+        this.size = referent.bytesOnDisk();
+    }
+
+    public void deleteOnCleanup()
+    {
+        deleteOnCleanup = true;
+    }
+
+    public void cleanup() throws IOException
+    {
+        if (deleteOnCleanup)
+        {
+            // this is tricky because the mmapping might not have been finalized yet,
+            // and delete will fail until it is.  additionally, we need to make sure to
+            // delete the data file first, so on restart the others will be recognized as GCable
+            // even if the compaction marker gets deleted next.
+            timer.schedule(new CleanupTask(), RETRY_DELAY);
+        }
+    }
+
+    private class CleanupTask extends TimerTask
+    {
+        int attempts = 0;
+
+        @Override
+        public void run()
+        {
+            File datafile = new File(path);
+            if (!datafile.delete())
+            {
+                if (attempts++ < DeletionService.MAX_RETRIES)
+                {
+                    timer.schedule(this, RETRY_DELAY);
+                    return;
+                }
+                else
+                {
+                    throw new RuntimeException("Unable to delete " + path);
+                }
+            }
+            try
+            {
+                FileUtils.deleteWithConfirm(new File(SSTable.indexFilename(path)));
+                FileUtils.deleteWithConfirm(new File(SSTable.filterFilename(path)));
+                FileUtils.deleteWithConfirm(new File(SSTable.compactedFilename(path)));
+            }
+            catch (IOException e)
+            {
+                throw new IOError(e);
+            }
+            tracker.spaceReclaimed(size);
+            logger.info("Deleted " + path);
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
index 3f24d583..e0487f74 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
@@ -1 +1,572 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.*;
+import java.util.*;
+import java.lang.ref.ReferenceQueue;
+import java.lang.ref.Reference;
+import java.nio.channels.FileChannel;
+import java.nio.MappedByteBuffer;
+
+import org.apache.log4j.Logger;
+
+import org.apache.commons.lang.StringUtils;
+
+import org.apache.cassandra.cache.InstrumentedCache;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.Pair;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.io.ICompactSerializer2;
+import org.apache.cassandra.io.util.BufferedRandomAccessFile;
+import org.apache.cassandra.io.util.FileDataInput;
+import org.apache.cassandra.io.util.MappedFileDataInput;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * SSTableReaders are open()ed by Table.onStart; after that they are created by SSTableWriter.renameAndOpen.
+ * Do not re-call open() on existing SSTable files; use the references kept by ColumnFamilyStore post-start instead.
+ */
+public class SSTableReader extends SSTable implements Comparable<SSTableReader>
+{
+    private static final Logger logger = Logger.getLogger(SSTableReader.class);
+
+    // `finalizers` is required to keep the PhantomReferences alive after the enclosing SSTR is itself
+    // unreferenced.  otherwise they will never get enqueued.
+    private static final Set<Reference<SSTableReader>> finalizers = new HashSet<Reference<SSTableReader>>();
+    private static final ReferenceQueue<SSTableReader> finalizerQueue = new ReferenceQueue<SSTableReader>()
+    {{
+        Runnable runnable = new Runnable()
+        {
+            public void run()
+            {
+                while (true)
+                {
+                    SSTableDeletingReference r = null;
+                    try
+                    {
+                        r = (SSTableDeletingReference) finalizerQueue.remove();
+                        finalizers.remove(r);
+                    }
+                    catch (InterruptedException e)
+                    {
+                        throw new RuntimeException(e);
+                    }
+                    try
+                    {
+                        r.cleanup();
+                    }
+                    catch (IOException e)
+                    {
+                        logger.error("Error deleting " + r.path, e);
+                    }
+                }
+            }
+        };
+        new Thread(runnable, "SSTABLE-DELETER").start();
+    }};
+    private static final long BUFFER_SIZE = Integer.MAX_VALUE;
+
+    public static int indexInterval()
+    {
+        return INDEX_INTERVAL;
+    }
+
+    public static long getApproximateKeyCount(Iterable<SSTableReader> sstables)
+    {
+        long count = 0;
+
+        for (SSTableReader sstable : sstables)
+        {
+            int indexKeyCount = sstable.getIndexPositions().size();
+            count = count + (indexKeyCount + 1) * INDEX_INTERVAL;
+            if (logger.isDebugEnabled())
+                logger.debug("index size for bloom filter calc for file  : " + sstable.getFilename() + "   : " + count);
+        }
+
+        return count;
+    }
+
+    public static SSTableReader open(Descriptor desc) throws IOException
+    {
+        return open(desc.filenameFor(COMPONENT_DATA));
+    }
+
+    public static SSTableReader open(String dataFileName) throws IOException
+    {
+        return open(dataFileName, StorageService.getPartitioner());
+    }
+
+    public static SSTableReader open(String dataFileName, IPartitioner partitioner) throws IOException
+    {
+        assert partitioner != null;
+
+        long start = System.currentTimeMillis();
+        SSTableReader sstable = new SSTableReader(dataFileName, partitioner);
+        logger.info("Sampling index for " + dataFileName);
+        sstable.loadIndexFile();
+        sstable.loadBloomFilter();
+
+        if (logger.isDebugEnabled())
+            logger.debug("INDEX LOAD TIME for "  + dataFileName + ": " + (System.currentTimeMillis() - start) + " ms.");
+
+        return sstable;
+    }
+
+    private volatile SSTableDeletingReference phantomReference;
+    // jvm can only map up to 2GB at a time, so we split index/data into segments of that size when using mmap i/o
+    private final MappedByteBuffer[] indexBuffers;
+    private final MappedByteBuffer[] buffers;
+
+    private InstrumentedCache<Pair<Descriptor,DecoratedKey>,PositionSize> keyCache;
+
+    SSTableReader(String filename,
+                  IPartitioner partitioner,
+                  List<KeyPosition> indexPositions, Map<KeyPosition, PositionSize> spannedIndexDataPositions,
+                  BloomFilter bloomFilter)
+    throws IOException
+    {
+        super(filename, partitioner);
+
+        if (DatabaseDescriptor.getIndexAccessMode() == DatabaseDescriptor.DiskAccessMode.mmap)
+        {
+            long indexLength = new File(indexFilename()).length();
+            int bufferCount = 1 + (int) (indexLength / BUFFER_SIZE);
+            indexBuffers = new MappedByteBuffer[bufferCount];
+            long remaining = indexLength;
+            for (int i = 0; i < bufferCount; i++)
+            {
+                indexBuffers[i] = mmap(indexFilename(), i * BUFFER_SIZE, (int) Math.min(remaining, BUFFER_SIZE));
+                remaining -= BUFFER_SIZE;
+            }
+        }
+        else
+        {
+            assert DatabaseDescriptor.getIndexAccessMode() == DatabaseDescriptor.DiskAccessMode.standard;
+            indexBuffers = null;
+        }
+
+        if (DatabaseDescriptor.getDiskAccessMode() == DatabaseDescriptor.DiskAccessMode.mmap)
+        {
+            int bufferCount = 1 + (int) (new File(getFilename()).length() / BUFFER_SIZE);
+            buffers = new MappedByteBuffer[bufferCount];
+            long remaining = length();
+            for (int i = 0; i < bufferCount; i++)
+            {
+                buffers[i] = mmap(getFilename(), i * BUFFER_SIZE, (int) Math.min(remaining, BUFFER_SIZE));
+                remaining -= BUFFER_SIZE;
+            }
+        }
+        else
+        {
+            assert DatabaseDescriptor.getDiskAccessMode() == DatabaseDescriptor.DiskAccessMode.standard;
+            buffers = null;
+        }
+
+        this.indexPositions = indexPositions;
+        this.spannedIndexDataPositions = spannedIndexDataPositions;
+        this.bf = bloomFilter;
+    }
+
+    public void setTrackedBy(SSTableTracker tracker)
+    {
+        phantomReference = new SSTableDeletingReference(tracker, this, finalizerQueue);
+        finalizers.add(phantomReference);
+        keyCache = tracker.getKeyCache();
+    }
+
+    private static MappedByteBuffer mmap(String filename, long start, int size) throws IOException
+    {
+        RandomAccessFile raf;
+        try
+        {
+            raf = new RandomAccessFile(filename, "r");
+        }
+        catch (FileNotFoundException e)
+        {
+            throw new IOError(e);
+        }
+
+        try
+        {
+            return raf.getChannel().map(FileChannel.MapMode.READ_ONLY, start, size);
+        }
+        finally
+        {
+            raf.close();
+        }
+    }
+
+    private SSTableReader(String filename, IPartitioner partitioner) throws IOException
+    {
+        this(filename, partitioner, null, null, null);
+    }
+
+    public List<KeyPosition> getIndexPositions()
+    {
+        return indexPositions;
+    }
+
+    public long estimatedKeys()
+    {
+        return indexPositions.size() * INDEX_INTERVAL;
+    }
+
+    void loadBloomFilter() throws IOException
+    {
+        DataInputStream stream = new DataInputStream(new FileInputStream(filterFilename()));
+        try
+        {
+            bf = BloomFilter.serializer().deserialize(stream);
+        }
+        finally
+        {
+            stream.close();
+        }
+    }
+
+    void loadIndexFile() throws IOException
+    {
+        indexPositions = new ArrayList<KeyPosition>();
+        // we read the positions in a BRAF so we don't have to worry about an entry spanning a mmap boundary.
+        // any entries that do, we force into the in-memory sample so key lookup can always bsearch within
+        // a single mmapped segment.
+        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(), "r");
+        try
+        {
+            int i = 0;
+            long indexSize = input.length();
+            while (true)
+            {
+                long indexPosition = input.getFilePointer();
+                if (indexPosition == indexSize)
+                {
+                    break;
+                }
+                DecoratedKey decoratedKey = partitioner.convertFromDiskFormat(input.readUTF());
+                long dataPosition = input.readLong();
+                long nextIndexPosition = input.getFilePointer();
+                boolean spannedEntry = bufferIndex(indexPosition) != bufferIndex(nextIndexPosition);
+                if (i++ % INDEX_INTERVAL == 0 || spannedEntry)
+                {
+                    KeyPosition info;
+                    info = new KeyPosition(decoratedKey, indexPosition);
+                    indexPositions.add(info);
+
+                    if (spannedEntry)
+                    {
+                        if (spannedIndexDataPositions == null)
+                        {
+                            spannedIndexDataPositions = new HashMap<KeyPosition, PositionSize>();
+                        }
+                        // read the next index entry to see how big the row is corresponding to the current, mmap-segment-spanning one
+                        input.readUTF();
+                        long nextDataPosition = input.readLong();
+                        input.seek(nextIndexPosition);
+                        spannedIndexDataPositions.put(info, new PositionSize(dataPosition, nextDataPosition - dataPosition));
+                    }
+                }
+            }
+        }
+        finally
+        {
+            input.close();
+        }
+    }
+
+    /** get the position in the index file to start scanning to find the given key (at most indexInterval keys away) */
+    private KeyPosition getIndexScanPosition(DecoratedKey decoratedKey)
+    {
+        assert indexPositions != null && indexPositions.size() > 0;
+        int index = Collections.binarySearch(indexPositions, new KeyPosition(decoratedKey, -1));
+        if (index < 0)
+        {
+            // binary search gives us the first index _greater_ than the key searched for,
+            // i.e., its insertion position
+            int greaterThan = (index + 1) * -1;
+            if (greaterThan == 0)
+                return null;
+            return indexPositions.get(greaterThan - 1);
+        }
+        else
+        {
+            return indexPositions.get(index);
+        }
+    }
+
+    /**
+     * returns the position in the data file to find the given key, or -1 if the key is not present
+     */
+    public PositionSize getPosition(DecoratedKey decoratedKey) throws IOException
+    {
+        // first, check bloom filter
+        if (!bf.isPresent(partitioner.convertToDiskFormat(decoratedKey)))
+            return null;
+
+        // next, the key cache
+        Pair<Descriptor, DecoratedKey> unifiedKey = new Pair<Descriptor, DecoratedKey>(desc, decoratedKey);
+        if (keyCache != null && keyCache.getCapacity() > 0)
+        {
+            PositionSize cachedPosition = keyCache.get(unifiedKey);
+            if (cachedPosition != null)
+            {
+                return cachedPosition;
+            }
+        }
+
+        // next, see if the sampled index says it's impossible for the key to be present
+        KeyPosition sampledPosition = getIndexScanPosition(decoratedKey);
+        if (sampledPosition == null)
+        {
+            return null;
+        }
+
+        // handle exact sampled index hit
+        if (spannedIndexDataPositions != null)
+        {
+            PositionSize info = spannedIndexDataPositions.get(sampledPosition);
+            if (info != null)
+                return info;
+        }
+
+        // scan the on-disk index, starting at the nearest sampled position
+        long p = sampledPosition.position;
+        FileDataInput input;
+        if (indexBuffers == null)
+        {
+            input = new BufferedRandomAccessFile(indexFilename(), "r");
+            ((BufferedRandomAccessFile)input).seek(p);
+        }
+        else
+        {
+            input = new MappedFileDataInput(indexBuffers[bufferIndex(p)], indexFilename(), (int)(p % BUFFER_SIZE));
+        }
+        try
+        {
+            int i = 0;
+            do
+            {
+                DecoratedKey indexDecoratedKey;
+                try
+                {
+                    indexDecoratedKey = partitioner.convertFromDiskFormat(input.readUTF());
+                }
+                catch (EOFException e)
+                {
+                    return null;
+                }
+                long position = input.readLong();
+                int v = indexDecoratedKey.compareTo(decoratedKey);
+                if (v == 0)
+                {
+                    PositionSize info;
+                    if (!input.isEOF())
+                    {
+                        int utflen = input.readUnsignedShort();
+                        if (utflen != input.skipBytes(utflen))
+                            throw new EOFException();
+                        info = new PositionSize(position, input.readLong() - position);
+                    }
+                    else
+                    {
+                        info = new PositionSize(position, length() - position);
+                    }
+                    if (keyCache != null && keyCache.getCapacity() > 0)
+                        keyCache.put(unifiedKey, info);
+                    return info;
+                }
+                if (v > 0)
+                    return null;
+            } while  (++i < INDEX_INTERVAL);
+        }
+        finally
+        {
+            input.close();
+        }
+        return null;
+    }
+
+    /** like getPosition, but if key is not found will return the location of the first key _greater_ than the desired one, or -1 if no such key exists. */
+    public long getNearestPosition(DecoratedKey decoratedKey) throws IOException
+    {
+        KeyPosition sampledPosition = getIndexScanPosition(decoratedKey);
+        if (sampledPosition == null)
+        {
+            return 0;
+        }
+
+        // can't use a MappedFileDataInput here, since we might cross a segment boundary while scanning
+        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(), "r");
+        input.seek(sampledPosition.position);
+        try
+        {
+            while (true)
+            {
+                DecoratedKey indexDecoratedKey;
+                try
+                {
+                    indexDecoratedKey = partitioner.convertFromDiskFormat(input.readUTF());
+                }
+                catch (EOFException e)
+                {
+                    return -1;
+                }
+                long position = input.readLong();
+                int v = indexDecoratedKey.compareTo(decoratedKey);
+                if (v >= 0)
+                    return position;
+            }
+        }
+        finally
+        {
+            input.close();
+        }
+    }
+
+    public long length()
+    {
+        return new File(getFilename()).length();
+    }
+
+    public int compareTo(SSTableReader o)
+    {
+        return desc.generation - o.desc.generation;
+    }
+
+    public void markCompacted() throws IOException
+    {
+        if (logger.isDebugEnabled())
+            logger.debug("Marking " + getFilename() + " compacted");
+        if (!new File(compactedFilename()).createNewFile())
+        {
+            throw new IOException("Unable to create compaction marker");
+        }
+        phantomReference.deleteOnCleanup();
+    }
+
+    /** obviously only for testing */
+    public void forceBloomFilterFailures()
+    {
+        bf = BloomFilter.alwaysMatchingBloomFilter();
+    }
+
+    public IPartitioner getPartitioner()
+    {
+        return partitioner;
+    }
+
+    public SSTableScanner getScanner(int bufferSize) throws IOException
+    {
+        return new SSTableScanner(this, bufferSize);
+    }
+
+    public FileDataInput getFileDataInput(DecoratedKey decoratedKey, int bufferSize) throws IOException
+    {
+        PositionSize info = getPosition(decoratedKey);
+        if (info == null)
+            return null;
+
+        if (buffers == null || (bufferIndex(info.position) != bufferIndex(info.position + info.size)))
+        {
+            BufferedRandomAccessFile file = new BufferedRandomAccessFile(getFilename(), "r", bufferSize);
+            file.seek(info.position);
+            return file;
+        }
+        return new MappedFileDataInput(buffers[bufferIndex(info.position)], getFilename(), (int) (info.position % BUFFER_SIZE));
+    }
+
+    static int bufferIndex(long position)
+    {
+        return (int) (position / BUFFER_SIZE);
+    }
+
+    public AbstractType getColumnComparator()
+    {
+        return DatabaseDescriptor.getComparator(getTableName(), getColumnFamilyName());
+    }
+
+    public ColumnFamily makeColumnFamily()
+    {
+        return ColumnFamily.create(getTableName(), getColumnFamilyName());
+    }
+
+    public ICompactSerializer2<IColumn> getColumnSerializer()
+    {
+        return DatabaseDescriptor.getColumnFamilyType(getTableName(), getColumnFamilyName()).equals("Standard")
+               ? Column.serializer()
+               : SuperColumn.serializer(getColumnComparator());
+    }
+}
+
+class FileSSTableMap
+{
+    private final Map<String, SSTableReader> map = new NonBlockingHashMap<String, SSTableReader>();
+
+    public SSTableReader get(String filename)
+    {
+        try
+        {
+            return map.get(new File(filename).getCanonicalPath());
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public SSTableReader put(String filename, SSTableReader value)
+    {
+        try
+        {
+            return map.put(new File(filename).getCanonicalPath(), value);
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public Collection<SSTableReader> values()
+    {
+        return map.values();
+    }
+
+    public void clear()
+    {
+        map.clear();
+    }
+
+    public void remove(String filename) throws IOException
+    {
+        map.remove(new File(filename).getCanonicalPath());
+    }
+
+    @Override
+    public String toString()
+    {
+        return "FileSSTableMap {" + StringUtils.join(map.keySet(), ", ") + "}";
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableScanner.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableScanner.java
index 3f24d583..1f192b08 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableScanner.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableScanner.java
@@ -1 +1,152 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.IOException;
+import java.io.Closeable;
+import java.io.IOError;
+import java.util.Iterator;
+import java.util.Arrays;
+
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.io.IteratingRow;
+import org.apache.cassandra.io.util.BufferedRandomAccessFile;
+import org.apache.cassandra.io.util.FileDataInput;
+
+import org.apache.log4j.Logger;
+
+
+public class SSTableScanner implements Iterator<IteratingRow>, Closeable
+{
+    private static Logger logger = Logger.getLogger(SSTableScanner.class);
+
+    private final BufferedRandomAccessFile file;
+    private final SSTableReader sstable;
+    private IteratingRow row;
+    private boolean exhausted = false;
+    private Iterator<IteratingRow> iterator;
+
+    /**
+     * @param sstable SSTable to scan.
+     */
+    SSTableScanner(SSTableReader sstable, int bufferSize) throws IOException
+    {
+        this.file = new BufferedRandomAccessFile(sstable.getFilename(), "r", bufferSize);
+        this.sstable = sstable;
+    }
+
+    public void close() throws IOException
+    {
+        file.close();
+    }
+
+    public void seekTo(DecoratedKey seekKey)
+    {
+        try
+        {
+            long position = sstable.getNearestPosition(seekKey);
+            if (position < 0)
+            {
+                exhausted = true;
+                return;
+            }
+            file.seek(position);
+            row = null;
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException("corrupt sstable", e);
+        }
+    }
+
+    public long getFileLength()
+    {
+        try
+        {
+            return file.length();
+        }
+        catch (IOException e)
+        {
+            throw new IOError(e);
+        }
+    }
+
+    public long getFilePointer()
+    {
+        return file.getFilePointer();
+    }
+
+    public boolean hasNext()
+    {
+        if (iterator == null)
+            iterator = exhausted ? Arrays.asList(new IteratingRow[0]).iterator() : new KeyScanningIterator();
+        return iterator.hasNext();
+    }
+
+    public IteratingRow next()
+    {
+        if (iterator == null)
+            iterator = exhausted ? Arrays.asList(new IteratingRow[0]).iterator() : new KeyScanningIterator();
+        return iterator.next();
+    }
+
+    public void remove()
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    private class KeyScanningIterator implements Iterator<IteratingRow>
+    {
+        public boolean hasNext()
+        {
+            try
+            {
+                if (row == null)
+                    return !file.isEOF();
+                return row.getEndPosition() < file.length();
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+
+        public IteratingRow next()
+        {
+            try
+            {
+                if (row != null)
+                    row.skipRemaining();
+                assert !file.isEOF();
+                return row = new IteratingRow(file, sstable);
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+
+        public void remove()
+        {
+            throw new UnsupportedOperationException();
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableTracker.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableTracker.java
index e69de29b..fd5feaf9 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableTracker.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableTracker.java
@@ -0,0 +1,183 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.cassandra.io.sstable;
+
+import java.util.*;
+import java.io.IOException;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.cassandra.cache.JMXInstrumentedCache;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.utils.Pair;
+
+import org.apache.log4j.Logger;
+
+public class SSTableTracker implements Iterable<SSTableReader>
+{
+    private static final Logger logger = Logger.getLogger(SSTableTracker.class);
+
+    private volatile Set<SSTableReader> sstables;
+    private final AtomicLong liveSize = new AtomicLong();
+    private final AtomicLong totalSize = new AtomicLong();
+
+    private final String ksname;
+    private final String cfname;
+
+    private final JMXInstrumentedCache<Pair<SSTable.Descriptor,DecoratedKey>,SSTable.PositionSize> keyCache;
+    private final JMXInstrumentedCache<String, ColumnFamily> rowCache;
+
+    public SSTableTracker(String ksname, String cfname)
+    {
+        this.ksname = ksname;
+        this.cfname = cfname;
+        sstables = Collections.emptySet();
+        keyCache = new JMXInstrumentedCache<Pair<SSTable.Descriptor,DecoratedKey>,SSTable.PositionSize>(ksname, cfname + "KeyCache", 0);
+        rowCache = new JMXInstrumentedCache<String, ColumnFamily>(ksname, cfname + "RowCache", 0);
+    }
+
+    public synchronized void replace(Collection<SSTableReader> oldSSTables, Iterable<SSTableReader> replacements) throws IOException
+    {
+        Set<SSTableReader> sstablesNew = new HashSet<SSTableReader>(sstables);
+
+        for (SSTableReader sstable : replacements)
+        {
+            assert sstable.getIndexPositions() != null;
+            sstablesNew.add(sstable);
+            long size = sstable.bytesOnDisk();
+            liveSize.addAndGet(size);
+            totalSize.addAndGet(size);
+            sstable.setTrackedBy(this);
+        }
+
+        for (SSTableReader sstable : oldSSTables)
+        {
+            boolean removed = sstablesNew.remove(sstable);
+            assert removed;
+            sstable.markCompacted();
+            liveSize.addAndGet(-sstable.bytesOnDisk());
+        }
+
+        sstables = Collections.unmodifiableSet(sstablesNew);
+        updateCacheSizes();
+    }
+
+    public synchronized void add(Iterable<SSTableReader> sstables)
+    {
+        assert sstables != null;
+        try
+        {
+            replace(Collections.<SSTableReader>emptyList(), sstables);
+        }
+        catch (IOException e)
+        {
+            throw new AssertionError(e);
+        }
+    }
+
+    public synchronized void markCompacted(Collection<SSTableReader> compacted) throws IOException
+    {
+        replace(compacted, Collections.<SSTableReader>emptyList());
+    }
+
+    /**
+     * Resizes the key and row caches based on the current key estimate.
+     */
+    public synchronized void updateCacheSizes()
+    {
+        long keys = estimatedKeys();
+        
+        int keyCacheSize = DatabaseDescriptor.getKeysCachedFor(ksname, cfname, keys);
+        if (keyCacheSize != keyCache.getCapacity())
+        {
+            // update cache size for the new key volume
+            if (logger.isDebugEnabled())
+                logger.debug("key cache capacity for " + cfname + " is " + keyCacheSize);
+            keyCache.setCapacity(keyCacheSize);
+        }
+
+        int rowCacheSize = DatabaseDescriptor.getRowsCachedFor(ksname, cfname, keys);
+        if (rowCacheSize != rowCache.getCapacity())
+        {   
+            if (logger.isDebugEnabled())
+                logger.debug("row cache capacity for " + cfname + " is " + rowCacheSize);
+            rowCache.setCapacity(rowCacheSize);
+        }
+    }
+
+    // the modifiers create new, unmodifiable objects each time; the volatile fences the assignment
+    // so we don't need any further synchronization for the common case here
+    public Set<SSTableReader> getSSTables()
+    {
+        return sstables;
+    }
+
+    public int size()
+    {
+        return sstables.size();
+    }
+
+    public Iterator<SSTableReader> iterator()
+    {
+        return sstables.iterator();
+    }
+
+    public synchronized void clearUnsafe()
+    {
+        sstables = Collections.emptySet();
+    }
+
+    public JMXInstrumentedCache<String, ColumnFamily> getRowCache()
+    {
+        return rowCache;
+    }
+
+    public long estimatedKeys()
+    {
+        long n = 0;
+        for (SSTableReader sstable : this)
+        {
+            n += sstable.estimatedKeys();
+        }
+        return n;
+    }
+
+    public long getLiveSize()
+    {
+        return liveSize.get();
+    }
+
+    public long getTotalSize()
+    {
+        return totalSize.get();
+    }
+
+    public void spaceReclaimed(long size)
+    {
+        totalSize.addAndGet(-size);
+    }
+
+    public JMXInstrumentedCache<Pair<SSTable.Descriptor, DecoratedKey>, SSTable.PositionSize> getKeyCache()
+    {
+        return keyCache;
+    }
+}
+
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
index 3f24d583..9f2e4cb1 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
@@ -1 +1,184 @@
   + native
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.DataOutputStream;
+import java.io.FileOutputStream;
+import java.io.IOError;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.util.BufferedRandomAccessFile;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class SSTableWriter extends SSTable
+{
+    private static Logger logger = Logger.getLogger(SSTableWriter.class);
+
+    private long keysWritten;
+    private BufferedRandomAccessFile dataFile;
+    private BufferedRandomAccessFile indexFile;
+    private DecoratedKey lastWrittenKey;
+    private BloomFilter bf;
+
+    public SSTableWriter(String filename, long keyCount, IPartitioner partitioner) throws IOException
+    {
+        super(filename, partitioner);
+        dataFile = new BufferedRandomAccessFile(getFilename(), "rw", (int)(DatabaseDescriptor.getFlushDataBufferSizeInMB() * 1024 * 1024));
+        indexFile = new BufferedRandomAccessFile(indexFilename(), "rw", (int)(DatabaseDescriptor.getFlushIndexBufferSizeInMB() * 1024 * 1024));
+        bf = BloomFilter.getFilter(keyCount, 15);
+    }
+
+    private long beforeAppend(DecoratedKey decoratedKey) throws IOException
+    {
+        if (decoratedKey == null)
+        {
+            throw new IOException("Keys must not be null.");
+        }
+        if (lastWrittenKey != null && lastWrittenKey.compareTo(decoratedKey) > 0)
+        {
+            logger.info("Last written key : " + lastWrittenKey);
+            logger.info("Current key : " + decoratedKey);
+            logger.info("Writing into file " + getFilename());
+            throw new IOException("Keys must be written in ascending order.");
+        }
+        return (lastWrittenKey == null) ? 0 : dataFile.getFilePointer();
+    }
+
+    private void afterAppend(DecoratedKey decoratedKey, long dataPosition, int dataSize) throws IOException
+    {
+        String diskKey = partitioner.convertToDiskFormat(decoratedKey);
+        bf.add(diskKey);
+        lastWrittenKey = decoratedKey;
+        long indexPosition = indexFile.getFilePointer();
+        indexFile.writeUTF(diskKey);
+        indexFile.writeLong(dataPosition);
+        if (logger.isTraceEnabled())
+            logger.trace("wrote " + decoratedKey + " at " + dataPosition);
+        if (logger.isTraceEnabled())
+            logger.trace("wrote index of " + decoratedKey + " at " + indexPosition);
+
+        boolean spannedEntry = SSTableReader.bufferIndex(indexPosition) != SSTableReader.bufferIndex(indexFile.getFilePointer());
+        if (keysWritten++ % INDEX_INTERVAL == 0 || spannedEntry)
+        {
+            if (indexPositions == null)
+            {
+                indexPositions = new ArrayList<KeyPosition>();
+            }
+            KeyPosition info = new KeyPosition(decoratedKey, indexPosition);
+            indexPositions.add(info);
+
+            if (spannedEntry)
+            {
+                if (spannedIndexDataPositions == null)
+                {
+                    spannedIndexDataPositions = new HashMap<KeyPosition, PositionSize>();
+                }
+                spannedIndexDataPositions.put(info, new PositionSize(dataPosition, dataSize));
+            }
+        }
+    }
+
+    // TODO make this take a DataOutputStream and wrap the byte[] version to combine them
+    public void append(DecoratedKey decoratedKey, DataOutputBuffer buffer) throws IOException
+    {
+        long currentPosition = beforeAppend(decoratedKey);
+        dataFile.writeUTF(partitioner.convertToDiskFormat(decoratedKey));
+        int length = buffer.getLength();
+        assert length > 0;
+        dataFile.writeInt(length);
+        dataFile.write(buffer.getData(), 0, length);
+        afterAppend(decoratedKey, currentPosition, length);
+    }
+
+    public void append(DecoratedKey decoratedKey, byte[] value) throws IOException
+    {
+        long currentPosition = beforeAppend(decoratedKey);
+        dataFile.writeUTF(partitioner.convertToDiskFormat(decoratedKey));
+        assert value.length > 0;
+        dataFile.writeInt(value.length);
+        dataFile.write(value);
+        afterAppend(decoratedKey, currentPosition, value.length);
+    }
+
+    public SSTableReader closeAndOpenReader() throws IOException
+    {
+        // bloom filter
+        FileOutputStream fos = new FileOutputStream(filterFilename());
+        DataOutputStream stream = new DataOutputStream(fos);
+        BloomFilter.serializer().serialize(bf, stream);
+        stream.flush();
+        fos.getFD().sync();
+        stream.close();
+
+        // index
+        indexFile.getChannel().force(true);
+        indexFile.close();
+
+        // main data
+        dataFile.close(); // calls force
+
+        String newpath = getFilename();
+        rename(indexFilename());
+        rename(filterFilename());
+        newpath = rename(newpath); // important to do this last since index & filter file names are derived from it
+
+        return new SSTableReader(newpath, partitioner, indexPositions, spannedIndexDataPositions, bf);
+    }
+
+    static String rename(String tmpFilename)
+    {
+        String filename = tmpFilename.replace("-" + SSTable.TEMPFILE_MARKER, "");
+        try
+        {
+            FBUtilities.renameWithConfirm(tmpFilename, filename);
+        }
+        catch (IOException e)
+        {
+            throw new IOError(e);
+        }
+        return filename;
+    }
+
+    public long getFilePointer()
+    {
+        return dataFile.getFilePointer();
+    }
+    
+    public static SSTableReader renameAndOpen(String dataFileName) throws IOException
+    {
+        SSTableWriter.rename(indexFilename(dataFileName));
+        SSTableWriter.rename(filterFilename(dataFileName));
+        dataFileName = SSTableWriter.rename(dataFileName);
+        return SSTableReader.open(dataFileName,
+                                  StorageService.getPartitioner());
+    }
+
+}
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/AntiEntropyService.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/AntiEntropyService.java
index 9ebd6f37..7fab3534 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/AntiEntropyService.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/AntiEntropyService.java
@@ -33,8 +33,8 @@
 import org.apache.cassandra.dht.Token;
 import org.apache.cassandra.io.CompactionIterator.CompactedRow;
 import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.io.SSTable;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.streaming.StreamOut;
 import org.apache.cassandra.net.IVerbHandler;
 import org.apache.cassandra.net.Message;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
index 043bdabb..14bda6b8 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
@@ -39,8 +39,8 @@
 import org.apache.cassandra.db.*;
 import org.apache.cassandra.dht.*;
 import org.apache.cassandra.gms.*;
-import org.apache.cassandra.io.SSTable;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.locator.*;
 import org.apache.cassandra.net.*;
 import org.apache.cassandra.service.AntiEntropyService.TreeRequestVerbHandler;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/PendingFile.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/PendingFile.java
index c5019d65..9ebe2382 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/PendingFile.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/PendingFile.java
@@ -26,7 +26,7 @@
 import java.io.IOException;
 
 import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.sstable.SSTable;
 
 class PendingFile
 {
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamCompletionHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamCompletionHandler.java
index 662ea5e4..02b97ac1 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamCompletionHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamCompletionHandler.java
@@ -1,6 +1,4 @@
-package org.apache.cassandra.streaming;
-/*
- * 
+/**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -17,9 +15,9 @@
  * KIND, either express or implied.  See the License for the
  * specific language governing permissions and limitations
  * under the License.
- * 
  */
 
+package org.apache.cassandra.streaming;
 
 import java.io.File;
 import java.io.IOException;
@@ -28,8 +26,8 @@
 import org.apache.log4j.Logger;
 
 import org.apache.cassandra.db.Table;
-import org.apache.cassandra.io.SSTableReader;
-import org.apache.cassandra.io.SSTableWriter;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableWriter;
 import org.apache.cassandra.net.MessagingService;
 import org.apache.cassandra.streaming.IStreamComplete;
 import org.apache.cassandra.streaming.StreamInManager;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamInitiateVerbHandler.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamInitiateVerbHandler.java
index 8c7d57e5..971b85e6 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamInitiateVerbHandler.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamInitiateVerbHandler.java
@@ -1,6 +1,4 @@
-package org.apache.cassandra.streaming;
-/*
- * 
+/**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -17,9 +15,9 @@
  * KIND, either express or implied.  See the License for the
  * specific language governing permissions and limitations
  * under the License.
- * 
  */
 
+package org.apache.cassandra.streaming;
 
 import java.io.*;
 import java.net.InetAddress;
@@ -30,7 +28,7 @@
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.db.ColumnFamilyStore;
 import org.apache.cassandra.db.Table;
-import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.sstable.SSTable;
 import org.apache.cassandra.net.IVerbHandler;
 import org.apache.cassandra.net.Message;
 import org.apache.cassandra.net.MessagingService;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamOut.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamOut.java
index 9ed98e6f..65b6de9f 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamOut.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamOut.java
@@ -1,6 +1,4 @@
-package org.apache.cassandra.streaming;
-/*
- * 
+/**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -17,9 +15,9 @@
  * KIND, either express or implied.  See the License for the
  * specific language governing permissions and limitations
  * under the License.
- * 
  */
 
+package org.apache.cassandra.streaming;
 
 import java.net.InetAddress;
 import java.util.*;
@@ -35,8 +33,8 @@
 import org.apache.cassandra.dht.Range;
 import org.apache.cassandra.streaming.StreamInitiateMessage;
 import org.apache.cassandra.db.Table;
-import org.apache.cassandra.io.SSTable;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.net.Message;
 import org.apache.cassandra.net.MessagingService;
 import org.apache.cassandra.streaming.StreamOutManager;
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableExport.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableExport.java
index efd2b74b..0cf984a1 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableExport.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableExport.java
@@ -30,9 +30,9 @@
 import org.apache.cassandra.db.marshal.AbstractType;
 import org.apache.cassandra.dht.IPartitioner;
 import org.apache.cassandra.io.IteratingRow;
-import org.apache.cassandra.io.SSTable;
-import org.apache.cassandra.io.SSTableReader;
-import org.apache.cassandra.io.SSTableScanner;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableScanner;
 import org.apache.cassandra.io.util.BufferedRandomAccessFile;
 import org.apache.cassandra.service.StorageService;
 
diff --git a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableImport.java b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableImport.java
index 779fafda..5be02bd6 100644
--- a/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableImport.java
+++ b/incubator/cassandra/trunk/src/java/org/apache/cassandra/tools/SSTableImport.java
@@ -28,7 +28,7 @@
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.dht.IPartitioner;
 import org.apache.cassandra.io.util.DataOutputBuffer;
-import org.apache.cassandra.io.SSTableWriter;
+import org.apache.cassandra.io.sstable.SSTableWriter;
 import static org.apache.cassandra.utils.FBUtilities.hexToBytes;
 import org.apache.commons.cli.*;
 import org.json.simple.JSONArray;
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
index 4a4abbb2..8749685b 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
@@ -40,7 +40,7 @@
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.db.filter.SliceQueryFilter;
 import org.apache.cassandra.db.filter.NamesQueryFilter;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 
 public class ColumnFamilyStoreTest extends CleanupHelper
 {
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java
index 2bdcd51b..f7e7585c 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java
@@ -32,7 +32,7 @@
 import org.apache.cassandra.CleanupHelper;
 import org.apache.cassandra.db.filter.IdentityQueryFilter;
 import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.utils.FBUtilities;
 
 import static junit.framework.Assert.assertEquals;
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/CompactionsTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/CompactionsTest.java
index 32b9eceb..d6e5c657 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/CompactionsTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/CompactionsTest.java
@@ -30,7 +30,7 @@
 
 import org.junit.Test;
 
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.CleanupHelper;
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.utils.FBUtilities;
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/TableTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/TableTest.java
index cffb7dd5..8b658da8 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/TableTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/db/TableTest.java
@@ -41,10 +41,10 @@
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.db.filter.SliceQueryFilter;
 import org.apache.cassandra.db.marshal.LongType;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.IndexHelper;
+import org.apache.cassandra.io.sstable.SSTable;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.io.util.BufferedRandomAccessFile;
-import org.apache.cassandra.io.IndexHelper;
-import org.apache.cassandra.io.SSTable;
 
 public class TableTest extends CleanupHelper
 {
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/SSTableTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/SSTableTest.java
index ca732241..e69de29b 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/SSTableTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/SSTableTest.java
@@ -1,97 +0,0 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one
-* or more contributor license agreements.  See the NOTICE file
-* distributed with this work for additional information
-* regarding copyright ownership.  The ASF licenses this file
-* to you under the Apache License, Version 2.0 (the
-* "License"); you may not use this file except in compliance
-* with the License.  You may obtain a copy of the License at
-*
-*    http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing,
-* software distributed under the License is distributed on an
-* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-* KIND, either express or implied.  See the License for the
-* specific language governing permissions and limitations
-* under the License.
-*/
-package org.apache.cassandra.io;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.*;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.cassandra.CleanupHelper;
-import org.apache.cassandra.io.util.BufferedRandomAccessFile;
-import org.apache.cassandra.db.DecoratedKey;
-
-import com.google.common.base.Predicate;
-import com.google.common.base.Predicates;
-
-public class SSTableTest extends CleanupHelper
-{
-    @Test
-    public void testSingleWrite() throws IOException {
-        // write test data
-        String key = Integer.toString(1);
-        byte[] bytes = new byte[1024];
-        new Random().nextBytes(bytes);
-
-        TreeMap<String, byte[]> map = new TreeMap<String,byte[]>();
-        map.put(key, bytes);
-        SSTableReader ssTable = SSTableUtils.writeRawSSTable("Keyspace1", "Standard1", map);
-
-        // verify
-        verifySingle(ssTable, bytes, key);
-        ssTable = SSTableReader.open(ssTable.getDescriptor()); // read the index from disk
-        verifySingle(ssTable, bytes, key);
-    }
-
-    private void verifySingle(SSTableReader sstable, byte[] bytes, String key) throws IOException
-    {
-        BufferedRandomAccessFile file = new BufferedRandomAccessFile(sstable.getFilename(), "r");
-        file.seek(sstable.getPosition(sstable.partitioner.decorateKey(key)).position);
-        assert key.equals(file.readUTF());
-        int size = file.readInt();
-        byte[] bytes2 = new byte[size];
-        file.readFully(bytes2);
-        assert Arrays.equals(bytes2, bytes);
-    }
-
-    @Test
-    public void testManyWrites() throws IOException {
-        TreeMap<String, byte[]> map = new TreeMap<String,byte[]>();
-        for ( int i = 100; i < 1000; ++i )
-        {
-            map.put(Integer.toString(i), ("Avinash Lakshman is a good man: " + i).getBytes());
-        }
-
-        // write
-        SSTableReader ssTable = SSTableUtils.writeRawSSTable("Keyspace1", "Standard2", map);
-
-        // verify
-        verifyMany(ssTable, map);
-        ssTable = SSTableReader.open(ssTable.getDescriptor()); // read the index from disk
-        verifyMany(ssTable, map);
-    }
-
-    private void verifyMany(SSTableReader sstable, TreeMap<String, byte[]> map) throws IOException
-    {
-        List<String> keys = new ArrayList<String>(map.keySet());
-        Collections.shuffle(keys);
-        BufferedRandomAccessFile file = new BufferedRandomAccessFile(sstable.getFilename(), "r");
-        for (String key : keys)
-        {
-            file.seek(sstable.getPosition(sstable.partitioner.decorateKey(key)).position);
-            assert key.equals(file.readUTF());
-            int size = file.readInt();
-            byte[] bytes2 = new byte[size];
-            file.readFully(bytes2);
-            assert Arrays.equals(bytes2, map.get(key));
-        }
-    }
-}
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/SSTableUtils.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/SSTableUtils.java
index 0089d9b9..e69de29b 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/SSTableUtils.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/SSTableUtils.java
@@ -1,109 +0,0 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one
-* or more contributor license agreements.  See the NOTICE file
-* distributed with this work for additional information
-* regarding copyright ownership.  The ASF licenses this file
-* to you under the Apache License, Version 2.0 (the
-* "License"); you may not use this file except in compliance
-* with the License.  You may obtain a copy of the License at
-*
-*    http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing,
-* software distributed under the License is distributed on an
-* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-* KIND, either express or implied.  See the License for the
-* specific language governing permissions and limitations
-* under the License.
-*/
-package org.apache.cassandra.io;
-
-import java.io.File;
-import java.io.IOException;
-
-import java.util.Map;
-import java.util.SortedMap;
-import java.util.Set;
-import java.util.TreeMap;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.Column;
-import org.apache.cassandra.db.ColumnFamily;
-import org.apache.cassandra.db.Table;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.io.util.DataOutputBuffer;
-
-/**
- * TODO: These methods imitate Memtable.writeSortedKeys to some degree, but
- * because it is so monolithic, we can't reuse much.
- */
-public class SSTableUtils
-{
-    // first configured table and cf
-    public static String TABLENAME;
-    public static String CFNAME;
-    static
-    {
-        try
-        {
-            TABLENAME = DatabaseDescriptor.getTables().iterator().next();
-            CFNAME = Table.open(TABLENAME).getColumnFamilies().iterator().next();
-        }
-        catch(IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    public static File tempSSTableFile(String tablename, String cfname) throws IOException
-    {
-        File tempdir = File.createTempFile(tablename, cfname);
-        if(!tempdir.delete() || !tempdir.mkdir())
-            throw new IOException("Temporary directory creation failed.");
-        tempdir.deleteOnExit();
-        File tabledir = new File(tempdir, tablename);
-        tabledir.mkdir();
-        tabledir.deleteOnExit();
-        File datafile = new File(new SSTable.Descriptor(tabledir, tablename, cfname, 0,
-                                                        false).filenameFor("Data.db"));
-        assert datafile.createNewFile();
-        datafile.deleteOnExit();
-        return datafile;
-    }
-
-    public static SSTableReader writeSSTable(Set<String> keys) throws IOException
-    {
-        TreeMap<String, ColumnFamily> map = new TreeMap<String, ColumnFamily>();
-        for (String key : keys)
-        {
-            ColumnFamily cf = ColumnFamily.create(TABLENAME, CFNAME);
-            cf.addColumn(new Column(key.getBytes(), key.getBytes(), 0));
-            map.put(key, cf);
-        }
-        return writeSSTable(map);
-    }
-
-    public static SSTableReader writeSSTable(SortedMap<String, ColumnFamily> entries) throws IOException
-    {
-        TreeMap<String, byte[]> map = new TreeMap<String, byte[]>();
-        for (Map.Entry<String, ColumnFamily> entry : entries.entrySet())
-        {
-            DataOutputBuffer buffer = new DataOutputBuffer();
-            ColumnFamily.serializer().serializeWithIndexes(entry.getValue(), buffer);
-            map.put(entry.getKey(), buffer.getData());
-        }
-        return writeRawSSTable(TABLENAME, CFNAME, map);
-    }
-
-    public static SSTableReader writeRawSSTable(String tablename, String cfname, SortedMap<String, byte[]> entries) throws IOException
-    {
-        File f = tempSSTableFile(tablename, cfname);
-        SSTableWriter writer = new SSTableWriter(f.getAbsolutePath(), entries.size(), StorageService.getPartitioner());
-        for (Map.Entry<String, byte[]> entry : entries.entrySet())
-            writer.append(writer.partitioner.decorateKey(entry.getKey()),
-                          entry.getValue());
-        new File(writer.indexFilename()).deleteOnExit();
-        new File(writer.filterFilename()).deleteOnExit();
-        return writer.closeAndOpenReader();
-    }
-}
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/StreamingTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/StreamingTest.java
index e837af21..448fe5b3 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/StreamingTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/StreamingTest.java
@@ -26,8 +26,8 @@
 import org.apache.cassandra.CleanupHelper;
 import org.apache.cassandra.Util;
 import org.apache.cassandra.db.*;
-import org.apache.cassandra.io.SSTableUtils;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableUtils;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.streaming.StreamOut;
 import org.apache.cassandra.utils.FBUtilities;
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java
index e69de29b..80726f51 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java
@@ -0,0 +1,98 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.*;
+
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.io.util.BufferedRandomAccessFile;
+import org.apache.cassandra.db.DecoratedKey;
+
+import com.google.common.base.Predicate;
+import com.google.common.base.Predicates;
+
+public class SSTableTest extends CleanupHelper
+{
+    @Test
+    public void testSingleWrite() throws IOException {
+        // write test data
+        String key = Integer.toString(1);
+        byte[] bytes = new byte[1024];
+        new Random().nextBytes(bytes);
+
+        TreeMap<String, byte[]> map = new TreeMap<String,byte[]>();
+        map.put(key, bytes);
+        SSTableReader ssTable = SSTableUtils.writeRawSSTable("Keyspace1", "Standard1", map);
+
+        // verify
+        verifySingle(ssTable, bytes, key);
+        ssTable = SSTableReader.open(ssTable.getDescriptor()); // read the index from disk
+        verifySingle(ssTable, bytes, key);
+    }
+
+    private void verifySingle(SSTableReader sstable, byte[] bytes, String key) throws IOException
+    {
+        BufferedRandomAccessFile file = new BufferedRandomAccessFile(sstable.getFilename(), "r");
+        file.seek(sstable.getPosition(sstable.partitioner.decorateKey(key)).position);
+        assert key.equals(file.readUTF());
+        int size = file.readInt();
+        byte[] bytes2 = new byte[size];
+        file.readFully(bytes2);
+        assert Arrays.equals(bytes2, bytes);
+    }
+
+    @Test
+    public void testManyWrites() throws IOException {
+        TreeMap<String, byte[]> map = new TreeMap<String,byte[]>();
+        for ( int i = 100; i < 1000; ++i )
+        {
+            map.put(Integer.toString(i), ("Avinash Lakshman is a good man: " + i).getBytes());
+        }
+
+        // write
+        SSTableReader ssTable = SSTableUtils.writeRawSSTable("Keyspace1", "Standard2", map);
+
+        // verify
+        verifyMany(ssTable, map);
+        ssTable = SSTableReader.open(ssTable.getDescriptor()); // read the index from disk
+        verifyMany(ssTable, map);
+    }
+
+    private void verifyMany(SSTableReader sstable, TreeMap<String, byte[]> map) throws IOException
+    {
+        List<String> keys = new ArrayList<String>(map.keySet());
+        Collections.shuffle(keys);
+        BufferedRandomAccessFile file = new BufferedRandomAccessFile(sstable.getFilename(), "r");
+        for (String key : keys)
+        {
+            file.seek(sstable.getPosition(sstable.partitioner.decorateKey(key)).position);
+            assert key.equals(file.readUTF());
+            int size = file.readInt();
+            byte[] bytes2 = new byte[size];
+            file.readFully(bytes2);
+            assert Arrays.equals(bytes2, map.get(key));
+        }
+    }
+}
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java
index 3f24d583..bde5c3e9 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableUtils.java
@@ -1 +1,111 @@
   + native
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+
+package org.apache.cassandra.io.sstable;
+
+import java.io.File;
+import java.io.IOException;
+
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Column;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+
+/**
+ * TODO: These methods imitate Memtable.writeSortedKeys to some degree, but
+ * because it is so monolithic, we can't reuse much.
+ */
+public class SSTableUtils
+{
+    // first configured table and cf
+    public static String TABLENAME;
+    public static String CFNAME;
+    static
+    {
+        try
+        {
+            TABLENAME = DatabaseDescriptor.getTables().iterator().next();
+            CFNAME = Table.open(TABLENAME).getColumnFamilies().iterator().next();
+        }
+        catch(IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public static File tempSSTableFile(String tablename, String cfname) throws IOException
+    {
+        File tempdir = File.createTempFile(tablename, cfname);
+        if(!tempdir.delete() || !tempdir.mkdir())
+            throw new IOException("Temporary directory creation failed.");
+        tempdir.deleteOnExit();
+        File tabledir = new File(tempdir, tablename);
+        tabledir.mkdir();
+        tabledir.deleteOnExit();
+        File datafile = new File(new SSTable.Descriptor(tabledir, tablename, cfname, 0,
+                                                        false).filenameFor("Data.db"));
+        assert datafile.createNewFile();
+        datafile.deleteOnExit();
+        return datafile;
+    }
+
+    public static SSTableReader writeSSTable(Set<String> keys) throws IOException
+    {
+        TreeMap<String, ColumnFamily> map = new TreeMap<String, ColumnFamily>();
+        for (String key : keys)
+        {
+            ColumnFamily cf = ColumnFamily.create(TABLENAME, CFNAME);
+            cf.addColumn(new Column(key.getBytes(), key.getBytes(), 0));
+            map.put(key, cf);
+        }
+        return writeSSTable(map);
+    }
+
+    public static SSTableReader writeSSTable(SortedMap<String, ColumnFamily> entries) throws IOException
+    {
+        TreeMap<String, byte[]> map = new TreeMap<String, byte[]>();
+        for (Map.Entry<String, ColumnFamily> entry : entries.entrySet())
+        {
+            DataOutputBuffer buffer = new DataOutputBuffer();
+            ColumnFamily.serializer().serializeWithIndexes(entry.getValue(), buffer);
+            map.put(entry.getKey(), buffer.getData());
+        }
+        return writeRawSSTable(TABLENAME, CFNAME, map);
+    }
+
+    public static SSTableReader writeRawSSTable(String tablename, String cfname, SortedMap<String, byte[]> entries) throws IOException
+    {
+        File f = tempSSTableFile(tablename, cfname);
+        SSTableWriter writer = new SSTableWriter(f.getAbsolutePath(), entries.size(), StorageService.getPartitioner());
+        for (Map.Entry<String, byte[]> entry : entries.entrySet())
+            writer.append(writer.partitioner.decorateKey(entry.getKey()),
+                          entry.getValue());
+        new File(writer.indexFilename()).deleteOnExit();
+        new File(writer.filterFilename()).deleteOnExit();
+        return writer.closeAndOpenReader();
+    }
+}
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/streaming/BootstrapTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/streaming/BootstrapTest.java
index c568d52f..73909442 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/streaming/BootstrapTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/streaming/BootstrapTest.java
@@ -28,7 +28,7 @@
 import java.util.Map;
 
 import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.sstable.SSTable;
 
 import org.junit.Test;
 
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableExportTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
index d8a6c278..3754af4f 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
@@ -28,10 +28,10 @@
 import org.apache.cassandra.db.filter.NamesQueryFilter;
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.io.SSTableReader;
-import org.apache.cassandra.io.SSTableWriter;
+import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableWriter;
 import org.apache.cassandra.io.util.DataOutputBuffer;
-import static org.apache.cassandra.io.SSTableUtils.tempSSTableFile;
+import static org.apache.cassandra.io.sstable.SSTableUtils.tempSSTableFile;
 import static org.apache.cassandra.utils.FBUtilities.hexToBytes;
 import static org.junit.Assert.assertTrue;
 
diff --git a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableImportTest.java b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
index fe200349..61794ec4 100644
--- a/incubator/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
+++ b/incubator/cassandra/trunk/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
@@ -26,9 +26,9 @@
 import org.apache.cassandra.db.IColumn;
 import org.apache.cassandra.db.filter.NamesQueryFilter;
 import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import static org.apache.cassandra.utils.FBUtilities.hexToBytes;
-import static org.apache.cassandra.io.SSTableUtils.tempSSTableFile;
+import static org.apache.cassandra.io.sstable.SSTableUtils.tempSSTableFile;
 import org.json.simple.parser.ParseException;
 import org.junit.Test;
 
