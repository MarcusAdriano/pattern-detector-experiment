diff --git a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/DocValuesConsumer.java b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
index 8055b88d..82a7e360 100644
--- a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
+++ b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
@@ -19,16 +19,18 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.lucene40.values.Writer;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.MergeState;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 
 /**
- * Abstract API that consumes {@link DocValue}s.
+ * Abstract API that consumes {@link IndexableField}s.
  * {@link DocValuesConsumer} are always associated with a specific field and
  * segments. Concrete implementations of this API write the given
  * {@link IndexableField} into a implementation specific format depending on
@@ -38,7 +40,6 @@
  */
 public abstract class DocValuesConsumer {
 
-  protected Source currentMergeSource;
   protected final BytesRef spare = new BytesRef();
 
   /**
@@ -86,8 +87,8 @@ public void merge(MergeState mergeState, DocValues[] docValues) throws IOExcepti
       final org.apache.lucene.index.MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(readerIDX);
       if (docValues[readerIDX] != null) {
         hasMerged = true;
-        merge(new SingleSubMergeState(docValues[readerIDX], mergeState.docBase[readerIDX], reader.reader.maxDoc(),
-                                    reader.liveDocs));
+        merge(docValues[readerIDX], mergeState.docBase[readerIDX],
+              reader.reader.maxDoc(), reader.liveDocs);
         mergeState.checkAbort.work(reader.reader.maxDoc());
       }
     }
@@ -98,73 +99,33 @@ public void merge(MergeState mergeState, DocValues[] docValues) throws IOExcepti
   }
 
   /**
-   * Merges the given {@link SingleSubMergeState} into this {@link DocValuesConsumer}.
+   * Merges the given {@link DocValues} into this {@link DocValuesConsumer}.
    * 
-   * @param state
-   *          the {@link SingleSubMergeState} to merge
    * @throws IOException
    *           if an {@link IOException} occurs
    */
-  protected void merge(SingleSubMergeState state) throws IOException {
+  protected void merge(DocValues reader, int docBase, int docCount, Bits liveDocs) throws IOException {
     // This enables bulk copies in subclasses per MergeState, subclasses can
     // simply override this and decide if they want to merge
     // segments using this generic implementation or if a bulk merge is possible
     // / feasible.
-    final Source source = state.reader.getDirectSource();
+    final Source source = reader.getDirectSource();
     assert source != null;
-    setNextMergeSource(source); // set the current enum we are working on - the
-    // impl. will get the correct reference for the type
-    // it supports
-    int docID = state.docBase;
-    final Bits liveDocs = state.liveDocs;
-    final int docCount = state.docCount;
+    int docID = docBase;
+    FieldType ft = new FieldType(StringField.TYPE_UNSTORED);
+    ft.setDocValueType(reader.type());
+    final Field scratchField = new Field("", "", ft);
     for (int i = 0; i < docCount; i++) {
       if (liveDocs == null || liveDocs.get(i)) {
-        mergeDoc(docID++, i);
+        mergeDoc(scratchField, source, docID++, i);
       }
     }
   }
 
-  /**
-   * Records the specified <tt>long</tt> value for the docID or throws an
-   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
-   * <tt>long</tt> values.
-   * 
-   * @throws UnsupportedOperationException
-   *           if this writer doesn't record <tt>long</tt> values
-   */
-  protected void add(int docID, long value) throws IOException {
-    throw new UnsupportedOperationException("override this method to support integer types");
-  }
-
-  /**
-   * Records the specified <tt>double</tt> value for the docID or throws an
-   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
-   * <tt>double</tt> values.
-   * 
-   * @throws UnsupportedOperationException
-   *           if this writer doesn't record <tt>double</tt> values
-   */
-  protected void add(int docID, double value) throws IOException {
-    throw new UnsupportedOperationException("override this method to support floating point types");
-  }
-
-  /**
-   * Records the specified {@link BytesRef} value for the docID or throws an
-   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
-   * {@link BytesRef} values.
-   * 
-   * @throws UnsupportedOperationException
-   *           if this writer doesn't record {@link BytesRef} values
-   */
-  protected void add(int docID, BytesRef value) throws IOException {
-    throw new UnsupportedOperationException("override this method to support byte types");
-  }
-
   /**
    * Merges a document with the given <code>docID</code>. The methods
    * implementation obtains the value for the <i>sourceDoc</i> id from the
-   * current {@link Source} set to <i>setNextMergeSource(Source)</i>.
+   * current {@link Source}.
    * <p>
    * This method is used during merging to provide implementation agnostic
    * default merge implementation.
@@ -176,67 +137,29 @@ protected void add(int docID, BytesRef value) throws IOException {
    * ID must always be greater than the previous ID or <tt>0</tt> if called the
    * first time.
    */
-  protected void mergeDoc(int docID, int sourceDoc)
+  protected void mergeDoc(Field scratchField, Source source, int docID, int sourceDoc)
       throws IOException {
-    switch(currentMergeSource.type()) {
+    switch(source.type()) {
     case BYTES_FIXED_DEREF:
     case BYTES_FIXED_SORTED:
     case BYTES_FIXED_STRAIGHT:
     case BYTES_VAR_DEREF:
     case BYTES_VAR_SORTED:
     case BYTES_VAR_STRAIGHT:
-      add(docID, currentMergeSource.getBytes(sourceDoc, spare));
+      scratchField.setValue(source.getBytes(sourceDoc, spare));
       break;
     case FIXED_INTS_16:
     case FIXED_INTS_32:
     case FIXED_INTS_64:
     case FIXED_INTS_8:
     case VAR_INTS:
-      add(docID, currentMergeSource.getInt(sourceDoc));
+      scratchField.setValue(source.getInt(sourceDoc));
       break;
     case FLOAT_32:
     case FLOAT_64:
-      add(docID, currentMergeSource.getFloat(sourceDoc));
+      scratchField.setValue(source.getFloat(sourceDoc));
       break;
     }
-  }
-  
-  /**
-   * Sets the next {@link Source} to consume values from on calls to
-   * {@link #mergeDoc(int, int)}
-   * 
-   * @param mergeSource
-   *          the next {@link Source}, this must not be null
-   */
-  protected final void setNextMergeSource(Source mergeSource) {
-    currentMergeSource = mergeSource;
-  }
-
-  /**
-   * Specialized auxiliary MergeState is necessary since we don't want to
-   * exploit internals up to the codecs consumer. An instance of this class is
-   * created for each merged low level {@link IndexReader} we are merging to
-   * support low level bulk copies.
-   */
-  public static class SingleSubMergeState {
-    /**
-     * the source reader for this MergeState - merged values should be read from
-     * this instance
-     */
-    public final DocValues reader;
-    /** the absolute docBase for this MergeState within the resulting segment */
-    public final int docBase;
-    /** the number of documents in this MergeState */
-    public final int docCount;
-    /** the not deleted bits for this MergeState */
-    public final Bits liveDocs;
-
-    public SingleSubMergeState(DocValues reader, int docBase, int docCount, Bits liveDocs) {
-      assert reader != null;
-      this.reader = reader;
-      this.docBase = docBase;
-      this.docCount = docCount;
-      this.liveDocs = liveDocs;
-    }
+    add(docID, scratchField);
   }
 }
diff --git a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java
index f17f2b61..eefd72bf 100644
--- a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java
+++ b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java
@@ -23,6 +23,7 @@
 import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.document.Field;
 import org.apache.lucene.index.DocValues.SortedSource;
 import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocValues.Type;
@@ -295,7 +296,6 @@ protected IndexOutput getOrCreateIndexOut() throws IOException {
      * Must be called only with increasing docIDs. It's OK for some docIDs to be
      * skipped; they will be filled with 0 bytes.
      */
-    @Override
     protected
     abstract void add(int docID, BytesRef bytes) throws IOException;
 
@@ -303,16 +303,13 @@ protected IndexOutput getOrCreateIndexOut() throws IOException {
     public abstract void finish(int docCount) throws IOException;
 
     @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
-      add(docID, currentMergeSource.getBytes(sourceDoc, bytesRef));
+    protected void mergeDoc(Field scratchField, Source source, int docID, int sourceDoc) throws IOException {
+      add(docID, source.getBytes(sourceDoc, bytesRef));
     }
 
     @Override
     public void add(int docID, IndexableField docValue) throws IOException {
-      final BytesRef ref = docValue.binaryValue();
-      if (ref != null) {
-        add(docID, ref);
-      }
+      add(docID, docValue.binaryValue());
     }
   }
 
diff --git a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java
index 9a05e788..1b0c2e7a 100644
--- a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java
+++ b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java
@@ -17,27 +17,29 @@
  * limitations under the License.
  */
 
-import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
-
 import java.io.IOException;
 
 import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
 import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
 import org.apache.lucene.codecs.lucene40.values.Bytes.BytesWriterBase;
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.document.Field;
 import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
+import org.apache.lucene.util.ByteBlockPool;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.PagedBytes;
 
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
+
 // Simplest storage: stores fixed length byte[] per
 // document, with no dedup and no sorting.
 /**
@@ -133,7 +135,7 @@ public Writer(Directory dir, String id, String codecName, int version, Counter b
 
 
     @Override
-    protected void merge(SingleSubMergeState state) throws IOException {
+    protected void merge(DocValues readerIn, int docBase, int docCount, Bits liveDocs) throws IOException {
       datOut = getOrCreateDataOut();
       boolean success = false;
       try {
@@ -141,8 +143,8 @@ protected void merge(SingleSubMergeState state) throws IOException {
           datOut.writeInt(size);
         }
 
-        if (state.liveDocs == null && tryBulkMerge(state.reader)) {
-          FixedStraightReader reader = (FixedStraightReader) state.reader;
+        if (liveDocs == null && tryBulkMerge(readerIn)) {
+          FixedStraightReader reader = (FixedStraightReader) readerIn;
           final int maxDocs = reader.maxDoc;
           if (maxDocs == 0) {
             return;
@@ -154,9 +156,9 @@ protected void merge(SingleSubMergeState state) throws IOException {
             throw new IllegalArgumentException("expected bytes size=" + size
                 + " but got " + reader.size);
            }
-          if (lastDocID+1 < state.docBase) {
-            fill(datOut, state.docBase);
-            lastDocID = state.docBase-1;
+          if (lastDocID+1 < docBase) {
+            fill(datOut, docBase);
+            lastDocID = docBase-1;
           }
           // TODO should we add a transfer to API to each reader?
           final IndexInput cloneData = reader.cloneData();
@@ -168,7 +170,7 @@ protected void merge(SingleSubMergeState state) throws IOException {
         
           lastDocID += maxDocs;
         } else {
-          super.merge(state);
+          super.merge(readerIn, docBase, docCount, liveDocs);
         }
         success = true;
       } finally {
@@ -184,9 +186,9 @@ protected boolean tryBulkMerge(DocValues docValues) {
     }
     
     @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+    protected void mergeDoc(Field scratchField, Source source, int docID, int sourceDoc) throws IOException {
       assert lastDocID < docID;
-      setMergeBytes(sourceDoc);
+      setMergeBytes(source, sourceDoc);
       if (size == -1) {
         size = bytesRef.length;
         datOut.writeInt(size);
@@ -199,8 +201,8 @@ protected void mergeDoc(int docID, int sourceDoc) throws IOException {
       lastDocID = docID;
     }
     
-    protected void setMergeBytes(int sourceDoc) {
-      currentMergeSource.getBytes(sourceDoc, bytesRef);
+    protected void setMergeBytes(Source source, int sourceDoc) {
+      source.getBytes(sourceDoc, bytesRef);
     }
 
     // Fills up to but not including this docID
diff --git a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java
index 07a78fc9..a7419e1b 100644
--- a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java
+++ b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java
@@ -87,10 +87,7 @@ protected void add(int docID, double v) throws IOException {
     
     @Override
     public void add(int docID, IndexableField docValue) throws IOException {
-      Number number = docValue.numericValue();
-      if (number != null) {
-        add(docID, number.doubleValue());
-      }
+      add(docID, docValue.numericValue().doubleValue());
     }
     
     @Override
@@ -100,8 +97,8 @@ protected boolean tryBulkMerge(DocValues docValues) {
     }
     
     @Override
-    protected void setMergeBytes(int sourceDoc) {
-      final double value = currentMergeSource.getFloat(sourceDoc);
+    protected void setMergeBytes(Source source, int sourceDoc) {
+      final double value = source.getFloat(sourceDoc);
       template.toBytes(value, bytesRef);
     }
   }
diff --git a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java
index 62b1a6b7..066bb4a7 100644
--- a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java
+++ b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java
@@ -20,6 +20,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexableField;
@@ -103,7 +104,6 @@ protected IntsWriter(Directory dir, String id, String codecName,
       template = DocValuesArray.TEMPLATES.get(valueType);
     }
     
-    @Override
     protected void add(int docID, long v) throws IOException {
       template.toBytes(v, bytesRef);
       add(docID, bytesRef);
@@ -111,15 +111,12 @@ protected void add(int docID, long v) throws IOException {
 
     @Override
     public void add(int docID, IndexableField docValue) throws IOException {
-      final Number number = docValue.numericValue();
-      if (number != null) {
-        add(docID, number.longValue());
-      }
+      add(docID, docValue.numericValue().longValue());
     }
     
     @Override
-    protected void setMergeBytes(int sourceDoc) {
-      final long value = currentMergeSource.getInt(sourceDoc);
+    protected void setMergeBytes(Source source, int sourceDoc) {
+      final long value = source.getInt(sourceDoc);
       template.toBytes(value, bytesRef);
     }
     
diff --git a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java
index f5b4ae36..ccd4f602 100644
--- a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java
+++ b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java
@@ -20,6 +20,7 @@
 
 import org.apache.lucene.codecs.lucene40.values.DocValuesArray.LongValues;
 import org.apache.lucene.codecs.lucene40.values.FixedStraightBytesImpl.FixedBytesWriterBase;
+import org.apache.lucene.document.Field;
 import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.index.DocValues;
@@ -62,7 +63,6 @@ protected PackedIntsWriter(Directory dir, String id, Counter bytesUsed,
       bytesRef = new BytesRef(8);
     }
 
-    @Override
     protected void add(int docID, long v) throws IOException {
       assert lastDocId < docID;
       if (!started) {
@@ -113,10 +113,10 @@ public void finish(int docCount) throws IOException {
     }
 
     @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+    protected void mergeDoc(Field scratchField, Source source, int docID, int sourceDoc) throws IOException {
       assert docID > lastDocId : "docID: " + docID
           + " must be greater than the last added doc id: " + lastDocId;
-        add(docID, currentMergeSource.getInt(sourceDoc));
+        add(docID, source.getInt(sourceDoc));
     }
 
     private void writePackedInts(IndexOutput datOut, int docCount) throws IOException {
@@ -152,10 +152,7 @@ private void writePackedInts(IndexOutput datOut, int docCount) throws IOExceptio
 
     @Override
     public void add(int docID, IndexableField docValue) throws IOException {
-      final Number number = docValue.numericValue();
-      if (number != null) {
-        add(docID, number.longValue());
-      }
+      add(docID, docValue.numericValue().longValue());
     }
   }
 
diff --git a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java
index 80924f21..2902801c 100644
--- a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java
+++ b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java
@@ -22,21 +22,25 @@
 import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
 import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
 import org.apache.lucene.codecs.lucene40.values.Bytes.BytesWriterBase;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
 import org.apache.lucene.util.ByteBlockPool;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.PagedBytes;
 import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.packed.PackedInts;
 import org.apache.lucene.util.packed.PackedInts.ReaderIterator;
+import org.apache.lucene.util.packed.PackedInts;
 
 // Variable length byte[] per document, no sharing
 
@@ -93,21 +97,21 @@ protected void add(int docID, BytesRef bytes) throws IOException {
     }
     
     @Override
-    protected void merge(SingleSubMergeState state) throws IOException {
+    protected void merge(DocValues readerIn, int docBase, int docCount, Bits liveDocs) throws IOException {
       merge = true;
       datOut = getOrCreateDataOut();
       boolean success = false;
       try {
-        if (state.liveDocs == null && state.reader instanceof VarStraightReader) {
+        if (liveDocs == null && readerIn instanceof VarStraightReader) {
           // bulk merge since we don't have any deletes
-          VarStraightReader reader = (VarStraightReader) state.reader;
+          VarStraightReader reader = (VarStraightReader) readerIn;
           final int maxDocs = reader.maxDoc;
           if (maxDocs == 0) {
             return;
           }
-          if (lastDocID+1 < state.docBase) {
-            fill(state.docBase, address);
-            lastDocID = state.docBase-1;
+          if (lastDocID+1 < docBase) {
+            fill(docBase, address);
+            lastDocID = docBase-1;
           }
           final long numDataBytes;
           final IndexInput cloneIdx = reader.cloneIndex();
@@ -137,7 +141,7 @@ protected void merge(SingleSubMergeState state) throws IOException {
             IOUtils.close(cloneData);  
           }
         } else {
-          super.merge(state);
+          super.merge(readerIn, docBase, docCount, liveDocs);
         }
         success = true;
       } finally {
@@ -148,10 +152,10 @@ protected void merge(SingleSubMergeState state) throws IOException {
     }
     
     @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+    protected void mergeDoc(Field scratchField, Source source, int docID, int sourceDoc) throws IOException {
       assert merge;
       assert lastDocID < docID;
-      currentMergeSource.getBytes(sourceDoc, bytesRef);
+      source.getBytes(sourceDoc, bytesRef);
       if (bytesRef.length == 0) {
         return; // default
       }
@@ -226,7 +230,7 @@ public long ramBytesUsed() {
   }
 
   public static class VarStraightReader extends BytesReaderBase {
-    private final int maxDoc;
+    final int maxDoc;
 
     VarStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
       super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_VAR_STRAIGHT);
diff --git a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/document/Field.java b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/document/Field.java
index 5056b1ea..8e8362c0 100644
--- a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/document/Field.java
+++ b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/document/Field.java
@@ -226,6 +226,16 @@ public void setValue(byte[] value) {
     fieldsData = new BytesRef(value);
   }
   
+  // nocommit jdocs warning that we hold ref
+  public void setValue(BytesRef value) {
+    fieldsData = value;
+  }
+
+  // nocommit jdocs
+  public void setValue(Number value) {
+    fieldsData = value;
+  }
+
   /**
    * Expert: sets the token stream to be used for indexing and causes
    * isIndexed() and isTokenized() to return true. May be combined with stored
@@ -267,13 +277,13 @@ public void setBoost(float boost) {
     this.boost = boost;
   }
   
-  public boolean numeric() {
-    return false;
-  }
-
   public Number numericValue() {
+    if (fieldsData instanceof Number) {
+      return (Number) fieldsData;
+    } else {
     return null;
   }
+  }
 
   public NumericField.DataType numericDataType() {
     return null;
diff --git a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/document/NumericField.java b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/document/NumericField.java
index e0b52cf7..4cb4df90 100644
--- a/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/document/NumericField.java
+++ b/lucene/dev/branches/lucene3453/lucene/src/java/org/apache/lucene/document/NumericField.java
@@ -312,11 +312,6 @@ public DataType numericDataType() {
     return dataType;
   }
 
-  @Override
-  public boolean numeric() {
-    return true;
-  }
-
   /**
    * Initializes the field with the supplied <code>long</code> value.
    * 
